[
    "{'paper': {'id': '2512.16776', 'authors': [{'_id': '6944bd29fbf17e708e185f72', 'name': 'Kling Team', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f73', 'name': 'Jialu Chen', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f74', 'name': 'Yuanzheng Ci', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f75', 'name': 'Xiangyu Du', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f76', 'name': 'Zipeng Feng', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f77', 'name': 'Kun Gai', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f78', 'name': 'Sainan Guo', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f79', 'name': 'Feng Han', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f7a', 'name': 'Jingbin He', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f7b', 'name': 'Kang He', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f7c', 'name': 'Xiao Hu', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f7d', 'name': 'Xiaohua Hu', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f7e', 'name': 'Boyuan Jiang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f7f', 'name': 'Fangyuan Kong', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f80', 'name': 'Hang Li', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f81', 'name': 'Jie Li', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f82', 'name': 'Qingyu Li', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f83', 'name': 'Shen Li', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f84', 'name': 'Xiaohan Li', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f85', 'name': 'Yan Li', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f86', 'name': 'Jiajun Liang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f87', 'name': 'Borui Liao', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f88', 'name': 'Yiqiao Liao', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f89', 'name': 'Weihong Lin', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f8a', 'name': 'Quande Liu', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f8b', 'name': 'Xiaokun Liu', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f8c', 'name': 'Yilun Liu', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f8d', 'name': 'Yuliang Liu', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f8e', 'name': 'Shun Lu', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f8f', 'name': 'Hangyu Mao', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f90', 'name': 'Yunyao Mao', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f91', 'name': 'Haodong Ouyang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f92', 'name': 'Wenyu Qin', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f93', 'name': 'Wanqi Shi', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f94', 'name': 'Xiaoyu Shi', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f95', 'name': 'Lianghao Su', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f96', 'name': 'Haozhi Sun', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f97', 'name': 'Peiqin Sun', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f98', 'name': 'Pengfei Wan', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f99', 'name': 'Chao Wang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f9a', 'name': 'Chenyu Wang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f9b', 'name': 'Meng Wang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f9c', 'name': 'Qiulin Wang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f9d', 'name': 'Runqi Wang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f9e', 'name': 'Xintao Wang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185f9f', 'name': 'Xuebo Wang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fa0', 'name': 'Zekun Wang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fa1', 'name': 'Min Wei', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fa2', 'name': 'Tiancheng Wen', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fa3', 'name': 'Guohao Wu', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fa4', 'name': 'Xiaoshi Wu', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fa5', 'name': 'Zhenhua Wu', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fa6', 'name': 'Da Xie', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fa7', 'name': 'Yingtong Xiong', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fa8', 'name': 'Yulong Xu', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fa9', 'name': 'Sile Yang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185faa', 'name': 'Zikang Yang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fab', 'name': 'Weicai Ye', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fac', 'name': 'Ziyang Yuan', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fad', 'name': 'Shenglong Zhang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fae', 'name': 'Shuaiyu Zhang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185faf', 'name': 'Yuanxing Zhang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fb0', 'name': 'Yufan Zhang', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fb1', 'name': 'Wenzheng Zhao', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fb2', 'name': 'Ruiliang Zhou', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fb3', 'name': 'Yan Zhou', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fb4', 'name': 'Guosheng Zhu', 'hidden': False}, {'_id': '6944bd29fbf17e708e185fb5', 'name': 'Yongjie Zhu', 'hidden': False}], 'publishedAt': '2025-12-18T17:08:12.000Z', 'submittedOnDailyAt': '2025-12-19T00:19:38.857Z', 'title': 'Kling-Omni Technical Report', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.', 'upvotes': 104, 'discussionId': '6944bd29fbf17e708e185fb6', 'ai_summary': 'Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.', 'ai_keywords': ['generative framework', 'multimodal visual language inputs', 'end-to-end', 'video generation', 'editing', 'intelligent reasoning', 'unified multimodal representation', 'cinematic-quality', 'efficient large-scale pre-training', 'inference optimizations', 'in-context generation', 'reasoning-based editing', 'multimodal instruction following', 'multimodal world simulators'], 'organization': {'_id': '662c559b322afcbae51b3c8b', 'name': 'KlingTeam', 'fullname': 'Kling Team', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg'}}, 'publishedAt': '2025-12-18T12:08:12.000Z', 'title': 'Kling-Omni Technical Report', 'summary': 'We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 187}, 'organization': {'_id': '662c559b322afcbae51b3c8b', 'name': 'KlingTeam', 'fullname': 'Kling Team', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.15745', 'authors': [{'_id': '6944bdd4fbf17e708e185fb8', 'name': 'Tiwei Bie', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fb9', 'name': 'Maosong Cao', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fba', 'name': 'Kun Chen', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fbb', 'name': 'Lun Du', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fbc', 'user': {'_id': '6909e9153e33b19bf2f71b05', 'avatarUrl': '/avatars/8fa060fa5c21ce57c2e5f87a0835af07.svg', 'isPro': False, 'fullname': 'Mingliang Gong', 'user': 'bright-ai-infra', 'type': 'user'}, 'name': 'Mingliang Gong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-19T09:49:22.650Z', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fbd', 'user': {'_id': '64a68e21d09682887d9ed95a', 'avatarUrl': '/avatars/2d7be7d9221b7a59ecbeb5383f70d83d.svg', 'isPro': False, 'fullname': 'Zhuocheng Gong', 'user': 'gzhch', 'type': 'user'}, 'name': 'Zhuochen Gong', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T09:56:22.280Z', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fbe', 'name': 'Yanmei Gu', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fbf', 'name': 'Jiaqi Hu', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fc0', 'user': {'_id': '625f8e6f673e5862a8c07f1a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1677306231208-625f8e6f673e5862a8c07f1a.jpeg', 'isPro': False, 'fullname': 'Bill H', 'user': 'lccurious', 'type': 'user'}, 'name': 'Zenan Huang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T09:56:17.788Z', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fc1', 'name': 'Zhenzhong Lan', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fc2', 'name': 'Chengxi Li', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fc3', 'user': {'_id': '64c07b488e2612254361153b', 'avatarUrl': '/avatars/ade0f783cc4c2d3e73f402637f595471.svg', 'isPro': False, 'fullname': 'chongxuan li', 'user': 'zhenxuan00', 'type': 'user'}, 'name': 'Chongxuan Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-19T09:56:49.796Z', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fc4', 'name': 'Jianguo Li', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fc5', 'name': 'Zehuan Li', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fc6', 'user': {'_id': '68ef4ecddfc956769ea3e909', 'avatarUrl': '/avatars/2448a4805102351ef4a6ece7d7f88b02.svg', 'isPro': False, 'fullname': 'Huabin liu', 'user': 'liuhuabin1229', 'type': 'user'}, 'name': 'Huabin Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-19T09:58:07.116Z', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fc7', 'user': {'_id': '646ed6708d316fde87b3eee3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6PjHGz3w2iPYa8eZCP2Qc.jpeg', 'isPro': False, 'fullname': 'Liulin', 'user': 'Ulov888', 'type': 'user'}, 'name': 'Ling Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T10:49:51.184Z', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fc8', 'user': {'_id': '690077c498a1389fbb6e9a5d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/690077c498a1389fbb6e9a5d/JPf4OH46m0uU7CH9aO4Yo.jpeg', 'isPro': False, 'fullname': 'Guoshan Lu', 'user': 'luguoshan', 'type': 'user'}, 'name': 'Guoshan Lu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T09:56:23.767Z', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fc9', 'name': 'Xiaocheng Lu', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fca', 'name': 'Yuxin Ma', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fcb', 'user': {'_id': '64d2eaee80ce1029be72bdcd', 'avatarUrl': '/avatars/6917aa66459d6f9a4d6437381c496bdd.svg', 'isPro': False, 'fullname': 'Jianfeng Tan', 'user': 'jianfengt', 'type': 'user'}, 'name': 'Jianfeng Tan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-19T09:57:09.384Z', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fcc', 'name': 'Lanning Wei', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fcd', 'user': {'_id': '64b8c89052b7353d8c6a1013', 'avatarUrl': '/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg', 'isPro': False, 'fullname': 'Ji-Rong Wen', 'user': 'jrwen', 'type': 'user'}, 'name': 'Ji-Rong Wen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-19T09:57:23.843Z', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fce', 'name': 'Yipeng Xing', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fcf', 'name': 'Xiaolu Zhang', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fd0', 'user': {'_id': '6725f5a7f05f62659e3615f3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/iiXdzhaSnExw2dwQQtLZ8.png', 'isPro': False, 'fullname': 'Junbo Zhao', 'user': 'jakezhao2024', 'type': 'user'}, 'name': 'Junbo Zhao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T09:56:20.782Z', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fd1', 'name': 'Da Zheng', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fd2', 'name': 'Jun Zhou', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fd3', 'user': {'_id': '63eb008e5c837d9968f1eb71', 'avatarUrl': '/avatars/ae43c3f5ab87b82f4bad25c65ac55d01.svg', 'isPro': False, 'fullname': 'Junlin Zhou', 'user': 'jlzhou', 'type': 'user'}, 'name': 'Junlin Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-19T12:13:31.053Z', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fd4', 'name': 'Zhanchao Zhou', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fd5', 'name': 'Liwang Zhu', 'hidden': False}, {'_id': '6944bdd4fbf17e708e185fd6', 'user': {'_id': '673b5f24e863f1d28b402efc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/19gUgtPEY3-FtY0sNlI_-.png', 'isPro': False, 'fullname': 'yihongzhuang', 'user': 'utdawn', 'type': 'user'}, 'name': 'Yihong Zhuang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T09:56:19.300Z', 'hidden': False}], 'publishedAt': '2025-12-10T09:26:18.000Z', 'submittedOnDailyAt': '2025-12-19T00:22:16.609Z', 'title': 'LLaDA2.0: Scaling Up Diffusion Language Models to 100B', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.', 'upvotes': 50, 'discussionId': '6944bdd5fbf17e708e185fd7', 'githubRepo': 'https://github.com/inclusionAI/LLaDA2.0', 'githubRepoAddedBy': 'user', 'ai_summary': 'LLaDA2.0 converts auto-regressive models into discrete diffusion large language models with a novel training scheme, achieving superior performance and efficiency at scale.', 'ai_keywords': ['discrete diffusion large language models', 'dLLM', 'auto-regressive models', 'knowledge inheritance', 'progressive adaptation', 'efficiency-aware design', 'block-level WSD', 'block diffusion', 'full-sequence diffusion', 'post-training alignment', 'SFT', 'DPO', 'Mixture-of-Experts', 'parallel decoding'], 'githubStars': 157, 'organization': {'_id': '67c1d682826160b28f778510', 'name': 'antgroup', 'fullname': 'Ant Group', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg'}}, 'publishedAt': '2025-12-10T04:26:18.000Z', 'title': 'LLaDA2.0: Scaling Up Diffusion Language Models to 100B', 'summary': 'This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15745.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 187}, 'organization': {'_id': '67c1d682826160b28f778510', 'name': 'antgroup', 'fullname': 'Ant Group', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16922', 'authors': [{'_id': '6944c39ffbf17e708e18605d', 'user': {'_id': '63f233820a16587ea967adc2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63f233820a16587ea967adc2/1nSoZofPV7UseXzjI2qAH.png', 'isPro': False, 'fullname': 'Sihan XU', 'user': 'sihanxu', 'type': 'user'}, 'name': 'Sihan Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:57:39.350Z', 'hidden': False}, {'_id': '6944c39ffbf17e708e18605e', 'name': 'Ziqiao Ma', 'hidden': False}, {'_id': '6944c39ffbf17e708e18605f', 'name': 'Wenhao Chai', 'hidden': False}, {'_id': '6944c39ffbf17e708e186060', 'name': 'Xuweiyi Chen', 'hidden': False}, {'_id': '6944c39ffbf17e708e186061', 'user': {'_id': '66608add236f958513d21d2e', 'avatarUrl': '/avatars/53eca0891c98cbb93be899885160a983.svg', 'isPro': False, 'fullname': 'Weiyang Jin', 'user': 'Wayne-King', 'type': 'user'}, 'name': 'Weiyang Jin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T09:56:57.800Z', 'hidden': False}, {'_id': '6944c39ffbf17e708e186062', 'name': 'Joyce Chai', 'hidden': False}, {'_id': '6944c39ffbf17e708e186063', 'name': 'Saining Xie', 'hidden': False}, {'_id': '6944c39ffbf17e708e186064', 'name': 'Stella X. Yu', 'hidden': False}], 'publishedAt': '2025-12-18T18:59:58.000Z', 'submittedOnDailyAt': '2025-12-19T01:03:18.428Z', 'title': 'Next-Embedding Prediction Makes Strong Vision Learners', 'submittedOnDailyBy': {'_id': '66608add236f958513d21d2e', 'avatarUrl': '/avatars/53eca0891c98cbb93be899885160a983.svg', 'isPro': False, 'fullname': 'Weiyang Jin', 'user': 'Wayne-King', 'type': 'user'}, 'summary': 'Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.', 'upvotes': 42, 'discussionId': '6944c3a0fbf17e708e186065', 'projectPage': 'https://sihanxu.me/nepa', 'githubRepo': 'https://github.com/SihanXU/nepa', 'githubRepoAddedBy': 'user', 'ai_summary': 'Generative pretraining using next embedding prediction outperforms traditional self-supervised methods in visual learning tasks, achieving high accuracy on ImageNet and effective transfer to semantic segmentation.', 'ai_keywords': ['generative pretraining', 'predictive tasks', 'Next-Embedding Predictive Autoregression (NEPA)', 'causal masking', 'stop gradient', 'Transformer', 'ImageNet-1k', 'top-1 accuracy', 'ViT-B', 'ViT-L', 'semantic segmentation', 'ADE20K'], 'githubStars': 30, 'organization': {'_id': '66df3cb0cf19a8918414cbfe', 'name': 'SixAILab', 'fullname': 'SixAILab', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/63f233820a16587ea967adc2/FSRWuJTgSvG0HFKm9_K4Z.jpeg'}}, 'publishedAt': '2025-12-18T13:59:58.000Z', 'title': 'Next-Embedding Prediction Makes Strong Vision Learners', 'summary': 'Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16922.png', 'numComments': 1, 'submittedBy': {'_id': '66608add236f958513d21d2e', 'avatarUrl': '/avatars/53eca0891c98cbb93be899885160a983.svg', 'fullname': 'Weiyang Jin', 'name': 'Wayne-King', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5}, 'organization': {'_id': '66df3cb0cf19a8918414cbfe', 'name': 'SixAILab', 'fullname': 'SixAILab', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/63f233820a16587ea967adc2/FSRWuJTgSvG0HFKm9_K4Z.jpeg'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.16915', 'authors': [{'_id': '6944c28efbf17e708e18603e', 'name': 'Guibao Shen', 'hidden': False}, {'_id': '6944c28efbf17e708e18603f', 'name': 'Yihua Du', 'hidden': False}, {'_id': '6944c28efbf17e708e186040', 'name': 'Wenhang Ge', 'hidden': False}, {'_id': '6944c28efbf17e708e186041', 'name': 'Jing He', 'hidden': False}, {'_id': '6944c28efbf17e708e186042', 'name': 'Chirui Chang', 'hidden': False}, {'_id': '6944c28efbf17e708e186043', 'name': 'Donghao Zhou', 'hidden': False}, {'_id': '6944c28efbf17e708e186044', 'name': 'Zhen Yang', 'hidden': False}, {'_id': '6944c28efbf17e708e186045', 'name': 'Luozhou Wang', 'hidden': False}, {'_id': '6944c28efbf17e708e186046', 'name': 'Xin Tao', 'hidden': False}, {'_id': '6944c28efbf17e708e186047', 'name': 'Ying-Cong Chen', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65d5aa45dca2a85f0fe895f3/0qAjEuYcHrF_fPEhemUU6.mp4'], 'publishedAt': '2025-12-18T18:59:50.000Z', 'submittedOnDailyAt': '2025-12-19T00:51:00.837Z', 'title': 'StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors', 'submittedOnDailyBy': {'_id': '65d5aa45dca2a85f0fe895f3', 'avatarUrl': '/avatars/a3cbcade6ea101e99f58641aa409fdfe.svg', 'isPro': False, 'fullname': 'Guibao SHEN', 'user': 'PaulSHEN1', 'type': 'user'}, 'summary': \"The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.\", 'upvotes': 31, 'discussionId': '6944c28efbf17e708e186048', 'ai_summary': 'StereoPilot, a feed-forward model leveraging a learnable domain switcher and cycle consistency loss, synthesizes high-quality stereo video directly without depth maps, outperforming existing methods in visual fidelity and computational efficiency.', 'ai_keywords': ['StereoPilot', 'learnable domain switcher', 'cycle consistency loss', 'feed-forward model', 'stereo video conversion', 'UniStereo', 'stereo formats', 'visual fidelity', 'computational efficiency'], 'organization': {'_id': '65ad19cac14c3cf579ad9b68', 'name': 'HKUSTGZ', 'fullname': 'HKUSTGZ'}}, 'publishedAt': '2025-12-18T13:59:50.000Z', 'title': 'StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors', 'summary': \"The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65d5aa45dca2a85f0fe895f3/0qAjEuYcHrF_fPEhemUU6.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16915.png', 'numComments': 1, 'submittedBy': {'_id': '65d5aa45dca2a85f0fe895f3', 'avatarUrl': '/avatars/a3cbcade6ea101e99f58641aa409fdfe.svg', 'fullname': 'Guibao SHEN', 'name': 'PaulSHEN1', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'organization': {'_id': '65ad19cac14c3cf579ad9b68', 'name': 'HKUSTGZ', 'fullname': 'HKUSTGZ'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.13507', 'authors': [{'_id': '6942c6c1fb33037a39577c81', 'name': 'Heyi Chen', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c82', 'name': 'Siyan Chen', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c83', 'name': 'Xin Chen', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c84', 'name': 'Yanfei Chen', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c85', 'name': 'Ying Chen', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c86', 'name': 'Zhuo Chen', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c87', 'name': 'Feng Cheng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c88', 'name': 'Tianheng Cheng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c89', 'name': 'Xinqi Cheng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c8a', 'name': 'Xuyan Chi', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c8b', 'name': 'Jian Cong', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c8c', 'name': 'Jing Cui', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c8d', 'name': 'Qinpeng Cui', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c8e', 'name': 'Qide Dong', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c8f', 'name': 'Junliang Fan', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c90', 'name': 'Jing Fang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c91', 'name': 'Zetao Fang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c92', 'name': 'Chengjian Feng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c93', 'name': 'Han Feng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c94', 'name': 'Mingyuan Gao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c95', 'name': 'Yu Gao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c96', 'name': 'Dong Guo', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c97', 'name': 'Qiushan Guo', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c98', 'name': 'Boyang Hao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c99', 'name': 'Qingkai Hao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c9a', 'name': 'Bibo He', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c9b', 'name': 'Qian He', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c9c', 'name': 'Tuyen Hoang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c9d', 'name': 'Ruoqing Hu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c9e', 'name': 'Xi Hu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577c9f', 'name': 'Weilin Huang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ca0', 'name': 'Zhaoyang Huang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ca1', 'name': 'Zhongyi Huang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ca2', 'name': 'Donglei Ji', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ca3', 'name': 'Siqi Jiang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ca4', 'name': 'Wei Jiang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ca5', 'name': 'Yunpu Jiang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ca6', 'name': 'Zhuo Jiang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ca7', 'name': 'Ashley Kim', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ca8', 'name': 'Jianan Kong', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ca9', 'name': 'Zhichao Lai', 'hidden': False}, {'_id': '6942c6c1fb33037a39577caa', 'name': 'Shanshan Lao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cab', 'name': 'Yichong Leng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cac', 'name': 'Ai Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cad', 'name': 'Feiya Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cae', 'name': 'Gen Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577caf', 'name': 'Huixia Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cb0', 'name': 'JiaShi Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cb1', 'name': 'Liang Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cb2', 'name': 'Ming Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cb3', 'name': 'Shanshan Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cb4', 'name': 'Tao Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cb5', 'name': 'Xian Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cb6', 'name': 'Xiaojie Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cb7', 'name': 'Xiaoyang Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cb8', 'name': 'Xingxing Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cb9', 'name': 'Yameng Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cba', 'name': 'Yifu Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cbb', 'name': 'Yiying Li', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cbc', 'name': 'Chao Liang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cbd', 'name': 'Han Liang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cbe', 'name': 'Jianzhong Liang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cbf', 'name': 'Ying Liang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cc0', 'name': 'Zhiqiang Liang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cc1', 'name': 'Wang Liao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cc2', 'name': 'Yalin Liao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cc3', 'name': 'Heng Lin', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cc4', 'name': 'Kengyu Lin', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cc5', 'name': 'Shanchuan Lin', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cc6', 'name': 'Xi Lin', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cc7', 'name': 'Zhijie Lin', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cc8', 'name': 'Feng Ling', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cc9', 'name': 'Fangfang Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cca', 'name': 'Gaohong Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ccb', 'name': 'Jiawei Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ccc', 'name': 'Jie Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ccd', 'name': 'Jihao Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cce', 'name': 'Shouda Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ccf', 'name': 'Shu Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cd0', 'name': 'Sichao Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cd1', 'name': 'Songwei Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cd2', 'name': 'Xin Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cd3', 'name': 'Xue Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cd4', 'name': 'Yibo Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cd5', 'name': 'Zikun Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cd6', 'name': 'Zuxi Liu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cd7', 'name': 'Junlin Lyu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cd8', 'name': 'Lecheng Lyu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cd9', 'name': 'Qian Lyu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cda', 'name': 'Han Mu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cdb', 'name': 'Xiaonan Nie', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cdc', 'name': 'Jingzhe Ning', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cdd', 'name': 'Xitong Pan', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cde', 'name': 'Yanghua Peng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cdf', 'name': 'Lianke Qin', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ce0', 'name': 'Xueqiong Qu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ce1', 'name': 'Yuxi Ren', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ce2', 'name': 'Kai Shen', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ce3', 'name': 'Guang Shi', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ce4', 'name': 'Lei Shi', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ce5', 'name': 'Yan Song', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ce6', 'name': 'Yinglong Song', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ce7', 'name': 'Fan Sun', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ce8', 'name': 'Li Sun', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ce9', 'name': 'Renfei Sun', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cea', 'name': 'Yan Sun', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ceb', 'name': 'Zeyu Sun', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cec', 'name': 'Wenjing Tang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577ced', 'name': 'Yaxue Tang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cee', 'name': 'Zirui Tao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cef', 'name': 'Feng Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cf0', 'name': 'Furui Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cf1', 'name': 'Jinran Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cf2', 'name': 'Junkai Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cf3', 'name': 'Ke Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cf4', 'name': 'Kexin Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cf5', 'name': 'Qingyi Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cf6', 'name': 'Rui Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cf7', 'name': 'Sen Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cf8', 'name': 'Shuai Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cf9', 'name': 'Tingru Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cfa', 'name': 'Weichen Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cfb', 'name': 'Xin Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cfc', 'name': 'Yanhui Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cfd', 'name': 'Yue Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cfe', 'name': 'Yuping Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577cff', 'name': 'Yuxuan Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d00', 'name': 'Ziyu Wang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d01', 'name': 'Guoqiang Wei', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d02', 'name': 'Wanru Wei', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d03', 'name': 'Di Wu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d04', 'name': 'Guohong Wu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d05', 'name': 'Hanjie Wu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d06', 'name': 'Jian Wu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d07', 'user': {'_id': '6381c5d63680a7cf34e08ca9', 'avatarUrl': '/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg', 'isPro': False, 'fullname': 'wujie10558@gmail.com', 'user': 'wujie10', 'type': 'user'}, 'name': 'Jie Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T09:07:28.238Z', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d08', 'name': 'Ruolan Wu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d09', 'name': 'Xinglong Wu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d0a', 'name': 'Yonghui Wu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d0b', 'name': 'Ruiqi Xia', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d0c', 'name': 'Liang Xiang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d0d', 'name': 'Fei Xiao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d0e', 'name': 'XueFeng Xiao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d0f', 'name': 'Pan Xie', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d10', 'name': 'Shuangyi Xie', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d11', 'name': 'Shuang Xu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d12', 'name': 'Jinlan Xue', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d13', 'name': 'Shen Yan', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d14', 'name': 'Bangbang Yang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d15', 'name': 'Ceyuan Yang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d16', 'name': 'Jiaqi Yang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d17', 'name': 'Runkai Yang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d18', 'name': 'Tao Yang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d19', 'name': 'Yang Yang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d1a', 'name': 'Yihang Yang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d1b', 'name': 'ZhiXian Yang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d1c', 'name': 'Ziyan Yang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d1d', 'name': 'Songting Yao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d1e', 'name': 'Yifan Yao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d1f', 'name': 'Zilyu Ye', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d20', 'name': 'Bowen Yu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d21', 'name': 'Jian Yu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d22', 'name': 'Chujie Yuan', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d23', 'name': 'Linxiao Yuan', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d24', 'name': 'Sichun Zeng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d25', 'name': 'Weihong Zeng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d26', 'name': 'Xuejiao Zeng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d27', 'name': 'Yan Zeng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d28', 'name': 'Chuntao Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d29', 'name': 'Heng Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d2a', 'name': 'Jingjie Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d2b', 'name': 'Kuo Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d2c', 'name': 'Liang Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d2d', 'name': 'Liying Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d2e', 'name': 'Manlin Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d2f', 'name': 'Ting Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d30', 'name': 'Weida Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d31', 'name': 'Xiaohe Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d32', 'name': 'Xinyan Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d33', 'name': 'Yan Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d34', 'name': 'Yuan Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d35', 'name': 'Zixiang Zhang', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d36', 'name': 'Fengxuan Zhao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d37', 'name': 'Huating Zhao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d38', 'name': 'Yang Zhao', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d39', 'name': 'Hao Zheng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d3a', 'name': 'Jianbin Zheng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d3b', 'name': 'Xiaozheng Zheng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d3c', 'name': 'Yangyang Zheng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d3d', 'name': 'Yijie Zheng', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d3e', 'name': 'Jiexin Zhou', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d3f', 'name': 'Jiahui Zhu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d40', 'name': 'Kuan Zhu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d41', 'name': 'Shenhan Zhu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d42', 'name': 'Wenjia Zhu', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d43', 'name': 'Benhui Zou', 'hidden': False}, {'_id': '6942c6c1fb33037a39577d44', 'name': 'Feilong Zuo', 'hidden': False}], 'publishedAt': '2025-12-15T16:36:52.000Z', 'submittedOnDailyAt': '2025-12-19T01:03:13.713Z', 'title': 'Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model', 'submittedOnDailyBy': {'_id': '6381c5d63680a7cf34e08ca9', 'avatarUrl': '/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg', 'isPro': False, 'fullname': 'wujie10558@gmail.com', 'user': 'wujie10', 'type': 'user'}, 'summary': 'Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.', 'upvotes': 29, 'discussionId': '6942c6c1fb33037a39577d45', 'projectPage': 'https://seed.bytedance.com/seedance1_5_pro', 'ai_summary': 'Seedance 1.5 pro, a dual-branch Diffusion Transformer model, achieves high-quality audio-visual synchronization and generation through cross-modal integration, post-training optimizations, and an acceleration framework.', 'ai_keywords': ['Diffusion Transformer', 'cross-modal joint module', 'Supervised Fine-Tuning', 'Reinforcement Learning from Human Feedback', 'multi-dimensional reward models', 'multilingual and dialect lip-syncing', 'dynamic cinematic camera control', 'narrative coherence'], 'organization': {'_id': '67d1140985ea0644e2f14b99', 'name': 'ByteDance-Seed', 'fullname': 'ByteDance Seed', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png'}}, 'publishedAt': '2025-12-15T11:36:52.000Z', 'title': 'Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model', 'summary': 'Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13507.png', 'numComments': 1, 'submittedBy': {'_id': '6381c5d63680a7cf34e08ca9', 'avatarUrl': '/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg', 'fullname': 'wujie10558@gmail.com', 'name': 'wujie10', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 10}, 'organization': {'_id': '67d1140985ea0644e2f14b99', 'name': 'ByteDance-Seed', 'fullname': 'ByteDance Seed', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.16913', 'authors': [{'_id': '6944c315fbf17e708e18604a', 'name': 'Xin Lin', 'hidden': False}, {'_id': '6944c315fbf17e708e18604b', 'name': 'Meixi Song', 'hidden': False}, {'_id': '6944c315fbf17e708e18604c', 'name': 'Dizhe Zhang', 'hidden': False}, {'_id': '6944c315fbf17e708e18604d', 'name': 'Wenxuan Lu', 'hidden': False}, {'_id': '6944c315fbf17e708e18604e', 'user': {'_id': '641d211e353524fe41f16387', 'avatarUrl': '/avatars/c6e72c82c029b415a035beebee50b52c.svg', 'isPro': True, 'fullname': 'Haodong Li', 'user': 'haodongli', 'type': 'user'}, 'name': 'Haodong Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:57:41.262Z', 'hidden': False}, {'_id': '6944c315fbf17e708e18604f', 'name': 'Bo Du', 'hidden': False}, {'_id': '6944c315fbf17e708e186050', 'name': 'Ming-Hsuan Yang', 'hidden': False}, {'_id': '6944c315fbf17e708e186051', 'name': 'Truong Nguyen', 'hidden': False}, {'_id': '6944c315fbf17e708e186052', 'name': 'Lu Qi', 'hidden': False}], 'publishedAt': '2025-12-18T18:59:29.000Z', 'submittedOnDailyAt': '2025-12-19T00:45:20.122Z', 'title': 'Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation', 'submittedOnDailyBy': {'_id': '68e786cbfa2b7fd74a46eb23', 'avatarUrl': '/avatars/8292af16a47ae15b389f17adb67f3e3a.svg', 'isPro': True, 'fullname': 'Insta360-Research', 'user': 'Insta360-Research', 'type': 'user'}, 'summary': 'In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP_website/ {https://insta360-research-team.github.io/DAP\\\\_website/}', 'upvotes': 25, 'discussionId': '6944c316fbf17e708e186053', 'ai_summary': 'A panoramic metric depth foundation model using DINOv3-Large and a three-stage pseudo-label pipeline achieves robust performance across diverse real-world scenes.', 'ai_keywords': ['DINOv3-Large', 'pseudo-label curation pipeline', 'range mask head', 'sharpness-centric optimization', 'geometry-centric optimization', 'Stanford2D3D', 'Matterport3D', 'Deep360']}, 'publishedAt': '2025-12-18T13:59:29.000Z', 'title': 'Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation', 'summary': 'In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP_website/ {https://insta360-research-team.github.io/DAP\\\\_website/}', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16913.png', 'numComments': 1, 'submittedBy': {'_id': '68e786cbfa2b7fd74a46eb23', 'avatarUrl': '/avatars/8292af16a47ae15b389f17adb67f3e3a.svg', 'fullname': 'Insta360-Research', 'name': 'Insta360-Research', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 12}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16923', 'authors': [{'_id': '6944d824fbf17e708e1860f3', 'user': {'_id': '6391eaf6a22277aa7d6ece6f', 'avatarUrl': '/avatars/7a709e7039dac07c7ca24d5e23f7785e.svg', 'isPro': False, 'fullname': 'Chun-Wei Tuan Mu', 'user': 'rayray9999', 'type': 'user'}, 'name': 'Chun-Wei Tuan Mu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:57:31.371Z', 'hidden': False}, {'_id': '6944d824fbf17e708e1860f4', 'name': 'Jia-Bin Huang', 'hidden': False}, {'_id': '6944d824fbf17e708e1860f5', 'name': 'Yu-Lun Liu', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/9Vljy4RtRgMfLQ2TrP_x5.mp4'], 'publishedAt': '2025-12-18T18:59:59.000Z', 'submittedOnDailyAt': '2025-12-19T02:15:37.076Z', 'title': 'Generative Refocusing: Flexible Defocus Control from a Single Image', 'submittedOnDailyBy': {'_id': '6459d5da3b6fafd9664807ab', 'avatarUrl': '/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg', 'isPro': False, 'fullname': 'Yu-Lun Liu', 'user': 'yulunliu', 'type': 'user'}, 'summary': 'Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.', 'upvotes': 21, 'discussionId': '6944d825fbf17e708e1860f6', 'projectPage': 'https://generative-refocusing.github.io/', 'githubRepo': 'https://github.com/rayray9999/Genfocus', 'githubRepoAddedBy': 'user', 'ai_summary': 'Generative Refocusing uses semi-supervised learning with DeblurNet and BokehNet to achieve high-quality single-image refocusing with controllable bokeh and text-guided adjustments.', 'ai_keywords': ['DeblurNet', 'BokehNet', 'semi-supervised training', 'defocus deblurring', 'bokeh synthesis', 'EXIF metadata', 'text-guided adjustments'], 'githubStars': 16}, 'publishedAt': '2025-12-18T13:59:59.000Z', 'title': 'Generative Refocusing: Flexible Defocus Control from a Single Image', 'summary': 'Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/9Vljy4RtRgMfLQ2TrP_x5.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16923.png', 'numComments': 1, 'submittedBy': {'_id': '6459d5da3b6fafd9664807ab', 'avatarUrl': '/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg', 'fullname': 'Yu-Lun Liu', 'name': 'yulunliu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16625', 'authors': [{'_id': '6944bdf2fbf17e708e185fd9', 'name': 'Linghui Shen', 'hidden': False}, {'_id': '6944bdf2fbf17e708e185fda', 'name': 'Mingyue Cui', 'hidden': False}, {'_id': '6944bdf2fbf17e708e185fdb', 'user': {'_id': '634cfebc350bcee9bed20a4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png', 'isPro': False, 'fullname': 'Xingyi Yang', 'user': 'adamdad', 'type': 'user'}, 'name': 'Xingyi Yang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:58:09.375Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/e0DL12ili-5bxUdYoakWm.png'], 'publishedAt': '2025-12-18T15:01:44.000Z', 'submittedOnDailyAt': '2025-12-19T00:30:00.085Z', 'title': 'DeContext as Defense: Safe Image Editing in Diffusion Transformers', 'submittedOnDailyBy': {'_id': '634cfebc350bcee9bed20a4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png', 'isPro': False, 'fullname': 'Xingyi Yang', 'user': 'adamdad', 'type': 'user'}, 'summary': \"In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.\", 'upvotes': 21, 'discussionId': '6944bdf3fbf17e708e185fdc', 'projectPage': 'https://linghuiishen.github.io/decontext_project_page/', 'githubRepo': 'https://github.com/LinghuiiShen/DeContext', 'githubRepoAddedBy': 'user', 'ai_summary': 'DeContext defends against unauthorized in-context image editing by weakening cross-attention pathways in multimodal attention layers, preserving visual quality while blocking unwanted modifications.', 'ai_keywords': ['in-context diffusion models', 'multimodal attention layers', 'cross-attention pathways', 'DeContext', 'denoising steps', 'transformer blocks', 'Flux Kontext', 'Step1X-Edit'], 'githubStars': 6, 'organization': {'_id': '646ecc368d316fde87b3b6e3', 'name': 'PolyUHK', 'fullname': 'The Hong Kong Polytechnic University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/646ecbc0cbb7bb996513e298/Akb4zKqIP9kb9PQoUPUmj.jpeg'}}, 'publishedAt': '2025-12-18T10:01:44.000Z', 'title': 'DeContext as Defense: Safe Image Editing in Diffusion Transformers', 'summary': \"In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/e0DL12ili-5bxUdYoakWm.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16625.png', 'numComments': 1, 'submittedBy': {'_id': '634cfebc350bcee9bed20a4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png', 'fullname': 'Xingyi Yang', 'name': 'adamdad', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 20}, 'organization': {'_id': '646ecc368d316fde87b3b6e3', 'name': 'PolyUHK', 'fullname': 'The Hong Kong Polytechnic University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/646ecbc0cbb7bb996513e298/Akb4zKqIP9kb9PQoUPUmj.jpeg'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.16301', 'authors': [{'_id': '6944be0ffbf17e708e185fde', 'user': {'_id': '63724cfada3183d9d53f2009', 'avatarUrl': '/avatars/17838fcf244ecf8d139343bb6c6d8562.svg', 'isPro': False, 'fullname': 'Patrick Jiang', 'user': 'pat-jj', 'type': 'user'}, 'name': 'Pengcheng Jiang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:58:04.988Z', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fdf', 'user': {'_id': '650488e454b989666d042a49', 'avatarUrl': '/avatars/3dc79c6f1a9dce872636dddd38a04670.svg', 'isPro': False, 'fullname': 'Jiacheng Lin', 'user': 'linjc16', 'type': 'user'}, 'name': 'Jiacheng Lin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:58:02.806Z', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fe0', 'name': 'Zhiyi Shi', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fe1', 'name': 'Zifeng Wang', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fe2', 'name': 'Luxi He', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fe3', 'name': 'Yichen Wu', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fe4', 'name': 'Ming Zhong', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fe5', 'user': {'_id': '649c5cf5c1ae48cf4d7dda34', 'avatarUrl': '/avatars/a2264945f9f876b690017a93f225f937.svg', 'isPro': False, 'fullname': 'Peiyang Song', 'user': 'p-song1', 'type': 'user'}, 'name': 'Peiyang Song', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:58:06.854Z', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fe6', 'name': 'Qizheng Zhang', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fe7', 'name': 'Heng Wang', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fe8', 'user': {'_id': '66a3f1c4c38ce500371fd8d4', 'avatarUrl': '/avatars/381de938091f1a5c179eef72aa247bbf.svg', 'isPro': False, 'fullname': 'Xueqiang Xu', 'user': 'XueqiangXu', 'type': 'user'}, 'name': 'Xueqiang Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:58:00.653Z', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fe9', 'name': 'Hanwen Xu', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fea', 'name': 'Pengrui Han', 'hidden': False}, {'_id': '6944be0ffbf17e708e185feb', 'name': 'Dylan Zhang', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fec', 'name': 'Jiashuo Sun', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fed', 'name': 'Chaoqi Yang', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fee', 'name': 'Kun Qian', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fef', 'name': 'Tian Wang', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ff0', 'name': 'Changran Hu', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ff1', 'name': 'Manling Li', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ff2', 'name': 'Quanzheng Li', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ff3', 'name': 'Hao Peng', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ff4', 'name': 'Sheng Wang', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ff5', 'name': 'Jingbo Shang', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ff6', 'name': 'Chao Zhang', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ff7', 'name': 'Jiaxuan You', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ff8', 'name': 'Liyuan Liu', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ff9', 'name': 'Pan Lu', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ffa', 'name': 'Yu Zhang', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ffb', 'name': 'Heng Ji', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ffc', 'name': 'Yejin Choi', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ffd', 'name': 'Dawn Song', 'hidden': False}, {'_id': '6944be0ffbf17e708e185ffe', 'name': 'Jimeng Sun', 'hidden': False}, {'_id': '6944be0ffbf17e708e185fff', 'name': 'Jiawei Han', 'hidden': False}], 'publishedAt': '2025-12-18T08:38:51.000Z', 'submittedOnDailyAt': '2025-12-19T00:23:16.990Z', 'title': 'Adaptation of Agentic AI', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.', 'upvotes': 21, 'discussionId': '6944be10fbf17e708e186000', 'githubRepo': 'https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI', 'githubRepoAddedBy': 'user', 'ai_summary': 'This paper presents a framework for agent and tool adaptation in agentic AI systems, clarifying design strategies and identifying open challenges for improving AI capabilities.', 'ai_keywords': ['agentic AI systems', 'foundation models', 'agent adaptations', 'tool adaptations', 'tool-execution-signaled', 'agent-output-signaled', 'agent-agnostic', 'agent-supervised'], 'githubStars': 257}, 'publishedAt': '2025-12-18T03:38:51.000Z', 'title': 'Adaptation of Agentic AI', 'summary': 'Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16301.png', 'numComments': 3, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 187}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.16905', 'authors': [{'_id': '6944c650fbf17e708e18608c', 'name': 'Kaixin Ding', 'hidden': False}, {'_id': '6944c650fbf17e708e18608d', 'name': 'Yang Zhou', 'hidden': False}, {'_id': '6944c650fbf17e708e18608e', 'name': 'Xi Chen', 'hidden': False}, {'_id': '6944c650fbf17e708e18608f', 'name': 'Miao Yang', 'hidden': False}, {'_id': '6944c650fbf17e708e186090', 'name': 'Jiarong Ou', 'hidden': False}, {'_id': '6944c650fbf17e708e186091', 'name': 'Rui Chen', 'hidden': False}, {'_id': '6944c650fbf17e708e186092', 'name': 'Xin Tao', 'hidden': False}, {'_id': '6944c650fbf17e708e186093', 'name': 'Hengshuang Zhao', 'hidden': False}], 'publishedAt': '2025-12-18T18:57:58.000Z', 'submittedOnDailyAt': '2025-12-19T01:16:07.574Z', 'title': 'Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection', 'submittedOnDailyBy': {'_id': '64e9c855233101ed99ca2315', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ozwIxW0ofJMR6kj623p1T.png', 'isPro': False, 'fullname': 'YANG, Zhenya', 'user': 'ANIYA673', 'type': 'user'}, 'summary': \"Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.\", 'upvotes': 18, 'discussionId': '6944c651fbf17e708e186094', 'ai_summary': 'Alchemist, a meta-gradient-based framework, automatically selects high-quality subsets from large-scale text-image datasets to improve visual quality and training efficiency in Text-to-Image models.', 'ai_keywords': ['Text-to-Image (T2I)', 'Imagen', 'Stable Diffusion', 'FLUX', 'data selection', 'data filtering', 'meta-learning', 'meta-gradient-based', 'data rating', 'data pruning', 'lightweight rater', 'gradient information', 'multi-granularity perception', 'Shift-Gsampling', 'visual quality', 'downstream performance']}, 'publishedAt': '2025-12-18T13:57:58.000Z', 'title': 'Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection', 'summary': \"Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16905.png', 'numComments': 1, 'submittedBy': {'_id': '64e9c855233101ed99ca2315', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ozwIxW0ofJMR6kj623p1T.png', 'fullname': 'YANG, Zhenya', 'name': 'ANIYA673', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16636', 'authors': [{'_id': '6944eb49fbf17e708e186136', 'name': 'Giorgos Petsangourakis', 'hidden': False}, {'_id': '6944eb49fbf17e708e186137', 'name': 'Christos Sgouropoulos', 'hidden': False}, {'_id': '6944eb49fbf17e708e186138', 'name': 'Bill Psomas', 'hidden': False}, {'_id': '6944eb49fbf17e708e186139', 'name': 'Theodoros Giannakopoulos', 'hidden': False}, {'_id': '6944eb49fbf17e708e18613a', 'name': 'Giorgos Sfikas', 'hidden': False}, {'_id': '6944eb49fbf17e708e18613b', 'name': 'Ioannis Kakogeorgiou', 'hidden': False}], 'publishedAt': '2025-12-18T15:10:42.000Z', 'submittedOnDailyAt': '2025-12-19T03:38:51.121Z', 'title': 'REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion', 'submittedOnDailyBy': {'_id': '661ba524bd9243bf7e598355', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/661ba524bd9243bf7e598355/i77yD4XgJn2vUbn_mIsT8.jpeg', 'isPro': False, 'fullname': 'Ioannis Kakogeorgiou', 'user': 'gkakogeorgiou', 'type': 'user'}, 'summary': 'Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .', 'upvotes': 18, 'discussionId': '6944eb4afbf17e708e18613c', 'githubRepo': 'https://github.com/giorgospets/reglue', 'githubRepoAddedBy': 'user', 'ai_summary': 'REGLUE, a unified latent diffusion framework, enhances image synthesis by jointly modeling VAE latents, patch-level VFM semantics, and global tokens, improving semantic supervision and convergence.', 'ai_keywords': ['Latent diffusion models', 'VAE', 'Vision Foundation Models', 'representation alignment', 'convolutional semantic compressor', 'nonlinear aggregation', 'multi-layer VFM features', 'FID', 'SiT backbone', 'external alignment loss'], 'githubStars': 0}, 'publishedAt': '2025-12-18T10:10:42.000Z', 'title': 'REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion', 'summary': 'Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16636.png', 'numComments': 1, 'submittedBy': {'_id': '661ba524bd9243bf7e598355', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/661ba524bd9243bf7e598355/i77yD4XgJn2vUbn_mIsT8.jpeg', 'fullname': 'Ioannis Kakogeorgiou', 'name': 'gkakogeorgiou', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16561', 'authors': [{'_id': '6944b692fbf17e708e185f41', 'user': {'_id': '6707f8800812a88fd6d87ebe', 'avatarUrl': '/avatars/28cd99359799ce85d5fe503144b776d8.svg', 'isPro': True, 'fullname': 'YUXIN WANG', 'user': 'yuxinhk', 'type': 'user'}, 'name': 'Yuxin Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:58:20.875Z', 'hidden': False}, {'_id': '6944b692fbf17e708e185f42', 'name': 'Lei Ke', 'hidden': False}, {'_id': '6944b692fbf17e708e185f43', 'name': 'Boqiang Zhang', 'hidden': False}, {'_id': '6944b692fbf17e708e185f44', 'name': 'Tianyuan Qu', 'hidden': False}, {'_id': '6944b692fbf17e708e185f45', 'name': 'Hanxun Yu', 'hidden': False}, {'_id': '6944b692fbf17e708e185f46', 'name': 'Zhenpeng Huang', 'hidden': False}, {'_id': '6944b692fbf17e708e185f47', 'name': 'Meng Yu', 'hidden': False}, {'_id': '6944b692fbf17e708e185f48', 'name': 'Dan Xu', 'hidden': False}, {'_id': '6944b692fbf17e708e185f49', 'name': 'Dong Yu', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/iKx30aOl3GEAMR_6_DHBR.mp4'], 'publishedAt': '2025-12-18T14:03:44.000Z', 'submittedOnDailyAt': '2025-12-19T02:30:09.104Z', 'title': 'N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models', 'submittedOnDailyBy': {'_id': '6258a6455ea3a0a9b6de3f22', 'avatarUrl': '/avatars/6eeed72a97fb24465e5e65583fbe50cf.svg', 'isPro': False, 'fullname': 'Lei Ke', 'user': 'lkeab', 'type': 'user'}, 'summary': 'While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.', 'upvotes': 16, 'discussionId': '6944b692fbf17e708e185f4a', 'ai_summary': 'N3D-VLM integrates native 3D perception and reasoning in vision-language models, enabling precise 3D localization and spatial understanding with a large-scale dataset.', 'ai_keywords': ['3D object perception', '3D-aware visual reasoning', '3D grounding', 'spatial understanding', 'native 3D object perception', '3D object localization', 'explicit reasoning', 'depth estimation', 'chain-of-thought reasoning', '3D spatial reasoning', 'vision-language model'], 'organization': {'_id': '66543b6e420092799d2f625c', 'name': 'tencent', 'fullname': 'Tencent', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png'}}, 'publishedAt': '2025-12-18T09:03:44.000Z', 'title': 'N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models', 'summary': 'While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/iKx30aOl3GEAMR_6_DHBR.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16561.png', 'numComments': 1, 'submittedBy': {'_id': '6258a6455ea3a0a9b6de3f22', 'avatarUrl': '/avatars/6eeed72a97fb24465e5e65583fbe50cf.svg', 'fullname': 'Lei Ke', 'name': 'lkeab', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 11}, 'organization': {'_id': '66543b6e420092799d2f625c', 'name': 'tencent', 'fullname': 'Tencent', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16924', 'authors': [{'_id': '6944cb02fbf17e708e1860bf', 'name': 'Hanlin Wang', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860c0', 'name': 'Hao Ouyang', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860c1', 'user': {'_id': '64981bea09cea550852652af', 'avatarUrl': '/avatars/df528e9008972c8e5ae4d278e617476c.svg', 'isPro': False, 'fullname': 'Qiuyu Wang', 'user': 'qiuyuu', 'type': 'user'}, 'name': 'Qiuyu Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:57:37.494Z', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860c2', 'name': 'Yue Yu', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860c3', 'name': 'Yihao Meng', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860c4', 'name': 'Wen Wang', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860c5', 'name': 'Ka Leong Cheng', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860c6', 'name': 'Shuailei Ma', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860c7', 'name': 'Qingyan Bai', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860c8', 'name': 'Yixuan Li', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860c9', 'name': 'Cheng Chen', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860ca', 'name': 'Yanhong Zeng', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860cb', 'name': 'Xing Zhu', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860cc', 'name': 'Yujun Shen', 'hidden': False}, {'_id': '6944cb02fbf17e708e1860cd', 'name': 'Qifeng Chen', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/656084f44e8918182d4f07c8/E1DcYH7ixuiqkGk34FuWa.mp4'], 'publishedAt': '2025-12-18T18:59:59.000Z', 'submittedOnDailyAt': '2025-12-19T03:25:56.892Z', 'title': 'The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text', 'submittedOnDailyBy': {'_id': '656084f44e8918182d4f07c8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/akAvCUCi7eR31PWOXrVPw.jpeg', 'isPro': False, 'fullname': 'Yihao Meng', 'user': 'Yhmeng1106', 'type': 'user'}, 'summary': 'We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.', 'upvotes': 14, 'discussionId': '6944cb03fbf17e708e1860ce', 'projectPage': 'https://worldcanvas.github.io/', 'githubRepo': 'https://github.com/pPetrichor/WorldCanvas', 'githubRepoAddedBy': 'user', 'ai_summary': 'WorldCanvas generates coherent, controllable world events using a multimodal framework that integrates text, trajectories, and reference images.', 'ai_keywords': ['promptable world events', 'user-directed simulation', 'trajectories', 'natural language', 'semantic intent', 'reference images', 'visual grounding', 'multi-agent interactions', 'object entry/exit', 'reference-guided appearance', 'counterintuitive events', 'temporal coherence', 'emergent consistency', 'world models', 'interactive simulators'], 'githubStars': 20, 'organization': {'_id': '67c1d682826160b28f778510', 'name': 'antgroup', 'fullname': 'Ant Group', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg'}}, 'publishedAt': '2025-12-18T13:59:59.000Z', 'title': 'The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text', 'summary': 'We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/656084f44e8918182d4f07c8/E1DcYH7ixuiqkGk34FuWa.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16924.png', 'numComments': 1, 'submittedBy': {'_id': '656084f44e8918182d4f07c8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/akAvCUCi7eR31PWOXrVPw.jpeg', 'fullname': 'Yihao Meng', 'name': 'Yhmeng1106', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'organization': {'_id': '67c1d682826160b28f778510', 'name': 'antgroup', 'fullname': 'Ant Group', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16918', 'authors': [{'_id': '6944d2c9fbf17e708e1860e3', 'name': 'Chaoyang Wang', 'hidden': False}, {'_id': '6944d2c9fbf17e708e1860e4', 'name': 'Kaituo Feng', 'hidden': False}, {'_id': '6944d2c9fbf17e708e1860e5', 'name': 'Dongyang Chen', 'hidden': False}, {'_id': '6944d2c9fbf17e708e1860e6', 'name': 'Zhongyu Wang', 'hidden': False}, {'_id': '6944d2c9fbf17e708e1860e7', 'name': 'Zhixun Li', 'hidden': False}, {'_id': '6944d2c9fbf17e708e1860e8', 'name': 'Sicheng Gao', 'hidden': False}, {'_id': '6944d2c9fbf17e708e1860e9', 'name': 'Meng Meng', 'hidden': False}, {'_id': '6944d2c9fbf17e708e1860ea', 'name': 'Xu Zhou', 'hidden': False}, {'_id': '6944d2c9fbf17e708e1860eb', 'name': 'Manyuan Zhang', 'hidden': False}, {'_id': '6944d2c9fbf17e708e1860ec', 'name': 'Yuzhang Shang', 'hidden': False}, {'_id': '6944d2c9fbf17e708e1860ed', 'name': 'Xiangyu Yue', 'hidden': False}], 'publishedAt': '2025-12-18T18:59:55.000Z', 'submittedOnDailyAt': '2025-12-19T01:52:10.597Z', 'title': 'AdaTooler-V: Adaptive Tool-Use for Images and Videos', 'submittedOnDailyBy': {'_id': '67079840a9bcb7459b8d2a46', 'avatarUrl': '/avatars/32466863c5554f20cb2775b138832ac3.svg', 'isPro': False, 'fullname': 'Kaituo Feng', 'user': 'KaituoFeng', 'type': 'user'}, 'summary': 'Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.', 'upvotes': 10, 'discussionId': '6944d2c9fbf17e708e1860ee', 'ai_summary': 'AdaTooler-V, a multimodal large language model, adaptively uses vision tools based on reinforcement learning, improving performance and reducing unnecessary tool invocations in visual reasoning tasks.', 'ai_keywords': ['multimodal large language models', 'multimodal interleaved chain-of-thought', 'vision tool interactions', 'reinforcement learning', 'adaptive tool-use', 'Tool Benefit Score', 'AdaTooler-V-CoT-100k', 'AdaTooler-V-300k', 'visual reasoning tasks', 'high-resolution benchmark']}, 'publishedAt': '2025-12-18T13:59:55.000Z', 'title': 'AdaTooler-V: Adaptive Tool-Use for Images and Videos', 'summary': 'Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16918.png', 'numComments': 1, 'submittedBy': {'_id': '67079840a9bcb7459b8d2a46', 'avatarUrl': '/avatars/32466863c5554f20cb2775b138832ac3.svg', 'fullname': 'Kaituo Feng', 'name': 'KaituoFeng', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16649', 'authors': [{'_id': '6944b6f3fbf17e708e185f4c', 'name': 'Bingxiang He', 'hidden': False}, {'_id': '6944b6f3fbf17e708e185f4d', 'name': 'Zekai Qu', 'hidden': False}, {'_id': '6944b6f3fbf17e708e185f4e', 'name': 'Zeyuan Liu', 'hidden': False}, {'_id': '6944b6f3fbf17e708e185f4f', 'name': 'Yinghao Chen', 'hidden': False}, {'_id': '6944b6f3fbf17e708e185f50', 'name': 'Yuxin Zuo', 'hidden': False}, {'_id': '6944b6f3fbf17e708e185f51', 'name': 'Cheng Qian', 'hidden': False}, {'_id': '6944b6f3fbf17e708e185f52', 'name': 'Kaiyan Zhang', 'hidden': False}, {'_id': '6944b6f3fbf17e708e185f53', 'name': 'Weize Chen', 'hidden': False}, {'_id': '6944b6f3fbf17e708e185f54', 'name': 'Chaojun Xiao', 'hidden': False}, {'_id': '6944b6f3fbf17e708e185f55', 'name': 'Ganqu Cui', 'hidden': False}, {'_id': '6944b6f3fbf17e708e185f56', 'name': 'Ning Ding', 'hidden': False}, {'_id': '6944b6f3fbf17e708e185f57', 'name': 'Zhiyuan Liu', 'hidden': False}], 'publishedAt': '2025-12-18T15:21:25.000Z', 'submittedOnDailyAt': '2025-12-19T09:19:59.748Z', 'title': 'JustRL: Scaling a 1.5B LLM with a Simple RL Recipe', 'submittedOnDailyBy': {'_id': '64c5e944979493279b700cb2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vjFuPWw8Vl7b7gXB19Sk-.jpeg', 'isPro': False, 'fullname': 'Bingxiang He', 'user': 'hbx', 'type': 'user'}, 'summary': \"Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: Is this complexity necessary? We present JustRL, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\\\% and 64.3\\\\% average accuracy across nine mathematical benchmarks) while using 2times less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.\", 'upvotes': 10, 'discussionId': '6944b6f3fbf17e708e185f58', 'projectPage': 'https://www.notion.so/JustRL-Scaling-a-1-5B-LLM-with-a-Simple-RL-Recipe-24f6198b0b6b80e48e74f519bfdaf0a8', 'githubRepo': 'https://github.com/thunlp/JustRL', 'githubRepoAddedBy': 'user', 'ai_summary': 'JustRL achieves state-of-the-art performance on reasoning models with minimal complexity, using single-stage training and fixed hyperparameters, outperforming sophisticated approaches in terms of compute and stability.', 'ai_keywords': ['reinforcement learning', 'large language models', 'multi-stage training', 'dynamic hyperparameter schedules', 'curriculum learning', 'single-stage training', 'fixed hyperparameters', 'reasoning models', 'mathematical benchmarks', 'explicit length penalties', 'robust verifiers', 'stable baseline'], 'organization': {'_id': '628735cbc83a2d6ab8d14a66', 'name': 'Tsinghua', 'fullname': 'Tsinghua University'}}, 'publishedAt': '2025-12-18T10:21:25.000Z', 'title': 'JustRL: Scaling a 1.5B LLM with a Simple RL Recipe', 'summary': \"Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: Is this complexity necessary? We present JustRL, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\\\% and 64.3\\\\% average accuracy across nine mathematical benchmarks) while using 2times less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16649.png', 'numComments': 2, 'submittedBy': {'_id': '64c5e944979493279b700cb2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vjFuPWw8Vl7b7gXB19Sk-.jpeg', 'fullname': 'Bingxiang He', 'name': 'hbx', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'organization': {'_id': '628735cbc83a2d6ab8d14a66', 'name': 'Tsinghua', 'fullname': 'Tsinghua University'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16912', 'authors': [{'_id': '6944c358fbf17e708e186055', 'name': 'Peter Chen', 'hidden': False}, {'_id': '6944c358fbf17e708e186056', 'name': 'Xiaopeng Li', 'hidden': False}, {'_id': '6944c358fbf17e708e186057', 'name': 'Ziniu Li', 'hidden': False}, {'_id': '6944c358fbf17e708e186058', 'name': 'Wotao Yin', 'hidden': False}, {'_id': '6944c358fbf17e708e186059', 'name': 'Xi Chen', 'hidden': False}, {'_id': '6944c358fbf17e708e18605a', 'name': 'Tianyi Lin', 'hidden': False}], 'publishedAt': '2025-12-18T18:59:27.000Z', 'submittedOnDailyAt': '2025-12-19T00:46:12.489Z', 'title': 'Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward', 'submittedOnDailyBy': {'_id': '678323cb4bd851a06acb936f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/amD4z7Yr6mCh-mQ0_Rnb1.png', 'isPro': False, 'fullname': 'Peter L. Chen', 'user': 'PeterLauLukCh', 'type': 'user'}, 'summary': 'This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.', 'upvotes': 8, 'discussionId': '6944c358fbf17e708e18605b', 'ai_summary': 'Reinforcement learning with verifiable rewards improves LLM reasoning through spurious rewards and entropy minimization, despite seemingly paradoxical effects, by reducing clipping bias and policy entropy.', 'ai_keywords': ['reinforcement learning', 'verifiable rewards', 'RLVR', 'Large Language Models', 'LLMs', 'spurious rewards', 'entropy minimization', 'policy entropy', 'clipping bias', 'reward-misalignment model'], 'organization': {'_id': '63f68badb607296857bb2441', 'name': 'columbia', 'fullname': 'Columbia University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/USuDBWwfOGNrQ0SZGFxDF.png'}}, 'publishedAt': '2025-12-18T13:59:27.000Z', 'title': 'Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward', 'summary': 'This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16912.png', 'numComments': 1, 'submittedBy': {'_id': '678323cb4bd851a06acb936f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/amD4z7Yr6mCh-mQ0_Rnb1.png', 'fullname': 'Peter L. Chen', 'name': 'PeterLauLukCh', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'organization': {'_id': '63f68badb607296857bb2441', 'name': 'columbia', 'fullname': 'Columbia University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/USuDBWwfOGNrQ0SZGFxDF.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16864', 'authors': [{'_id': '6944ce43fbf17e708e1860d8', 'user': {'_id': '66e79b3c1c79fc2e51dc1d60', 'avatarUrl': '/avatars/8706336e9e7a417505c9bb32583a662f.svg', 'isPro': False, 'fullname': 'QU Tianyuan', 'user': 'TainU', 'type': 'user'}, 'name': 'Tianyuan Qu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:57:33.807Z', 'hidden': False}, {'_id': '6944ce43fbf17e708e1860d9', 'name': 'Lei Ke', 'hidden': False}, {'_id': '6944ce43fbf17e708e1860da', 'name': 'Xiaohang Zhan', 'hidden': False}, {'_id': '6944ce43fbf17e708e1860db', 'name': 'Longxiang Tang', 'hidden': False}, {'_id': '6944ce43fbf17e708e1860dc', 'name': 'Yuqi Liu', 'hidden': False}, {'_id': '6944ce43fbf17e708e1860dd', 'name': 'Bohao Peng', 'hidden': False}, {'_id': '6944ce43fbf17e708e1860de', 'name': 'Bei Yu', 'hidden': False}, {'_id': '6944ce43fbf17e708e1860df', 'name': 'Dong Yu', 'hidden': False}, {'_id': '6944ce43fbf17e708e1860e0', 'name': 'Jiaya Jia', 'hidden': False}], 'publishedAt': '2025-12-18T18:34:23.000Z', 'submittedOnDailyAt': '2025-12-19T01:35:30.623Z', 'title': 'RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing', 'submittedOnDailyBy': {'_id': '66e79b3c1c79fc2e51dc1d60', 'avatarUrl': '/avatars/8706336e9e7a417505c9bb32583a662f.svg', 'isPro': False, 'fullname': 'QU Tianyuan', 'user': 'TainU', 'type': 'user'}, 'summary': 'Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io', 'upvotes': 8, 'discussionId': '6944ce43fbf17e708e1860e1', 'projectPage': 'https://replan-iv-edit.github.io/', 'githubRepo': 'https://github.com/dvlab-research/RePlan', 'githubRepoAddedBy': 'user', 'ai_summary': 'RePlan, a plan-then-execute framework, enhances instruction-based image editing by combining a vision-language planner with a diffusion editor, achieving superior performance in complex and intricate editing tasks using limited data.', 'ai_keywords': ['instruction-based image editing', 'vision-language planner', 'diffusion editor', 'step-by-step reasoning', 'attention-region injection mechanism', 'GRPO-based reinforcement learning', 'IV-Edit benchmark', 'regional precision', 'overall fidelity'], 'githubStars': 8}, 'publishedAt': '2025-12-18T13:34:23.000Z', 'title': 'RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing', 'summary': 'Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16864.png', 'numComments': 1, 'submittedBy': {'_id': '66e79b3c1c79fc2e51dc1d60', 'avatarUrl': '/avatars/8706336e9e7a417505c9bb32583a662f.svg', 'fullname': 'QU Tianyuan', 'name': 'TainU', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.16900', 'authors': [{'_id': '6944c40afbf17e708e186067', 'name': 'Shuyuan Tu', 'hidden': False}, {'_id': '6944c40afbf17e708e186068', 'name': 'Yueming Pan', 'hidden': False}, {'_id': '6944c40afbf17e708e186069', 'name': 'Yinming Huang', 'hidden': False}, {'_id': '6944c40afbf17e708e18606a', 'name': 'Xintong Han', 'hidden': False}, {'_id': '6944c40afbf17e708e18606b', 'name': 'Zhen Xing', 'hidden': False}, {'_id': '6944c40afbf17e708e18606c', 'name': 'Qi Dai', 'hidden': False}, {'_id': '6944c40afbf17e708e18606d', 'name': 'Kai Qiu', 'hidden': False}, {'_id': '6944c40afbf17e708e18606e', 'name': 'Chong Luo', 'hidden': False}, {'_id': '6944c40afbf17e708e18606f', 'name': 'Zuxuan Wu', 'hidden': False}], 'publishedAt': '2025-12-18T18:56:05.000Z', 'submittedOnDailyAt': '2025-12-19T04:30:34.828Z', 'title': 'FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction', 'submittedOnDailyBy': {'_id': '66da6972eae491c64243e8f3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SgX1j3QGKMk_YFTnM9Wr_.png', 'isPro': False, 'fullname': 'Shuyuan Tu', 'user': 'FrancisRing', 'type': 'user'}, 'summary': 'Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.', 'upvotes': 7, 'discussionId': '6944c40bfbf17e708e186070', 'projectPage': 'https://francis-rings.github.io/FlashPortrait/', 'githubRepo': 'https://github.com/Francis-Rings/FlashPortrait', 'githubRepoAddedBy': 'user', 'ai_summary': 'FlashPortrait is a diffusion-based video transformer for long-portrait animation that ensures ID consistency and achieves 6x acceleration through a dynamic sliding-window scheme and higher-order latent derivatives.', 'ai_keywords': ['video diffusion transformer', 'ID-preserving', 'infinite-length videos', 'identity-agnostic facial expression features', 'Normalized Facial Expression Block', 'diffusion latents', 'dynamic sliding-window scheme', 'weighted blending', 'latent variation rate', 'derivative magnitude ratio', 'higher-order latent derivatives'], 'githubStars': 19, 'organization': {'_id': '643cb0625fcffe09fb6ca688', 'name': 'Fudan-University', 'fullname': 'Fudan University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png'}}, 'publishedAt': '2025-12-18T13:56:05.000Z', 'title': 'FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction', 'summary': 'Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16900.png', 'numComments': 1, 'submittedBy': {'_id': '66da6972eae491c64243e8f3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SgX1j3QGKMk_YFTnM9Wr_.png', 'fullname': 'Shuyuan Tu', 'name': 'FrancisRing', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}, 'organization': {'_id': '643cb0625fcffe09fb6ca688', 'name': 'Fudan-University', 'fullname': 'Fudan University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16501', 'authors': [{'_id': '6944c0f0fbf17e708e186028', 'name': 'Beitong Zhou', 'hidden': False}, {'_id': '6944c0f0fbf17e708e186029', 'name': 'Zhexiao Huang', 'hidden': False}, {'_id': '6944c0f0fbf17e708e18602a', 'name': 'Yuan Guo', 'hidden': False}, {'_id': '6944c0f0fbf17e708e18602b', 'user': {'_id': '60d2a2984956988b63753371', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg', 'isPro': False, 'fullname': 'Zhangxuan Gu', 'user': 'zhangxgu', 'type': 'user'}, 'name': 'Zhangxuan Gu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:57:56.975Z', 'hidden': False}, {'_id': '6944c0f0fbf17e708e18602c', 'name': 'Tianyu Xia', 'hidden': False}, {'_id': '6944c0f0fbf17e708e18602d', 'name': 'Zichen Luo', 'hidden': False}, {'_id': '6944c0f0fbf17e708e18602e', 'name': 'Fei Tang', 'hidden': False}, {'_id': '6944c0f0fbf17e708e18602f', 'name': 'Dehan Kong', 'hidden': False}, {'_id': '6944c0f0fbf17e708e186030', 'name': 'Yanyi Shang', 'hidden': False}, {'_id': '6944c0f0fbf17e708e186031', 'name': 'Suling Ou', 'hidden': False}, {'_id': '6944c0f0fbf17e708e186032', 'name': 'Zhenlin Guo', 'hidden': False}, {'_id': '6944c0f0fbf17e708e186033', 'name': 'Changhua Meng', 'hidden': False}, {'_id': '6944c0f0fbf17e708e186034', 'name': 'Shuheng Shen', 'hidden': False}], 'publishedAt': '2025-12-18T13:09:09.000Z', 'submittedOnDailyAt': '2025-12-19T03:40:54.482Z', 'title': 'VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks', 'submittedOnDailyBy': {'_id': '60d2a2984956988b63753371', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg', 'isPro': False, 'fullname': 'Zhangxuan Gu', 'user': 'zhangxgu', 'type': 'user'}, 'summary': 'GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.', 'upvotes': 7, 'discussionId': '6944c0f0fbf17e708e186035', 'projectPage': 'https://ui-venus.github.io/VenusBench-GD/', 'ai_summary': 'VenusBench-GD is a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, offering a hierarchical evaluation framework with extensive data coverage and rich annotations.', 'ai_keywords': ['GUI grounding', 'VenusBench-GD', 'cross-platform benchmark', 'high-quality data construction pipeline', 'hierarchical task taxonomy', 'multimodal models', 'GUI-specialized models', 'overfitting', 'robustness', 'evaluation frameworks'], 'organization': {'_id': '67aea5c8f086ab0f70ed97c9', 'name': 'inclusionAI', 'fullname': 'inclusionAI', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg'}}, 'publishedAt': '2025-12-18T08:09:09.000Z', 'title': 'VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks', 'summary': 'GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16501.png', 'numComments': 1, 'submittedBy': {'_id': '60d2a2984956988b63753371', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg', 'fullname': 'Zhangxuan Gu', 'name': 'zhangxgu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'organization': {'_id': '67aea5c8f086ab0f70ed97c9', 'name': 'inclusionAI', 'fullname': 'inclusionAI', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.16378', 'authors': [{'_id': '6945080bfbf17e708e1861c0', 'user': {'_id': '66309b3833ccd9e68c5d5171', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg', 'isPro': False, 'fullname': 'Sara Papi', 'user': 'spapi', 'type': 'user'}, 'name': 'Sara Papi', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:57:12.964Z', 'hidden': False}, {'_id': '6945080bfbf17e708e1861c1', 'user': {'_id': '647224b7609ae9f5636b3f32', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/647224b7609ae9f5636b3f32/-GJ6KFEvgTHdqJURl_I_J.jpeg', 'isPro': False, 'fullname': 'Javier Garca Gilabert', 'user': 'javi8979', 'type': 'user'}, 'name': 'Javier Garcia Gilabert', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T10:49:52.784Z', 'hidden': False}, {'_id': '6945080bfbf17e708e1861c2', 'name': 'Zachary Hopton', 'hidden': False}, {'_id': '6945080bfbf17e708e1861c3', 'name': 'Vilm Zouhar', 'hidden': False}, {'_id': '6945080bfbf17e708e1861c4', 'name': 'Carlos Escolano', 'hidden': False}, {'_id': '6945080bfbf17e708e1861c5', 'name': 'Gerard I. Gllego', 'hidden': False}, {'_id': '6945080bfbf17e708e1861c6', 'name': 'Jorge Iranzo-Snchez', 'hidden': False}, {'_id': '6945080bfbf17e708e1861c7', 'name': 'Ahrii Kim', 'hidden': False}, {'_id': '6945080bfbf17e708e1861c8', 'name': 'Dominik Machek', 'hidden': False}, {'_id': '6945080bfbf17e708e1861c9', 'name': 'Patricia Schmidtova', 'hidden': False}, {'_id': '6945080bfbf17e708e1861ca', 'name': 'Maike Zfle', 'hidden': False}], 'publishedAt': '2025-12-18T10:21:14.000Z', 'submittedOnDailyAt': '2025-12-19T09:41:51.939Z', 'title': 'Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs', 'submittedOnDailyBy': {'_id': '647224b7609ae9f5636b3f32', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/647224b7609ae9f5636b3f32/-GJ6KFEvgTHdqJURl_I_J.jpeg', 'isPro': False, 'fullname': 'Javier Garca Gilabert', 'user': 'javi8979', 'type': 'user'}, 'summary': 'As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.', 'upvotes': 7, 'discussionId': '6945080cfbf17e708e1861cb', 'githubRepo': 'https://github.com/sarapapi/hearing2translate', 'githubRepoAddedBy': 'user', 'ai_summary': 'Hearing to Translate benchmarks SpeechLLMs and cascaded systems for speech-to-text translation, finding that cascaded systems are more reliable overall and highlighting the importance of integrating LLMs for high-quality translation.', 'ai_keywords': ['Large Language Models', 'SpeechLLMs', 'speech-to-text translation', 'transcription-based pipelines', 'cascaded architectures', 'speech foundation models', 'multilingual LLMs', 'disfluent speech', 'noisy speech', 'long-form speech'], 'githubStars': 9}, 'publishedAt': '2025-12-18T05:21:14.000Z', 'title': 'Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs', 'summary': 'As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16378.png', 'numComments': 1, 'submittedBy': {'_id': '647224b7609ae9f5636b3f32', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/647224b7609ae9f5636b3f32/-GJ6KFEvgTHdqJURl_I_J.jpeg', 'fullname': 'Javier Garca Gilabert', 'name': 'javi8979', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 8}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.16899', 'authors': [{'_id': '6944cae4fbf17e708e1860b7', 'name': 'Yushi Hu', 'hidden': False}, {'_id': '6944cae4fbf17e708e1860b8', 'name': 'Reyhane Askari-Hemmat', 'hidden': False}, {'_id': '6944cae4fbf17e708e1860b9', 'name': 'Melissa Hall', 'hidden': False}, {'_id': '6944cae4fbf17e708e1860ba', 'name': 'Emily Dinan', 'hidden': False}, {'_id': '6944cae4fbf17e708e1860bb', 'name': 'Luke Zettlemoyer', 'hidden': False}, {'_id': '6944cae4fbf17e708e1860bc', 'name': 'Marjan Ghazvininejad', 'hidden': False}], 'publishedAt': '2025-12-18T18:56:04.000Z', 'submittedOnDailyAt': '2025-12-19T07:31:45.856Z', 'title': 'Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image', 'submittedOnDailyBy': {'_id': '62b1474bdcbad6848a91a54e', 'avatarUrl': '/avatars/d7308899b46232cad4a48a0e876449a8.svg', 'isPro': False, 'fullname': 'Yushi Hu', 'user': 'yushihu', 'type': 'user'}, 'summary': 'Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.', 'upvotes': 6, 'discussionId': '6944cae5fbf17e708e1860bd', 'githubRepo': 'https://github.com/facebookresearch/MMRB2/tree/main', 'githubRepoAddedBy': 'user', 'ai_summary': 'Multimodal RewardBench 2 (MMRB2) is a benchmark for reward models on multimodal understanding and generation tasks, featuring expert-annotated preferences and state-of-the-art model evaluations.', 'ai_keywords': ['reward models', 'large language models', 'omni models', 'Multimodal RewardBench 2', 'MMRB2', 'text-to-image', 'image editing', 'interleaved generation', 'multimodal reasoning', 'multimodal LLM-as-a-judge', 'Best-of-N sampling'], 'githubStars': 5, 'organization': {'_id': '5e63d8713071d5be688861b8', 'name': 'facebook', 'fullname': 'AI at Meta', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png'}}, 'publishedAt': '2025-12-18T13:56:04.000Z', 'title': 'Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image', 'summary': 'Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16899.png', 'numComments': 1, 'submittedBy': {'_id': '62b1474bdcbad6848a91a54e', 'avatarUrl': '/avatars/d7308899b46232cad4a48a0e876449a8.svg', 'fullname': 'Yushi Hu', 'name': 'yushihu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}, 'organization': {'_id': '5e63d8713071d5be688861b8', 'name': 'facebook', 'fullname': 'AI at Meta', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16921', 'authors': [{'_id': '6944c582fbf17e708e186079', 'name': 'Qihao Liu', 'hidden': False}, {'_id': '6944c582fbf17e708e18607a', 'name': 'Chengzhi Mao', 'hidden': False}, {'_id': '6944c582fbf17e708e18607b', 'name': 'Yaojie Liu', 'hidden': False}, {'_id': '6944c582fbf17e708e18607c', 'name': 'Alan Yuille', 'hidden': False}, {'_id': '6944c582fbf17e708e18607d', 'name': 'Wen-Sheng Chu', 'hidden': False}], 'publishedAt': '2025-12-18T18:59:57.000Z', 'submittedOnDailyAt': '2025-12-19T00:58:38.732Z', 'title': 'Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification', 'submittedOnDailyBy': {'_id': '639f1e519f1f2baab2f00d22', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg', 'isPro': True, 'fullname': 'Qihao Liu', 'user': 'QHL067', 'type': 'user'}, 'summary': 'Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.', 'upvotes': 5, 'discussionId': '6944c583fbf17e708e18607e', 'projectPage': 'https://auditdm.github.io/', 'ai_summary': 'AuditDM, an automated framework using reinforcement learning, identifies and rectifies failure modes in multimodal LLMs by generating challenging examples, leading to improved performance across benchmarks.', 'ai_keywords': ['AuditDM', 'multimodal LLMs', 'reinforcement learning', 'challenging questions', 'counterfactual images', 'model weaknesses', 'annotation-free data', 'rectification', 'Gemma-3', 'PaliGemma-2', 'benchmark', 'model auditing', 'model diagnosis', 'improvement'], 'organization': {'_id': '5e6aca39878b8b2bf9806447', 'name': 'google', 'fullname': 'Google', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png'}}, 'publishedAt': '2025-12-18T13:59:57.000Z', 'title': 'Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification', 'summary': 'Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16921.png', 'numComments': 1, 'submittedBy': {'_id': '639f1e519f1f2baab2f00d22', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg', 'fullname': 'Qihao Liu', 'name': 'QHL067', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5}, 'organization': {'_id': '5e6aca39878b8b2bf9806447', 'name': 'google', 'fullname': 'Google', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.11251', 'authors': [{'_id': '6940cc4e65f1e24a1177feaf', 'name': 'Yunkai Zhang', 'hidden': False}, {'_id': '6940cc4e65f1e24a1177feb0', 'name': 'Yawen Zhang', 'hidden': False}, {'_id': '6940cc4e65f1e24a1177feb1', 'name': 'Ming Zheng', 'hidden': False}, {'_id': '6940cc4e65f1e24a1177feb2', 'name': 'Kezhen Chen', 'hidden': False}, {'_id': '6940cc4e65f1e24a1177feb3', 'name': 'Chongyang Gao', 'hidden': False}, {'_id': '6940cc4e65f1e24a1177feb4', 'name': 'Ruian Ge', 'hidden': False}, {'_id': '6940cc4e65f1e24a1177feb5', 'name': 'Siyuan Teng', 'hidden': False}, {'_id': '6940cc4e65f1e24a1177feb6', 'name': 'Amine Jelloul', 'hidden': False}, {'_id': '6940cc4e65f1e24a1177feb7', 'name': 'Jinmeng Rao', 'hidden': False}, {'_id': '6940cc4e65f1e24a1177feb8', 'name': 'Xiaoyuan Guo', 'hidden': False}, {'_id': '6940cc4e65f1e24a1177feb9', 'name': 'Chiang-Wei Fang', 'hidden': False}, {'_id': '6940cc4e65f1e24a1177feba', 'name': 'Zeyu Zheng', 'hidden': False}, {'_id': '6940cc4e65f1e24a1177febb', 'name': 'Jie Yang', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64e7bc340d1cc5d5e39df58e/abgwq1BM5N6jxOGYJSBdd.jpeg'], 'publishedAt': '2025-12-12T03:18:00.000Z', 'submittedOnDailyAt': '2025-12-19T04:19:10.950Z', 'title': 'Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language', 'submittedOnDailyBy': {'_id': '64e7bc340d1cc5d5e39df58e', 'avatarUrl': '/avatars/4408a4239fb99fab16fcb3b3ab296252.svg', 'isPro': False, 'fullname': 'Yunkai Zhang', 'user': 'zhykoties', 'type': 'user'}, 'summary': 'Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose Insight Miner, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce TS-InsightsAvailable at \\\\href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel agentic workflow, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA liu2023llava and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality.', 'upvotes': 4, 'discussionId': '6940cc4e65f1e24a1177fec7', 'ai_summary': 'Insight Miner, a large-scale multimodal model, generates high-quality time-series descriptions using a novel agentic workflow and outperforms existing models with the help of the TS-Insights dataset.', 'ai_keywords': ['multimodal model', 'time-series descriptions', 'domain-specific knowledge', 'TS-Insights', 'agentic workflow', 'feature extraction', 'trend descriptions', 'GPT-4', 'instruction tuning', 'LLaVA', 'LLMs', 'time series analysis'], 'organization': {'_id': '5e6aca39878b8b2bf9806447', 'name': 'google', 'fullname': 'Google', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png'}}, 'publishedAt': '2025-12-11T22:18:00.000Z', 'title': 'Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language', 'summary': 'Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose Insight Miner, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce TS-InsightsAvailable at \\\\href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel agentic workflow, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA liu2023llava and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64e7bc340d1cc5d5e39df58e/abgwq1BM5N6jxOGYJSBdd.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11251.png', 'numComments': 1, 'submittedBy': {'_id': '64e7bc340d1cc5d5e39df58e', 'avatarUrl': '/avatars/4408a4239fb99fab16fcb3b3ab296252.svg', 'fullname': 'Yunkai Zhang', 'name': 'zhykoties', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'organization': {'_id': '5e6aca39878b8b2bf9806447', 'name': 'google', 'fullname': 'Google', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16920', 'authors': [{'_id': '694562cafbf17e708e1862de', 'name': 'Jinjie Mai', 'hidden': False}, {'_id': '694562cafbf17e708e1862df', 'name': 'Chaoyang Wang', 'hidden': False}, {'_id': '694562cafbf17e708e1862e0', 'name': 'Guocheng Gordon Qian', 'hidden': False}, {'_id': '694562cafbf17e708e1862e1', 'name': 'Willi Menapace', 'hidden': False}, {'_id': '694562cafbf17e708e1862e2', 'name': 'Sergey Tulyakov', 'hidden': False}, {'_id': '694562cafbf17e708e1862e3', 'name': 'Bernard Ghanem', 'hidden': False}, {'_id': '694562cafbf17e708e1862e4', 'name': 'Peter Wonka', 'hidden': False}, {'_id': '694562cafbf17e708e1862e5', 'name': 'Ashkan Mirzaei', 'hidden': False}], 'publishedAt': '2025-12-18T18:59:57.000Z', 'submittedOnDailyAt': '2025-12-19T12:06:58.822Z', 'title': 'EasyV2V: A High-quality Instruction-based Video Editing Framework', 'submittedOnDailyBy': {'_id': '634db15dd00bb5d92c3bd94f', 'avatarUrl': '/avatars/9a6a1231bc5205911272d83527593f1a.svg', 'isPro': False, 'fullname': 'Ashkan Mirzaei', 'user': 'ashmrz', 'type': 'user'}, 'summary': 'While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce EasyV2V, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/', 'upvotes': 2, 'discussionId': '694562cafbf17e708e1862e6', 'ai_summary': 'EasyV2V framework enhances video editing by combining diverse data sources, leveraging pretrained text-to-video models with LoRA fine-tuning, and implementing unified spatiotemporal control, achieving top results.', 'ai_keywords': ['EasyV2V', 'fast inverses', 'single-frame supervision', 'pseudo pairs', 'dense-captioned clips', 'transition supervision', 'pretrained text-to-video models', 'LoRA fine-tuning', 'spatiotemporal control', 'mask mechanism', 'reference images'], 'organization': {'_id': '63c87c41cd6a490608ce31d1', 'name': 'snap-research', 'fullname': 'Snap Research', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png'}}, 'publishedAt': '2025-12-18T13:59:57.000Z', 'title': 'EasyV2V: A High-quality Instruction-based Video Editing Framework', 'summary': 'While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce EasyV2V, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16920.png', 'numComments': 1, 'submittedBy': {'_id': '634db15dd00bb5d92c3bd94f', 'avatarUrl': '/avatars/9a6a1231bc5205911272d83527593f1a.svg', 'fullname': 'Ashkan Mirzaei', 'name': 'ashmrz', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'organization': {'_id': '63c87c41cd6a490608ce31d1', 'name': 'snap-research', 'fullname': 'Snap Research', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.16767', 'authors': [{'_id': '6944eaa1fbf17e708e18612e', 'user': {'_id': '65fba7bd0b78c48c9e39e0f1', 'avatarUrl': '/avatars/ca432ead932cb7c641f9375b9653ad0d.svg', 'isPro': False, 'fullname': 'Zhiyang Guo', 'user': 'jasongzy', 'type': 'user'}, 'name': 'Zhiyang Guo', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:57:24.640Z', 'hidden': False}, {'_id': '6944eaa1fbf17e708e18612f', 'name': 'Ori Zhang', 'hidden': False}, {'_id': '6944eaa1fbf17e708e186130', 'name': 'Jax Xiang', 'hidden': False}, {'_id': '6944eaa1fbf17e708e186131', 'name': 'Alan Zhao', 'hidden': False}, {'_id': '6944eaa1fbf17e708e186132', 'name': 'Wengang Zhou', 'hidden': False}, {'_id': '6944eaa1fbf17e708e186133', 'name': 'Houqiang Li', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65fba7bd0b78c48c9e39e0f1/BbxctuNHdcilfCFjikDIl.mp4'], 'publishedAt': '2025-12-18T17:01:44.000Z', 'submittedOnDailyAt': '2025-12-19T03:37:14.230Z', 'title': 'Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation', 'submittedOnDailyBy': {'_id': '65fba7bd0b78c48c9e39e0f1', 'avatarUrl': '/avatars/ca432ead932cb7c641f9375b9653ad0d.svg', 'isPro': False, 'fullname': 'Zhiyang Guo', 'user': 'jasongzy', 'type': 'user'}, 'summary': 'Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.', 'upvotes': 2, 'discussionId': '6944eaa2fbf17e708e186134', 'projectPage': 'https://jasongzy.github.io/Make-It-Poseable/', 'githubRepo': 'https://github.com/jasongzy/Make-It-Poseable', 'githubRepoAddedBy': 'user', 'ai_summary': 'A novel feed-forward framework, Make-It-Poseable, reformulates character posing as a latent-space transformation problem, using a latent posing transformer and dense pose representation to achieve superior posing quality and extend to 3D editing applications.', 'ai_keywords': ['feed-forward framework', 'latent-space transformation', 'latent posing transformer', 'shape tokens', 'dense pose representation', 'latent-space supervision', 'adaptive completion module'], 'githubStars': 2, 'organization': {'_id': '66543b6e420092799d2f625c', 'name': 'tencent', 'fullname': 'Tencent', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png'}}, 'publishedAt': '2025-12-18T12:01:44.000Z', 'title': 'Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation', 'summary': 'Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65fba7bd0b78c48c9e39e0f1/BbxctuNHdcilfCFjikDIl.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16767.png', 'numComments': 1, 'submittedBy': {'_id': '65fba7bd0b78c48c9e39e0f1', 'avatarUrl': '/avatars/ca432ead932cb7c641f9375b9653ad0d.svg', 'fullname': 'Zhiyang Guo', 'name': 'jasongzy', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'organization': {'_id': '66543b6e420092799d2f625c', 'name': 'tencent', 'fullname': 'Tencent', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.12576', 'authors': [{'_id': '6944dd07fbf17e708e186109', 'user': {'_id': '625d1201943e346492b31c7d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1666359066419-625d1201943e346492b31c7d.png', 'isPro': False, 'fullname': 'wenxueru', 'user': 'Aunderline', 'type': 'user'}, 'name': 'Xueru Wen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:57:29.270Z', 'hidden': False}, {'_id': '6944dd07fbf17e708e18610a', 'name': 'Jie Lou', 'hidden': False}, {'_id': '6944dd07fbf17e708e18610b', 'name': 'Yanjiang Liu', 'hidden': False}, {'_id': '6944dd07fbf17e708e18610c', 'name': 'Hongyu Lin', 'hidden': False}, {'_id': '6944dd07fbf17e708e18610d', 'name': 'Ben He', 'hidden': False}, {'_id': '6944dd07fbf17e708e18610e', 'name': 'Xianpei Han', 'hidden': False}, {'_id': '6944dd07fbf17e708e18610f', 'name': 'Le Sun', 'hidden': False}, {'_id': '6944dd07fbf17e708e186110', 'name': 'Yaojie Lu', 'hidden': False}, {'_id': '6944dd07fbf17e708e186111', 'name': 'Debing Zhang', 'hidden': False}], 'publishedAt': '2025-12-14T07:03:51.000Z', 'submittedOnDailyAt': '2025-12-19T02:35:36.047Z', 'title': 'Coupled Variational Reinforcement Learning for Language Model General Reasoning', 'submittedOnDailyBy': {'_id': '625d1201943e346492b31c7d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1666359066419-625d1201943e346492b31c7d.png', 'isPro': False, 'fullname': 'wenxueru', 'user': 'Aunderline', 'type': 'user'}, 'summary': 'While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\\\b{Coupled Variational Reinforcement Learning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\\\% over the base model and achieves an additional 2.3\\\\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.', 'upvotes': 2, 'discussionId': '6944dd07fbf17e708e186112', 'ai_summary': 'CoVRL, a hybrid approach combining variational inference and reinforcement learning, enhances language model reasoning by coupling prior and posterior distributions, improving performance and coherence.', 'ai_keywords': ['reinforcement learning', 'verifier-free RL', 'LLMs', 'reasoning traces', 'variational inference', 'prior distributions', 'posterior distributions', 'composite distribution', 'efficient exploration', 'thought-answer coherence', 'mathematical reasoning', 'general reasoning benchmarks']}, 'publishedAt': '2025-12-14T02:03:51.000Z', 'title': 'Coupled Variational Reinforcement Learning for Language Model General Reasoning', 'summary': 'While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\\\b{Coupled Variational Reinforcement Learning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\\\% over the base model and achieves an additional 2.3\\\\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12576.png', 'numComments': 1, 'submittedBy': {'_id': '625d1201943e346492b31c7d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1666359066419-625d1201943e346492b31c7d.png', 'fullname': 'wenxueru', 'name': 'Aunderline', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.16670', 'authors': [{'_id': '694526cbfbf17e708e186298', 'name': 'Ole Beisswenger', 'hidden': False}, {'_id': '694526cbfbf17e708e186299', 'user': {'_id': '631b6dbcbf1351ed2bd05be1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/631b6dbcbf1351ed2bd05be1/_kzcDD4qCifsb-651_LmY.png', 'isPro': False, 'fullname': 'Jan-Niklas Dihlmann', 'user': 'JDihlmann', 'type': 'user'}, 'name': 'Jan-Niklas Dihlmann', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T10:49:54.174Z', 'hidden': False}, {'_id': '694526cbfbf17e708e18629a', 'name': 'Hendrik P. A. Lensch', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/631b6dbcbf1351ed2bd05be1/Db47aIqWCyyqvI6LyLv6i.mp4'], 'publishedAt': '2025-12-18T15:41:08.000Z', 'submittedOnDailyAt': '2025-12-19T07:52:11.154Z', 'title': 'FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering', 'submittedOnDailyBy': {'_id': '631b6dbcbf1351ed2bd05be1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/631b6dbcbf1351ed2bd05be1/_kzcDD4qCifsb-651_LmY.png', 'isPro': False, 'fullname': 'Jan-Niklas Dihlmann', 'user': 'JDihlmann', 'type': 'user'}, 'summary': 'Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.', 'upvotes': 1, 'discussionId': '694526cbfbf17e708e18629b', 'ai_summary': 'FrameDiffuser is an autoregressive neural rendering framework that generates temporally consistent photorealistic frames using G-buffer data and previous frame outputs, leveraging ControlNet and ControlLoRA for structural and temporal coherence.', 'ai_keywords': ['neural rendering', 'G-buffer', 'diffusion-based approaches', 'RGBX', 'DiffusionRenderer', 'autoregressive framework', 'temporal consistency', 'ControlNet', 'ControlLoRA', 'three-stage training', 'structural guidance', 'temporal coherence', 'environment-specific training']}, 'publishedAt': '2025-12-18T10:41:08.000Z', 'title': 'FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering', 'summary': 'Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/631b6dbcbf1351ed2bd05be1/Db47aIqWCyyqvI6LyLv6i.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16670.png', 'numComments': 1, 'submittedBy': {'_id': '631b6dbcbf1351ed2bd05be1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/631b6dbcbf1351ed2bd05be1/_kzcDD4qCifsb-651_LmY.png', 'fullname': 'Jan-Niklas Dihlmann', 'name': 'JDihlmann', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.16615', 'authors': [{'_id': '6944e9effbf17e708e186127', 'user': {'_id': '64a63f9449b08110f761cd73', 'avatarUrl': '/avatars/61860202fc818b105ef24e74dd4f7d3c.svg', 'isPro': False, 'fullname': 'Yifan Zhou', 'user': 'SingleZombie', 'type': 'user'}, 'name': 'Yifan Zhou', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-19T08:57:27.191Z', 'hidden': False}, {'_id': '6944e9effbf17e708e186128', 'name': 'Zeqi Xiao', 'hidden': False}, {'_id': '6944e9effbf17e708e186129', 'name': 'Tianyi Wei', 'hidden': False}, {'_id': '6944e9effbf17e708e18612a', 'name': 'Shuai Yang', 'hidden': False}, {'_id': '6944e9effbf17e708e18612b', 'name': 'Xingang Pan', 'hidden': False}], 'publishedAt': '2025-12-18T14:53:12.000Z', 'submittedOnDailyAt': '2025-12-19T03:31:26.755Z', 'title': 'Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers', 'submittedOnDailyBy': {'_id': '64a63f9449b08110f761cd73', 'avatarUrl': '/avatars/61860202fc818b105ef24e74dd4f7d3c.svg', 'isPro': False, 'fullname': 'Yifan Zhou', 'user': 'SingleZombie', 'type': 'user'}, 'summary': 'Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA', 'upvotes': 1, 'discussionId': '6944e9effbf17e708e18612c', 'ai_summary': 'Log-linear Sparse Attention (LLSA) improves the efficiency of diffusion transformers by reducing computational costs for long token sequences through a hierarchical structure, enhancing training speed without sacrificing generation quality.', 'ai_keywords': ['Diffusion Transformers (DiTs)', 'self-attention', 'Top-K sparse attention', 'block-wise representation', 'log-linear complexity', 'hierarchical structure', 'hierarchical Top-K selection', 'Hierarchical KV Enrichment mechanism', 'GPU implementation', 'sparse indices', 'pixel-space image generation'], 'organization': {'_id': '6508b28cf36bb51c50faad98', 'name': 'NanyangTechnologicalUniversity', 'fullname': 'Nanyang Technological University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png'}}, 'publishedAt': '2025-12-18T09:53:12.000Z', 'title': 'Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers', 'summary': 'Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16615.png', 'numComments': 1, 'submittedBy': {'_id': '64a63f9449b08110f761cd73', 'avatarUrl': '/avatars/61860202fc818b105ef24e74dd4f7d3c.svg', 'fullname': 'Yifan Zhou', 'name': 'SingleZombie', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'organization': {'_id': '6508b28cf36bb51c50faad98', 'name': 'NanyangTechnologicalUniversity', 'fullname': 'Nanyang Technological University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.14884', 'authors': [{'_id': '69452064fbf17e708e18622c', 'name': 'Huzheng Yang', 'hidden': False}, {'_id': '69452064fbf17e708e18622d', 'name': 'Katherine Xu', 'hidden': False}, {'_id': '69452064fbf17e708e18622e', 'name': 'Andrew Lu', 'hidden': False}, {'_id': '69452064fbf17e708e18622f', 'name': 'Michael D. Grossberg', 'hidden': False}, {'_id': '69452064fbf17e708e186230', 'name': 'Yutong Bai', 'hidden': False}, {'_id': '69452064fbf17e708e186231', 'name': 'Jianbo Shi', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/66b00989491b555fefc42631/837CqJ1DFxxTeYIlWE5xw.mp4'], 'publishedAt': '2025-12-16T20:03:23.000Z', 'submittedOnDailyAt': '2025-12-19T07:32:11.804Z', 'title': 'Vibe Spaces for Creatively Connecting and Expressing Visual Concepts', 'submittedOnDailyBy': {'_id': '66b00989491b555fefc42631', 'avatarUrl': '/avatars/23893c0162ae48a4eb9d2938c6a80732.svg', 'isPro': False, 'fullname': 'Huzheng Yang', 'user': 'huzey', 'type': 'user'}, 'summary': 'Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributes -- their vibe. We introduce Vibe Blending, a novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distant concepts in latent space. We propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP, enabling smooth and semantically consistent transitions between concepts. To evaluate creative quality, we design a cognitively inspired framework combining human judgments, LLM reasoning, and a geometric path-based difficulty score. We find that Vibe Space produces blends that humans consistently rate as more creative and coherent than current methods.', 'upvotes': 1, 'discussionId': '69452065fbf17e708e186232', 'projectPage': 'https://huzeyann.github.io/VibeSpace-webpage/', 'githubRepo': 'https://github.com/huzeyann/VibeSpace', 'githubRepoAddedBy': 'user', 'ai_summary': 'Vibe Blending uses Vibe Space, a hierarchical graph manifold, to generate coherent image hybrids by learning geodesics in feature spaces, outperforming current methods in creativity and coherence.', 'ai_keywords': ['Vibe Blending', 'Vibe Space', 'hierarchical graph manifold', 'geodesics', 'feature spaces', 'CLIP', 'LLM reasoning', 'geometric path-based difficulty score'], 'githubStars': 0}, 'publishedAt': '2025-12-16T15:03:23.000Z', 'title': 'Vibe Spaces for Creatively Connecting and Expressing Visual Concepts', 'summary': 'Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributes -- their vibe. We introduce Vibe Blending, a novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distant concepts in latent space. We propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP, enabling smooth and semantically consistent transitions between concepts. To evaluate creative quality, we design a cognitively inspired framework combining human judgments, LLM reasoning, and a geometric path-based difficulty score. We find that Vibe Space produces blends that humans consistently rate as more creative and coherent than current methods.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/66b00989491b555fefc42631/837CqJ1DFxxTeYIlWE5xw.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14884.png', 'numComments': 1, 'submittedBy': {'_id': '66b00989491b555fefc42631', 'avatarUrl': '/avatars/23893c0162ae48a4eb9d2938c6a80732.svg', 'fullname': 'Huzheng Yang', 'name': 'huzey', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.15907', 'authors': [{'_id': '6944be82fbf17e708e18601b', 'name': 'Tejas Anvekar', 'hidden': False}, {'_id': '6944be82fbf17e708e18601c', 'name': 'Juhna Park', 'hidden': False}, {'_id': '6944be82fbf17e708e18601d', 'name': 'Aparna Garimella', 'hidden': False}, {'_id': '6944be82fbf17e708e18601e', 'name': 'Vivek Gupta', 'hidden': False}], 'publishedAt': '2025-12-17T19:20:20.000Z', 'submittedOnDailyAt': '2025-12-19T00:25:09.474Z', 'title': 'TabReX : Tabular Referenceless eXplainable Evaluation', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.', 'upvotes': 0, 'discussionId': '6944be83fbf17e708e18601f', 'projectPage': 'https://coral-lab-asu.github.io/TabReX/', 'githubRepo': 'https://github.com/CoRAL-ASU/TabReX', 'githubRepoAddedBy': 'user', 'ai_summary': 'TabReX is a reference-less framework using graph-based reasoning to evaluate the quality of tables generated by LLMs, offering structural and factual fidelity scores.', 'ai_keywords': ['TabReX', 'canonical knowledge graphs', 'LLM-guided matching', 'rubric-aware scores', 'TabReX-Bench', 'planner-driven perturbation types', 'expert rankings', 'fine-grained model-vs-prompt analysis'], 'githubStars': 1}, 'publishedAt': '2025-12-17T14:20:20.000Z', 'title': 'TabReX : Tabular Referenceless eXplainable Evaluation', 'summary': 'Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15907.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 187}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.15528', 'authors': [{'_id': '6944c6aefbf17e708e1860a2', 'name': 'Daiqing Wu', 'hidden': False}, {'_id': '6944c6aefbf17e708e1860a3', 'name': 'Dongbao Yang', 'hidden': False}, {'_id': '6944c6aefbf17e708e1860a4', 'name': 'Can Ma. Yu Zhou', 'hidden': False}], 'publishedAt': '2025-12-17T15:30:50.000Z', 'submittedOnDailyAt': '2025-12-19T01:03:29.739Z', 'title': 'EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration', 'submittedOnDailyBy': {'_id': '642d26527f9efee76b861df2', 'avatarUrl': '/avatars/a7e646e8ca91189054cd7bbf1997e92a.svg', 'isPro': False, 'fullname': 'Wu Daiqing', 'user': 'wudq', 'type': 'user'}, 'summary': \"Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.\", 'upvotes': 0, 'discussionId': '6944c6aefbf17e708e1860a5', 'githubRepo': 'https://github.com/wdqqdw/EmoCaliber', 'githubRepoAddedBy': 'user', 'ai_summary': 'EmoCaliber, a confidence-aware Multimodal Large Language Model, enhances Visual Emotion Comprehension by verbalizing confidence in emotion predictions, leading to improved reliability and accuracy.', 'ai_keywords': ['Multimodal Large Language Models', 'Visual Emotion Comprehension', 'VEC', 'emotion categories', 'confidence estimation', 'structured reasoning', 'VECBench'], 'githubStars': 1}, 'publishedAt': '2025-12-17T10:30:50.000Z', 'title': 'EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration', 'summary': \"Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15528.png', 'numComments': 1, 'submittedBy': {'_id': '642d26527f9efee76b861df2', 'avatarUrl': '/avatars/a7e646e8ca91189054cd7bbf1997e92a.svg', 'fullname': 'Wu Daiqing', 'name': 'wudq', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}"
]