[
    "{'paper': {'id': '2504.06263', 'authors': [{'_id': '67f5e3701b29460f6a087954', 'name': 'Yiying Yang', 'hidden': False}, {'_id': '67f5e3701b29460f6a087955', 'user': {'_id': '64b914c8ace99c0723ad83a9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg', 'isPro': False, 'fullname': 'Wei Cheng', 'user': 'wchengad', 'type': 'user'}, 'name': 'Wei Cheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T07:34:51.924Z', 'hidden': False}, {'_id': '67f5e3701b29460f6a087956', 'user': {'_id': '6485b08e687d9e0c759121b0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg', 'isPro': False, 'fullname': 'sijin', 'user': 'CH3COOK', 'type': 'user'}, 'name': 'Sijin Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T07:34:48.985Z', 'hidden': False}, {'_id': '67f5e3701b29460f6a087957', 'name': 'Xianfang Zeng', 'hidden': False}, {'_id': '67f5e3701b29460f6a087958', 'name': 'Jiaxu Zhang', 'hidden': False}, {'_id': '67f5e3701b29460f6a087959', 'name': 'Liao Wang', 'hidden': False}, {'_id': '67f5e3701b29460f6a08795a', 'name': 'Gang Yu', 'hidden': False}, {'_id': '67f5e3701b29460f6a08795b', 'name': 'Xingjun Ma', 'hidden': False}, {'_id': '67f5e3701b29460f6a08795c', 'name': 'Yu-Gang Jiang', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/goYk4m9KxOQvIKfjUy5Rn.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/oqnAsMzmh3Vtx0OIPv5ai.gif'], 'publishedAt': '2025-04-08T17:59:49.000Z', 'submittedOnDailyAt': '2025-04-09T01:51:00.484Z', 'title': 'OmniSVG: A Unified Scalable Vector Graphics Generation Model', 'submittedOnDailyBy': {'_id': '6485b08e687d9e0c759121b0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg', 'isPro': False, 'fullname': 'sijin', 'user': 'CH3COOK', 'type': 'user'}, 'summary': 'Scalable Vector Graphics (SVG) is an important image format widely adopted in\\ngraphic design because of their resolution independence and editability. The\\nstudy of generating high-quality SVG has continuously drawn attention from both\\ndesigners and researchers in the AIGC community. However, existing methods\\neither produces unstructured outputs with huge computational cost or is limited\\nto generating monochrome icons of over-simplified structures. To produce\\nhigh-quality and complex SVG, we propose OmniSVG, a unified framework that\\nleverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal\\nSVG generation. By parameterizing SVG commands and coordinates into discrete\\ntokens, OmniSVG decouples structural logic from low-level geometry for\\nefficient training while maintaining the expressiveness of complex SVG\\nstructure. To further advance the development of SVG synthesis, we introduce\\nMMSVG-2M, a multimodal dataset with two million richly annotated SVG assets,\\nalong with a standardized evaluation protocol for conditional SVG generation\\ntasks. Extensive experiments show that OmniSVG outperforms existing methods and\\ndemonstrates its potential for integration into professional SVG design\\nworkflows.', 'upvotes': 68, 'discussionId': '67f5e3751b29460f6a087aa7', 'projectPage': 'https://omnisvg.github.io/', 'githubRepo': 'https://github.com/OmniSVG/OmniSVG'}, 'publishedAt': '2025-04-08T13:59:49.000Z', 'title': 'OmniSVG: A Unified Scalable Vector Graphics Generation Model', 'summary': 'Scalable Vector Graphics (SVG) is an important image format widely adopted in\\ngraphic design because of their resolution independence and editability. The\\nstudy of generating high-quality SVG has continuously drawn attention from both\\ndesigners and researchers in the AIGC community. However, existing methods\\neither produces unstructured outputs with huge computational cost or is limited\\nto generating monochrome icons of over-simplified structures. To produce\\nhigh-quality and complex SVG, we propose OmniSVG, a unified framework that\\nleverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal\\nSVG generation. By parameterizing SVG commands and coordinates into discrete\\ntokens, OmniSVG decouples structural logic from low-level geometry for\\nefficient training while maintaining the expressiveness of complex SVG\\nstructure. To further advance the development of SVG synthesis, we introduce\\nMMSVG-2M, a multimodal dataset with two million richly annotated SVG assets,\\nalong with a standardized evaluation protocol for conditional SVG generation\\ntasks. Extensive experiments show that OmniSVG outperforms existing methods and\\ndemonstrates its potential for integration into professional SVG design\\nworkflows.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/goYk4m9KxOQvIKfjUy5Rn.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/oqnAsMzmh3Vtx0OIPv5ai.gif'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06263.png', 'numComments': 1, 'submittedBy': {'_id': '6485b08e687d9e0c759121b0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg', 'fullname': 'sijin', 'name': 'CH3COOK', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2504.05599', 'authors': [{'_id': '67f61a98af81b0685bf055cf', 'name': 'Yi Peng', 'hidden': False}, {'_id': '67f61a98af81b0685bf055d0', 'name': 'Chris', 'hidden': False}, {'_id': '67f61a98af81b0685bf055d1', 'name': 'Xiaokun Wang', 'hidden': False}, {'_id': '67f61a98af81b0685bf055d2', 'name': 'Yichen Wei', 'hidden': False}, {'_id': '67f61a98af81b0685bf055d3', 'name': 'Jiangbo Pei', 'hidden': False}, {'_id': '67f61a98af81b0685bf055d4', 'name': 'Weijie Qiu', 'hidden': False}, {'_id': '67f61a98af81b0685bf055d5', 'name': 'Ai Jian', 'hidden': False}, {'_id': '67f61a98af81b0685bf055d6', 'name': 'Yunzhuo Hao', 'hidden': False}, {'_id': '67f61a98af81b0685bf055d7', 'name': 'Jiachun Pan', 'hidden': False}, {'_id': '67f61a98af81b0685bf055d8', 'name': 'Tianyidan Xie', 'hidden': False}, {'_id': '67f61a98af81b0685bf055d9', 'name': 'Li Ge', 'hidden': False}, {'_id': '67f61a98af81b0685bf055da', 'name': 'Rongxian Zhuang', 'hidden': False}, {'_id': '67f61a98af81b0685bf055db', 'name': 'Xuchen Song', 'hidden': False}, {'_id': '67f61a98af81b0685bf055dc', 'name': 'Yang Liu', 'hidden': False}, {'_id': '67f61a98af81b0685bf055dd', 'name': 'Yahui Zhou', 'hidden': False}], 'publishedAt': '2025-04-08T01:19:20.000Z', 'submittedOnDailyAt': '2025-04-09T05:32:09.323Z', 'title': 'Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought', 'submittedOnDailyBy': {'_id': '6462b241b438438da3c25a5d', 'avatarUrl': '/avatars/606a67f1be639c9a5e36f293abd5f27a.svg', 'isPro': False, 'fullname': 'Xuchen Song', 'user': 'xuchensong', 'type': 'user'}, 'summary': 'We introduce Skywork R1V, a multimodal reasoning model extending the an\\nR1-series Large language models (LLM) to visual modalities via an efficient\\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\\nR1V facilitates seamless multimodal adaptation without necessitating retraining\\nof either the foundational language model or the vision encoder. To strengthen\\nvisual-text alignment, we propose a hybrid optimization strategy that combines\\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\\n(GRPO), significantly enhancing cross-modal integration efficiency.\\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\\napproach for reasoning data generation. This approach dynamically optimizes\\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\\nSkywork R1V, with only 38B parameters, delivers competitive performance,\\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\\nweights have been publicly released to promote openness and reproducibility.', 'upvotes': 49, 'discussionId': '67f61a9daf81b0685bf05731', 'githubRepo': 'https://github.com/SkyworkAI/Skywork-R1V'}, 'publishedAt': '2025-04-07T21:19:20.000Z', 'title': 'Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought', 'summary': 'We introduce Skywork R1V, a multimodal reasoning model extending the an\\nR1-series Large language models (LLM) to visual modalities via an efficient\\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\\nR1V facilitates seamless multimodal adaptation without necessitating retraining\\nof either the foundational language model or the vision encoder. To strengthen\\nvisual-text alignment, we propose a hybrid optimization strategy that combines\\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\\n(GRPO), significantly enhancing cross-modal integration efficiency.\\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\\napproach for reasoning data generation. This approach dynamically optimizes\\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\\nSkywork R1V, with only 38B parameters, delivers competitive performance,\\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\\nweights have been publicly released to promote openness and reproducibility.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05599.png', 'numComments': 2, 'submittedBy': {'_id': '6462b241b438438da3c25a5d', 'avatarUrl': '/avatars/606a67f1be639c9a5e36f293abd5f27a.svg', 'fullname': 'Xuchen Song', 'name': 'xuchensong', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.06261', 'authors': [{'_id': '67f60df2d0df7eccaae93eb0', 'name': 'Gleb Rodionov', 'hidden': False}, {'_id': '67f60df2d0df7eccaae93eb1', 'user': {'_id': '6261af8040e04009e813a43d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6261af8040e04009e813a43d/Cc4I98uaePHE-YglLodlZ.jpeg', 'isPro': False, 'fullname': 'Roman Garipov', 'user': 'garipovroma', 'type': 'user'}, 'name': 'Roman Garipov', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T14:36:56.157Z', 'hidden': False}, {'_id': '67f60df2d0df7eccaae93eb2', 'name': 'Alina Shutova', 'hidden': False}, {'_id': '67f60df2d0df7eccaae93eb3', 'name': 'George Yakushev', 'hidden': False}, {'_id': '67f60df2d0df7eccaae93eb4', 'name': 'Vage Egiazarian', 'hidden': False}, {'_id': '67f60df2d0df7eccaae93eb5', 'name': 'Anton Sinitsin', 'hidden': False}, {'_id': '67f60df2d0df7eccaae93eb6', 'name': 'Denis Kuznedelev', 'hidden': False}, {'_id': '67f60df2d0df7eccaae93eb7', 'name': 'Dan Alistarh', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64ef52c2718f94ae8e78a5e7/79i2ecMWxf7tpYS3hvRP4.qt'], 'publishedAt': '2025-04-08T17:59:41.000Z', 'submittedOnDailyAt': '2025-04-09T04:36:08.129Z', 'title': 'Hogwild! Inference: Parallel LLM Generation via Concurrent Attention', 'submittedOnDailyBy': {'_id': '64ef52c2718f94ae8e78a5e7', 'avatarUrl': '/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg', 'isPro': False, 'fullname': 'Alistarh', 'user': 'd-alistarh', 'type': 'user'}, 'summary': 'Large Language Models (LLMs) have demonstrated the ability to tackle\\nincreasingly complex tasks through advanced reasoning, long-form content\\ngeneration, and tool use. Solving these tasks often involves long\\ninference-time computations. In human problem solving, a common strategy to\\nexpedite work is collaboration: by dividing the problem into sub-tasks,\\nexploring different strategies concurrently, etc. Recent research has shown\\nthat LLMs can also operate in parallel by implementing explicit cooperation\\nframeworks, such as voting mechanisms or the explicit creation of independent\\nsub-tasks that can be executed in parallel. However, each of these frameworks\\nmay not be suitable for all types of tasks, which can hinder their\\napplicability. In this work, we propose a different design approach: we run LLM\\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\\nattention cache and prompt these workers to decide how best to collaborate. Our\\napproach allows the instances to come up with their own collaboration strategy\\nfor the problem at hand, all the while \"seeing\" each other\\'s partial progress\\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\\nparallel LLM inference engine where multiple instances of the same LLM run in\\nparallel with the same attention cache, with \"instant\" access to each other\\'s\\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\\nutilization. We find that modern reasoning-capable LLMs can perform inference\\nwith shared Key-Value cache out of the box, without additional fine-tuning.', 'upvotes': 48, 'discussionId': '67f60df3d0df7eccaae93eff', 'githubRepo': 'https://github.com/eqimp/hogwild_llm'}, 'publishedAt': '2025-04-08T13:59:41.000Z', 'title': 'Hogwild! Inference: Parallel LLM Generation via Concurrent Attention', 'summary': 'Large Language Models (LLMs) have demonstrated the ability to tackle\\nincreasingly complex tasks through advanced reasoning, long-form content\\ngeneration, and tool use. Solving these tasks often involves long\\ninference-time computations. In human problem solving, a common strategy to\\nexpedite work is collaboration: by dividing the problem into sub-tasks,\\nexploring different strategies concurrently, etc. Recent research has shown\\nthat LLMs can also operate in parallel by implementing explicit cooperation\\nframeworks, such as voting mechanisms or the explicit creation of independent\\nsub-tasks that can be executed in parallel. However, each of these frameworks\\nmay not be suitable for all types of tasks, which can hinder their\\napplicability. In this work, we propose a different design approach: we run LLM\\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\\nattention cache and prompt these workers to decide how best to collaborate. Our\\napproach allows the instances to come up with their own collaboration strategy\\nfor the problem at hand, all the while \"seeing\" each other\\'s partial progress\\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\\nparallel LLM inference engine where multiple instances of the same LLM run in\\nparallel with the same attention cache, with \"instant\" access to each other\\'s\\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\\nutilization. We find that modern reasoning-capable LLMs can perform inference\\nwith shared Key-Value cache out of the box, without additional fine-tuning.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64ef52c2718f94ae8e78a5e7/79i2ecMWxf7tpYS3hvRP4.qt'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06261.png', 'numComments': 2, 'submittedBy': {'_id': '64ef52c2718f94ae8e78a5e7', 'avatarUrl': '/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg', 'fullname': 'Alistarh', 'name': 'd-alistarh', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.05979', 'authors': [{'_id': '67f5d5416ceb820f2006d8a2', 'name': 'Sixiang Chen', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8a3', 'user': {'_id': '63fccdac93b993a4ebd7789a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg', 'isPro': False, 'fullname': 'Jinbin Bai', 'user': 'BryanW', 'type': 'user'}, 'name': 'Jinbin Bai', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T07:35:08.303Z', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8a4', 'name': 'Zhuoran Zhao', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8a5', 'name': 'Tian Ye', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8a6', 'user': {'_id': '656724074f6ec72017754d33', 'avatarUrl': '/avatars/e61de248f6f53719b2375077340dd033.svg', 'isPro': False, 'fullname': 'QingyuShi', 'user': 'QingyuShi', 'type': 'user'}, 'name': 'Qingyu Shi', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T07:35:04.572Z', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8a7', 'user': {'_id': '67136093d2e50f1e8c9fad52', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png', 'isPro': False, 'fullname': 'Donghao Zhou', 'user': 'donghao-zhou', 'type': 'user'}, 'name': 'Donghao Zhou', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T07:35:02.400Z', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8a8', 'name': 'Wenhao Chai', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8a9', 'name': 'Xin Lin', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8aa', 'name': 'Jianzong Wu', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8ab', 'name': 'Chao Tang', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8ac', 'name': 'Shilin Xu', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8ad', 'name': 'Tao Zhang', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8ae', 'name': 'Haobo Yuan', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8af', 'name': 'Yikang Zhou', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8b0', 'name': 'Wei Chow', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8b1', 'name': 'Linfeng Li', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8b2', 'name': 'Xiangtai Li', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8b3', 'name': 'Lei Zhu', 'hidden': False}, {'_id': '67f5d5416ceb820f2006d8b4', 'name': 'Lu Qi', 'hidden': False}], 'publishedAt': '2025-04-08T12:34:36.000Z', 'submittedOnDailyAt': '2025-04-09T00:39:48.924Z', 'title': 'An Empirical Study of GPT-4o Image Generation Capabilities', 'submittedOnDailyBy': {'_id': '63fccdac93b993a4ebd7789a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg', 'isPro': False, 'fullname': 'Jinbin Bai', 'user': 'BryanW', 'type': 'user'}, 'summary': \"The landscape of image generation has rapidly evolved, from early GAN-based\\napproaches to diffusion models and, most recently, to unified generative\\narchitectures that seek to bridge understanding and generation tasks. Recent\\nadvances, especially the GPT-4o, have demonstrated the feasibility of\\nhigh-fidelity multimodal generation, their architectural design remains\\nmysterious and unpublished. This prompts the question of whether image and text\\ngeneration have already been successfully integrated into a unified framework\\nfor those methods. In this work, we conduct an empirical study of GPT-4o's\\nimage generation capabilities, benchmarking it against leading open-source and\\ncommercial models. Our evaluation covers four main categories, including\\ntext-to-image, image-to-image, image-to-3D, and image-to-X generation, with\\nmore than 20 tasks. Our analysis highlights the strengths and limitations of\\nGPT-4o under various settings, and situates it within the broader evolution of\\ngenerative modeling. Through this investigation, we identify promising\\ndirections for future unified generative models, emphasizing the role of\\narchitectural design and data scaling.\", 'upvotes': 43, 'discussionId': '67f5d5496ceb820f2006da78'}, 'publishedAt': '2025-04-08T08:34:36.000Z', 'title': 'An Empirical Study of GPT-4o Image Generation Capabilities', 'summary': \"The landscape of image generation has rapidly evolved, from early GAN-based\\napproaches to diffusion models and, most recently, to unified generative\\narchitectures that seek to bridge understanding and generation tasks. Recent\\nadvances, especially the GPT-4o, have demonstrated the feasibility of\\nhigh-fidelity multimodal generation, their architectural design remains\\nmysterious and unpublished. This prompts the question of whether image and text\\ngeneration have already been successfully integrated into a unified framework\\nfor those methods. In this work, we conduct an empirical study of GPT-4o's\\nimage generation capabilities, benchmarking it against leading open-source and\\ncommercial models. Our evaluation covers four main categories, including\\ntext-to-image, image-to-image, image-to-3D, and image-to-X generation, with\\nmore than 20 tasks. Our analysis highlights the strengths and limitations of\\nGPT-4o under various settings, and situates it within the broader evolution of\\ngenerative modeling. Through this investigation, we identify promising\\ndirections for future unified generative models, emphasizing the role of\\narchitectural design and data scaling.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05979.png', 'numComments': 1, 'submittedBy': {'_id': '63fccdac93b993a4ebd7789a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg', 'fullname': 'Jinbin Bai', 'name': 'BryanW', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2504.05535', 'authors': [{'_id': '67f630091aed1b4344b57c1b', 'name': 'M-A-P Team', 'hidden': False}, {'_id': '67f630091aed1b4344b57c1c', 'user': {'_id': '656d97b10bbc114fe64a96c5', 'avatarUrl': '/avatars/fd23bae1d85c5b96c42064a5ddcfad41.svg', 'isPro': False, 'fullname': 'SiweiWu', 'user': 'SiweiWu', 'type': 'user'}, 'name': 'Siwei Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T14:36:41.051Z', 'hidden': False}, {'_id': '67f630091aed1b4344b57c1d', 'user': {'_id': '6704ee27386892c420db1938', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg', 'isPro': False, 'fullname': 'JinCheng Ren', 'user': 'JinChengRen', 'type': 'user'}, 'name': 'Jincheng Ren', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T09:48:17.939Z', 'hidden': False}, {'_id': '67f630091aed1b4344b57c1e', 'user': {'_id': '654907a4a1faff97850c4eff', 'avatarUrl': '/avatars/458c90151614bc7f116943b6e67d6b8a.svg', 'isPro': False, 'fullname': 'du', 'user': 'dododododo', 'type': 'user'}, 'name': 'Xinrun Du', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T14:36:47.680Z', 'hidden': False}, {'_id': '67f630091aed1b4344b57c1f', 'name': 'Shuyue Guo', 'hidden': False}, {'_id': '67f630091aed1b4344b57c20', 'name': 'Xingwei Qu', 'hidden': False}, {'_id': '67f630091aed1b4344b57c21', 'name': 'Yiming Liang', 'hidden': False}, {'_id': '67f630091aed1b4344b57c22', 'name': 'Jie Liu', 'hidden': False}, {'_id': '67f630091aed1b4344b57c23', 'name': 'Yunwen Li', 'hidden': False}, {'_id': '67f630091aed1b4344b57c24', 'user': {'_id': '64ab99dcb76bfd863eba64c1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg', 'isPro': False, 'fullname': 'TY.Zheng', 'user': 'aaabiao', 'type': 'user'}, 'name': 'Tianyu Zheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T09:48:16.069Z', 'hidden': False}, {'_id': '67f630091aed1b4344b57c25', 'user': {'_id': '67f654ecb88bc093ada9da3f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Anh0oLDzzHCeSmEzwAmYd.png', 'isPro': False, 'fullname': 'boyuFeng', 'user': 'FWORKS', 'type': 'user'}, 'name': 'Boyu Feng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T14:36:53.597Z', 'hidden': False}, {'_id': '67f630091aed1b4344b57c26', 'name': 'Huaqing Yuan', 'hidden': False}, {'_id': '67f630091aed1b4344b57c27', 'name': 'Zenith Wang', 'hidden': False}, {'_id': '67f630091aed1b4344b57c28', 'name': 'Jiaheng Liu', 'hidden': False}, {'_id': '67f630091aed1b4344b57c29', 'name': 'Wenhao Huang', 'hidden': False}, {'_id': '67f630091aed1b4344b57c2a', 'name': 'Chenglin Cai', 'hidden': False}, {'_id': '67f630091aed1b4344b57c2b', 'name': 'Haoran Que', 'hidden': False}, {'_id': '67f630091aed1b4344b57c2c', 'name': 'Jian Yang', 'hidden': False}, {'_id': '67f630091aed1b4344b57c2d', 'name': 'Yuelin Bai', 'hidden': False}, {'_id': '67f630091aed1b4344b57c2e', 'name': 'Zekun Moore Wang', 'hidden': False}, {'_id': '67f630091aed1b4344b57c2f', 'user': {'_id': '62a80fe3ac97233f1625235a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg', 'isPro': False, 'fullname': 'Zhouliang Yu', 'user': 'zhouliang', 'type': 'user'}, 'name': 'Zhouliang Yu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T14:36:43.986Z', 'hidden': False}, {'_id': '67f630091aed1b4344b57c30', 'name': 'Qunshu Lin', 'hidden': False}, {'_id': '67f630091aed1b4344b57c31', 'name': 'Ding Pan', 'hidden': False}, {'_id': '67f630091aed1b4344b57c32', 'name': 'Yuchen Jiang', 'hidden': False}, {'_id': '67f630091aed1b4344b57c33', 'name': 'Tiannan Wang', 'hidden': False}, {'_id': '67f630091aed1b4344b57c34', 'name': 'Wangchunshu Zhou', 'hidden': False}, {'_id': '67f630091aed1b4344b57c35', 'name': 'Shenzhi Wang', 'hidden': False}, {'_id': '67f630091aed1b4344b57c36', 'user': {'_id': '6444e7765691ca69b0d95856', 'avatarUrl': '/avatars/4ae5001e7c7dea89c4deaf2b05436857.svg', 'isPro': False, 'fullname': 'Xingyuan Bu', 'user': 'sefira32', 'type': 'user'}, 'name': 'Xingyuan Bu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T14:36:50.729Z', 'hidden': False}, {'_id': '67f630091aed1b4344b57c37', 'user': {'_id': '6417d9ea8f689506e7148417', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg', 'isPro': False, 'fullname': 'minghao', 'user': 'Liam-Liu', 'type': 'user'}, 'name': 'Minghao Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T09:48:19.721Z', 'hidden': False}, {'_id': '67f630091aed1b4344b57c38', 'name': 'Guoyin Wang', 'hidden': False}, {'_id': '67f630091aed1b4344b57c39', 'name': 'Ge Zhang', 'hidden': False}, {'_id': '67f630091aed1b4344b57c3a', 'name': 'Chenghua Lin', 'hidden': False}], 'publishedAt': '2025-04-07T22:15:51.000Z', 'submittedOnDailyAt': '2025-04-09T07:58:42.888Z', 'title': 'COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for\\n  Alignment with Human Values', 'submittedOnDailyBy': {'_id': '656d97b10bbc114fe64a96c5', 'avatarUrl': '/avatars/fd23bae1d85c5b96c42064a5ddcfad41.svg', 'isPro': False, 'fullname': 'SiweiWu', 'user': 'SiweiWu', 'type': 'user'}, 'summary': 'Aligning large language models (LLMs) with human preferences has achieved\\nremarkable success. However, existing Chinese preference datasets are limited\\nby small scale, narrow domain coverage, and lack of rigorous data validation.\\nAdditionally, the reliance on human annotators for instruction and response\\nlabeling significantly constrains the scalability of human preference datasets.\\nTo address these challenges, we design an LLM-based Chinese preference dataset\\nannotation pipeline with no human intervention. Specifically, we crawled and\\ncarefully filtered 92k high-quality Chinese queries and employed 15 mainstream\\nLLMs to generate and score chosen-rejected response pairs. Based on it, we\\nintroduce COIG-P (Chinese Open Instruction Generalist - Preference), a\\nhigh-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese\\npreference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel,\\nand Role. Building upon COIG-P, to reduce the overhead of using LLMs for\\nscoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously\\nconstructed a Chinese Reward Benchmark (CRBench). Evaluation results based on\\nAlignBench liu2024alignbenchbenchmarkingchinesealignment show that that\\nCOIG-P significantly outperforms other Chinese preference datasets, and it\\nbrings significant performance improvements ranging from 2% to 12% for the\\nQwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results\\non CRBench demonstrate that our CRM has a strong and robust scoring ability. We\\napply it to filter chosen-rejected response pairs in a test split of COIG-P,\\nand our experiments show that it is comparable to GPT-4o in identifying\\nlow-quality samples while maintaining efficiency and cost-effectiveness. Our\\ncodes and data are released in\\nhttps://github.com/multimodal-art-projection/COIG-P.', 'upvotes': 30, 'discussionId': '67f6300b1aed1b4344b57cd0'}, 'publishedAt': '2025-04-07T18:15:51.000Z', 'title': 'COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for\\n  Alignment with Human Values', 'summary': 'Aligning large language models (LLMs) with human preferences has achieved\\nremarkable success. However, existing Chinese preference datasets are limited\\nby small scale, narrow domain coverage, and lack of rigorous data validation.\\nAdditionally, the reliance on human annotators for instruction and response\\nlabeling significantly constrains the scalability of human preference datasets.\\nTo address these challenges, we design an LLM-based Chinese preference dataset\\nannotation pipeline with no human intervention. Specifically, we crawled and\\ncarefully filtered 92k high-quality Chinese queries and employed 15 mainstream\\nLLMs to generate and score chosen-rejected response pairs. Based on it, we\\nintroduce COIG-P (Chinese Open Instruction Generalist - Preference), a\\nhigh-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese\\npreference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel,\\nand Role. Building upon COIG-P, to reduce the overhead of using LLMs for\\nscoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously\\nconstructed a Chinese Reward Benchmark (CRBench). Evaluation results based on\\nAlignBench liu2024alignbenchbenchmarkingchinesealignment show that that\\nCOIG-P significantly outperforms other Chinese preference datasets, and it\\nbrings significant performance improvements ranging from 2% to 12% for the\\nQwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results\\non CRBench demonstrate that our CRM has a strong and robust scoring ability. We\\napply it to filter chosen-rejected response pairs in a test split of COIG-P,\\nand our experiments show that it is comparable to GPT-4o in identifying\\nlow-quality samples while maintaining efficiency and cost-effectiveness. Our\\ncodes and data are released in\\nhttps://github.com/multimodal-art-projection/COIG-P.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05535.png', 'numComments': 1, 'submittedBy': {'_id': '656d97b10bbc114fe64a96c5', 'avatarUrl': '/avatars/fd23bae1d85c5b96c42064a5ddcfad41.svg', 'fullname': 'SiweiWu', 'name': 'SiweiWu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 15}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2504.02160', 'authors': [{'_id': '67efd1cd40e0a904109cac33', 'user': {'_id': '660114b38ae190912a61be5d', 'avatarUrl': '/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg', 'isPro': False, 'fullname': 'ShaojinWu', 'user': 'fenfan', 'type': 'user'}, 'name': 'Shaojin Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-08T06:54:08.610Z', 'hidden': False}, {'_id': '67efd1cd40e0a904109cac34', 'name': 'Mengqi Huang', 'hidden': False}, {'_id': '67efd1cd40e0a904109cac35', 'user': {'_id': '635634171c93c1ef4e9eb1c2', 'avatarUrl': '/avatars/66b31b801960612057ecfd1e26410075.svg', 'isPro': False, 'fullname': 'wuwenxu', 'user': 'wuwx', 'type': 'user'}, 'name': 'Wenxu Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-08T06:54:06.171Z', 'hidden': False}, {'_id': '67efd1cd40e0a904109cac36', 'name': 'Yufeng Cheng', 'hidden': False}, {'_id': '67efd1cd40e0a904109cac37', 'name': 'Fei Ding', 'hidden': False}, {'_id': '67efd1cd40e0a904109cac38', 'name': 'Qian He', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/660114b38ae190912a61be5d/eLRIcZxGD19KgExwoJTKW.mp4'], 'publishedAt': '2025-04-02T22:20:21.000Z', 'submittedOnDailyAt': '2025-04-09T02:18:09.429Z', 'title': 'Less-to-More Generalization: Unlocking More Controllability by\\n  In-Context Generation', 'submittedOnDailyBy': {'_id': '660114b38ae190912a61be5d', 'avatarUrl': '/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg', 'isPro': False, 'fullname': 'ShaojinWu', 'user': 'fenfan', 'type': 'user'}, 'summary': 'Although subject-driven generation has been extensively explored in image\\ngeneration due to its wide applications, it still has challenges in data\\nscalability and subject expansibility. For the first challenge, moving from\\ncurating single-subject datasets to multiple-subject ones and scaling them is\\nparticularly difficult. For the second, most recent methods center on\\nsingle-subject generation, making it hard to apply when dealing with\\nmulti-subject scenarios. In this study, we propose a highly-consistent data\\nsynthesis pipeline to tackle this challenge. This pipeline harnesses the\\nintrinsic in-context generation capabilities of diffusion transformers and\\ngenerates high-consistency multi-subject paired data. Additionally, we\\nintroduce UNO, which consists of progressive cross-modal alignment and\\nuniversal rotary position embedding. It is a multi-image conditioned\\nsubject-to-image model iteratively trained from a text-to-image model.\\nExtensive experiments show that our method can achieve high consistency while\\nensuring controllability in both single-subject and multi-subject driven\\ngeneration.', 'upvotes': 22, 'discussionId': '67efd1d140e0a904109cad62', 'projectPage': 'https://bytedance.github.io/UNO/', 'githubRepo': 'https://github.com/bytedance/UNO', 'ai_keywords': ['diffusion transformers', 'in-context generation', 'multi-subject paired data', 'UNO', 'progressive cross-modal alignment', 'universal rotary position embedding', 'multi-image conditioned', 'subject-to-image model', 'text-to-image model']}, 'publishedAt': '2025-04-02T18:20:21.000Z', 'title': 'Less-to-More Generalization: Unlocking More Controllability by\\n  In-Context Generation', 'summary': 'Although subject-driven generation has been extensively explored in image\\ngeneration due to its wide applications, it still has challenges in data\\nscalability and subject expansibility. For the first challenge, moving from\\ncurating single-subject datasets to multiple-subject ones and scaling them is\\nparticularly difficult. For the second, most recent methods center on\\nsingle-subject generation, making it hard to apply when dealing with\\nmulti-subject scenarios. In this study, we propose a highly-consistent data\\nsynthesis pipeline to tackle this challenge. This pipeline harnesses the\\nintrinsic in-context generation capabilities of diffusion transformers and\\ngenerates high-consistency multi-subject paired data. Additionally, we\\nintroduce UNO, which consists of progressive cross-modal alignment and\\nuniversal rotary position embedding. It is a multi-image conditioned\\nsubject-to-image model iteratively trained from a text-to-image model.\\nExtensive experiments show that our method can achieve high consistency while\\nensuring controllability in both single-subject and multi-subject driven\\ngeneration.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/660114b38ae190912a61be5d/eLRIcZxGD19KgExwoJTKW.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02160.png', 'numComments': 1, 'submittedBy': {'_id': '660114b38ae190912a61be5d', 'avatarUrl': '/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg', 'fullname': 'ShaojinWu', 'name': 'fenfan', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2504.02810', 'authors': [{'_id': '67f099de103cb604facd26cd', 'user': {'_id': '63453f02a05b51f7ded3c579', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/u7p-wTj8zyeWpSFRI8Go0.png', 'isPro': False, 'fullname': 'Andy Lin', 'user': 'pkuHaowei', 'type': 'user'}, 'name': 'Haowei Lin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-06T08:11:10.909Z', 'hidden': False}, {'_id': '67f099de103cb604facd26ce', 'name': 'Xiangyu Wang', 'hidden': False}, {'_id': '67f099de103cb604facd26cf', 'name': 'Ruilin Yan', 'hidden': False}, {'_id': '67f099de103cb604facd26d0', 'name': 'Baizhou Huang', 'hidden': False}, {'_id': '67f099de103cb604facd26d1', 'name': 'Haotian Ye', 'hidden': False}, {'_id': '67f099de103cb604facd26d2', 'name': 'Jianhua Zhu', 'hidden': False}, {'_id': '67f099de103cb604facd26d3', 'name': 'Zihao Wang', 'hidden': False}, {'_id': '67f099de103cb604facd26d4', 'name': 'James Zou', 'hidden': False}, {'_id': '67f099de103cb604facd26d5', 'name': 'Jianzhu Ma', 'hidden': False}, {'_id': '67f099de103cb604facd26d6', 'user': {'_id': '64683a5776bb704aa14588b7', 'avatarUrl': '/avatars/e532756f52c5b95981470ace41a10556.svg', 'isPro': False, 'fullname': 'Yitao Liang', 'user': 'YitaoLiang', 'type': 'user'}, 'name': 'Yitao Liang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T07:38:09.311Z', 'hidden': False}], 'publishedAt': '2025-04-03T17:54:18.000Z', 'submittedOnDailyAt': '2025-04-09T01:08:45.803Z', 'title': 'Generative Evaluation of Complex Reasoning in Large Language Models', 'submittedOnDailyBy': {'_id': '63453f02a05b51f7ded3c579', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/u7p-wTj8zyeWpSFRI8Go0.png', 'isPro': False, 'fullname': 'Andy Lin', 'user': 'pkuHaowei', 'type': 'user'}, 'summary': \"With powerful large language models (LLMs) demonstrating superhuman reasoning\\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\\nmerely recall answers from their extensive, web-scraped training datasets?\\nPublicly released benchmarks inevitably become contaminated once incorporated\\ninto subsequent LLM training sets, undermining their reliability as faithful\\nassessments. To address this, we introduce KUMO, a generative evaluation\\nframework designed specifically for assessing reasoning in LLMs. KUMO\\nsynergistically combines LLMs with symbolic engines to dynamically produce\\ndiverse, multi-turn reasoning tasks that are partially observable and\\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\\ngenerates novel tasks across open-ended domains, compelling models to\\ndemonstrate genuine generalization rather than memorization. We evaluated 23\\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\\nbenchmarking their reasoning abilities against university students. Our\\nfindings reveal that many LLMs have outperformed university-level performance\\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\\ntasks correlates strongly with results on newly released real-world reasoning\\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\\ngenuine LLM reasoning capabilities.\", 'upvotes': 10, 'discussionId': '67f099e1103cb604facd280e', 'githubRepo': 'https://github.com/linhaowei1/kumo', 'ai_keywords': ['large language models (LLMs)', 'superhuman reasoning capabilities', 'web-scraped training datasets', 'generative evaluation framework', 'symbolic engines', 'multi-turn reasoning tasks', 'partially observable', 'adjustable in difficulty', 'automated pipeline', 'open-ended domains', 'genuine generalization', 'memorization', 'reasoning abilities', 'reasoning-scaled LLMs', 'university-level performance', 'complex reasoning challenges', 'real-world reasoning benchmarks']}, 'publishedAt': '2025-04-03T13:54:18.000Z', 'title': 'Generative Evaluation of Complex Reasoning in Large Language Models', 'summary': \"With powerful large language models (LLMs) demonstrating superhuman reasoning\\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\\nmerely recall answers from their extensive, web-scraped training datasets?\\nPublicly released benchmarks inevitably become contaminated once incorporated\\ninto subsequent LLM training sets, undermining their reliability as faithful\\nassessments. To address this, we introduce KUMO, a generative evaluation\\nframework designed specifically for assessing reasoning in LLMs. KUMO\\nsynergistically combines LLMs with symbolic engines to dynamically produce\\ndiverse, multi-turn reasoning tasks that are partially observable and\\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\\ngenerates novel tasks across open-ended domains, compelling models to\\ndemonstrate genuine generalization rather than memorization. We evaluated 23\\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\\nbenchmarking their reasoning abilities against university students. Our\\nfindings reveal that many LLMs have outperformed university-level performance\\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\\ntasks correlates strongly with results on newly released real-world reasoning\\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\\ngenuine LLM reasoning capabilities.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02810.png', 'numComments': 3, 'submittedBy': {'_id': '63453f02a05b51f7ded3c579', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/u7p-wTj8zyeWpSFRI8Go0.png', 'fullname': 'Andy Lin', 'name': 'pkuHaowei', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2504.05594', 'authors': [{'_id': '67f5dc86015730c161ce291b', 'name': 'Qi Mao', 'hidden': False}, {'_id': '67f5dc86015730c161ce291c', 'name': 'Lan Chen', 'hidden': False}, {'_id': '67f5dc86015730c161ce291d', 'name': 'Yuchao Gu', 'hidden': False}, {'_id': '67f5dc86015730c161ce291e', 'name': 'Mike Zheng Shou', 'hidden': False}, {'_id': '67f5dc86015730c161ce291f', 'name': 'Ming-Hsuan Yang', 'hidden': False}], 'publishedAt': '2025-04-08T01:02:50.000Z', 'submittedOnDailyAt': '2025-04-09T01:06:41.710Z', 'title': 'Tuning-Free Image Editing with Fidelity and Editability via Unified\\n  Latent Diffusion Model', 'submittedOnDailyBy': {'_id': '640d704c8036cc2142299c19', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg', 'isPro': False, 'fullname': 'Lan Chen', 'user': 'Orannue', 'type': 'user'}, 'summary': 'Balancing fidelity and editability is essential in text-based image editing\\n(TIE), where failures commonly lead to over- or under-editing issues. Existing\\nmethods typically rely on attention injections for structure preservation and\\nleverage the inherent text alignment capabilities of pre-trained text-to-image\\n(T2I) models for editability, but they lack explicit and unified mechanisms to\\nproperly balance these two objectives. In this work, we introduce UnifyEdit, a\\ntuning-free method that performs diffusion latent optimization to enable a\\nbalanced integration of fidelity and editability within a unified framework.\\nUnlike direct attention injections, we develop two attention-based constraints:\\na self-attention (SA) preservation constraint for structural fidelity, and a\\ncross-attention (CA) alignment constraint to enhance text alignment for\\nimproved editability. However, simultaneously applying both constraints can\\nlead to gradient conflicts, where the dominance of one constraint results in\\nover- or under-editing. To address this challenge, we introduce an adaptive\\ntime-step scheduler that dynamically adjusts the influence of these\\nconstraints, guiding the diffusion latent toward an optimal balance. Extensive\\nquantitative and qualitative experiments validate the effectiveness of our\\napproach, demonstrating its superiority in achieving a robust balance between\\nstructure preservation and text alignment across various editing tasks,\\noutperforming other state-of-the-art methods. The source code will be available\\nat https://github.com/CUC-MIPG/UnifyEdit.', 'upvotes': 9, 'discussionId': '67f5dc89015730c161ce2a50'}, 'publishedAt': '2025-04-07T21:02:50.000Z', 'title': 'Tuning-Free Image Editing with Fidelity and Editability via Unified\\n  Latent Diffusion Model', 'summary': 'Balancing fidelity and editability is essential in text-based image editing\\n(TIE), where failures commonly lead to over- or under-editing issues. Existing\\nmethods typically rely on attention injections for structure preservation and\\nleverage the inherent text alignment capabilities of pre-trained text-to-image\\n(T2I) models for editability, but they lack explicit and unified mechanisms to\\nproperly balance these two objectives. In this work, we introduce UnifyEdit, a\\ntuning-free method that performs diffusion latent optimization to enable a\\nbalanced integration of fidelity and editability within a unified framework.\\nUnlike direct attention injections, we develop two attention-based constraints:\\na self-attention (SA) preservation constraint for structural fidelity, and a\\ncross-attention (CA) alignment constraint to enhance text alignment for\\nimproved editability. However, simultaneously applying both constraints can\\nlead to gradient conflicts, where the dominance of one constraint results in\\nover- or under-editing. To address this challenge, we introduce an adaptive\\ntime-step scheduler that dynamically adjusts the influence of these\\nconstraints, guiding the diffusion latent toward an optimal balance. Extensive\\nquantitative and qualitative experiments validate the effectiveness of our\\napproach, demonstrating its superiority in achieving a robust balance between\\nstructure preservation and text alignment across various editing tasks,\\noutperforming other state-of-the-art methods. The source code will be available\\nat https://github.com/CUC-MIPG/UnifyEdit.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05594.png', 'numComments': 1, 'submittedBy': {'_id': '640d704c8036cc2142299c19', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg', 'fullname': 'Lan Chen', 'name': 'Orannue', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.06148', 'authors': [{'_id': '67f6310fe30d3e5d13a9cbfc', 'user': {'_id': '673deee2afdcf84dddf74827', 'avatarUrl': '/avatars/d2e051ddef816342aa52b98ded109e66.svg', 'isPro': False, 'fullname': 'XxZheng', 'user': 'Fengx1nn', 'type': 'user'}, 'name': 'Xiangxi Zheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T09:48:14.241Z', 'hidden': False}, {'_id': '67f6310fe30d3e5d13a9cbfd', 'name': 'Linjie Li', 'hidden': False}, {'_id': '67f6310fe30d3e5d13a9cbfe', 'name': 'Zhengyuan Yang', 'hidden': False}, {'_id': '67f6310fe30d3e5d13a9cbff', 'name': 'Ping Yu', 'hidden': False}, {'_id': '67f6310fe30d3e5d13a9cc00', 'name': 'Alex Jinpeng Wang', 'hidden': False}, {'_id': '67f6310fe30d3e5d13a9cc01', 'name': 'Rui Yan', 'hidden': False}, {'_id': '67f6310fe30d3e5d13a9cc02', 'name': 'Yuan Yao', 'hidden': False}, {'_id': '67f6310fe30d3e5d13a9cc03', 'name': 'Lijuan Wang', 'hidden': False}], 'publishedAt': '2025-04-08T15:43:01.000Z', 'submittedOnDailyAt': '2025-04-09T07:04:38.598Z', 'title': 'V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric\\n  Capabilities in Multimodal Large Language Models', 'submittedOnDailyBy': {'_id': '62333a88fd7bb4a39b92d387', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png', 'isPro': False, 'fullname': 'Alex Jinpeng Wang', 'user': 'Awiny', 'type': 'user'}, 'summary': 'Recent advancements in Multimodal Large Language Models (MLLMs) have led to\\nsignificant improvements across various multimodal benchmarks. However, as\\nevaluations shift from static datasets to open-world, dynamic environments,\\ncurrent game-based benchmarks remain inadequate because they lack\\nvisual-centric tasks and fail to assess the diverse reasoning skills required\\nfor real-world decision-making. To address this, we introduce Visual-centric\\nMultiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework\\ndesigned to assess visual reasoning capabilities of MLLMs. V-MAGE features five\\ndiverse games with 30+ handcrafted levels, testing models on core visual skills\\nsuch as positioning, trajectory tracking, timing, and visual memory, alongside\\nhigher-level reasoning like long-term planning and deliberation. We use V-MAGE\\nto evaluate leading MLLMs, revealing significant challenges in their visual\\nperception and reasoning. In all game environments, the top-performing MLLMs,\\nas determined by Elo rating comparisons, exhibit a substantial performance gap\\ncompared to humans. Our findings highlight critical limitations, including\\nvarious types of perceptual errors made by the models, and suggest potential\\navenues for improvement from an agent-centric perspective, such as refining\\nagent strategies and addressing perceptual inaccuracies. Code is available at\\nhttps://github.com/CSU-JPG/V-MAGE.', 'upvotes': 7, 'discussionId': '67f63111e30d3e5d13a9cc85', 'projectPage': 'https://csu-jpg.github.io/V-MAGE/', 'githubRepo': 'https://github.com/CSU-JPG/V-MAGE'}, 'publishedAt': '2025-04-08T11:43:01.000Z', 'title': 'V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric\\n  Capabilities in Multimodal Large Language Models', 'summary': 'Recent advancements in Multimodal Large Language Models (MLLMs) have led to\\nsignificant improvements across various multimodal benchmarks. However, as\\nevaluations shift from static datasets to open-world, dynamic environments,\\ncurrent game-based benchmarks remain inadequate because they lack\\nvisual-centric tasks and fail to assess the diverse reasoning skills required\\nfor real-world decision-making. To address this, we introduce Visual-centric\\nMultiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework\\ndesigned to assess visual reasoning capabilities of MLLMs. V-MAGE features five\\ndiverse games with 30+ handcrafted levels, testing models on core visual skills\\nsuch as positioning, trajectory tracking, timing, and visual memory, alongside\\nhigher-level reasoning like long-term planning and deliberation. We use V-MAGE\\nto evaluate leading MLLMs, revealing significant challenges in their visual\\nperception and reasoning. In all game environments, the top-performing MLLMs,\\nas determined by Elo rating comparisons, exhibit a substantial performance gap\\ncompared to humans. Our findings highlight critical limitations, including\\nvarious types of perceptual errors made by the models, and suggest potential\\navenues for improvement from an agent-centric perspective, such as refining\\nagent strategies and addressing perceptual inaccuracies. Code is available at\\nhttps://github.com/CSU-JPG/V-MAGE.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06148.png', 'numComments': 1, 'submittedBy': {'_id': '62333a88fd7bb4a39b92d387', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png', 'fullname': 'Alex Jinpeng Wang', 'name': 'Awiny', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.06232', 'authors': [{'_id': '67f6406e49525c856f4705c4', 'name': 'Jiazi Bu', 'hidden': False}, {'_id': '67f6406e49525c856f4705c5', 'name': 'Pengyang Ling', 'hidden': False}, {'_id': '67f6406e49525c856f4705c6', 'name': 'Yujie Zhou', 'hidden': False}, {'_id': '67f6406e49525c856f4705c7', 'name': 'Pan Zhang', 'hidden': False}, {'_id': '67f6406e49525c856f4705c8', 'name': 'Tong Wu', 'hidden': False}, {'_id': '67f6406e49525c856f4705c9', 'name': 'Xiaoyi Dong', 'hidden': False}, {'_id': '67f6406e49525c856f4705ca', 'name': 'Yuhang Zang', 'hidden': False}, {'_id': '67f6406e49525c856f4705cb', 'name': 'Yuhang Cao', 'hidden': False}, {'_id': '67f6406e49525c856f4705cc', 'name': 'Dahua Lin', 'hidden': False}, {'_id': '67f6406e49525c856f4705cd', 'name': 'Jiaqi Wang', 'hidden': False}], 'publishedAt': '2025-04-08T17:30:40.000Z', 'submittedOnDailyAt': '2025-04-09T08:11:30.876Z', 'title': 'HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned\\n  Guidance', 'submittedOnDailyBy': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'isPro': True, 'fullname': 'Jiaqi Wang', 'user': 'myownskyW7', 'type': 'user'}, 'summary': \"Text-to-image (T2I) diffusion/flow models have drawn considerable attention\\nrecently due to their remarkable ability to deliver flexible visual creations.\\nStill, high-resolution image synthesis presents formidable challenges due to\\nthe scarcity and complexity of high-resolution content. To this end, we present\\nHiFlow, a training-free and model-agnostic framework to unlock the resolution\\npotential of pre-trained flow models. Specifically, HiFlow establishes a\\nvirtual reference flow within the high-resolution space that effectively\\ncaptures the characteristics of low-resolution flow information, offering\\nguidance for high-resolution generation through three key aspects:\\ninitialization alignment for low-frequency consistency, direction alignment for\\nstructure preservation, and acceleration alignment for detail fidelity. By\\nleveraging this flow-aligned guidance, HiFlow substantially elevates the\\nquality of high-resolution image synthesis of T2I models and demonstrates\\nversatility across their personalized variants. Extensive experiments validate\\nHiFlow's superiority in achieving superior high-resolution image quality over\\ncurrent state-of-the-art methods.\", 'upvotes': 6, 'discussionId': '67f6407349525c856f470733'}, 'publishedAt': '2025-04-08T13:30:40.000Z', 'title': 'HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned\\n  Guidance', 'summary': \"Text-to-image (T2I) diffusion/flow models have drawn considerable attention\\nrecently due to their remarkable ability to deliver flexible visual creations.\\nStill, high-resolution image synthesis presents formidable challenges due to\\nthe scarcity and complexity of high-resolution content. To this end, we present\\nHiFlow, a training-free and model-agnostic framework to unlock the resolution\\npotential of pre-trained flow models. Specifically, HiFlow establishes a\\nvirtual reference flow within the high-resolution space that effectively\\ncaptures the characteristics of low-resolution flow information, offering\\nguidance for high-resolution generation through three key aspects:\\ninitialization alignment for low-frequency consistency, direction alignment for\\nstructure preservation, and acceleration alignment for detail fidelity. By\\nleveraging this flow-aligned guidance, HiFlow substantially elevates the\\nquality of high-resolution image synthesis of T2I models and demonstrates\\nversatility across their personalized variants. Extensive experiments validate\\nHiFlow's superiority in achieving superior high-resolution image quality over\\ncurrent state-of-the-art methods.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06232.png', 'numComments': 1, 'submittedBy': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'fullname': 'Jiaqi Wang', 'name': 'myownskyW7', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 17}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.00043', 'authors': [{'_id': '67ec9d4ad327ed17ec707488', 'name': 'Jixuan Leng', 'hidden': False}, {'_id': '67ec9d4ad327ed17ec707489', 'name': 'Chengsong Huang', 'hidden': False}, {'_id': '67ec9d4ad327ed17ec70748a', 'name': 'Langlin Huang', 'hidden': False}, {'_id': '67ec9d4ad327ed17ec70748b', 'name': 'Bill Yuchen Lin', 'hidden': False}, {'_id': '67ec9d4ad327ed17ec70748c', 'name': 'William W. Cohen', 'hidden': False}, {'_id': '67ec9d4ad327ed17ec70748d', 'name': 'Haohan Wang', 'hidden': False}, {'_id': '67ec9d4ad327ed17ec70748e', 'name': 'Jiaxin Huang', 'hidden': False}], 'publishedAt': '2025-03-30T20:03:36.000Z', 'submittedOnDailyAt': '2025-04-09T00:47:44.460Z', 'title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\\n  with Controllable Puzzle Generation', 'submittedOnDailyBy': {'_id': '64efbf39b3610349e84db417', 'avatarUrl': '/avatars/9e09a20e88f8cf5ce119efc0dadc3b7b.svg', 'isPro': False, 'fullname': 'Jiaxin Huang', 'user': 'teapot123', 'type': 'user'}, 'summary': 'Existing reasoning evaluation frameworks for Large Language Models (LLMs) and\\nLarge Vision-Language Models (LVLMs) predominantly either assess text-based\\nreasoning or vision-language understanding capabilities, with limited dynamic\\ninterplay between textual and visual constraints. To address this limitation,\\nwe introduce CrossWordBench, a benchmark designed to evaluate the reasoning\\ncapabilities of both LLMs and LVLMs through the medium of crossword puzzles-a\\ntask requiring multimodal adherence to semantic constraints from text-based\\nclues and intersectional constraints from visual grid structures.\\nCrossWordBench leverages a controllable puzzle generation framework that\\nproduces puzzles in multiple formats (text and image) and offers different\\nevaluation strategies ranging from direct puzzle solving to interactive modes.\\nOur extensive evaluation of over 20 models reveals that reasoning LLMs\\noutperform non-reasoning models substantially by effectively leveraging\\ncrossing-letter constraints. We further demonstrate that LVLMs struggle with\\nthe task, showing a strong correlation between their puzzle-solving performance\\nand grid-parsing accuracy. Our findings offer insights into the limitations of\\nthe reasoning capabilities of current LLMs and LVLMs, and provide an effective\\napproach for creating multimodal constrained tasks for future evaluations.', 'upvotes': 6, 'discussionId': '67ec9d4fd327ed17ec707598', 'ai_keywords': ['CrossWordBench', 'multimodal adherence', 'semantic constraints', 'intersectional constraints', 'controllable puzzle generation framework', 'direct puzzle solving', 'interactive modes', 'reasoning LLMs', 'non-reasoning models', 'crossing-letter constraints', 'grid-parsing accuracy', 'multimodal constrained tasks']}, 'publishedAt': '2025-03-30T16:03:36.000Z', 'title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\\n  with Controllable Puzzle Generation', 'summary': 'Existing reasoning evaluation frameworks for Large Language Models (LLMs) and\\nLarge Vision-Language Models (LVLMs) predominantly either assess text-based\\nreasoning or vision-language understanding capabilities, with limited dynamic\\ninterplay between textual and visual constraints. To address this limitation,\\nwe introduce CrossWordBench, a benchmark designed to evaluate the reasoning\\ncapabilities of both LLMs and LVLMs through the medium of crossword puzzles-a\\ntask requiring multimodal adherence to semantic constraints from text-based\\nclues and intersectional constraints from visual grid structures.\\nCrossWordBench leverages a controllable puzzle generation framework that\\nproduces puzzles in multiple formats (text and image) and offers different\\nevaluation strategies ranging from direct puzzle solving to interactive modes.\\nOur extensive evaluation of over 20 models reveals that reasoning LLMs\\noutperform non-reasoning models substantially by effectively leveraging\\ncrossing-letter constraints. We further demonstrate that LVLMs struggle with\\nthe task, showing a strong correlation between their puzzle-solving performance\\nand grid-parsing accuracy. Our findings offer insights into the limitations of\\nthe reasoning capabilities of current LLMs and LVLMs, and provide an effective\\napproach for creating multimodal constrained tasks for future evaluations.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00043.png', 'numComments': 1, 'submittedBy': {'_id': '64efbf39b3610349e84db417', 'avatarUrl': '/avatars/9e09a20e88f8cf5ce119efc0dadc3b7b.svg', 'fullname': 'Jiaxin Huang', 'name': 'teapot123', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.20533', 'authors': [{'_id': '67f62f3a28b4852d4761e842', 'name': 'Yijiong Yu', 'hidden': False}], 'publishedAt': '2025-03-26T13:28:57.000Z', 'submittedOnDailyAt': '2025-04-09T06:58:23.443Z', 'title': 'Accelerate Parallelizable Reasoning via Parallel Decoding within One\\n  Sequence', 'submittedOnDailyBy': {'_id': '6374c494958cd71fa7ea0a9d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/2YCKv6tXCZXtsIOFIIXjs.png', 'isPro': False, 'fullname': 'yuyijiong', 'user': 'yuyijiong', 'type': 'user'}, 'summary': 'Recent advances in reasoning models have demonstrated significant\\nimprovements in accuracy, particularly for complex tasks such as mathematical\\nreasoning, by employing detailed and comprehensive reasoning processes.\\nHowever, generating these lengthy reasoning sequences is computationally\\nexpensive and time-consuming. To address this inefficiency, we leverage the\\ninherent parallelizability of certain tasks to accelerate the reasoning\\nprocess. Specifically, when multiple parallel reasoning branches exist, we\\ndecode multiple tokens per step using a specialized attention mask, processing\\nthem within a single sequence, avoiding additional memory usage. Experimental\\nresults show that our method achieves over 100% speedup in decoding time while\\nmaintaining the answer quality.', 'upvotes': 6, 'discussionId': '67f62f3b28b4852d4761e87c'}, 'publishedAt': '2025-03-26T09:28:57.000Z', 'title': 'Accelerate Parallelizable Reasoning via Parallel Decoding within One\\n  Sequence', 'summary': 'Recent advances in reasoning models have demonstrated significant\\nimprovements in accuracy, particularly for complex tasks such as mathematical\\nreasoning, by employing detailed and comprehensive reasoning processes.\\nHowever, generating these lengthy reasoning sequences is computationally\\nexpensive and time-consuming. To address this inefficiency, we leverage the\\ninherent parallelizability of certain tasks to accelerate the reasoning\\nprocess. Specifically, when multiple parallel reasoning branches exist, we\\ndecode multiple tokens per step using a specialized attention mask, processing\\nthem within a single sequence, avoiding additional memory usage. Experimental\\nresults show that our method achieves over 100% speedup in decoding time while\\nmaintaining the answer quality.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20533.png', 'numComments': 1, 'submittedBy': {'_id': '6374c494958cd71fa7ea0a9d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/2YCKv6tXCZXtsIOFIIXjs.png', 'fullname': 'yuyijiong', 'name': 'yuyijiong', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 43}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.05520', 'authors': [{'_id': '67f5e81488885c9950c7ab7e', 'user': {'_id': '62e1b3cb3eb0730f621a83f6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg', 'isPro': False, 'fullname': 'Taiwei Shi', 'user': 'MaksimSTW', 'type': 'user'}, 'name': 'Taiwei Shi', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-09T07:34:46.737Z', 'hidden': False}, {'_id': '67f5e81488885c9950c7ab7f', 'name': 'Yiyang Wu', 'hidden': False}, {'_id': '67f5e81488885c9950c7ab80', 'name': 'Linxin Song', 'hidden': False}, {'_id': '67f5e81488885c9950c7ab81', 'name': 'Tianyi Zhou', 'hidden': False}, {'_id': '67f5e81488885c9950c7ab82', 'name': 'Jieyu Zhao', 'hidden': False}], 'publishedAt': '2025-04-07T21:31:31.000Z', 'submittedOnDailyAt': '2025-04-09T13:29:28.521Z', 'title': 'Efficient Reinforcement Finetuning via Adaptive Curriculum Learning', 'submittedOnDailyBy': {'_id': '62e1b3cb3eb0730f621a83f6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg', 'isPro': False, 'fullname': 'Taiwei Shi', 'user': 'MaksimSTW', 'type': 'user'}, 'summary': \"Reinforcement finetuning (RFT) has shown great potential for enhancing the\\nmathematical reasoning capabilities of large language models (LLMs), but it is\\noften sample- and compute-inefficient, requiring extensive training. In this\\nwork, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a\\nmethod that significantly improves both the efficiency and final accuracy of\\nRFT through adaptive curriculum learning. AdaRFT dynamically adjusts the\\ndifficulty of training problems based on the model's recent reward signals,\\nensuring that the model consistently trains on tasks that are challenging but\\nsolvable. This adaptive sampling strategy accelerates learning by maintaining\\nan optimal difficulty range, avoiding wasted computation on problems that are\\ntoo easy or too hard. AdaRFT requires only a lightweight extension to standard\\nRFT algorithms like Proximal Policy Optimization (PPO), without modifying the\\nreward function or model architecture. Experiments on competition-level math\\ndatasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT\\nsignificantly improves both training efficiency and reasoning performance. We\\nevaluate AdaRFT across multiple data distributions and model sizes, showing\\nthat it reduces the number of training steps by up to 2x and improves accuracy\\nby a considerable margin, offering a more scalable and effective RFT framework.\", 'upvotes': 4, 'discussionId': '67f5e81588885c9950c7abaf'}, 'publishedAt': '2025-04-07T17:31:31.000Z', 'title': 'Efficient Reinforcement Finetuning via Adaptive Curriculum Learning', 'summary': \"Reinforcement finetuning (RFT) has shown great potential for enhancing the\\nmathematical reasoning capabilities of large language models (LLMs), but it is\\noften sample- and compute-inefficient, requiring extensive training. In this\\nwork, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a\\nmethod that significantly improves both the efficiency and final accuracy of\\nRFT through adaptive curriculum learning. AdaRFT dynamically adjusts the\\ndifficulty of training problems based on the model's recent reward signals,\\nensuring that the model consistently trains on tasks that are challenging but\\nsolvable. This adaptive sampling strategy accelerates learning by maintaining\\nan optimal difficulty range, avoiding wasted computation on problems that are\\ntoo easy or too hard. AdaRFT requires only a lightweight extension to standard\\nRFT algorithms like Proximal Policy Optimization (PPO), without modifying the\\nreward function or model architecture. Experiments on competition-level math\\ndatasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT\\nsignificantly improves both training efficiency and reasoning performance. We\\nevaluate AdaRFT across multiple data distributions and model sizes, showing\\nthat it reduces the number of training steps by up to 2x and improves accuracy\\nby a considerable margin, offering a more scalable and effective RFT framework.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05520.png', 'numComments': 1, 'submittedBy': {'_id': '62e1b3cb3eb0730f621a83f6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg', 'fullname': 'Taiwei Shi', 'name': 'MaksimSTW', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2504.06122', 'authors': [{'_id': '67f664cc902987500a49b197', 'name': 'Jingyuan Zhang', 'hidden': False}, {'_id': '67f664cc902987500a49b198', 'name': 'Qi Wang', 'hidden': False}, {'_id': '67f664cc902987500a49b199', 'name': 'Xingguang Ji', 'hidden': False}, {'_id': '67f664cc902987500a49b19a', 'name': 'Yahui Liu', 'hidden': False}, {'_id': '67f664cc902987500a49b19b', 'name': 'Yang Yue', 'hidden': False}, {'_id': '67f664cc902987500a49b19c', 'name': 'Fuzheng Zhang', 'hidden': False}, {'_id': '67f664cc902987500a49b19d', 'name': 'Di Zhang', 'hidden': False}, {'_id': '67f664cc902987500a49b19e', 'name': 'Guorui Zhou', 'hidden': False}, {'_id': '67f664cc902987500a49b19f', 'name': 'Kun Gai', 'hidden': False}], 'publishedAt': '2025-04-08T15:15:26.000Z', 'submittedOnDailyAt': '2025-04-09T10:45:37.912Z', 'title': 'Leanabell-Prover: Posttraining Scaling in Formal Reasoning', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': 'Recent advances in automated theorem proving (ATP) through LLMs have\\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\\nhas not yet be revolutionized by the recent posttraining scaling as\\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\\nmodels in natural languages.To begin, we continual train current ATP models\\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\\nadditional data aimed at incorporating cognitive behaviors that emulate human\\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\\nwith the use of outcome reward returned by Lean 4 compiler. Through our\\ndesigned continual training and reinforcement learning processes, we have\\nsuccessfully improved existing formal provers, including both\\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\\nrate (pass@32) on MiniF2F. This is an on-going project and we will\\nprogressively update our findings, release our data and training details.', 'upvotes': 3, 'discussionId': '67f664cd902987500a49b1d9'}, 'publishedAt': '2025-04-08T11:15:26.000Z', 'title': 'Leanabell-Prover: Posttraining Scaling in Formal Reasoning', 'summary': 'Recent advances in automated theorem proving (ATP) through LLMs have\\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\\nhas not yet be revolutionized by the recent posttraining scaling as\\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\\nmodels in natural languages.To begin, we continual train current ATP models\\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\\nadditional data aimed at incorporating cognitive behaviors that emulate human\\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\\nwith the use of outcome reward returned by Lean 4 compiler. Through our\\ndesigned continual training and reinforcement learning processes, we have\\nsuccessfully improved existing formal provers, including both\\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\\nrate (pass@32) on MiniF2F. This is an on-going project and we will\\nprogressively update our findings, release our data and training details.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06122.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6614}, 'isAuthorParticipating': False}"
]