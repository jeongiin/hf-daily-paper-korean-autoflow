[
    "{'paper': {'id': '2506.08007', 'authors': [{'_id': '684794553ec10bdd8ab4de1a', 'name': 'Qingxiu Dong', 'hidden': False}, {'_id': '684794553ec10bdd8ab4de1b', 'user': {'_id': '5df85abada6d0311fd3d5408', 'avatarUrl': '/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg', 'isPro': False, 'fullname': 'Li Dong', 'user': 'unilm', 'type': 'user'}, 'name': 'Li Dong', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:44:23.723Z', 'hidden': False}, {'_id': '684794553ec10bdd8ab4de1c', 'user': {'_id': '667119d6578448466d9531a6', 'avatarUrl': '/avatars/72c31909a5584b1306b6404b94a22b2a.svg', 'isPro': False, 'fullname': 'Yao Tang', 'user': 'YaoTang23', 'type': 'user'}, 'name': 'Yao Tang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:44:20.414Z', 'hidden': False}, {'_id': '684794553ec10bdd8ab4de1d', 'name': 'Tianzhu Ye', 'hidden': False}, {'_id': '684794553ec10bdd8ab4de1e', 'name': 'Yutao Sun', 'hidden': False}, {'_id': '684794553ec10bdd8ab4de1f', 'name': 'Zhifang Sui', 'hidden': False}, {'_id': '684794553ec10bdd8ab4de20', 'user': {'_id': '67ecd6178647cfa1775f75ed', 'avatarUrl': '/avatars/98882cc58dc0a5de94df765d523d92c9.svg', 'isPro': False, 'fullname': 'FW', 'user': 'frontierai', 'type': 'user'}, 'name': 'Furu Wei', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-10T02:11:34.050Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/OeDc7c4QFJOxdkJWvdbWB.png'], 'publishedAt': '2025-06-09T17:59:53.000Z', 'submittedOnDailyAt': '2025-06-10T00:43:01.816Z', 'title': 'Reinforcement Pre-Training', 'submittedOnDailyBy': {'_id': '5df85abada6d0311fd3d5408', 'avatarUrl': '/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg', 'isPro': False, 'fullname': 'Li Dong', 'user': 'unilm', 'type': 'user'}, 'summary': 'In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling\\nparadigm for large language models and reinforcement learning (RL).\\nSpecifically, we reframe next-token prediction as a reasoning task trained\\nusing RL, where it receives verifiable rewards for correctly predicting the\\nnext token for a given context. RPT offers a scalable method to leverage vast\\namounts of text data for general-purpose RL, rather than relying on\\ndomain-specific annotated answers. By incentivizing the capability of\\nnext-token reasoning, RPT significantly improves the language modeling accuracy\\nof predicting the next tokens. Moreover, RPT provides a strong pre-trained\\nfoundation for further reinforcement fine-tuning. The scaling curves show that\\nincreased training compute consistently improves the next-token prediction\\naccuracy. The results position RPT as an effective and promising scaling\\nparadigm to advance language model pre-training.', 'upvotes': 129, 'discussionId': '684794553ec10bdd8ab4de21', 'ai_summary': 'Reinforcement Pre-Training (RPT) improves language model accuracy through reinforcement learning and offers a scalable method for leveraging text data for general-purpose RL.', 'ai_keywords': ['Reinforcement Pre-Training (RPT)', 'next-token prediction', 'reasoning task', 'reinforcement learning (RL)', 'verifiable rewards', 'language modeling accuracy', 'reinforcement fine-tuning', 'scaling curves']}, 'publishedAt': '2025-06-09T13:59:53.000Z', 'title': 'Reinforcement Pre-Training', 'summary': 'In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling\\nparadigm for large language models and reinforcement learning (RL).\\nSpecifically, we reframe next-token prediction as a reasoning task trained\\nusing RL, where it receives verifiable rewards for correctly predicting the\\nnext token for a given context. RPT offers a scalable method to leverage vast\\namounts of text data for general-purpose RL, rather than relying on\\ndomain-specific annotated answers. By incentivizing the capability of\\nnext-token reasoning, RPT significantly improves the language modeling accuracy\\nof predicting the next tokens. Moreover, RPT provides a strong pre-trained\\nfoundation for further reinforcement fine-tuning. The scaling curves show that\\nincreased training compute consistently improves the next-token prediction\\naccuracy. The results position RPT as an effective and promising scaling\\nparadigm to advance language model pre-training.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/OeDc7c4QFJOxdkJWvdbWB.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08007.png', 'numComments': 3, 'submittedBy': {'_id': '5df85abada6d0311fd3d5408', 'avatarUrl': '/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg', 'fullname': 'Li Dong', 'name': 'unilm', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 28}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.07044', 'authors': [{'_id': '684795093ec10bdd8ab4de43', 'name': 'LASA Team', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de44', 'user': {'_id': '64118689756b9e455c7eac62', 'avatarUrl': '/avatars/cdb3da22593facf545a0bafbf548b07e.svg', 'isPro': False, 'fullname': 'Xu Weiwen', 'user': 'xww033', 'type': 'user'}, 'name': 'Weiwen Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:44:07.459Z', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de45', 'user': {'_id': '604f67ef0fe8ff3ec13d71ef', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png', 'isPro': False, 'fullname': 'Hou Pong (Ken) Chan', 'user': 'kenchan0226', 'type': 'user'}, 'name': 'Hou Pong Chan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:44:05.163Z', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de46', 'name': 'Long Li', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de47', 'name': 'Mahani Aljunied', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de48', 'name': 'Ruifeng Yuan', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de49', 'user': {'_id': '61e09ec13a1781f66b4e9ae2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg', 'isPro': False, 'fullname': 'Jianyu Wang', 'user': 'Jianyu', 'type': 'user'}, 'name': 'Jianyu Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:44:03.340Z', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de4a', 'user': {'_id': '63108cc834c7d77420b0fd68', 'avatarUrl': '/avatars/2721e573a417a8ec0b81ee048c4b42ba.svg', 'isPro': False, 'fullname': 'chenghao xiao', 'user': 'gowitheflow', 'type': 'user'}, 'name': 'Chenghao Xiao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T10:59:59.163Z', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de4b', 'name': 'Guizhen Chen', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de4c', 'name': 'Chaoqun Liu', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de4d', 'name': 'Zhaodonghui Li', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de4e', 'name': 'Yu Sun', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de4f', 'name': 'Junao Shen', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de50', 'name': 'Chaojun Wang', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de51', 'name': 'Jie Tan', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de52', 'name': 'Deli Zhao', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de53', 'name': 'Tingyang Xu', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de54', 'user': {'_id': '64b7cd74ff6d81ae297feded', 'avatarUrl': '/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg', 'isPro': False, 'fullname': 'ZHANG HAO', 'user': '26hzhang', 'type': 'user'}, 'name': 'Hao Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T11:20:59.561Z', 'hidden': False}, {'_id': '684795093ec10bdd8ab4de55', 'user': {'_id': '642eecbf9b2484d7d8526781', 'avatarUrl': '/avatars/773606f4a37d48861ec4f0f2df8a956f.svg', 'isPro': False, 'fullname': 'Yu Rong', 'user': 'Swrooy', 'type': 'user'}, 'name': 'Yu Rong', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:44:01.224Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/604f67ef0fe8ff3ec13d71ef/R3ajyza5JHjd8tOwwV2ht.png'], 'publishedAt': '2025-06-08T08:47:30.000Z', 'submittedOnDailyAt': '2025-06-10T00:48:48.080Z', 'title': 'Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\\n  Understanding and Reasoning', 'submittedOnDailyBy': {'_id': '604f67ef0fe8ff3ec13d71ef', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png', 'isPro': False, 'fullname': 'Hou Pong (Ken) Chan', 'user': 'kenchan0226', 'type': 'user'}, 'summary': \"Multimodal Large Language Models (MLLMs) have demonstrated impressive\\ncapabilities in understanding common visual elements, largely due to their\\nlarge-scale datasets and advanced training strategies. However, their\\neffectiveness in medical applications remains limited due to the inherent\\ndiscrepancies between data and tasks in medical scenarios and those in the\\ngeneral domain. Concretely, existing medical MLLMs face the following critical\\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\\nheightened susceptibility to hallucinations due to suboptimal data curation\\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\\nscenarios. To address these challenges, we first propose a comprehensive data\\ncuration procedure that (1) efficiently acquires rich medical knowledge data\\nnot only from medical imaging but also from extensive medical texts and\\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\\nquestion answering (VQA), and reasoning samples. As a result, we build a\\nmultimodal dataset enriched with extensive medical knowledge. Building on the\\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\\nundergoes multi-stage training to embed medical expertise and enhance its\\ntask-solving capabilities progressively. Besides, we preliminarily explore the\\npotential of applying reinforcement learning with verifiable rewards paradigm\\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\\nand textual medical benchmarks for standardized, fair, and efficient model\\nassessment. We evaluate the performance of Lingshu on three fundamental medical\\ntasks, multimodal QA, text-based QA, and medical report generation. The results\\nshow that Lingshu consistently outperforms the existing open-source multimodal\\nmodels on most tasks ...\", 'upvotes': 59, 'discussionId': '684795093ec10bdd8ab4de56', 'ai_summary': 'A medical-specialized multimodal large language model, Lingshu, is introduced with enhanced data curation and reinforcement learning to address limitations in medical applications.', 'ai_keywords': ['Multimodal Large Language Models', 'MLLMs', 'medical knowledge', 'hallucinations', 'data curation', 'medical texts', 'general-domain data', 'accurate medical captions', 'visual question answering', 'VQA', 'reasoning capabilities', 'multi-stage training', 'medical expertise', 'reinforcement learning', 'verifiable rewards paradigm', 'MedEvalKit', 'multimodal QA', 'text-based QA', 'medical report generation']}, 'publishedAt': '2025-06-08T04:47:30.000Z', 'title': 'Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\\n  Understanding and Reasoning', 'summary': \"Multimodal Large Language Models (MLLMs) have demonstrated impressive\\ncapabilities in understanding common visual elements, largely due to their\\nlarge-scale datasets and advanced training strategies. However, their\\neffectiveness in medical applications remains limited due to the inherent\\ndiscrepancies between data and tasks in medical scenarios and those in the\\ngeneral domain. Concretely, existing medical MLLMs face the following critical\\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\\nheightened susceptibility to hallucinations due to suboptimal data curation\\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\\nscenarios. To address these challenges, we first propose a comprehensive data\\ncuration procedure that (1) efficiently acquires rich medical knowledge data\\nnot only from medical imaging but also from extensive medical texts and\\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\\nquestion answering (VQA), and reasoning samples. As a result, we build a\\nmultimodal dataset enriched with extensive medical knowledge. Building on the\\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\\nundergoes multi-stage training to embed medical expertise and enhance its\\ntask-solving capabilities progressively. Besides, we preliminarily explore the\\npotential of applying reinforcement learning with verifiable rewards paradigm\\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\\nand textual medical benchmarks for standardized, fair, and efficient model\\nassessment. We evaluate the performance of Lingshu on three fundamental medical\\ntasks, multimodal QA, text-based QA, and medical report generation. The results\\nshow that Lingshu consistently outperforms the existing open-source multimodal\\nmodels on most tasks ...\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/604f67ef0fe8ff3ec13d71ef/R3ajyza5JHjd8tOwwV2ht.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07044.png', 'numComments': 2, 'submittedBy': {'_id': '604f67ef0fe8ff3ec13d71ef', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png', 'fullname': 'Hou Pong (Ken) Chan', 'name': 'kenchan0226', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.07900', 'authors': [{'_id': '6847924d3ec10bdd8ab4ddb9', 'name': 'MiniCPM Team', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddba', 'user': {'_id': '608f6d72283d0a8d7be9d1f9', 'avatarUrl': '/avatars/7f499a37019359a3c488ba6cc11751fc.svg', 'isPro': False, 'fullname': 'Chaojun XIAO', 'user': 'xcjthu', 'type': 'user'}, 'name': 'Chaojun Xiao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T11:00:03.612Z', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddbb', 'name': 'Yuxuan Li', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddbc', 'name': 'Xu Han', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddbd', 'name': 'Yuzhuo Bai', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddbe', 'name': 'Jie Cai', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddbf', 'name': 'Haotian Chen', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddc0', 'name': 'Wentong Chen', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddc1', 'name': 'Xin Cong', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddc2', 'name': 'Ganqu Cui', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddc3', 'name': 'Ning Ding', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddc4', 'name': 'Shengdan Fan', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddc5', 'name': 'Yewei Fang', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddc6', 'name': 'Zixuan Fu', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddc7', 'name': 'Wenyu Guan', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddc8', 'name': 'Yitong Guan', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddc9', 'user': {'_id': '66add9c413ac672510f6cdba', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66add9c413ac672510f6cdba/ubTWt8XN7aDdSihYAORah.png', 'isPro': False, 'fullname': 'guojunshao', 'user': 'guojunshaoyao', 'type': 'user'}, 'name': 'Junshao Guo', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T11:00:01.410Z', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddca', 'name': 'Yufeng Han', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddcb', 'name': 'Bingxiang He', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddcc', 'name': 'Yuxiang Huang', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddcd', 'name': 'Cunliang Kong', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddce', 'name': 'Qiuzuo Li', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddcf', 'name': 'Siyuan Li', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddd0', 'name': 'Wenhao Li', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddd1', 'name': 'Yanghao Li', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddd2', 'name': 'Yishan Li', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddd3', 'name': 'Zhen Li', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddd4', 'name': 'Dan Liu', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddd5', 'name': 'Biyuan Lin', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddd6', 'name': 'Yankai Lin', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddd7', 'name': 'Xiang Long', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddd8', 'name': 'Quanyu Lu', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddd9', 'name': 'Yaxi Lu', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddda', 'name': 'Peiyan Luo', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dddb', 'name': 'Hongya Lyu', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dddc', 'name': 'Litu Ou', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dddd', 'name': 'Yinxu Pan', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddde', 'name': 'Zekai Qu', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dddf', 'name': 'Qundong Shi', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dde0', 'name': 'Zijun Song', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dde1', 'name': 'Jiayuan Su', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dde2', 'name': 'Zhou Su', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dde3', 'name': 'Ao Sun', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dde4', 'name': 'Xianghui Sun', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dde5', 'name': 'Peijun Tang', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dde6', 'name': 'Fangzheng Wang', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dde7', 'name': 'Feng Wang', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dde8', 'name': 'Shuo Wang', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dde9', 'user': {'_id': '63be286fb3b8c44f8cecc16f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63be286fb3b8c44f8cecc16f/1CIkfEKoTnBYdYDSuQ8AT.jpeg', 'isPro': False, 'fullname': 'Yudong Wang', 'user': 'BigDong', 'type': 'user'}, 'name': 'Yudong Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:45:33.890Z', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddea', 'name': 'Yesai Wu', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddeb', 'name': 'Zhenyu Xiao', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddec', 'name': 'Jie Xie', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4dded', 'name': 'Zihao Xie', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddee', 'name': 'Yukun Yan', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddef', 'name': 'Jiarui Yuan', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddf0', 'name': 'Kaihuo Zhang', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddf1', 'name': 'Lei Zhang', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddf2', 'name': 'Linyue Zhang', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddf3', 'name': 'Xueren Zhang', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddf4', 'name': 'Yudi Zhang', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddf5', 'name': 'Hengyu Zhao', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddf6', 'name': 'Weilin Zhao', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddf7', 'name': 'Weilun Zhao', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddf8', 'name': 'Yuanqian Zhao', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddf9', 'name': 'Zhi Zheng', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddfa', 'name': 'Ge Zhou', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddfb', 'name': 'Jie Zhou', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddfc', 'name': 'Wei Zhou', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddfd', 'name': 'Zihan Zhou', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddfe', 'name': 'Zixuan Zhou', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4ddff', 'name': 'Zhiyuan Liu', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4de00', 'name': 'Guoyang Zeng', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4de01', 'name': 'Chao Jia', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4de02', 'name': 'Dahai Li', 'hidden': False}, {'_id': '6847924d3ec10bdd8ab4de03', 'name': 'Maosong Sun', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/608f6d72283d0a8d7be9d1f9/NF1aHsqQbJ_Dl18__cn2H.qt'], 'publishedAt': '2025-06-09T16:16:50.000Z', 'submittedOnDailyAt': '2025-06-10T00:50:56.021Z', 'title': 'MiniCPM4: Ultra-Efficient LLMs on End Devices', 'submittedOnDailyBy': {'_id': '608f6d72283d0a8d7be9d1f9', 'avatarUrl': '/avatars/7f499a37019359a3c488ba6cc11751fc.svg', 'isPro': False, 'fullname': 'Chaojun XIAO', 'user': 'xcjthu', 'type': 'user'}, 'summary': 'This paper introduces MiniCPM4, a highly efficient large language model (LLM)\\ndesigned explicitly for end-side devices. We achieve this efficiency through\\nsystematic innovation in four key dimensions: model architecture, training\\ndata, training algorithms, and inference systems. Specifically, in terms of\\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\\nmechanism that accelerates both prefilling and decoding phases for long-context\\nprocessing. Regarding training data, we propose UltraClean, an efficient and\\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\\na comprehensive supervised fine-tuning dataset. These datasets enable\\nsatisfactory model performance to be achieved using just 8 trillion training\\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\\npre-training strategy search, and improve existing post-training methods by\\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\\nCPM.cu that integrates sparse attention, model quantization, and speculative\\nsampling to achieve efficient prefilling and decoding. To meet diverse\\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\\noutperforms open-source models of similar size across multiple benchmarks,\\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\\ndemonstrates significant speed improvements over Qwen3-8B when processing long\\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\\napplications, including trustworthy survey generation and tool use with model\\ncontext protocol, clearly showcasing its broad usability.', 'upvotes': 48, 'discussionId': '6847924e3ec10bdd8ab4de04', 'projectPage': 'https://huggingface.co/collections/openbmb/minicpm4-6841ab29d180257e940baa9b', 'githubRepo': 'https://github.com/openbmb/minicpm', 'ai_summary': 'MiniCPM4, a highly efficient large language model for end-side devices, achieves superior performance using innovations in sparse attention, pre-training datasets, training algorithms, and inference systems.', 'ai_keywords': ['InfLLM v2', 'sparse attention mechanism', 'UltraClean', 'UltraChat v2', 'prefilling', 'decoding', 'long-context processing', 'ModelTunnel v2', 'chunk-wise rollout', 'data-efficient tenary LLM', 'BitCPM', 'CPM.cu', 'model quantization', 'speculative sampling']}, 'publishedAt': '2025-06-09T12:16:50.000Z', 'title': 'MiniCPM4: Ultra-Efficient LLMs on End Devices', 'summary': 'This paper introduces MiniCPM4, a highly efficient large language model (LLM)\\ndesigned explicitly for end-side devices. We achieve this efficiency through\\nsystematic innovation in four key dimensions: model architecture, training\\ndata, training algorithms, and inference systems. Specifically, in terms of\\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\\nmechanism that accelerates both prefilling and decoding phases for long-context\\nprocessing. Regarding training data, we propose UltraClean, an efficient and\\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\\na comprehensive supervised fine-tuning dataset. These datasets enable\\nsatisfactory model performance to be achieved using just 8 trillion training\\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\\npre-training strategy search, and improve existing post-training methods by\\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\\nCPM.cu that integrates sparse attention, model quantization, and speculative\\nsampling to achieve efficient prefilling and decoding. To meet diverse\\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\\noutperforms open-source models of similar size across multiple benchmarks,\\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\\ndemonstrates significant speed improvements over Qwen3-8B when processing long\\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\\napplications, including trustworthy survey generation and tool use with model\\ncontext protocol, clearly showcasing its broad usability.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/608f6d72283d0a8d7be9d1f9/NF1aHsqQbJ_Dl18__cn2H.qt'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07900.png', 'numComments': 1, 'submittedBy': {'_id': '608f6d72283d0a8d7be9d1f9', 'avatarUrl': '/avatars/7f499a37019359a3c488ba6cc11751fc.svg', 'fullname': 'Chaojun XIAO', 'name': 'xcjthu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.06444', 'authors': [{'_id': '68479c1e3ec10bdd8ab4de9d', 'name': 'Ruizhong Qiu', 'hidden': False}, {'_id': '68479c1e3ec10bdd8ab4de9e', 'name': 'Gaotang Li', 'hidden': False}, {'_id': '68479c1e3ec10bdd8ab4de9f', 'name': 'Tianxin Wei', 'hidden': False}, {'_id': '68479c1e3ec10bdd8ab4dea0', 'name': 'Jingrui He', 'hidden': False}, {'_id': '68479c1e3ec10bdd8ab4dea1', 'name': 'Hanghang Tong', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65370d95019de94263ad34a7/Uhd5i-LpHkPBjdpr4KJLi.jpeg'], 'publishedAt': '2025-06-06T18:05:45.000Z', 'submittedOnDailyAt': '2025-06-10T01:29:36.727Z', 'title': 'Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\\n  Assurance', 'submittedOnDailyBy': {'_id': '65370d95019de94263ad34a7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65370d95019de94263ad34a7/fH9SoJfX7yifXup0DvXkm.jpeg', 'isPro': False, 'fullname': 'Ruizhong Qiu', 'user': 'q-rz', 'type': 'user'}, 'summary': \"Existing safety assurance research has primarily focused on training-phase\\nalignment to instill safe behaviors into LLMs. However, recent studies have\\nexposed these methods' susceptibility to diverse jailbreak attacks.\\nConcurrently, inference scaling has significantly advanced LLM reasoning\\ncapabilities but remains unexplored in the context of safety assurance.\\nAddressing this gap, our work pioneers inference scaling for robust and\\neffective LLM safety against emerging threats. We reveal that conventional\\ninference scaling techniques, despite their success in reasoning tasks, perform\\npoorly in safety contexts, even falling short of basic approaches like\\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\\nchallenge, the exploration--efficiency dilemma, arising from the high\\ncomputational overhead associated with frequent process reward model (PRM)\\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\\nscaling paradigm tailored explicitly for safety assurance. Central to our\\napproach is the introduction of a multifurcation reward model (MRM) that\\nsignificantly reduces the required number of reward model evaluations. To\\noperationalize this paradigm, we further propose: (i) a partial supervision\\ntraining objective for MRM, (ii) a conservative exploration constraint to\\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\\ncaching strategy that facilitates cache sharing across sequences during tree\\nsearch. Extensive experiments validate the effectiveness of our method.\\nAdditionally, we publicly release our trained multifurcation reward model\\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\\nto accelerate future research in LLM safety. Our code, model, and data are\\npublicly available at https://github.com/q-rz/saffron , and our project\\nhomepage is at https://q-rz.github.io/p/saffron .\", 'upvotes': 43, 'discussionId': '68479c1e3ec10bdd8ab4dea2', 'projectPage': 'https://q-rz.github.io/p/saffron', 'githubRepo': 'https://github.com/q-rz/saffron', 'ai_summary': 'SAFFRON, a novel inference scaling paradigm, enhances LLM safety by reducing reward model evaluations through a multifurcation reward model and other optimizations.', 'ai_keywords': ['LLMs', 'inference scaling', 'safety assurance', 'jailbreak attacks', 'Best-of-N Sampling', 'process reward model', 'exploration--efficiency dilemma', 'multifurcation reward model', 'partial supervision training', 'conservative exploration constraint', 'Trie-based key--value caching', 'Safety4M dataset']}, 'publishedAt': '2025-06-06T14:05:45.000Z', 'title': 'Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\\n  Assurance', 'summary': \"Existing safety assurance research has primarily focused on training-phase\\nalignment to instill safe behaviors into LLMs. However, recent studies have\\nexposed these methods' susceptibility to diverse jailbreak attacks.\\nConcurrently, inference scaling has significantly advanced LLM reasoning\\ncapabilities but remains unexplored in the context of safety assurance.\\nAddressing this gap, our work pioneers inference scaling for robust and\\neffective LLM safety against emerging threats. We reveal that conventional\\ninference scaling techniques, despite their success in reasoning tasks, perform\\npoorly in safety contexts, even falling short of basic approaches like\\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\\nchallenge, the exploration--efficiency dilemma, arising from the high\\ncomputational overhead associated with frequent process reward model (PRM)\\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\\nscaling paradigm tailored explicitly for safety assurance. Central to our\\napproach is the introduction of a multifurcation reward model (MRM) that\\nsignificantly reduces the required number of reward model evaluations. To\\noperationalize this paradigm, we further propose: (i) a partial supervision\\ntraining objective for MRM, (ii) a conservative exploration constraint to\\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\\ncaching strategy that facilitates cache sharing across sequences during tree\\nsearch. Extensive experiments validate the effectiveness of our method.\\nAdditionally, we publicly release our trained multifurcation reward model\\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\\nto accelerate future research in LLM safety. Our code, model, and data are\\npublicly available at https://github.com/q-rz/saffron , and our project\\nhomepage is at https://q-rz.github.io/p/saffron .\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65370d95019de94263ad34a7/Uhd5i-LpHkPBjdpr4KJLi.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06444.png', 'numComments': 1, 'submittedBy': {'_id': '65370d95019de94263ad34a7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65370d95019de94263ad34a7/fH9SoJfX7yifXup0DvXkm.jpeg', 'fullname': 'Ruizhong Qiu', 'name': 'q-rz', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.07977', 'authors': [{'_id': '684792f03ec10bdd8ab4de06', 'name': 'Jingjing Chang', 'hidden': False}, {'_id': '684792f03ec10bdd8ab4de07', 'user': {'_id': '647469b9a51711a3b58bda2b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/647469b9a51711a3b58bda2b/yeDf8Sa8IDEQyney1dGC9.jpeg', 'isPro': False, 'fullname': 'Yixiao Fang', 'user': 'fangyixiao', 'type': 'user'}, 'name': 'Yixiao Fang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:44:54.679Z', 'hidden': False}, {'_id': '684792f03ec10bdd8ab4de08', 'name': 'Peng Xing', 'hidden': False}, {'_id': '684792f03ec10bdd8ab4de09', 'name': 'Shuhan Wu', 'hidden': False}, {'_id': '684792f03ec10bdd8ab4de0a', 'user': {'_id': '64b914c8ace99c0723ad83a9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg', 'isPro': False, 'fullname': 'Wei Cheng', 'user': 'wchengad', 'type': 'user'}, 'name': 'Wei Cheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:45:23.322Z', 'hidden': False}, {'_id': '684792f03ec10bdd8ab4de0b', 'name': 'Rui Wang', 'hidden': False}, {'_id': '684792f03ec10bdd8ab4de0c', 'name': 'Xianfang Zeng', 'hidden': False}, {'_id': '684792f03ec10bdd8ab4de0d', 'name': 'Gang Yu', 'hidden': False}, {'_id': '684792f03ec10bdd8ab4de0e', 'name': 'Hai-Bao Chen', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/Qq0Ue6mPEkoDJkIMGjRJ0.jpeg', 'https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ywhobM8HPQHIRy4CRBkLI.jpeg'], 'publishedAt': '2025-06-09T17:50:21.000Z', 'submittedOnDailyAt': '2025-06-10T00:52:27.518Z', 'title': 'OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation', 'submittedOnDailyBy': {'_id': '64b914c8ace99c0723ad83a9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg', 'isPro': False, 'fullname': 'Wei Cheng', 'user': 'wchengad', 'type': 'user'}, 'summary': 'Text-to-image (T2I) models have garnered significant attention for generating\\nhigh-quality images aligned with text prompts. However, rapid T2I model\\nadvancements reveal limitations in early benchmarks, lacking comprehensive\\nevaluations, for example, the evaluation on reasoning, text rendering and\\nstyle. Notably, recent state-of-the-art models, with their rich knowledge\\nmodeling capabilities, show promising results on the image generation problems\\nrequiring strong reasoning ability, yet existing evaluation systems have not\\nadequately addressed this frontier. To systematically address these gaps, we\\nintroduce OneIG-Bench, a meticulously designed comprehensive benchmark\\nframework for fine-grained evaluation of T2I models across multiple dimensions,\\nincluding prompt-image alignment, text rendering precision, reasoning-generated\\ncontent, stylization, and diversity. By structuring the evaluation, this\\nbenchmark enables in-depth analysis of model performance, helping researchers\\nand practitioners pinpoint strengths and bottlenecks in the full pipeline of\\nimage generation. Specifically, OneIG-Bench enables flexible evaluation by\\nallowing users to focus on a particular evaluation subset. Instead of\\ngenerating images for the entire set of prompts, users can generate images only\\nfor the prompts associated with the selected dimension and complete the\\ncorresponding evaluation accordingly. Our codebase and dataset are now publicly\\navailable to facilitate reproducible evaluation studies and cross-model\\ncomparisons within the T2I research community.', 'upvotes': 36, 'discussionId': '684792f03ec10bdd8ab4de0f', 'projectPage': 'https://oneig-bench.github.io/', 'githubRepo': 'https://github.com/OneIG-Bench/OneIG-Benchmark', 'ai_summary': 'OneIG-Bench is a comprehensive benchmark framework for evaluating text-to-image models across multiple dimensions including reasoning, text rendering, and diversity.', 'ai_keywords': ['text-to-image (T2I) models', 'prompt-image alignment', 'text rendering precision', 'reasoning-generated content', 'stylization', 'diversity']}, 'publishedAt': '2025-06-09T13:50:21.000Z', 'title': 'OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation', 'summary': 'Text-to-image (T2I) models have garnered significant attention for generating\\nhigh-quality images aligned with text prompts. However, rapid T2I model\\nadvancements reveal limitations in early benchmarks, lacking comprehensive\\nevaluations, for example, the evaluation on reasoning, text rendering and\\nstyle. Notably, recent state-of-the-art models, with their rich knowledge\\nmodeling capabilities, show promising results on the image generation problems\\nrequiring strong reasoning ability, yet existing evaluation systems have not\\nadequately addressed this frontier. To systematically address these gaps, we\\nintroduce OneIG-Bench, a meticulously designed comprehensive benchmark\\nframework for fine-grained evaluation of T2I models across multiple dimensions,\\nincluding prompt-image alignment, text rendering precision, reasoning-generated\\ncontent, stylization, and diversity. By structuring the evaluation, this\\nbenchmark enables in-depth analysis of model performance, helping researchers\\nand practitioners pinpoint strengths and bottlenecks in the full pipeline of\\nimage generation. Specifically, OneIG-Bench enables flexible evaluation by\\nallowing users to focus on a particular evaluation subset. Instead of\\ngenerating images for the entire set of prompts, users can generate images only\\nfor the prompts associated with the selected dimension and complete the\\ncorresponding evaluation accordingly. Our codebase and dataset are now publicly\\navailable to facilitate reproducible evaluation studies and cross-model\\ncomparisons within the T2I research community.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/Qq0Ue6mPEkoDJkIMGjRJ0.jpeg', 'https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ywhobM8HPQHIRy4CRBkLI.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07977.png', 'numComments': 1, 'submittedBy': {'_id': '64b914c8ace99c0723ad83a9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg', 'fullname': 'Wei Cheng', 'name': 'wchengad', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.07491', 'authors': [{'_id': '684799083ec10bdd8ab4de8a', 'user': {'_id': '63efbb1efc92a63ac81126d0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1676655314726-noauth.jpeg', 'isPro': True, 'fullname': 'Yongsen Mao', 'user': 'ysmao', 'type': 'user'}, 'name': 'Yongsen Mao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:43:08.018Z', 'hidden': False}, {'_id': '684799083ec10bdd8ab4de8b', 'name': 'Junhao Zhong', 'hidden': False}, {'_id': '684799083ec10bdd8ab4de8c', 'name': 'Chuan Fang', 'hidden': False}, {'_id': '684799083ec10bdd8ab4de8d', 'user': {'_id': '6437c0ead38ce48bdd4b0067', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6437c0ead38ce48bdd4b0067/9HdcjbD0ugiPypLDtQl8E.png', 'isPro': False, 'fullname': 'Jia Zheng', 'user': 'bertjiazheng', 'type': 'user'}, 'name': 'Jia Zheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:43:11.939Z', 'hidden': False}, {'_id': '684799083ec10bdd8ab4de8e', 'name': 'Rui Tang', 'hidden': False}, {'_id': '684799083ec10bdd8ab4de8f', 'name': 'Hao Zhu', 'hidden': False}, {'_id': '684799083ec10bdd8ab4de90', 'name': 'Ping Tan', 'hidden': False}, {'_id': '684799083ec10bdd8ab4de91', 'name': 'Zihan Zhou', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6437c0ead38ce48bdd4b0067/0ppuxf3I81w8mWqDgei0W.mp4'], 'publishedAt': '2025-06-09T07:10:58.000Z', 'submittedOnDailyAt': '2025-06-10T01:04:13.223Z', 'title': 'SpatialLM: Training Large Language Models for Structured Indoor Modeling', 'submittedOnDailyBy': {'_id': '6437c0ead38ce48bdd4b0067', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6437c0ead38ce48bdd4b0067/9HdcjbD0ugiPypLDtQl8E.png', 'isPro': False, 'fullname': 'Jia Zheng', 'user': 'bertjiazheng', 'type': 'user'}, 'summary': 'SpatialLM is a large language model designed to process 3D point cloud data\\nand generate structured 3D scene understanding outputs. These outputs include\\narchitectural elements like walls, doors, windows, and oriented object boxes\\nwith their semantic categories. Unlike previous methods which exploit\\ntask-specific network designs, our model adheres to the standard multimodal LLM\\narchitecture and is fine-tuned directly from open-source LLMs.\\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\\nground-truth 3D annotations, and conduct a careful study on various modeling\\nand training decisions. On public benchmarks, our model gives state-of-the-art\\nperformance in layout estimation and competitive results in 3D object\\ndetection. With that, we show a feasible path for enhancing the spatial\\nunderstanding capabilities of modern LLMs for applications in augmented\\nreality, embodied robotics, and more.', 'upvotes': 26, 'discussionId': '684799083ec10bdd8ab4de92', 'projectPage': 'https://manycore-research.github.io/SpatialLM', 'githubRepo': 'https://github.com/manycore-research/SpatialLM/', 'ai_summary': 'SpatialLM, a multimodal large language model, processes 3D point cloud data to generate structured scene understanding outputs, achieving state-of-the-art performance in layout estimation and competitive results in 3D object detection.', 'ai_keywords': ['large language model', '3D point cloud', 'structured 3D scene understanding', 'multimodal LLM', 'fine-tuning', 'synthetic dataset', 'ground-truth 3D annotations', 'layout estimation', '3D object detection', 'augmented reality', 'embodied robotics']}, 'publishedAt': '2025-06-09T03:10:58.000Z', 'title': 'SpatialLM: Training Large Language Models for Structured Indoor Modeling', 'summary': 'SpatialLM is a large language model designed to process 3D point cloud data\\nand generate structured 3D scene understanding outputs. These outputs include\\narchitectural elements like walls, doors, windows, and oriented object boxes\\nwith their semantic categories. Unlike previous methods which exploit\\ntask-specific network designs, our model adheres to the standard multimodal LLM\\narchitecture and is fine-tuned directly from open-source LLMs.\\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\\nground-truth 3D annotations, and conduct a careful study on various modeling\\nand training decisions. On public benchmarks, our model gives state-of-the-art\\nperformance in layout estimation and competitive results in 3D object\\ndetection. With that, we show a feasible path for enhancing the spatial\\nunderstanding capabilities of modern LLMs for applications in augmented\\nreality, embodied robotics, and more.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6437c0ead38ce48bdd4b0067/0ppuxf3I81w8mWqDgei0W.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07491.png', 'numComments': 1, 'submittedBy': {'_id': '6437c0ead38ce48bdd4b0067', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6437c0ead38ce48bdd4b0067/9HdcjbD0ugiPypLDtQl8E.png', 'fullname': 'Jia Zheng', 'name': 'bertjiazheng', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 15}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.07803', 'authors': [{'_id': '6847d4103ec10bdd8ab4dfb1', 'user': {'_id': '6437d0a951c7ebfc813c735b', 'avatarUrl': '/avatars/6cbac4e4be5029655702c5d8b9046b90.svg', 'isPro': False, 'fullname': 'Allakhverdov Eduard', 'user': 'combat-helicopter', 'type': 'user'}, 'name': 'Eduard Allakhverdov', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:42:38.549Z', 'hidden': False}, {'_id': '6847d4103ec10bdd8ab4dfb2', 'name': 'Dmitrii Tarasov', 'hidden': False}, {'_id': '6847d4103ec10bdd8ab4dfb3', 'name': 'Elizaveta Goncharova', 'hidden': False}, {'_id': '6847d4103ec10bdd8ab4dfb4', 'name': 'Andrey Kuznetsov', 'hidden': False}], 'publishedAt': '2025-06-09T14:32:18.000Z', 'submittedOnDailyAt': '2025-06-10T05:17:41.513Z', 'title': 'Image Reconstruction as a Tool for Feature Analysis', 'submittedOnDailyBy': {'_id': '6310ff34bc152fa3e810c186', 'avatarUrl': '/avatars/bfd63bcd81548283f5e496e3693bf143.svg', 'isPro': False, 'fullname': 'Elizaveta Goncharova', 'user': 'Elizaveta', 'type': 'user'}, 'summary': 'Vision encoders are increasingly used in modern applications, from\\nvision-only models to multimodal systems such as vision-language models.\\nDespite their remarkable success, it remains unclear how these architectures\\nrepresent features internally. Here, we propose a novel approach for\\ninterpreting vision features via image reconstruction. We compare two related\\nmodel families, SigLIP and SigLIP2, which differ only in their training\\nobjective, and show that encoders pre-trained on image-based tasks retain\\nsignificantly more image information than those trained on non-image tasks such\\nas contrastive learning. We further apply our method to a range of vision\\nencoders, ranking them by the informativeness of their feature representations.\\nFinally, we demonstrate that manipulating the feature space yields predictable\\nchanges in reconstructed images, revealing that orthogonal rotations (rather\\nthan spatial transformations) control color encoding. Our approach can be\\napplied to any vision encoder, shedding light on the inner structure of its\\nfeature space. The code and model weights to reproduce the experiments are\\navailable in GitHub.', 'upvotes': 20, 'discussionId': '6847d4103ec10bdd8ab4dfb5', 'projectPage': 'https://fusionbrainlab.github.io/feature_analysis/', 'githubRepo': 'https://github.com/FusionBrainLab/feature_analysis', 'ai_summary': 'Image reconstruction reveals that vision encoders retain more image information after image-based tasks and that orthogonal rotations in feature space control color encoding.', 'ai_keywords': ['SigLIP', 'SigLIP2', 'vision encoders', 'image reconstruction', 'contrastive learning', 'feature representations']}, 'publishedAt': '2025-06-09T10:32:18.000Z', 'title': 'Image Reconstruction as a Tool for Feature Analysis', 'summary': 'Vision encoders are increasingly used in modern applications, from\\nvision-only models to multimodal systems such as vision-language models.\\nDespite their remarkable success, it remains unclear how these architectures\\nrepresent features internally. Here, we propose a novel approach for\\ninterpreting vision features via image reconstruction. We compare two related\\nmodel families, SigLIP and SigLIP2, which differ only in their training\\nobjective, and show that encoders pre-trained on image-based tasks retain\\nsignificantly more image information than those trained on non-image tasks such\\nas contrastive learning. We further apply our method to a range of vision\\nencoders, ranking them by the informativeness of their feature representations.\\nFinally, we demonstrate that manipulating the feature space yields predictable\\nchanges in reconstructed images, revealing that orthogonal rotations (rather\\nthan spatial transformations) control color encoding. Our approach can be\\napplied to any vision encoder, shedding light on the inner structure of its\\nfeature space. The code and model weights to reproduce the experiments are\\navailable in GitHub.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07803.png', 'numComments': 1, 'submittedBy': {'_id': '6310ff34bc152fa3e810c186', 'avatarUrl': '/avatars/bfd63bcd81548283f5e496e3693bf143.svg', 'fullname': 'Elizaveta Goncharova', 'name': 'Elizaveta', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 8}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.06205', 'authors': [{'_id': '6846ca9b3ec10bdd8ab4dbf4', 'user': {'_id': '66727b038171db46e7f4f242', 'avatarUrl': '/avatars/3f8ab656006533fe1a24dd48c9b0a1b6.svg', 'isPro': False, 'fullname': 'sc', 'user': 'sc-bd', 'type': 'user'}, 'name': 'Sheng Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T09:21:08.452Z', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dbf5', 'name': 'Peiyu He', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dbf6', 'name': 'Jiaxin Hu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dbf7', 'name': 'Ziyang Liu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dbf8', 'name': 'Yansheng Wang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dbf9', 'name': 'Tao Xu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dbfa', 'name': 'Chi Zhang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dbfb', 'name': 'Chongchong Zhang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dbfc', 'name': 'Chao An', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dbfd', 'name': 'Shiyu Cai', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dbfe', 'name': 'Duo Cao', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dbff', 'name': 'Kangping Chen', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc00', 'name': 'Shuai Chu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc01', 'name': 'Tianwei Chu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc02', 'name': 'Mingdi Dan', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc03', 'name': 'Min Du', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc04', 'name': 'Weiwei Fang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc05', 'name': 'Pengyou Fu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc06', 'name': 'Junkai Hu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc07', 'name': 'Xiaowei Jiang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc08', 'name': 'Zhaodi Jiang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc09', 'name': 'Fuxuan Li', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc0a', 'name': 'Jun Li', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc0b', 'name': 'Minghui Li', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc0c', 'name': 'Mingyao Li', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc0d', 'name': 'Yanchang Li', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc0e', 'name': 'Zhibin Li', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc0f', 'name': 'Guangming Liu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc10', 'name': 'Kairui Liu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc11', 'name': 'Lihao Liu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc12', 'name': 'Weizhi Liu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc13', 'name': 'Xiaoshun Liu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc14', 'name': 'Yufei Liu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc15', 'name': 'Yunfei Liu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc16', 'name': 'Qiang Lu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc17', 'name': 'Yuanfei Luo', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc18', 'name': 'Xiang Lv', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc19', 'name': 'Hongying Ma', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc1a', 'name': 'Sai Ma', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc1b', 'name': 'Lingxian Mi', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc1c', 'name': 'Sha Sa', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc1d', 'name': 'Hongxiang Shu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc1e', 'name': 'Lei Tian', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc1f', 'name': 'Chengzhi Wang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc20', 'name': 'Jiayu Wang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc21', 'name': 'Kaijie Wang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc22', 'name': 'Qingyi Wang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc23', 'name': 'Renwen Wang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc24', 'name': 'Tao Wang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc25', 'name': 'Wei Wang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc26', 'name': 'Xirui Wang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc27', 'name': 'Chao Wei', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc28', 'name': 'Xuguang Wei', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc29', 'name': 'Zijun Xia', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc2a', 'name': 'Zhaohao Xiao', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc2b', 'name': 'Tingshuai Yan', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc2c', 'name': 'Liyan Yang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc2d', 'name': 'Yifan Yang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc2e', 'name': 'Zhikai Yang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc2f', 'name': 'Zhong Yin', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc30', 'name': 'Li Yuan', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc31', 'name': 'Liuchun Yuan', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc32', 'name': 'Chi Zhang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc33', 'name': 'Jinyang Zhang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc34', 'name': 'Junhui Zhang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc35', 'name': 'Linge Zhang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc36', 'name': 'Zhenyi Zhang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc37', 'name': 'Zheyu Zhang', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc38', 'name': 'Dongjie Zhu', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc39', 'name': 'Hang Li', 'hidden': False}, {'_id': '6846ca9b3ec10bdd8ab4dc3a', 'name': 'Yangang Zhang', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/66727b038171db46e7f4f242/nrwuIsn9tQaR75nAuym-R.mp4'], 'publishedAt': '2025-06-06T16:08:47.000Z', 'submittedOnDailyAt': '2025-06-10T07:53:05.305Z', 'title': 'Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal\\n  Learning', 'submittedOnDailyBy': {'_id': '66727b038171db46e7f4f242', 'avatarUrl': '/avatars/3f8ab656006533fe1a24dd48c9b0a1b6.svg', 'isPro': False, 'fullname': 'sc', 'user': 'sc-bd', 'type': 'user'}, 'summary': 'Modern robot navigation systems encounter difficulties in diverse and complex\\nindoor environments. Traditional approaches rely on multiple modules with small\\nmodels or rule-based systems and thus lack adaptability to new environments. To\\naddress this, we developed Astra, a comprehensive dual-model architecture,\\nAstra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a\\nmultimodal LLM, processes vision and language inputs to perform self and goal\\nlocalization using a hybrid topological-semantic graph as the global map, and\\noutperforms traditional visual place recognition methods. Astra-Local, a\\nmultitask network, handles local path planning and odometry estimation. Its 4D\\nspatial-temporal encoder, trained through self-supervised learning, generates\\nrobust 4D features for downstream tasks. The planning head utilizes flow\\nmatching and a novel masked ESDF loss to minimize collision risks for\\ngenerating local trajectories, and the odometry head integrates multi-sensor\\ninputs via a transformer encoder to predict the relative pose of the robot.\\nDeployed on real in-house mobile robots, Astra achieves high end-to-end mission\\nsuccess rate across diverse indoor environments.', 'upvotes': 20, 'discussionId': '6846ca9b3ec10bdd8ab4dc3b', 'ai_summary': 'Astra, a dual-model architecture for mobile robot navigation, uses a multimodal LLM for global localization and a multitask network for local path planning and odometry estimation, achieving high success rates in diverse indoor environments.', 'ai_keywords': ['LLM', 'self and goal localization', 'hybrid topological-semantic graph', 'multimodal LLM', 'multitask network', '4D spatial-temporal encoder', 'self-supervised learning', '4D features', 'flow matching', 'masked ESDF loss', 'local trajectories', 'transformer encoder', 'relative pose prediction']}, 'publishedAt': '2025-06-06T12:08:47.000Z', 'title': 'Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal\\n  Learning', 'summary': 'Modern robot navigation systems encounter difficulties in diverse and complex\\nindoor environments. Traditional approaches rely on multiple modules with small\\nmodels or rule-based systems and thus lack adaptability to new environments. To\\naddress this, we developed Astra, a comprehensive dual-model architecture,\\nAstra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a\\nmultimodal LLM, processes vision and language inputs to perform self and goal\\nlocalization using a hybrid topological-semantic graph as the global map, and\\noutperforms traditional visual place recognition methods. Astra-Local, a\\nmultitask network, handles local path planning and odometry estimation. Its 4D\\nspatial-temporal encoder, trained through self-supervised learning, generates\\nrobust 4D features for downstream tasks. The planning head utilizes flow\\nmatching and a novel masked ESDF loss to minimize collision risks for\\ngenerating local trajectories, and the odometry head integrates multi-sensor\\ninputs via a transformer encoder to predict the relative pose of the robot.\\nDeployed on real in-house mobile robots, Astra achieves high end-to-end mission\\nsuccess rate across diverse indoor environments.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/66727b038171db46e7f4f242/nrwuIsn9tQaR75nAuym-R.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06205.png', 'numComments': 1, 'submittedBy': {'_id': '66727b038171db46e7f4f242', 'avatarUrl': '/avatars/3f8ab656006533fe1a24dd48c9b0a1b6.svg', 'fullname': 'sc', 'name': 'sc-bd', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.07986', 'authors': [{'_id': '68479b0f3ec10bdd8ab4de94', 'name': 'Zhengyao Lv', 'hidden': False}, {'_id': '68479b0f3ec10bdd8ab4de95', 'user': {'_id': '66221f7a608fb4e7f46ecadb', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yMCsWyTkL4Xdxe18uwMYR.jpeg', 'isPro': False, 'fullname': 'Tianlin Pan', 'user': 'ldiex', 'type': 'user'}, 'name': 'Tianlin Pan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T11:20:57.479Z', 'hidden': False}, {'_id': '68479b0f3ec10bdd8ab4de96', 'user': {'_id': '635f8ed47c05eb9f59963d3a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg', 'isPro': False, 'fullname': 'ChenyangSi', 'user': 'ChenyangSi', 'type': 'user'}, 'name': 'Chenyang Si', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:43:05.835Z', 'hidden': False}, {'_id': '68479b0f3ec10bdd8ab4de97', 'name': 'Zhaoxi Chen', 'hidden': False}, {'_id': '68479b0f3ec10bdd8ab4de98', 'name': 'Wangmeng Zuo', 'hidden': False}, {'_id': '68479b0f3ec10bdd8ab4de99', 'name': 'Ziwei Liu', 'hidden': False}, {'_id': '68479b0f3ec10bdd8ab4de9a', 'name': 'Kwan-Yee K. Wong', 'hidden': False}], 'publishedAt': '2025-06-09T17:54:04.000Z', 'submittedOnDailyAt': '2025-06-10T01:15:51.787Z', 'title': 'Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers', 'submittedOnDailyBy': {'_id': '645aff5121ab438e732c47c1', 'avatarUrl': '/avatars/23b2a853139b0f2ae1fa88e2bd4e0056.svg', 'isPro': False, 'fullname': 'Zhengyao Lv', 'user': 'cszy98', 'type': 'user'}, 'summary': 'Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\\nlike FLUX struggle with achieving precise alignment between text prompts and\\ngenerated content. We identify two key issues in the attention mechanism of\\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\\nimbalance between visual and textual modalities and 2) the lack of\\ntimestep-aware attention weighting, which hinder the alignment. To address\\nthese issues, we propose Temperature-Adjusted Cross-modal Attention\\n(TACA), a parameter-efficient method that dynamically rebalances multimodal\\ninteractions through temperature scaling and timestep-dependent adjustment.\\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\\nits ability to improve image-text alignment in terms of object appearance,\\nattribute binding, and spatial relationships. Our findings highlight the\\nimportance of balancing cross-modal attention in improving semantic fidelity in\\ntext-to-image diffusion models. Our codes are publicly available at\\nhttps://github.com/Vchitect/TACA', 'upvotes': 13, 'discussionId': '68479b0f3ec10bdd8ab4de9b', 'projectPage': 'https://vchitect.github.io/TACA/', 'githubRepo': 'https://github.com/Vchitect/TACA', 'ai_summary': 'Temperature-Adjusted Cross-modal Attention (TACA) enhances text-image alignment in diffusion models by dynamically rebalancing multimodal interactions through temperature scaling and timestep-dependent adjustment.', 'ai_keywords': ['Temperature-Adjusted Cross-modal Attention', 'TACA', 'multimodal interactions', 'temperature scaling', 'timestep-dependent adjustment', 'FLUX', 'SD3.5', 'T2I-CompBench', 'semantic fidelity', 'text-to-image diffusion models']}, 'publishedAt': '2025-06-09T13:54:04.000Z', 'title': 'Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers', 'summary': 'Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\\nlike FLUX struggle with achieving precise alignment between text prompts and\\ngenerated content. We identify two key issues in the attention mechanism of\\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\\nimbalance between visual and textual modalities and 2) the lack of\\ntimestep-aware attention weighting, which hinder the alignment. To address\\nthese issues, we propose Temperature-Adjusted Cross-modal Attention\\n(TACA), a parameter-efficient method that dynamically rebalances multimodal\\ninteractions through temperature scaling and timestep-dependent adjustment.\\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\\nits ability to improve image-text alignment in terms of object appearance,\\nattribute binding, and spatial relationships. Our findings highlight the\\nimportance of balancing cross-modal attention in improving semantic fidelity in\\ntext-to-image diffusion models. Our codes are publicly available at\\nhttps://github.com/Vchitect/TACA', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07986.png', 'numComments': 1, 'submittedBy': {'_id': '645aff5121ab438e732c47c1', 'avatarUrl': '/avatars/23b2a853139b0f2ae1fa88e2bd4e0056.svg', 'fullname': 'Zhengyao Lv', 'name': 'cszy98', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.07553', 'authors': [{'_id': '684794a43ec10bdd8ab4de24', 'user': {'_id': '65eaa07cb6c760d77468b4b6', 'avatarUrl': '/avatars/4a1aae58986b40444351e0a167ca807c.svg', 'isPro': False, 'fullname': 'Jingchao Wang', 'user': 'jcwang0602', 'type': 'user'}, 'name': 'Jingchao Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:44:15.857Z', 'hidden': False}, {'_id': '684794a43ec10bdd8ab4de25', 'user': {'_id': '65fd45473ccf43503350d837', 'avatarUrl': '/avatars/11b9679945b3c89b142f0d62a312f362.svg', 'isPro': False, 'fullname': 'Haote Yang', 'user': 'Hoter', 'type': 'user'}, 'name': 'Haote Yang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:44:18.060Z', 'hidden': False}, {'_id': '684794a43ec10bdd8ab4de26', 'name': 'Jiang Wu', 'hidden': False}, {'_id': '684794a43ec10bdd8ab4de27', 'name': 'Yifan He', 'hidden': False}, {'_id': '684794a43ec10bdd8ab4de28', 'name': 'Xingjian Wei', 'hidden': False}, {'_id': '684794a43ec10bdd8ab4de29', 'name': 'Yinfan Wang', 'hidden': False}, {'_id': '684794a43ec10bdd8ab4de2a', 'name': 'Chengjin Liu', 'hidden': False}, {'_id': '684794a43ec10bdd8ab4de2b', 'name': 'Lingli Ge', 'hidden': False}, {'_id': '684794a43ec10bdd8ab4de2c', 'name': 'Lijun Wu', 'hidden': False}, {'_id': '684794a43ec10bdd8ab4de2d', 'name': 'Bin Wang', 'hidden': False}, {'_id': '684794a43ec10bdd8ab4de2e', 'name': 'Dahua Lin', 'hidden': False}, {'_id': '684794a43ec10bdd8ab4de2f', 'name': 'Conghui He', 'hidden': False}], 'publishedAt': '2025-06-09T08:47:10.000Z', 'submittedOnDailyAt': '2025-06-10T00:49:58.326Z', 'title': 'GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular\\n  Structure Recognition', 'submittedOnDailyBy': {'_id': '65fd45473ccf43503350d837', 'avatarUrl': '/avatars/11b9679945b3c89b142f0d62a312f362.svg', 'isPro': False, 'fullname': 'Haote Yang', 'user': 'Hoter', 'type': 'user'}, 'summary': \"Optical Chemical Structure Recognition (OCSR) is crucial for digitizing\\nchemical knowledge by converting molecular images into machine-readable\\nformats. While recent vision-language models (VLMs) have shown potential in\\nthis task, their image-captioning approach often struggles with complex\\nmolecular structures and inconsistent annotations. To overcome these\\nchallenges, we introduce GTR-Mol-VLM, a novel framework featuring two key\\ninnovations: (1) the Graph Traversal as Visual Chain of Thought\\nmechanism that emulates human reasoning by incrementally parsing molecular\\ngraphs through sequential atom-bond predictions, and (2) the data-centric\\nprinciple of Faithfully Recognize What You've Seen, which addresses\\nthe mismatch between abbreviated structures in images and their expanded\\nannotations. To support model development, we constructed GTR-CoT-1.3M, a\\nlarge-scale instruction-tuning dataset with meticulously corrected annotations,\\nand introduced MolRec-Bench, the first benchmark designed for a fine-grained\\nevaluation of graph-parsing accuracy in OCSR. Comprehensive experiments\\ndemonstrate that GTR-Mol-VLM achieves superior results compared to specialist\\nmodels, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in\\nscenarios involving molecular images with functional group abbreviations,\\nGTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage\\npoints, both in SMILES-based and graph-based metrics. We hope that this work\\nwill drive OCSR technology to more effectively meet real-world needs, thereby\\nadvancing the fields of cheminformatics and AI for Science. We will release\\nGTR-CoT at https://github.com/opendatalab/GTR-CoT.\", 'upvotes': 12, 'discussionId': '684794a43ec10bdd8ab4de30', 'ai_summary': 'GTR-Mol-VLM, featuring graph traversal and data-centric principles, outperforms existing models in Optical Chemical Structure Recognition by accurately parsing molecular graphs and handling abbreviated structures.', 'ai_keywords': ['Graph Traversal as Visual Chain of Thought', \"Faithfully Recognize What You've Seen\", 'GTR-CoT-1.3M', 'MolRec-Bench', 'graph-parsing accuracy', 'Optical Chemical Structure Recognition', 'VLMs', 'SMILES-based', 'graph-based metrics']}, 'publishedAt': '2025-06-09T04:47:10.000Z', 'title': 'GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular\\n  Structure Recognition', 'summary': \"Optical Chemical Structure Recognition (OCSR) is crucial for digitizing\\nchemical knowledge by converting molecular images into machine-readable\\nformats. While recent vision-language models (VLMs) have shown potential in\\nthis task, their image-captioning approach often struggles with complex\\nmolecular structures and inconsistent annotations. To overcome these\\nchallenges, we introduce GTR-Mol-VLM, a novel framework featuring two key\\ninnovations: (1) the Graph Traversal as Visual Chain of Thought\\nmechanism that emulates human reasoning by incrementally parsing molecular\\ngraphs through sequential atom-bond predictions, and (2) the data-centric\\nprinciple of Faithfully Recognize What You've Seen, which addresses\\nthe mismatch between abbreviated structures in images and their expanded\\nannotations. To support model development, we constructed GTR-CoT-1.3M, a\\nlarge-scale instruction-tuning dataset with meticulously corrected annotations,\\nand introduced MolRec-Bench, the first benchmark designed for a fine-grained\\nevaluation of graph-parsing accuracy in OCSR. Comprehensive experiments\\ndemonstrate that GTR-Mol-VLM achieves superior results compared to specialist\\nmodels, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in\\nscenarios involving molecular images with functional group abbreviations,\\nGTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage\\npoints, both in SMILES-based and graph-based metrics. We hope that this work\\nwill drive OCSR technology to more effectively meet real-world needs, thereby\\nadvancing the fields of cheminformatics and AI for Science. We will release\\nGTR-CoT at https://github.com/opendatalab/GTR-CoT.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07553.png', 'numComments': 1, 'submittedBy': {'_id': '65fd45473ccf43503350d837', 'avatarUrl': '/avatars/11b9679945b3c89b142f0d62a312f362.svg', 'fullname': 'Haote Yang', 'name': 'Hoter', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.07530', 'authors': [{'_id': '68478dae3ec10bdd8ab4dd9b', 'name': 'Hongyu Wang', 'hidden': False}, {'_id': '68478dae3ec10bdd8ab4dd9c', 'name': 'Chuyan Xiong', 'hidden': False}, {'_id': '68478dae3ec10bdd8ab4dd9d', 'name': 'Ruiping Wang', 'hidden': False}, {'_id': '68478dae3ec10bdd8ab4dd9e', 'name': 'Xilin Chen', 'hidden': False}], 'publishedAt': '2025-06-09T08:15:11.000Z', 'submittedOnDailyAt': '2025-06-10T00:14:07.356Z', 'title': 'BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation', 'submittedOnDailyBy': {'_id': '63f71771d36951307fcb4dcd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg', 'isPro': False, 'fullname': 'Hongyu Wang', 'user': 'hongyuw', 'type': 'user'}, 'summary': \"Vision-Language-Action (VLA) models have shown impressive capabilities across\\na wide range of robotics manipulation tasks. However, their growing model size\\nposes significant challenges for deployment on resource-constrained robotic\\nsystems. While 1-bit pretraining has proven effective for enhancing the\\ninference efficiency of large language models with minimal performance loss,\\nits application to VLA models remains underexplored. In this work, we present\\nBitVLA, the first 1-bit VLA model for robotics manipulation, in which every\\nparameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint\\nof the vision encoder, we propose the distillation-aware training strategy that\\ncompresses the full-precision encoder to 1.58-bit weights. During this process,\\na full-precision encoder serves as a teacher model to better align latent\\nrepresentations. Despite the lack of large-scale robotics pretraining, BitVLA\\nachieves performance comparable to the state-of-the-art model OpenVLA-OFT with\\n4-bit post-training quantization on the LIBERO benchmark, while consuming only\\n29.8% of the memory. These results highlight BitVLA's promise for deployment on\\nmemory-constrained edge devices. We release the code and model weights in\\nhttps://github.com/ustcwhy/BitVLA.\", 'upvotes': 12, 'discussionId': '68478dae3ec10bdd8ab4dd9f', 'githubRepo': 'https://github.com/ustcwhy/BitVLA', 'ai_summary': 'BitVLA, a 1-bit VLA model with ternary parameters, achieves comparable performance to OpenVLA-OFT on LIBERO while using 29.8% less memory through distillation-aware training.', 'ai_keywords': ['VLA models', '1-bit pretraining', 'ternary parameters', 'distillation-aware training', 'vision encoder', 'full-precision encoder', 'latent representations', 'memory footprint', 'robotics manipulation', 'OpenVLA-OFT', 'LIBERO benchmark', 'memory-constrained edge devices']}, 'publishedAt': '2025-06-09T04:15:11.000Z', 'title': 'BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation', 'summary': \"Vision-Language-Action (VLA) models have shown impressive capabilities across\\na wide range of robotics manipulation tasks. However, their growing model size\\nposes significant challenges for deployment on resource-constrained robotic\\nsystems. While 1-bit pretraining has proven effective for enhancing the\\ninference efficiency of large language models with minimal performance loss,\\nits application to VLA models remains underexplored. In this work, we present\\nBitVLA, the first 1-bit VLA model for robotics manipulation, in which every\\nparameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint\\nof the vision encoder, we propose the distillation-aware training strategy that\\ncompresses the full-precision encoder to 1.58-bit weights. During this process,\\na full-precision encoder serves as a teacher model to better align latent\\nrepresentations. Despite the lack of large-scale robotics pretraining, BitVLA\\nachieves performance comparable to the state-of-the-art model OpenVLA-OFT with\\n4-bit post-training quantization on the LIBERO benchmark, while consuming only\\n29.8% of the memory. These results highlight BitVLA's promise for deployment on\\nmemory-constrained edge devices. We release the code and model weights in\\nhttps://github.com/ustcwhy/BitVLA.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07530.png', 'numComments': 1, 'submittedBy': {'_id': '63f71771d36951307fcb4dcd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg', 'fullname': 'Hongyu Wang', 'name': 'hongyuw', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 18}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.07712', 'authors': [{'_id': '684790cd3ec10bdd8ab4ddaa', 'user': {'_id': '66dfb6bac93721c02f75f37e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zzoX0tpkHCNsFGjtnATSt.png', 'isPro': False, 'fullname': 'Renjie', 'user': 'RogerLos', 'type': 'user'}, 'name': 'Renjie Luo', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-10T01:56:30.026Z', 'hidden': False}, {'_id': '684790cd3ec10bdd8ab4ddab', 'name': 'Jiaxi Li', 'hidden': False}, {'_id': '684790cd3ec10bdd8ab4ddac', 'user': {'_id': '65d7b983baa72790a1151923', 'avatarUrl': '/avatars/938531e84ca01a0c5a2a174057e3e9c5.svg', 'isPro': False, 'fullname': 'Chen Huang', 'user': 'Albus-Chen', 'type': 'user'}, 'name': 'Chen Huang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T09:19:43.446Z', 'hidden': False}, {'_id': '684790cd3ec10bdd8ab4ddad', 'name': 'Wei Lu', 'hidden': False}], 'publishedAt': '2025-06-09T12:56:41.000Z', 'submittedOnDailyAt': '2025-06-10T00:30:07.286Z', 'title': 'Through the Valley: Path to Effective Long CoT Training for Small\\n  Language Models', 'submittedOnDailyBy': {'_id': '66dfb6bac93721c02f75f37e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zzoX0tpkHCNsFGjtnATSt.png', 'isPro': False, 'fullname': 'Renjie', 'user': 'RogerLos', 'type': 'user'}, 'summary': 'Long chain-of-thought (CoT) supervision has become a common strategy to\\nenhance reasoning in language models. While effective for large models, we\\nidentify a phenomenon we call Long CoT Degradation, in which small language\\nmodels (SLMs; <=3B parameters) trained on limited long CoT data experience\\nsignificant performance deterioration. Through extensive experiments on the\\nQwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is\\nwidespread across SLMs. In some settings, models trained on only 8k long CoT\\nexamples lose up to 75% of their original performance before fine-tuning.\\nStrikingly, we further observe that for some particularly small models, even\\ntraining on 220k long CoT examples fails to recover or surpass their original\\nperformance prior to fine-tuning. Our analysis attributes this effect to error\\naccumulation: while longer responses increase the capacity for multi-step\\nreasoning, they also amplify the risk of compounding mistakes. Furthermore, we\\nfind that Long CoT Degradation may negatively impacts downstream reinforcement\\nlearning (RL), although this can be alleviated by sufficiently scaled\\nsupervised fine-tuning (SFT). Our findings challenge common assumptions about\\nthe benefits of long CoT training for SLMs and offer practical guidance for\\nbuilding more effective small-scale reasoning models.', 'upvotes': 10, 'discussionId': '684790cd3ec10bdd8ab4ddae', 'ai_summary': 'Small language models experience significant performance declines when trained on long chain-of-thought data due to error accumulation, impacting downstream reinforcement learning but potentially mitigated by extensive supervised fine-tuning.', 'ai_keywords': ['Long chain-of-thought', 'Long CoT Degradation', 'small language models', 'SLMs', 'Qwen2.5', 'LLaMA3', 'Gemma3', 'error accumulation', 'supervised fine-tuning', 'SFT', 'reinforcement learning']}, 'publishedAt': '2025-06-09T08:56:41.000Z', 'title': 'Through the Valley: Path to Effective Long CoT Training for Small\\n  Language Models', 'summary': 'Long chain-of-thought (CoT) supervision has become a common strategy to\\nenhance reasoning in language models. While effective for large models, we\\nidentify a phenomenon we call Long CoT Degradation, in which small language\\nmodels (SLMs; <=3B parameters) trained on limited long CoT data experience\\nsignificant performance deterioration. Through extensive experiments on the\\nQwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is\\nwidespread across SLMs. In some settings, models trained on only 8k long CoT\\nexamples lose up to 75% of their original performance before fine-tuning.\\nStrikingly, we further observe that for some particularly small models, even\\ntraining on 220k long CoT examples fails to recover or surpass their original\\nperformance prior to fine-tuning. Our analysis attributes this effect to error\\naccumulation: while longer responses increase the capacity for multi-step\\nreasoning, they also amplify the risk of compounding mistakes. Furthermore, we\\nfind that Long CoT Degradation may negatively impacts downstream reinforcement\\nlearning (RL), although this can be alleviated by sufficiently scaled\\nsupervised fine-tuning (SFT). Our findings challenge common assumptions about\\nthe benefits of long CoT training for SLMs and offer practical guidance for\\nbuilding more effective small-scale reasoning models.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07712.png', 'numComments': 1, 'submittedBy': {'_id': '66dfb6bac93721c02f75f37e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zzoX0tpkHCNsFGjtnATSt.png', 'fullname': 'Renjie', 'name': 'RogerLos', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.07298', 'authors': [{'_id': '684795e83ec10bdd8ab4de6a', 'user': {'_id': '62de9e6fdcdc9043efa8b756', 'avatarUrl': '/avatars/c26974c740633d143f7382f0858ea99a.svg', 'isPro': False, 'fullname': 'Yijia Dai', 'user': 'DaiYijia', 'type': 'user'}, 'name': 'Yijia Dai', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:43:53.562Z', 'hidden': False}, {'_id': '684795e83ec10bdd8ab4de6b', 'name': 'Zhaolin Gao', 'hidden': False}, {'_id': '684795e83ec10bdd8ab4de6c', 'name': 'Yahya Satter', 'hidden': False}, {'_id': '684795e83ec10bdd8ab4de6d', 'user': {'_id': '664f92095a60ca2484b90d7a', 'avatarUrl': '/avatars/3232bb702ed479ac821b7a5dfb457d0b.svg', 'isPro': False, 'fullname': 'Sarah Dean', 'user': 'sarahdean', 'type': 'user'}, 'name': 'Sarah Dean', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-06-10T10:25:24.393Z', 'hidden': False}, {'_id': '684795e83ec10bdd8ab4de6e', 'name': 'Jennifer J. Sun', 'hidden': False}], 'publishedAt': '2025-06-08T21:49:38.000Z', 'submittedOnDailyAt': '2025-06-10T01:00:39.516Z', 'title': 'Pre-trained Large Language Models Learn Hidden Markov Models In-context', 'submittedOnDailyBy': {'_id': '652eec0aabc673c4204c459e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/652eec0aabc673c4204c459e/9otSQFP8G3S8zarR1Y5rE.jpeg', 'isPro': False, 'fullname': 'Zhaolin Gao', 'user': 'GitBag', 'type': 'user'}, 'summary': 'Hidden Markov Models (HMMs) are foundational tools for modeling sequential\\ndata with latent Markovian structure, yet fitting them to real-world data\\nremains computationally challenging. In this work, we show that pre-trained\\nlarge language models (LLMs) can effectively model data generated by HMMs via\\nin-context learning (ICL)x2013their ability to infer patterns from\\nexamples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve\\npredictive accuracy approaching the theoretical optimum. We uncover novel\\nscaling trends influenced by HMM properties, and offer theoretical conjectures\\nfor these empirical observations. We also provide practical guidelines for\\nscientists on using ICL as a diagnostic tool for complex data. On real-world\\nanimal decision-making tasks, ICL achieves competitive performance with models\\ndesigned by human experts. To our knowledge, this is the first demonstration\\nthat ICL can learn and predict HMM-generated sequencesx2013an\\nadvance that deepens our understanding of in-context learning in LLMs and\\nestablishes its potential as a powerful tool for uncovering hidden structure in\\ncomplex scientific data.', 'upvotes': 10, 'discussionId': '684795e93ec10bdd8ab4de6f', 'githubRepo': 'https://github.com/DaiYijia02/icl-hmm', 'ai_summary': 'In-context learning in large language models can effectively model sequences generated by hidden Markov models, achieving predictive accuracy and uncovering scaling trends, thus demonstrating its potential as a diagnostic tool for complex scientific data.', 'ai_keywords': ['hidden Markov models', 'HMMs', 'large language models', 'LLMs', 'in-context learning', 'IC', 'predictive accuracy', 'theoretical optimum', 'synthetic HMMs', 'scaling trends', 'empirical observations', 'animal decision-making tasks', 'human experts']}, 'publishedAt': '2025-06-08T17:49:38.000Z', 'title': 'Pre-trained Large Language Models Learn Hidden Markov Models In-context', 'summary': 'Hidden Markov Models (HMMs) are foundational tools for modeling sequential\\ndata with latent Markovian structure, yet fitting them to real-world data\\nremains computationally challenging. In this work, we show that pre-trained\\nlarge language models (LLMs) can effectively model data generated by HMMs via\\nin-context learning (ICL)x2013their ability to infer patterns from\\nexamples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve\\npredictive accuracy approaching the theoretical optimum. We uncover novel\\nscaling trends influenced by HMM properties, and offer theoretical conjectures\\nfor these empirical observations. We also provide practical guidelines for\\nscientists on using ICL as a diagnostic tool for complex data. On real-world\\nanimal decision-making tasks, ICL achieves competitive performance with models\\ndesigned by human experts. To our knowledge, this is the first demonstration\\nthat ICL can learn and predict HMM-generated sequencesx2013an\\nadvance that deepens our understanding of in-context learning in LLMs and\\nestablishes its potential as a powerful tool for uncovering hidden structure in\\ncomplex scientific data.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07298.png', 'numComments': 2, 'submittedBy': {'_id': '652eec0aabc673c4204c459e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/652eec0aabc673c4204c459e/9otSQFP8G3S8zarR1Y5rE.jpeg', 'fullname': 'Zhaolin Gao', 'name': 'GitBag', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.06941', 'authors': [{'_id': '684797863ec10bdd8ab4de72', 'user': {'_id': '6520621836008ecc88699622', 'avatarUrl': '/avatars/b08c00af00f1736a4f4938443e575b0e.svg', 'isPro': False, 'fullname': 'Parshin Shojaee', 'user': 'parshinsh', 'type': 'user'}, 'name': 'Parshin Shojaee', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:43:32.697Z', 'hidden': False}, {'_id': '684797863ec10bdd8ab4de73', 'name': 'Iman Mirzadeh', 'hidden': False}, {'_id': '684797863ec10bdd8ab4de74', 'name': 'Keivan Alizadeh', 'hidden': False}, {'_id': '684797863ec10bdd8ab4de75', 'name': 'Maxwell Horton', 'hidden': False}, {'_id': '684797863ec10bdd8ab4de76', 'name': 'Samy Bengio', 'hidden': False}, {'_id': '684797863ec10bdd8ab4de77', 'name': 'Mehrdad Farajtabar', 'hidden': False}], 'publishedAt': '2025-06-07T22:42:29.000Z', 'submittedOnDailyAt': '2025-06-10T01:00:21.500Z', 'title': 'The Illusion of Thinking: Understanding the Strengths and Limitations of\\n  Reasoning Models via the Lens of Problem Complexity', 'submittedOnDailyBy': {'_id': '6520621836008ecc88699622', 'avatarUrl': '/avatars/b08c00af00f1736a4f4938443e575b0e.svg', 'isPro': False, 'fullname': 'Parshin Shojaee', 'user': 'parshinsh', 'type': 'user'}, 'summary': \"Recent generations of language models have introduced Large Reasoning Models\\n(LRMs) that generate detailed thinking processes before providing answers.\\nWhile these models demonstrate improved performance on reasoning benchmarks,\\ntheir fundamental capabilities, scaling properties, and limitations remain\\ninsufficiently understood. Current evaluations primarily focus on established\\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\\nevaluation paradigm often suffers from contamination and does not provide\\ninsights into the reasoning traces. In this work, we systematically investigate\\nthese gaps with the help of controllable puzzle environments that allow precise\\nmanipulation of complexity while maintaining consistent logical structures.\\nThis setup enables the analysis of not only final answers but also the internal\\nreasoning traces, offering insights into how LRMs think. Through extensive\\nexperiments, we show that LRMs face a complete accuracy collapse beyond certain\\ncomplexities. Moreover, they exhibit a counterintuitive scaling limit: their\\nreasoning effort increases with problem complexity up to a point, then declines\\ndespite having remaining token budget. By comparing LRMs with their standard\\nLLM counterparts under same inference compute, we identify three performance\\nregimes: (1) low-complexity tasks where standard models outperform LRMs, (2)\\nmedium-complexity tasks where LRMs demonstrates advantage, and (3)\\nhigh-complexity tasks where both models face complete collapse. We found that\\nLRMs have limitations in exact computation: they fail to use explicit\\nalgorithms and reason inconsistently across scales. We also investigate the\\nreasoning traces in more depth, studying the patterns of explored solutions and\\nanalyzing the models' computational behavior, shedding light on their\\nstrengths, limitations, and raising questions about their reasoning\\ncapabilities.\", 'upvotes': 9, 'discussionId': '684797863ec10bdd8ab4de78', 'ai_summary': 'Large Reasoning Models (LRMs) exhibit varying performance across task complexities, with limitations in exact computation and inconsistent reasoning, as assessed using controllable puzzle environments.', 'ai_keywords': ['Large Reasoning Models', 'LRMs', 'controllable puzzle environments', 'reasoning traces', 'standard LLMs', 'performance regimes', 'exact computation', 'reasoning capabilities']}, 'publishedAt': '2025-06-07T18:42:29.000Z', 'title': 'The Illusion of Thinking: Understanding the Strengths and Limitations of\\n  Reasoning Models via the Lens of Problem Complexity', 'summary': \"Recent generations of language models have introduced Large Reasoning Models\\n(LRMs) that generate detailed thinking processes before providing answers.\\nWhile these models demonstrate improved performance on reasoning benchmarks,\\ntheir fundamental capabilities, scaling properties, and limitations remain\\ninsufficiently understood. Current evaluations primarily focus on established\\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\\nevaluation paradigm often suffers from contamination and does not provide\\ninsights into the reasoning traces. In this work, we systematically investigate\\nthese gaps with the help of controllable puzzle environments that allow precise\\nmanipulation of complexity while maintaining consistent logical structures.\\nThis setup enables the analysis of not only final answers but also the internal\\nreasoning traces, offering insights into how LRMs think. Through extensive\\nexperiments, we show that LRMs face a complete accuracy collapse beyond certain\\ncomplexities. Moreover, they exhibit a counterintuitive scaling limit: their\\nreasoning effort increases with problem complexity up to a point, then declines\\ndespite having remaining token budget. By comparing LRMs with their standard\\nLLM counterparts under same inference compute, we identify three performance\\nregimes: (1) low-complexity tasks where standard models outperform LRMs, (2)\\nmedium-complexity tasks where LRMs demonstrates advantage, and (3)\\nhigh-complexity tasks where both models face complete collapse. We found that\\nLRMs have limitations in exact computation: they fail to use explicit\\nalgorithms and reason inconsistently across scales. We also investigate the\\nreasoning traces in more depth, studying the patterns of explored solutions and\\nanalyzing the models' computational behavior, shedding light on their\\nstrengths, limitations, and raising questions about their reasoning\\ncapabilities.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06941.png', 'numComments': 1, 'submittedBy': {'_id': '6520621836008ecc88699622', 'avatarUrl': '/avatars/b08c00af00f1736a4f4938443e575b0e.svg', 'fullname': 'Parshin Shojaee', 'name': 'parshinsh', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.06006', 'authors': [{'_id': '68480800308cb7e626e80e20', 'user': {'_id': '64686434f43574d9556b1fa6', 'avatarUrl': '/avatars/64183d643cfc3b274714a6167c354e39.svg', 'isPro': False, 'fullname': 'Yifu Qiu', 'user': 'yfqiu-nlp', 'type': 'user'}, 'name': 'Yifu Qiu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T10:59:54.843Z', 'hidden': False}, {'_id': '68480800308cb7e626e80e21', 'name': 'Yftah Ziser', 'hidden': False}, {'_id': '68480800308cb7e626e80e22', 'name': 'Anna Korhonen', 'hidden': False}, {'_id': '68480800308cb7e626e80e23', 'name': 'Shay B. Cohen', 'hidden': False}, {'_id': '68480800308cb7e626e80e24', 'user': {'_id': '60809ad44ad99100d63ce36a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1619040921084-noauth.jpeg', 'isPro': False, 'fullname': 'Edoardo Maria Ponti', 'user': 'ducdauge', 'type': 'user'}, 'name': 'Edoardo M. Ponti', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T10:59:52.211Z', 'hidden': False}], 'publishedAt': '2025-06-06T11:50:18.000Z', 'submittedOnDailyAt': '2025-06-10T09:02:07.285Z', 'title': 'Bootstrapping World Models from Dynamics Models in Multimodal Foundation\\n  Models', 'submittedOnDailyBy': {'_id': '60809ad44ad99100d63ce36a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1619040921084-noauth.jpeg', 'isPro': False, 'fullname': 'Edoardo Maria Ponti', 'user': 'ducdauge', 'type': 'user'}, 'summary': 'To what extent do vision-and-language foundation models possess a realistic\\nworld model (observation times action rightarrow observation) and a\\ndynamics model (observation times observation rightarrow action), when\\nactions are expressed through language? While open-source foundation models\\nstruggle with both, we find that fine-tuning them to acquire a dynamics model\\nthrough supervision is significantly easier than acquiring a world model. In\\nturn, dynamics models can be used to bootstrap world models through two main\\nstrategies: 1) weakly supervised learning from synthetic data and 2) inference\\ntime verification. Firstly, the dynamics model can annotate actions for\\nunlabelled pairs of video frame observations to expand the training data. We\\nfurther propose a new objective, where image tokens in observation pairs are\\nweighted by their importance, as predicted by a recognition model. Secondly,\\nthe dynamics models can assign rewards to multiple samples of the world model\\nto score them, effectively guiding search at inference time. We evaluate the\\nworld models resulting from both strategies through the task of action-centric\\nimage editing on Aurora-Bench. Our best model achieves a performance\\ncompetitive with state-of-the-art image editing models, improving on them by a\\nmargin of 15% on real-world subsets according to GPT4o-as-judge, and\\nachieving the best average human evaluation across all subsets of Aurora-Bench.', 'upvotes': 9, 'discussionId': '68480800308cb7e626e80e25', 'ai_summary': 'Foundation models can be fine-tuned to develop dynamics models more easily than world models, with dynamics models aiding world model development through weak supervision and inference time verification, leading to state-of-the-art performance in action-centric image editing.', 'ai_keywords': ['vision-and-language foundation models', 'realistic world model', 'dynamics model', 'fine-tuning', 'weakly supervised learning', 'synthetic data', 'inference time verification', 'image tokens', 'recognition model', 'action-centric image editing', 'Aurora-Bench', 'GPT4o-as-judge']}, 'publishedAt': '2025-06-06T07:50:18.000Z', 'title': 'Bootstrapping World Models from Dynamics Models in Multimodal Foundation\\n  Models', 'summary': 'To what extent do vision-and-language foundation models possess a realistic\\nworld model (observation times action rightarrow observation) and a\\ndynamics model (observation times observation rightarrow action), when\\nactions are expressed through language? While open-source foundation models\\nstruggle with both, we find that fine-tuning them to acquire a dynamics model\\nthrough supervision is significantly easier than acquiring a world model. In\\nturn, dynamics models can be used to bootstrap world models through two main\\nstrategies: 1) weakly supervised learning from synthetic data and 2) inference\\ntime verification. Firstly, the dynamics model can annotate actions for\\nunlabelled pairs of video frame observations to expand the training data. We\\nfurther propose a new objective, where image tokens in observation pairs are\\nweighted by their importance, as predicted by a recognition model. Secondly,\\nthe dynamics models can assign rewards to multiple samples of the world model\\nto score them, effectively guiding search at inference time. We evaluate the\\nworld models resulting from both strategies through the task of action-centric\\nimage editing on Aurora-Bench. Our best model achieves a performance\\ncompetitive with state-of-the-art image editing models, improving on them by a\\nmargin of 15% on real-world subsets according to GPT4o-as-judge, and\\nachieving the best average human evaluation across all subsets of Aurora-Bench.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06006.png', 'numComments': 1, 'submittedBy': {'_id': '60809ad44ad99100d63ce36a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1619040921084-noauth.jpeg', 'fullname': 'Edoardo Maria Ponti', 'name': 'ducdauge', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.05062', 'authors': [{'_id': '684269c12be20aeb4aa81708', 'user': {'_id': '63e8914accae1fe5c6176a55', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63e8914accae1fe5c6176a55/_gDjcqGKWKoSZrHSEIlAw.jpeg', 'isPro': False, 'fullname': 'Noy Sternlicht', 'user': 'noystl', 'type': 'user'}, 'name': 'Noy Sternlicht', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-06T07:40:27.279Z', 'hidden': False}, {'_id': '684269c12be20aeb4aa81709', 'name': 'Ariel Gera', 'hidden': False}, {'_id': '684269c12be20aeb4aa8170a', 'name': 'Roy Bar-Haim', 'hidden': False}, {'_id': '684269c12be20aeb4aa8170b', 'name': 'Tom Hope', 'hidden': False}, {'_id': '684269c12be20aeb4aa8170c', 'name': 'Noam Slonim', 'hidden': False}], 'publishedAt': '2025-06-05T14:06:51.000Z', 'submittedOnDailyAt': '2025-06-10T11:14:18.249Z', 'title': 'Debatable Intelligence: Benchmarking LLM Judges via Debate Speech\\n  Evaluation', 'submittedOnDailyBy': {'_id': '63e8914accae1fe5c6176a55', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63e8914accae1fe5c6176a55/_gDjcqGKWKoSZrHSEIlAw.jpeg', 'isPro': False, 'fullname': 'Noy Sternlicht', 'user': 'noystl', 'type': 'user'}, 'summary': 'We introduce Debate Speech Evaluation as a novel and challenging benchmark\\nfor assessing LLM judges. Evaluating debate speeches requires a deep\\nunderstanding of the speech at multiple levels, including argument strength and\\nrelevance, the coherence and organization of the speech, the appropriateness of\\nits style and tone, and so on. This task involves a unique set of cognitive\\nabilities that have previously received limited attention in systematic LLM\\nbenchmarking. To explore such skills, we leverage a dataset of over 600\\nmeticulously annotated debate speeches and present the first in-depth analysis\\nof how state-of-the-art LLMs compare to human judges on this task. Our findings\\nreveal a nuanced picture: while larger models can approximate individual human\\njudgments in some respects, they differ substantially in their overall judgment\\nbehavior. We also investigate the ability of frontier LLMs to generate\\npersuasive, opinionated speeches, showing that models may perform at a human\\nlevel on this task.', 'upvotes': 9, 'discussionId': '684269c22be20aeb4aa81737', 'projectPage': 'https://noy-sternlicht.github.io/Debatable-Intelligence-Web/', 'githubRepo': 'https://github.com/noy-sternlicht/Debatable-Intelligence', 'ai_summary': 'This study evaluates the performance of large language models in assessing debate speeches, a task requiring deep understanding of various aspects of speech, including argument strength and coherence, and compares it to human judges.', 'ai_keywords': ['LLM', 'large language models', 'debate speech evaluation', 'argument strength', 'speech coherence', 'opinionated speech generation']}, 'publishedAt': '2025-06-05T10:06:51.000Z', 'title': 'Debatable Intelligence: Benchmarking LLM Judges via Debate Speech\\n  Evaluation', 'summary': 'We introduce Debate Speech Evaluation as a novel and challenging benchmark\\nfor assessing LLM judges. Evaluating debate speeches requires a deep\\nunderstanding of the speech at multiple levels, including argument strength and\\nrelevance, the coherence and organization of the speech, the appropriateness of\\nits style and tone, and so on. This task involves a unique set of cognitive\\nabilities that have previously received limited attention in systematic LLM\\nbenchmarking. To explore such skills, we leverage a dataset of over 600\\nmeticulously annotated debate speeches and present the first in-depth analysis\\nof how state-of-the-art LLMs compare to human judges on this task. Our findings\\nreveal a nuanced picture: while larger models can approximate individual human\\njudgments in some respects, they differ substantially in their overall judgment\\nbehavior. We also investigate the ability of frontier LLMs to generate\\npersuasive, opinionated speeches, showing that models may perform at a human\\nlevel on this task.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05062.png', 'numComments': 1, 'submittedBy': {'_id': '63e8914accae1fe5c6176a55', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63e8914accae1fe5c6176a55/_gDjcqGKWKoSZrHSEIlAw.jpeg', 'fullname': 'Noy Sternlicht', 'name': 'noystl', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.07463', 'authors': [{'_id': '68478a493ec10bdd8ab4dd90', 'user': {'_id': '632c234f42c386ebd2710434', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg', 'isPro': False, 'fullname': 'Guang Liu', 'user': 'ZacLiu', 'type': 'user'}, 'name': 'Guang Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T09:19:47.891Z', 'hidden': False}, {'_id': '68478a493ec10bdd8ab4dd91', 'user': {'_id': '63a11ce02fabbbb899a01d58', 'avatarUrl': '/avatars/ee3d4088b6d32b2c18b8be91913e90dd.svg', 'isPro': False, 'fullname': 'ldwang', 'user': 'ldwang', 'type': 'user'}, 'name': 'Liangdong Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T09:19:50.398Z', 'hidden': False}, {'_id': '68478a493ec10bdd8ab4dd92', 'name': 'Jijie Li', 'hidden': False}, {'_id': '68478a493ec10bdd8ab4dd93', 'name': 'Yang Yu', 'hidden': False}, {'_id': '68478a493ec10bdd8ab4dd94', 'name': 'Yao Xu', 'hidden': False}, {'_id': '68478a493ec10bdd8ab4dd95', 'name': 'Jiabei Chen', 'hidden': False}, {'_id': '68478a493ec10bdd8ab4dd96', 'name': 'Yu Bai', 'hidden': False}, {'_id': '68478a493ec10bdd8ab4dd97', 'name': 'Feng Liao', 'hidden': False}, {'_id': '68478a493ec10bdd8ab4dd98', 'user': {'_id': '629aa3155ab4232a3fe0893e', 'avatarUrl': '/avatars/cf2d4a9295b5da9e2e4d2278bbb36040.svg', 'isPro': False, 'fullname': 'Yonghua Lin', 'user': 'Yonghua', 'type': 'user'}, 'name': 'Yonghua Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-06-10T09:39:44.976Z', 'hidden': False}], 'publishedAt': '2025-06-09T06:14:19.000Z', 'submittedOnDailyAt': '2025-06-10T01:15:00.564Z', 'title': 'CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large\\n  Language Models', 'submittedOnDailyBy': {'_id': '632c234f42c386ebd2710434', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg', 'isPro': False, 'fullname': 'Guang Liu', 'user': 'ZacLiu', 'type': 'user'}, 'summary': 'We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered\\nfor superior data quality and diverse human-like reasoning trajectory. CCI4.0\\noccupies roughly 35 TB of disk space and comprises two sub-datasets:\\nCCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a 5.2 TB carefully\\ncurated Chinese web corpus, a 22.5 TB English subset from Nemotron-CC, and\\ndiverse sources from math, wiki, arxiv, and code. Although these data are\\nmostly sourced from well-processed datasets, the quality standards of various\\ndomains are dynamic and require extensive expert experience and labor to\\nprocess. So, we propose a novel pipeline justifying data quality mainly based\\non models through two-stage deduplication, multiclassifier quality scoring, and\\ndomain-aware fluency filtering. We extract 4.5 billion pieces of\\nCoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the\\ndistillation of CoT from larger models, our proposed staged CoT extraction\\nexemplifies diverse reasoning patterns and significantly decreases the\\npossibility of hallucination. Empirical evaluations demonstrate that LLMs\\npre-trained in CCI4.0 benefit from cleaner, more reliable training signals,\\nyielding consistent improvements in downstream tasks, especially in math and\\ncode reflection tasks. Our results underscore the critical role of rigorous\\ndata curation and human thinking templates in advancing LLM performance,\\nshedding some light on automatically processing pretraining corpora.', 'upvotes': 8, 'discussionId': '68478a493ec10bdd8ab4dd99', 'projectPage': 'https://openseek.baai.ac.cn/', 'githubRepo': 'https://github.com/FlagAI-Open/OpenSeek', 'ai_summary': 'A large-scale bilingual pre-training dataset, CCI4.0, enhances data quality and diverse reasoning patterns for language models, leading to improved performance in downstream tasks like math and code reflection.', 'ai_keywords': ['pre-training dataset', 'bilingual pre-training', 'data quality', 'reasoning trajectory', 'deduplication', 'multiclassifier quality scoring', 'domain-aware fluency filtering', 'Chain-of-Thought', 'CoT extraction', 'language models', 'LLMs', 'downstream tasks', 'math tasks', 'code reflection tasks', 'data curation', 'human thinking templates']}, 'publishedAt': '2025-06-09T02:14:19.000Z', 'title': 'CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large\\n  Language Models', 'summary': 'We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered\\nfor superior data quality and diverse human-like reasoning trajectory. CCI4.0\\noccupies roughly 35 TB of disk space and comprises two sub-datasets:\\nCCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a 5.2 TB carefully\\ncurated Chinese web corpus, a 22.5 TB English subset from Nemotron-CC, and\\ndiverse sources from math, wiki, arxiv, and code. Although these data are\\nmostly sourced from well-processed datasets, the quality standards of various\\ndomains are dynamic and require extensive expert experience and labor to\\nprocess. So, we propose a novel pipeline justifying data quality mainly based\\non models through two-stage deduplication, multiclassifier quality scoring, and\\ndomain-aware fluency filtering. We extract 4.5 billion pieces of\\nCoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the\\ndistillation of CoT from larger models, our proposed staged CoT extraction\\nexemplifies diverse reasoning patterns and significantly decreases the\\npossibility of hallucination. Empirical evaluations demonstrate that LLMs\\npre-trained in CCI4.0 benefit from cleaner, more reliable training signals,\\nyielding consistent improvements in downstream tasks, especially in math and\\ncode reflection tasks. Our results underscore the critical role of rigorous\\ndata curation and human thinking templates in advancing LLM performance,\\nshedding some light on automatically processing pretraining corpora.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07463.png', 'numComments': 1, 'submittedBy': {'_id': '632c234f42c386ebd2710434', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg', 'fullname': 'Guang Liu', 'name': 'ZacLiu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 8}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.07309', 'authors': [{'_id': '6847b8793ec10bdd8ab4df4f', 'user': {'_id': '67f42bd98752b56bd349a9db', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_1jLlx7qqMDYb2ylZ2TCa.png', 'isPro': False, 'fullname': 'Yin Huang', 'user': 'MaggieHuang', 'type': 'user'}, 'name': 'Yin Huang', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-10T04:45:46.729Z', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df50', 'name': 'Yifan Ethan Xu', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df51', 'name': 'Kai Sun', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df52', 'name': 'Vera Yan', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df53', 'name': 'Alicia Sun', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df54', 'name': 'Haidar Khan', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df55', 'name': 'Jimmy Nguyen', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df56', 'name': 'Mohammad Kachuee', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df57', 'name': 'Zhaojiang Lin', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df58', 'name': 'Yue Liu', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df59', 'name': 'Aaron Colak', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df5a', 'name': 'Anuj Kumar', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df5b', 'name': 'Wen-tau Yih', 'hidden': False}, {'_id': '6847b8793ec10bdd8ab4df5c', 'name': 'Xin Luna Dong', 'hidden': False}], 'publishedAt': '2025-06-08T22:51:46.000Z', 'submittedOnDailyAt': '2025-06-10T03:17:46.849Z', 'title': 'ConfQA: Answer Only If You Are Confident', 'submittedOnDailyBy': {'_id': '67f42bd98752b56bd349a9db', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_1jLlx7qqMDYb2ylZ2TCa.png', 'isPro': False, 'fullname': 'Yin Huang', 'user': 'MaggieHuang', 'type': 'user'}, 'summary': 'Can we teach Large Language Models (LLMs) to refrain from hallucinating\\nfactual statements? In this paper we present a fine-tuning strategy that we\\ncall ConfQA, which can reduce hallucination rate from 20-40% to under 5% across\\nmultiple factuality benchmarks. The core idea is simple: when the LLM answers a\\nquestion correctly, it is trained to continue with the answer; otherwise, it is\\ntrained to admit \"I am unsure\". But there are two key factors that make the\\ntraining highly effective. First, we introduce a dampening prompt \"answer only\\nif you are confident\" to explicitly guide the behavior, without which\\nhallucination remains high as 15%-25%. Second, we leverage simple factual\\nstatements, specifically attribute values from knowledge graphs, to help LLMs\\ncalibrate the confidence, resulting in robust generalization across domains and\\nquestion types. Building on this insight, we propose the Dual Neural Knowledge\\nframework, which seamlessly select between internally parameterized neural\\nknowledge and externally recorded symbolic knowledge based on ConfQA\\'s\\nconfidence. The framework enables potential accuracy gains to beyond 95%, while\\nreducing unnecessary external retrievals by over 30%.', 'upvotes': 8, 'discussionId': '6847b87a3ec10bdd8ab4df5d', 'ai_summary': 'ConfQA fine-tuning strategy reduces factual statement hallucination in LLMs by 80%, using a dampening prompt and factual statements from knowledge graphs to improve confidence calibration and knowledge selection.', 'ai_keywords': ['Large Language Models', 'LLMs', 'fine-tuning', 'ConfQA', 'hallucination', 'factuality benchmarks', 'dampening prompt', 'factual statements', 'knowledge graphs', 'confidence calibration', 'Dual Neural Knowledge framework', 'neural knowledge', 'symbolic knowledge', 'accuracy gains', 'external retrievals']}, 'publishedAt': '2025-06-08T18:51:46.000Z', 'title': 'ConfQA: Answer Only If You Are Confident', 'summary': 'Can we teach Large Language Models (LLMs) to refrain from hallucinating\\nfactual statements? In this paper we present a fine-tuning strategy that we\\ncall ConfQA, which can reduce hallucination rate from 20-40% to under 5% across\\nmultiple factuality benchmarks. The core idea is simple: when the LLM answers a\\nquestion correctly, it is trained to continue with the answer; otherwise, it is\\ntrained to admit \"I am unsure\". But there are two key factors that make the\\ntraining highly effective. First, we introduce a dampening prompt \"answer only\\nif you are confident\" to explicitly guide the behavior, without which\\nhallucination remains high as 15%-25%. Second, we leverage simple factual\\nstatements, specifically attribute values from knowledge graphs, to help LLMs\\ncalibrate the confidence, resulting in robust generalization across domains and\\nquestion types. Building on this insight, we propose the Dual Neural Knowledge\\nframework, which seamlessly select between internally parameterized neural\\nknowledge and externally recorded symbolic knowledge based on ConfQA\\'s\\nconfidence. The framework enables potential accuracy gains to beyond 95%, while\\nreducing unnecessary external retrievals by over 30%.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07309.png', 'numComments': 1, 'submittedBy': {'_id': '67f42bd98752b56bd349a9db', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_1jLlx7qqMDYb2ylZ2TCa.png', 'fullname': 'Yin Huang', 'name': 'MaggieHuang', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.08012', 'authors': [{'_id': '684791b63ec10bdd8ab4ddb1', 'name': 'Penghao Wu', 'hidden': False}, {'_id': '684791b63ec10bdd8ab4ddb2', 'name': 'Shengnan Ma', 'hidden': False}, {'_id': '684791b63ec10bdd8ab4ddb3', 'name': 'Bo Wang', 'hidden': False}, {'_id': '684791b63ec10bdd8ab4ddb4', 'name': 'Jiaheng Yu', 'hidden': False}, {'_id': '684791b63ec10bdd8ab4ddb5', 'name': 'Lewei Lu', 'hidden': False}, {'_id': '684791b63ec10bdd8ab4ddb6', 'name': 'Ziwei Liu', 'hidden': False}], 'publishedAt': '2025-06-09T17:59:57.000Z', 'submittedOnDailyAt': '2025-06-10T00:35:58.769Z', 'title': 'GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\\n  Behavior', 'submittedOnDailyBy': {'_id': '64101f81b27543634e377fc1', 'avatarUrl': '/avatars/557dd9d4707e3b38e0805dfb87c08004.svg', 'isPro': False, 'fullname': 'Penghao Wu', 'user': 'craigwu', 'type': 'user'}, 'summary': 'Multimodal Large Language Models (MLLMs) have shown great potential in\\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\\nGUI models mostly rely on learning from nearly error-free offline trajectories,\\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\\npropose GUI-Reflection, a novel framework that explicitly integrates\\nself-reflection and error correction capabilities into end-to-end multimodal\\nGUI models throughout dedicated training stages: GUI-specific pre-training,\\noffline supervised fine-tuning (SFT), and online reflection tuning.\\nGUI-reflection enables self-reflection behavior emergence with fully automated\\ndata generation and learning processes without requiring any human annotation.\\nSpecifically, 1) we first propose scalable data pipelines to automatically\\nconstruct reflection and error correction data from existing successful\\ntrajectories. While existing GUI models mainly focus on grounding and UI\\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\\ndiverse and efficient environment for online training and data collection of\\nGUI models on mobile devices. 3) We also present an iterative online reflection\\ntuning algorithm leveraging the proposed environment, enabling the model to\\ncontinuously enhance its reflection and error correction abilities. Our\\nframework equips GUI agents with self-reflection and correction capabilities,\\npaving the way for more robust, adaptable, and intelligent GUI automation, with\\nall data, models, environments, and tools to be released publicly.', 'upvotes': 7, 'discussionId': '684791b63ec10bdd8ab4ddb7', 'projectPage': 'https://penghao-wu.github.io/GUI_Reflection/', 'githubRepo': 'https://github.com/penghao-wu/GUI_Reflection', 'ai_summary': 'GUI-Reflection enhances GUI automation by integrating self-reflection and error correction through scalable data pipelines and an iterative online tuning framework.', 'ai_keywords': ['multimodal large language models', 'graphical user interface', 'GUI automation', 'self-reflection', 'error correction', 'GUI-specific pre-training', 'supervised fine-tuning', 'online reflection tuning', 'reflection-oriented abilities', 'iterative online reflection tuning algorithm']}, 'publishedAt': '2025-06-09T13:59:57.000Z', 'title': 'GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\\n  Behavior', 'summary': 'Multimodal Large Language Models (MLLMs) have shown great potential in\\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\\nGUI models mostly rely on learning from nearly error-free offline trajectories,\\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\\npropose GUI-Reflection, a novel framework that explicitly integrates\\nself-reflection and error correction capabilities into end-to-end multimodal\\nGUI models throughout dedicated training stages: GUI-specific pre-training,\\noffline supervised fine-tuning (SFT), and online reflection tuning.\\nGUI-reflection enables self-reflection behavior emergence with fully automated\\ndata generation and learning processes without requiring any human annotation.\\nSpecifically, 1) we first propose scalable data pipelines to automatically\\nconstruct reflection and error correction data from existing successful\\ntrajectories. While existing GUI models mainly focus on grounding and UI\\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\\ndiverse and efficient environment for online training and data collection of\\nGUI models on mobile devices. 3) We also present an iterative online reflection\\ntuning algorithm leveraging the proposed environment, enabling the model to\\ncontinuously enhance its reflection and error correction abilities. Our\\nframework equips GUI agents with self-reflection and correction capabilities,\\npaving the way for more robust, adaptable, and intelligent GUI automation, with\\nall data, models, environments, and tools to be released publicly.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08012.png', 'numComments': 1, 'submittedBy': {'_id': '64101f81b27543634e377fc1', 'avatarUrl': '/avatars/557dd9d4707e3b38e0805dfb87c08004.svg', 'fullname': 'Penghao Wu', 'name': 'craigwu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 15}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.07434', 'authors': [{'_id': '684797f33ec10bdd8ab4de7a', 'user': {'_id': '6447ca6ca478b20f1755b294', 'avatarUrl': '/avatars/5049856b5ed1b74533fff902e14b4c7c.svg', 'isPro': False, 'fullname': 'Feifan Song', 'user': 'songff', 'type': 'user'}, 'name': 'Feifan Song', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:43:29.497Z', 'hidden': False}, {'_id': '684797f33ec10bdd8ab4de7b', 'user': {'_id': '67244a81aa8556c561925ab6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/w-vZ0uwYACagrNq-H1oyO.jpeg', 'isPro': False, 'fullname': 'Shaohang Wei', 'user': 'SylvainWei', 'type': 'user'}, 'name': 'Shaohang Wei', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:43:27.498Z', 'hidden': False}, {'_id': '684797f33ec10bdd8ab4de7c', 'name': 'Wen Luo', 'hidden': False}, {'_id': '684797f33ec10bdd8ab4de7d', 'name': 'Yuxuan Fan', 'hidden': False}, {'_id': '684797f33ec10bdd8ab4de7e', 'name': 'Tianyu Liu', 'hidden': False}, {'_id': '684797f33ec10bdd8ab4de7f', 'name': 'Guoyin Wang', 'hidden': False}, {'_id': '684797f33ec10bdd8ab4de80', 'name': 'Houfeng Wang', 'hidden': False}], 'publishedAt': '2025-06-09T05:21:22.000Z', 'submittedOnDailyAt': '2025-06-10T00:59:10.518Z', 'title': 'Well Begun is Half Done: Low-resource Preference Alignment by\\n  Weak-to-Strong Decoding', 'submittedOnDailyBy': {'_id': '6447ca6ca478b20f1755b294', 'avatarUrl': '/avatars/5049856b5ed1b74533fff902e14b4c7c.svg', 'isPro': False, 'fullname': 'Feifan Song', 'user': 'songff', 'type': 'user'}, 'summary': 'Large Language Models (LLMs) require alignment with human preferences to\\navoid generating offensive, false, or meaningless content. Recently,\\nlow-resource methods for LLM alignment have been popular, while still facing\\nchallenges in obtaining both high-quality and aligned content. Motivated by the\\nobservation that the difficulty of generating aligned responses is concentrated\\nat the beginning of decoding, we propose a novel framework, Weak-to-Strong\\nDecoding (WSD), to enhance the alignment ability of base models by the guidance\\nof a small aligned model. The small model first drafts well-aligned beginnings,\\nfollowed by the large base model to continue the rest, controlled by a\\nwell-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,\\nto fine-tune a small-sized Pilot-3B as the draft model, which effectively\\nenhances different base models under the WSD framework to outperform all\\nbaseline methods, while avoiding degradation on downstream tasks, termed as the\\nalignment tax. Extensive experiments are further conducted to examine the\\nimpact of different settings and time efficiency, as well as analyses on the\\nintrinsic mechanisms of WSD in depth.', 'upvotes': 7, 'discussionId': '684797f33ec10bdd8ab4de81', 'githubRepo': 'https://github.com/F2-Song/Weak-to-Strong-Decoding', 'ai_summary': 'A new decoding framework (Weak-to-Strong Decoding, WSD) enhances the alignment of large language models by using a small aligned model to draft responses, followed by the base model, with a design to prevent degradation in performance on downstream tasks.', 'ai_keywords': ['Large Language Models (LLMs)', 'LLM alignment', 'human preferences', 'low-resource methods', 'decoding', 'small aligned model', 'auto-switch mechanism', 'GenerAlign', 'Pilot-3B', 'draft model', 'alignment tax', 'intrinsic mechanisms']}, 'publishedAt': '2025-06-09T01:21:22.000Z', 'title': 'Well Begun is Half Done: Low-resource Preference Alignment by\\n  Weak-to-Strong Decoding', 'summary': 'Large Language Models (LLMs) require alignment with human preferences to\\navoid generating offensive, false, or meaningless content. Recently,\\nlow-resource methods for LLM alignment have been popular, while still facing\\nchallenges in obtaining both high-quality and aligned content. Motivated by the\\nobservation that the difficulty of generating aligned responses is concentrated\\nat the beginning of decoding, we propose a novel framework, Weak-to-Strong\\nDecoding (WSD), to enhance the alignment ability of base models by the guidance\\nof a small aligned model. The small model first drafts well-aligned beginnings,\\nfollowed by the large base model to continue the rest, controlled by a\\nwell-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,\\nto fine-tune a small-sized Pilot-3B as the draft model, which effectively\\nenhances different base models under the WSD framework to outperform all\\nbaseline methods, while avoiding degradation on downstream tasks, termed as the\\nalignment tax. Extensive experiments are further conducted to examine the\\nimpact of different settings and time efficiency, as well as analyses on the\\nintrinsic mechanisms of WSD in depth.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07434.png', 'numComments': 1, 'submittedBy': {'_id': '6447ca6ca478b20f1755b294', 'avatarUrl': '/avatars/5049856b5ed1b74533fff902e14b4c7c.svg', 'fullname': 'Feifan Song', 'name': 'songff', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.08010', 'authors': [{'_id': '6847ad3b3ec10bdd8ab4df06', 'name': 'Nick Jiang', 'hidden': False}, {'_id': '6847ad3b3ec10bdd8ab4df07', 'name': 'Amil Dravid', 'hidden': False}, {'_id': '6847ad3b3ec10bdd8ab4df08', 'name': 'Alexei Efros', 'hidden': False}, {'_id': '6847ad3b3ec10bdd8ab4df09', 'name': 'Yossi Gandelsman', 'hidden': False}], 'publishedAt': '2025-06-09T17:59:57.000Z', 'submittedOnDailyAt': '2025-06-10T04:42:01.785Z', 'title': \"Vision Transformers Don't Need Trained Registers\", 'submittedOnDailyBy': {'_id': '6398d9d168e3392256aaf952', 'avatarUrl': '/avatars/062363a9b5ebb26603c543a7fc3ee4ec.svg', 'isPro': False, 'fullname': 'Nick', 'user': 'nickjiang', 'type': 'user'}, 'summary': 'We investigate the mechanism underlying a previously identified phenomenon in\\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\\nsparse set of neurons is responsible for concentrating high-norm activations on\\noutlier tokens, leading to irregular attention patterns and degrading\\ndownstream visual processing. While the existing solution for removing these\\noutliers involves retraining models from scratch with additional learned\\nregister tokens, we use our findings to create a training-free approach to\\nmitigate these artifacts. By shifting the high-norm activations from our\\ndiscovered register neurons into an additional untrained token, we can mimic\\nthe effect of register tokens on a model already trained without registers. We\\ndemonstrate that our method produces cleaner attention and feature maps,\\nenhances performance over base models across multiple downstream visual tasks,\\nand achieves results comparable to models explicitly trained with register\\ntokens. We then extend test-time registers to off-the-shelf vision-language\\nmodels to improve their interpretability. Our results suggest that test-time\\nregisters effectively take on the role of register tokens at test-time,\\noffering a training-free solution for any pre-trained model released without\\nthem.', 'upvotes': 5, 'discussionId': '6847ad3c3ec10bdd8ab4df0a', 'ai_summary': 'A training-free method shifts high-norm activations in Vision Transformers to an untrained token, enhancing attention maps and performance across visual tasks, and improving interpretability in vision-language models.', 'ai_keywords': ['Vision Transformers', 'high-norm tokens', 'noisy attention maps', 'activations', 'neurons', 'irregular attention patterns', 'downstream visual processing', 'register tokens', 'feature maps', 'vision-language models', 'interpretability', 'test-time registers']}, 'publishedAt': '2025-06-09T13:59:57.000Z', 'title': \"Vision Transformers Don't Need Trained Registers\", 'summary': 'We investigate the mechanism underlying a previously identified phenomenon in\\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\\nsparse set of neurons is responsible for concentrating high-norm activations on\\noutlier tokens, leading to irregular attention patterns and degrading\\ndownstream visual processing. While the existing solution for removing these\\noutliers involves retraining models from scratch with additional learned\\nregister tokens, we use our findings to create a training-free approach to\\nmitigate these artifacts. By shifting the high-norm activations from our\\ndiscovered register neurons into an additional untrained token, we can mimic\\nthe effect of register tokens on a model already trained without registers. We\\ndemonstrate that our method produces cleaner attention and feature maps,\\nenhances performance over base models across multiple downstream visual tasks,\\nand achieves results comparable to models explicitly trained with register\\ntokens. We then extend test-time registers to off-the-shelf vision-language\\nmodels to improve their interpretability. Our results suggest that test-time\\nregisters effectively take on the role of register tokens at test-time,\\noffering a training-free solution for any pre-trained model released without\\nthem.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08010.png', 'numComments': 1, 'submittedBy': {'_id': '6398d9d168e3392256aaf952', 'avatarUrl': '/avatars/062363a9b5ebb26603c543a7fc3ee4ec.svg', 'fullname': 'Nick', 'name': 'nickjiang', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.08006', 'authors': [{'_id': '6847ae533ec10bdd8ab4df0c', 'name': 'Sicheng Mo', 'hidden': False}, {'_id': '6847ae533ec10bdd8ab4df0d', 'name': 'Ziyang Leng', 'hidden': False}, {'_id': '6847ae533ec10bdd8ab4df0e', 'name': 'Leon Liu', 'hidden': False}, {'_id': '6847ae533ec10bdd8ab4df0f', 'name': 'Weizhen Wang', 'hidden': False}, {'_id': '6847ae533ec10bdd8ab4df10', 'name': 'Honglin He', 'hidden': False}, {'_id': '6847ae533ec10bdd8ab4df11', 'name': 'Bolei Zhou', 'hidden': False}], 'publishedAt': '2025-06-09T17:59:52.000Z', 'submittedOnDailyAt': '2025-06-10T02:33:18.764Z', 'title': 'Dreamland: Controllable World Creation with Simulator and Generative\\n  Models', 'submittedOnDailyBy': {'_id': '637c94d3f219c71f93eda9ad', 'avatarUrl': '/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg', 'isPro': True, 'fullname': 'Sicheng Mo', 'user': 'Sichengmo', 'type': 'user'}, 'summary': 'Large-scale video generative models can synthesize diverse and realistic\\nvisual content for dynamic world creation, but they often lack element-wise\\ncontrollability, hindering their use in editing scenes and training embodied AI\\nagents. We propose Dreamland, a hybrid world generation framework combining the\\ngranular control of a physics-based simulator and the photorealistic content\\noutput of large-scale pretrained generative models. In particular, we design a\\nlayered world abstraction that encodes both pixel-level and object-level\\nsemantics and geometry as an intermediate representation to bridge the\\nsimulator and the generative model. This approach enhances controllability,\\nminimizes adaptation cost through early alignment with real-world\\ndistributions, and supports off-the-shelf use of existing and future pretrained\\ngenerative models. We further construct a D3Sim dataset to facilitate the\\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\\nthat Dreamland outperforms existing baselines with 50.8% improved image\\nquality, 17.9% stronger controllability, and has great potential to enhance\\nembodied agent training. Code and data will be made available.', 'upvotes': 5, 'discussionId': '6847ae533ec10bdd8ab4df12', 'projectPage': 'https://metadriverse.github.io/dreamland/', 'ai_summary': 'Dreamland, a hybrid framework, combines physics-based simulators and generative models to improve controllability and image quality in video generation.', 'ai_keywords': ['video generative models', 'physics-based simulator', 'photorealistic content', 'world abstraction', 'pixel-level semantics', 'object-level semantics', 'geometry', 'layered world abstraction', 'early alignment', 'D3Sim dataset', 'embodied agent training']}, 'publishedAt': '2025-06-09T13:59:52.000Z', 'title': 'Dreamland: Controllable World Creation with Simulator and Generative\\n  Models', 'summary': 'Large-scale video generative models can synthesize diverse and realistic\\nvisual content for dynamic world creation, but they often lack element-wise\\ncontrollability, hindering their use in editing scenes and training embodied AI\\nagents. We propose Dreamland, a hybrid world generation framework combining the\\ngranular control of a physics-based simulator and the photorealistic content\\noutput of large-scale pretrained generative models. In particular, we design a\\nlayered world abstraction that encodes both pixel-level and object-level\\nsemantics and geometry as an intermediate representation to bridge the\\nsimulator and the generative model. This approach enhances controllability,\\nminimizes adaptation cost through early alignment with real-world\\ndistributions, and supports off-the-shelf use of existing and future pretrained\\ngenerative models. We further construct a D3Sim dataset to facilitate the\\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\\nthat Dreamland outperforms existing baselines with 50.8% improved image\\nquality, 17.9% stronger controllability, and has great potential to enhance\\nembodied agent training. Code and data will be made available.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08006.png', 'numComments': 1, 'submittedBy': {'_id': '637c94d3f219c71f93eda9ad', 'avatarUrl': '/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg', 'fullname': 'Sicheng Mo', 'name': 'Sichengmo', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.08011', 'authors': [{'_id': '6847a7ae3ec10bdd8ab4deed', 'name': 'Yunfei Xie', 'hidden': False}, {'_id': '6847a7ae3ec10bdd8ab4deee', 'name': 'Yinsong Ma', 'hidden': False}, {'_id': '6847a7ae3ec10bdd8ab4deef', 'name': 'Shiyi Lan', 'hidden': False}, {'_id': '6847a7ae3ec10bdd8ab4def0', 'name': 'Alan Yuille', 'hidden': False}, {'_id': '6847a7ae3ec10bdd8ab4def1', 'name': 'Junfei Xiao', 'hidden': False}, {'_id': '6847a7ae3ec10bdd8ab4def2', 'name': 'Chen Wei', 'hidden': False}], 'publishedAt': '2025-06-09T17:59:57.000Z', 'submittedOnDailyAt': '2025-06-10T10:13:10.802Z', 'title': 'Play to Generalize: Learning to Reason Through Game Play', 'submittedOnDailyBy': {'_id': '65f6afa36ea40b9a29dd45ca', 'avatarUrl': '/avatars/a80a25fa70a89a3bf3d50ea2866f0df1.svg', 'isPro': True, 'fullname': 'Yunfei Xie', 'user': 'yunfeixie', 'type': 'user'}, 'summary': \"Developing generalizable reasoning capabilities in multimodal large language\\nmodels (MLLMs) remains challenging. Motivated by cognitive science literature\\nsuggesting that gameplay promotes transferable cognitive skills, we propose a\\nnovel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs\\ndevelop out-of-domain generalization of multimodal reasoning through playing\\narcade-like games. Specifically, we show that post-training a 7B-parameter MLLM\\nvia reinforcement learning (RL) on simple arcade-like games, e.g. Snake,\\nsignificantly enhances its downstream performance on multimodal math benchmarks\\nlike MathVista, and on multi-discipline questions like MMMU, without seeing any\\nworked solutions, equations, or diagrams during RL, suggesting the capture of\\ntransferable reasoning skills. Remarkably, our model outperforms specialist\\nmodels tuned on multimodal reasoning data in multimodal reasoning benchmarks,\\nwhile preserving the base model's performance on general visual benchmarks, a\\nchallenge where specialist models often fall short. Our findings suggest a new\\npost-training paradigm: synthetic, rule-based games can serve as controllable\\nand scalable pre-text tasks that unlock generalizable multimodal reasoning\\nabilities in MLLMs.\", 'upvotes': 4, 'discussionId': '6847a7ae3ec10bdd8ab4def3', 'projectPage': 'https://yunfeixie233.github.io/ViGaL/', 'githubRepo': 'https://github.com/yunfeixie233/ViGaL', 'ai_summary': 'Post-training multimodal large language models with reinforcement learning on arcade-like games enhances their multimodal reasoning abilities without domain-specific data.', 'ai_keywords': ['multimodal large language models', 'MLLMs', 'Visual Game Learning', 'ViGaL', 'reinforcement learning', 'RL', 'MathVista', 'MMMU', 'multimodal reasoning benchmarks', 'generalizable reasoning skills']}, 'publishedAt': '2025-06-09T13:59:57.000Z', 'title': 'Play to Generalize: Learning to Reason Through Game Play', 'summary': \"Developing generalizable reasoning capabilities in multimodal large language\\nmodels (MLLMs) remains challenging. Motivated by cognitive science literature\\nsuggesting that gameplay promotes transferable cognitive skills, we propose a\\nnovel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs\\ndevelop out-of-domain generalization of multimodal reasoning through playing\\narcade-like games. Specifically, we show that post-training a 7B-parameter MLLM\\nvia reinforcement learning (RL) on simple arcade-like games, e.g. Snake,\\nsignificantly enhances its downstream performance on multimodal math benchmarks\\nlike MathVista, and on multi-discipline questions like MMMU, without seeing any\\nworked solutions, equations, or diagrams during RL, suggesting the capture of\\ntransferable reasoning skills. Remarkably, our model outperforms specialist\\nmodels tuned on multimodal reasoning data in multimodal reasoning benchmarks,\\nwhile preserving the base model's performance on general visual benchmarks, a\\nchallenge where specialist models often fall short. Our findings suggest a new\\npost-training paradigm: synthetic, rule-based games can serve as controllable\\nand scalable pre-text tasks that unlock generalizable multimodal reasoning\\nabilities in MLLMs.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08011.png', 'numComments': 2, 'submittedBy': {'_id': '65f6afa36ea40b9a29dd45ca', 'avatarUrl': '/avatars/a80a25fa70a89a3bf3d50ea2866f0df1.svg', 'fullname': 'Yunfei Xie', 'name': 'yunfeixie', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.06266', 'authors': [{'_id': '6847b4b43ec10bdd8ab4df33', 'user': {'_id': '6337537b267cee4d068f604d', 'avatarUrl': '/avatars/15267f0759a6570c98ee6a150558fcc0.svg', 'isPro': False, 'fullname': 'Sabri Eyuboglu', 'user': 'sabrieyuboglu', 'type': 'user'}, 'name': 'Sabri Eyuboglu', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-10T04:29:41.818Z', 'hidden': False}, {'_id': '6847b4b43ec10bdd8ab4df34', 'name': 'Ryan Ehrlich', 'hidden': False}, {'_id': '6847b4b43ec10bdd8ab4df35', 'name': 'Simran Arora', 'hidden': False}, {'_id': '6847b4b43ec10bdd8ab4df36', 'name': 'Neel Guha', 'hidden': False}, {'_id': '6847b4b43ec10bdd8ab4df37', 'name': 'Dylan Zinsley', 'hidden': False}, {'_id': '6847b4b43ec10bdd8ab4df38', 'name': 'Emily Liu', 'hidden': False}, {'_id': '6847b4b43ec10bdd8ab4df39', 'name': 'Will Tennien', 'hidden': False}, {'_id': '6847b4b43ec10bdd8ab4df3a', 'name': 'Atri Rudra', 'hidden': False}, {'_id': '6847b4b43ec10bdd8ab4df3b', 'name': 'James Zou', 'hidden': False}, {'_id': '6847b4b43ec10bdd8ab4df3c', 'name': 'Azalia Mirhoseini', 'hidden': False}, {'_id': '6847b4b43ec10bdd8ab4df3d', 'name': 'Christopher Re', 'hidden': False}], 'publishedAt': '2025-06-06T17:48:23.000Z', 'submittedOnDailyAt': '2025-06-10T03:02:08.278Z', 'title': 'Cartridges: Lightweight and general-purpose long context representations\\n  via self-study', 'submittedOnDailyBy': {'_id': '6337537b267cee4d068f604d', 'avatarUrl': '/avatars/15267f0759a6570c98ee6a150558fcc0.svg', 'isPro': False, 'fullname': 'Sabri Eyuboglu', 'user': 'sabrieyuboglu', 'type': 'user'}, 'summary': \"Large language models are often used to answer queries grounded in large text\\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\\nentire corpus in the context window and leveraging in-context learning (ICL).\\nAlthough current models support contexts of 100K-1M tokens, this setup is\\ncostly to serve because the memory consumption of the KV cache scales with\\ninput length. We explore an alternative: training a smaller KV cache offline on\\neach corpus. At inference time, we load this trained KV cache, which we call a\\nCartridge, and decode a response. Critically, the cost of training a Cartridge\\ncan be amortized across all the queries referencing the same corpus. However,\\nwe find that the naive approach of training the Cartridge with next-token\\nprediction on the corpus is not competitive with ICL. Instead, we propose\\nself-study, a training recipe in which we generate synthetic conversations\\nabout the corpus and train the Cartridge with a context-distillation objective.\\nWe find that Cartridges trained with self-study replicate the functionality of\\nICL, while being significantly cheaper to serve. On challenging long-context\\nbenchmarks, Cartridges trained with self-study match ICL performance while\\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\\nextends the model's effective context length (e.g. from 128k to 484k tokens on\\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\\ntime without retraining.\", 'upvotes': 4, 'discussionId': '6847b4b43ec10bdd8ab4df3e', 'projectPage': 'https://hazyresearch.stanford.edu/blog/2025-06-08-cartridges', 'githubRepo': 'https://github.com/HazyResearch/cartridges', 'ai_summary': 'Training a smaller, offline KV cache (Cartridge) with a context-distillation objective (self-study) for large language models reduces serving costs, matches ICL performance, and extends effective context length.', 'ai_keywords': ['KV cache', 'Cartridge', 'in-context learning (ICL)', 'self-study', 'context-distillation objective', 'MTOB']}, 'publishedAt': '2025-06-06T13:48:23.000Z', 'title': 'Cartridges: Lightweight and general-purpose long context representations\\n  via self-study', 'summary': \"Large language models are often used to answer queries grounded in large text\\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\\nentire corpus in the context window and leveraging in-context learning (ICL).\\nAlthough current models support contexts of 100K-1M tokens, this setup is\\ncostly to serve because the memory consumption of the KV cache scales with\\ninput length. We explore an alternative: training a smaller KV cache offline on\\neach corpus. At inference time, we load this trained KV cache, which we call a\\nCartridge, and decode a response. Critically, the cost of training a Cartridge\\ncan be amortized across all the queries referencing the same corpus. However,\\nwe find that the naive approach of training the Cartridge with next-token\\nprediction on the corpus is not competitive with ICL. Instead, we propose\\nself-study, a training recipe in which we generate synthetic conversations\\nabout the corpus and train the Cartridge with a context-distillation objective.\\nWe find that Cartridges trained with self-study replicate the functionality of\\nICL, while being significantly cheaper to serve. On challenging long-context\\nbenchmarks, Cartridges trained with self-study match ICL performance while\\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\\nextends the model's effective context length (e.g. from 128k to 484k tokens on\\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\\ntime without retraining.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06266.png', 'numComments': 1, 'submittedBy': {'_id': '6337537b267cee4d068f604d', 'avatarUrl': '/avatars/15267f0759a6570c98ee6a150558fcc0.svg', 'fullname': 'Sabri Eyuboglu', 'name': 'sabrieyuboglu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.07527', 'authors': [{'_id': '6847dce63ec10bdd8ab4e011', 'user': {'_id': '659e3ea885956d2cccda2b9e', 'avatarUrl': '/avatars/f26d415f7c1b39550af2075769f38a91.svg', 'isPro': False, 'fullname': '', 'user': 'RoadQAQ', 'type': 'user'}, 'name': 'Lu Ma', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-10T07:21:11.443Z', 'hidden': False}, {'_id': '6847dce63ec10bdd8ab4e012', 'name': 'Hao Liang', 'hidden': False}, {'_id': '6847dce63ec10bdd8ab4e013', 'name': 'Meiyi Qiang', 'hidden': False}, {'_id': '6847dce63ec10bdd8ab4e014', 'name': 'Lexiang Tang', 'hidden': False}, {'_id': '6847dce63ec10bdd8ab4e015', 'name': 'Xiaochen Ma', 'hidden': False}, {'_id': '6847dce63ec10bdd8ab4e016', 'name': 'Zhen Hao Wong', 'hidden': False}, {'_id': '6847dce63ec10bdd8ab4e017', 'name': 'Junbo Niu', 'hidden': False}, {'_id': '6847dce63ec10bdd8ab4e018', 'name': 'Chengyu Shen', 'hidden': False}, {'_id': '6847dce63ec10bdd8ab4e019', 'name': 'Runming He', 'hidden': False}, {'_id': '6847dce63ec10bdd8ab4e01a', 'name': 'Bin Cui', 'hidden': False}, {'_id': '6847dce63ec10bdd8ab4e01b', 'name': 'Wentao Zhang', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/659e3ea885956d2cccda2b9e/Q_orxNfgmdXXT6I2ahrDs.jpeg'], 'publishedAt': '2025-06-09T08:11:20.000Z', 'submittedOnDailyAt': '2025-06-10T05:52:14.746Z', 'title': \"Learning What Reinforcement Learning Can't: Interleaved Online\\n  Fine-Tuning for Hardest Questions\", 'submittedOnDailyBy': {'_id': '659e3ea885956d2cccda2b9e', 'avatarUrl': '/avatars/f26d415f7c1b39550af2075769f38a91.svg', 'isPro': False, 'fullname': '', 'user': 'RoadQAQ', 'type': 'user'}, 'summary': \"Recent advances in large language model (LLM) reasoning have shown that\\nsophisticated behaviors such as planning and self-reflection can emerge through\\nreinforcement learning (RL). However, despite these successes, RL in its\\ncurrent form remains insufficient to induce capabilities that exceed the\\nlimitations of the base model, as it is primarily optimized based on existing\\nknowledge of the model rather than facilitating the acquisition of new\\ninformation. To address this limitation, we employ supervised fine-tuning (SFT)\\nto learn what RL cannot, which enables the incorporation of new knowledge and\\nreasoning patterns by leveraging high-quality demonstration data. We analyze\\nthe training dynamics of RL and SFT for LLM reasoning and find that RL excels\\nat maintaining and improving performance on questions within the model's\\noriginal capabilities, while SFT is more effective at enabling progress on\\nquestions beyond the current scope of the model. Motivated by the complementary\\nstrengths of RL and SFT, we introduce a novel training approach,\\nReLIFT (Reinforcement Learning Interleaved\\nwith Online Fine-Tuning). In ReLIFT, the model is primarily\\ntrained using RL, but when it encounters challenging questions, high-quality\\nsolutions are collected for fine-tuning, and the training process alternates\\nbetween RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT\\nachieves an average improvement of over +5.2 points across five\\ncompetition-level benchmarks and one out-of-distribution benchmark compared to\\nother zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both\\nRL and SFT while using only 13\\\\% of the detailed demonstration data,\\nhighlighting its scalability. These results provide compelling evidence that\\nReLIFT overcomes the fundamental limitations of RL and underscores the\\nsignificant potential.\", 'upvotes': 3, 'discussionId': '6847dce73ec10bdd8ab4e01c', 'githubRepo': 'https://github.com/TheRoadQaQ/ReLIFT', 'ai_summary': 'ReLIFT, a method combining reinforcement learning and supervised fine-tuning, enhances large language model reasoning by addressing limitations of RL through interleaved training, improving performance across benchmarks with minimal data.', 'ai_keywords': ['reinforcement learning', 'supervised fine-tuning', 'ReLIFT', 'large language model', 'reasoning', 'training dynamics', 'zero-RL models', 'competition-level benchmarks', 'out-of-distribution benchmark']}, 'publishedAt': '2025-06-09T04:11:20.000Z', 'title': \"Learning What Reinforcement Learning Can't: Interleaved Online\\n  Fine-Tuning for Hardest Questions\", 'summary': \"Recent advances in large language model (LLM) reasoning have shown that\\nsophisticated behaviors such as planning and self-reflection can emerge through\\nreinforcement learning (RL). However, despite these successes, RL in its\\ncurrent form remains insufficient to induce capabilities that exceed the\\nlimitations of the base model, as it is primarily optimized based on existing\\nknowledge of the model rather than facilitating the acquisition of new\\ninformation. To address this limitation, we employ supervised fine-tuning (SFT)\\nto learn what RL cannot, which enables the incorporation of new knowledge and\\nreasoning patterns by leveraging high-quality demonstration data. We analyze\\nthe training dynamics of RL and SFT for LLM reasoning and find that RL excels\\nat maintaining and improving performance on questions within the model's\\noriginal capabilities, while SFT is more effective at enabling progress on\\nquestions beyond the current scope of the model. Motivated by the complementary\\nstrengths of RL and SFT, we introduce a novel training approach,\\nReLIFT (Reinforcement Learning Interleaved\\nwith Online Fine-Tuning). In ReLIFT, the model is primarily\\ntrained using RL, but when it encounters challenging questions, high-quality\\nsolutions are collected for fine-tuning, and the training process alternates\\nbetween RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT\\nachieves an average improvement of over +5.2 points across five\\ncompetition-level benchmarks and one out-of-distribution benchmark compared to\\nother zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both\\nRL and SFT while using only 13\\\\% of the detailed demonstration data,\\nhighlighting its scalability. These results provide compelling evidence that\\nReLIFT overcomes the fundamental limitations of RL and underscores the\\nsignificant potential.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/659e3ea885956d2cccda2b9e/Q_orxNfgmdXXT6I2ahrDs.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07527.png', 'numComments': 1, 'submittedBy': {'_id': '659e3ea885956d2cccda2b9e', 'avatarUrl': '/avatars/f26d415f7c1b39550af2075769f38a91.svg', 'fullname': '', 'name': 'RoadQAQ', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.07848', 'authors': [{'_id': '6847de223ec10bdd8ab4e02a', 'name': 'Teng Hu', 'hidden': False}, {'_id': '6847de223ec10bdd8ab4e02b', 'name': 'Zhentao Yu', 'hidden': False}, {'_id': '6847de223ec10bdd8ab4e02c', 'name': 'Zhengguang Zhou', 'hidden': False}, {'_id': '6847de223ec10bdd8ab4e02d', 'name': 'Jiangning Zhang', 'hidden': False}, {'_id': '6847de223ec10bdd8ab4e02e', 'name': 'Yuan Zhou', 'hidden': False}, {'_id': '6847de223ec10bdd8ab4e02f', 'name': 'Qinglin Lu', 'hidden': False}, {'_id': '6847de223ec10bdd8ab4e030', 'name': 'Ran Yi', 'hidden': False}], 'publishedAt': '2025-06-09T15:11:09.000Z', 'submittedOnDailyAt': '2025-06-10T05:57:02.723Z', 'title': 'PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal\\n  Interaction and Enhancement', 'submittedOnDailyBy': {'_id': '63468720dd6d90d82ccf3450', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg', 'isPro': False, 'fullname': 'YSH', 'user': 'BestWishYsh', 'type': 'user'}, 'summary': 'Despite recent advances in video generation, existing models still lack\\nfine-grained controllability, especially for multi-subject customization with\\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\\nmulti-subject video customization framework that enables flexible and\\nidentity-consistent generation. To establish accurate correspondences between\\nsubject images and textual entities, we design a VLLM-based text-image fusion\\nmodule that embeds visual identities into the textual space for precise\\ngrounding. To further enhance identity preservation and subject interaction, we\\npropose a 3D-RoPE-based enhancement module that enables structured\\nbidirectional fusion between text and image embeddings. Moreover, we develop an\\nattention-inherited identity injection module to effectively inject fused\\nidentity features into the video generation process, mitigating identity drift.\\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\\ngrounding, segmentation, and a clique-based subject consolidation strategy to\\nproduce high-quality multi-subject data, effectively enhancing subject\\ndistinction and reducing ambiguity in downstream video generation. Extensive\\nexperiments demonstrate that PolyVivid achieves superior performance in\\nidentity fidelity, video realism, and subject alignment, outperforming existing\\nopen-source and commercial baselines.', 'upvotes': 2, 'discussionId': '6847de223ec10bdd8ab4e031', 'projectPage': 'https://sjtuplayer.github.io/projects/PolyVivid/', 'ai_summary': 'PolyVivid is a multi-subject video customization framework that uses text-image fusion, 3D-RoPE enhancement, attention-inherited identity injection, and MLLM-based data processing to ensure identity consistency and realistic video generation.', 'ai_keywords': ['VLLM-based text-image fusion', '3D-RoPE-based enhancement', 'attention-inherited identity injection', 'MLLM-based data pipeline', 'identity fidelity', 'video realism', 'subject alignment']}, 'publishedAt': '2025-06-09T11:11:09.000Z', 'title': 'PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal\\n  Interaction and Enhancement', 'summary': 'Despite recent advances in video generation, existing models still lack\\nfine-grained controllability, especially for multi-subject customization with\\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\\nmulti-subject video customization framework that enables flexible and\\nidentity-consistent generation. To establish accurate correspondences between\\nsubject images and textual entities, we design a VLLM-based text-image fusion\\nmodule that embeds visual identities into the textual space for precise\\ngrounding. To further enhance identity preservation and subject interaction, we\\npropose a 3D-RoPE-based enhancement module that enables structured\\nbidirectional fusion between text and image embeddings. Moreover, we develop an\\nattention-inherited identity injection module to effectively inject fused\\nidentity features into the video generation process, mitigating identity drift.\\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\\ngrounding, segmentation, and a clique-based subject consolidation strategy to\\nproduce high-quality multi-subject data, effectively enhancing subject\\ndistinction and reducing ambiguity in downstream video generation. Extensive\\nexperiments demonstrate that PolyVivid achieves superior performance in\\nidentity fidelity, video realism, and subject alignment, outperforming existing\\nopen-source and commercial baselines.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07848.png', 'numComments': 1, 'submittedBy': {'_id': '63468720dd6d90d82ccf3450', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg', 'fullname': 'YSH', 'name': 'BestWishYsh', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 56}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.07240', 'authors': [{'_id': '6847b6513ec10bdd8ab4df49', 'user': {'_id': '600bde0c2b417b1d53669bd0', 'avatarUrl': '/avatars/2d9704713630e96458368b47179c039c.svg', 'isPro': False, 'fullname': 'Roy Eisenstadt', 'user': 'royeis', 'type': 'user'}, 'name': 'Roy Eisenstadt', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-06-10T11:04:16.897Z', 'hidden': False}, {'_id': '6847b6513ec10bdd8ab4df4a', 'name': 'Itamar Zimerman', 'hidden': False}, {'_id': '6847b6513ec10bdd8ab4df4b', 'name': 'Lior Wolf', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/TYxLNOb6ac6HH-pR04f7W.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/CVHB0FrpM5va85IVFh2bT.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/c-o1T8-RuSJ222Fj79LDN.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/eKIDa2-ZEMdQdYOimgSpC.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/VI5bYWw7ZDAruk6rqNrMh.jpeg'], 'publishedAt': '2025-06-08T17:54:33.000Z', 'submittedOnDailyAt': '2025-06-10T03:10:26.281Z', 'title': 'Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path\\n  Lengths in LLMs', 'submittedOnDailyBy': {'_id': '65376feed325b3f02fb92c69', 'avatarUrl': '/avatars/e952918cf434d5302e9b1a404eccaf0e.svg', 'isPro': False, 'fullname': 'Itamar Zimerman', 'user': 'ItamarZ', 'type': 'user'}, 'summary': 'Recently, techniques such as explicit structured reasoning have demonstrated\\nstrong test-time scaling behavior by enforcing a separation between the model\\'s\\ninternal \"thinking\" process and the final response. A key factor influencing\\nanswer quality in this setting is the length of the thinking stage. When the\\nreasoning is too short, the model may fail to capture the complexity of the\\ntask. Conversely, when it is too long, the model may overthink, leading to\\nunnecessary computation and degraded performance. This paper explores and\\nexploits the underlying mechanisms by which LLMs understand and regulate the\\nlength of their reasoning during explicit thought processes. First, we show\\nthat LLMs encode their progress through the reasoning process and introduce an\\ninteractive progress bar visualization, which is then used to reveal insights\\non the model\\'s planning dynamics. Second, we manipulate the internal progress\\nencoding during inference to reduce unnecessary steps and generate a more\\nconcise and decisive chain of thoughts. Our empirical results demonstrate that\\nthis \"overclocking\" method mitigates overthinking, improves answer accuracy,\\nand reduces inference latency. Our code is publicly available.', 'upvotes': 2, 'discussionId': '6847b6513ec10bdd8ab4df4c', 'projectPage': 'https://royeisen.github.io/OverclockingLLMReasoning-paper/', 'githubRepo': 'https://github.com/royeisen/reasoning_loading_bar', 'ai_summary': 'LLMs regulate reasoning length through progress encoding, and manipulating this encoding improves accuracy and reduces inference time.', 'ai_keywords': ['explicit structured reasoning', 'LLMs', 'reasoning process', 'progress bar visualization', 'progress encoding', 'inference', 'overclocking', 'overthinking', 'answer accuracy', 'inference latency']}, 'publishedAt': '2025-06-08T13:54:33.000Z', 'title': 'Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path\\n  Lengths in LLMs', 'summary': 'Recently, techniques such as explicit structured reasoning have demonstrated\\nstrong test-time scaling behavior by enforcing a separation between the model\\'s\\ninternal \"thinking\" process and the final response. A key factor influencing\\nanswer quality in this setting is the length of the thinking stage. When the\\nreasoning is too short, the model may fail to capture the complexity of the\\ntask. Conversely, when it is too long, the model may overthink, leading to\\nunnecessary computation and degraded performance. This paper explores and\\nexploits the underlying mechanisms by which LLMs understand and regulate the\\nlength of their reasoning during explicit thought processes. First, we show\\nthat LLMs encode their progress through the reasoning process and introduce an\\ninteractive progress bar visualization, which is then used to reveal insights\\non the model\\'s planning dynamics. Second, we manipulate the internal progress\\nencoding during inference to reduce unnecessary steps and generate a more\\nconcise and decisive chain of thoughts. Our empirical results demonstrate that\\nthis \"overclocking\" method mitigates overthinking, improves answer accuracy,\\nand reduces inference latency. Our code is publicly available.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/TYxLNOb6ac6HH-pR04f7W.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/CVHB0FrpM5va85IVFh2bT.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/c-o1T8-RuSJ222Fj79LDN.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/eKIDa2-ZEMdQdYOimgSpC.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/VI5bYWw7ZDAruk6rqNrMh.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07240.png', 'numComments': 1, 'submittedBy': {'_id': '65376feed325b3f02fb92c69', 'avatarUrl': '/avatars/e952918cf434d5302e9b1a404eccaf0e.svg', 'fullname': 'Itamar Zimerman', 'name': 'ItamarZ', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.07160', 'authors': [{'_id': '6847c0263ec10bdd8ab4df60', 'user': {'_id': '627b73728b6ecd7ece822825', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg', 'isPro': False, 'fullname': 'Yikun Wang', 'user': 'LibraTree', 'type': 'user'}, 'name': 'Yikun Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:42:47.991Z', 'hidden': False}, {'_id': '6847c0263ec10bdd8ab4df61', 'name': 'Yibin Wang', 'hidden': False}, {'_id': '6847c0263ec10bdd8ab4df62', 'name': 'Dianyi Wang', 'hidden': False}, {'_id': '6847c0263ec10bdd8ab4df63', 'name': 'Zimian Peng', 'hidden': False}, {'_id': '6847c0263ec10bdd8ab4df64', 'name': 'Qipeng Guo', 'hidden': False}, {'_id': '6847c0263ec10bdd8ab4df65', 'name': 'Dacheng Tao', 'hidden': False}, {'_id': '6847c0263ec10bdd8ab4df66', 'name': 'Jiaqi Wang', 'hidden': False}], 'publishedAt': '2025-06-08T14:18:15.000Z', 'submittedOnDailyAt': '2025-06-10T04:04:49.808Z', 'title': 'GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\\n  Policy Optimization', 'submittedOnDailyBy': {'_id': '627b73728b6ecd7ece822825', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg', 'isPro': False, 'fullname': 'Yikun Wang', 'user': 'LibraTree', 'type': 'user'}, 'summary': 'Recent advances in large language models (LLMs) have demonstrated remarkable\\ncapabilities across diverse domains, particularly in mathematical reasoning,\\namid which geometry problem solving remains a challenging area where auxiliary\\nconstruction plays a enssential role. Existing approaches either achieve\\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\\nmassive computational costs. We posit that reinforcement learning with\\nverifiable reward (e.g., GRPO) offers a promising direction for training\\nsmaller models that effectively combine auxiliary construction with robust\\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\\npresents fundamental limitations due to its dependence on unconditional\\nrewards, which leads to indiscriminate and counterproductive auxiliary\\nconstructions. To address these challenges, we propose Group Contrastive Policy\\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\\nor negative reward signals for auxiliary construction based on contextual\\nutility, and a (2) length reward that promotes longer reasoning chains.\\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\\ngeometric reasoning models that judiciously determine when to employ auxiliary\\nconstruction. Our extensive empirical evaluation across popular geometric\\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\\nof 4.29% across all benchmarks.', 'upvotes': 2, 'discussionId': '6847c0273ec10bdd8ab4df67', 'ai_summary': 'A new reinforcement learning framework, Group Contrastive Policy Optimization (GCPO), enhances geometric reasoning in large language models with judicious auxiliary constructions, outperforming existing methods on benchmarks.', 'ai_keywords': ['Large Language Models (LLMs)', 'mathematical reasoning', 'geometry problem solving', 'reinforcement learning', 'verifiable reward', 'GRPO', 'Group Contrastive Policy Optimization (GCPO)', 'Group Contrastive Masking', 'length reward', 'GeometryZero', 'Geometry3K', 'MathVista']}, 'publishedAt': '2025-06-08T10:18:15.000Z', 'title': 'GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\\n  Policy Optimization', 'summary': 'Recent advances in large language models (LLMs) have demonstrated remarkable\\ncapabilities across diverse domains, particularly in mathematical reasoning,\\namid which geometry problem solving remains a challenging area where auxiliary\\nconstruction plays a enssential role. Existing approaches either achieve\\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\\nmassive computational costs. We posit that reinforcement learning with\\nverifiable reward (e.g., GRPO) offers a promising direction for training\\nsmaller models that effectively combine auxiliary construction with robust\\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\\npresents fundamental limitations due to its dependence on unconditional\\nrewards, which leads to indiscriminate and counterproductive auxiliary\\nconstructions. To address these challenges, we propose Group Contrastive Policy\\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\\nor negative reward signals for auxiliary construction based on contextual\\nutility, and a (2) length reward that promotes longer reasoning chains.\\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\\ngeometric reasoning models that judiciously determine when to employ auxiliary\\nconstruction. Our extensive empirical evaluation across popular geometric\\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\\nof 4.29% across all benchmarks.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07160.png', 'numComments': 1, 'submittedBy': {'_id': '627b73728b6ecd7ece822825', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg', 'fullname': 'Yikun Wang', 'name': 'LibraTree', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.04807', 'authors': [{'_id': '6847c0983ec10bdd8ab4df69', 'user': {'_id': '65fba5700b78c48c9e393a3e', 'avatarUrl': '/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg', 'isPro': False, 'fullname': 'Yuyi Zhang', 'user': 'ZZXF', 'type': 'user'}, 'name': 'Yuyi Zhang', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-06-10T05:42:47.055Z', 'hidden': False}, {'_id': '6847c0983ec10bdd8ab4df6a', 'user': {'_id': '6616c9e090d2013d26a54b47', 'avatarUrl': '/avatars/573064303dcdcf778e1fbbfcff3c9a2b.svg', 'isPro': False, 'fullname': 'Shi', 'user': 'shiyx1', 'type': 'user'}, 'name': 'Yongxin Shi', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-10T05:20:28.050Z', 'hidden': False}, {'_id': '6847c0983ec10bdd8ab4df6b', 'name': 'Peirong Zhang', 'hidden': False}, {'_id': '6847c0983ec10bdd8ab4df6c', 'name': 'Yixin Zhao', 'hidden': False}, {'_id': '6847c0983ec10bdd8ab4df6d', 'name': 'Zhenhua Yang', 'hidden': False}, {'_id': '6847c0983ec10bdd8ab4df6e', 'user': {'_id': '66a102960072f5db18e860e3', 'avatarUrl': '/avatars/7679eddb31153c6b868cf496833551d6.svg', 'isPro': False, 'fullname': 'Lianwen Jin', 'user': 'lianwen', 'type': 'user'}, 'name': 'Lianwen Jin', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-10T05:20:28.050Z', 'hidden': False}], 'publishedAt': '2025-06-05T09:33:06.000Z', 'submittedOnDailyAt': '2025-06-10T04:16:00.729Z', 'title': 'MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character\\n  Recognition with over 97K Categories', 'submittedOnDailyBy': {'_id': '65fba5700b78c48c9e393a3e', 'avatarUrl': '/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg', 'isPro': False, 'fullname': 'Yuyi Zhang', 'user': 'ZZXF', 'type': 'user'}, 'summary': 'Foundational to the Chinese language and culture, Chinese characters\\nencompass extraordinarily extensive and ever-expanding categories, with the\\nlatest Chinese GB18030-2022 standard containing 87,887 categories. The accurate\\nrecognition of this vast number of characters, termed mega-category\\nrecognition, presents a formidable yet crucial challenge for cultural heritage\\npreservation and digital applications. Despite significant advances in Optical\\nCharacter Recognition (OCR), mega-category recognition remains unexplored due\\nto the absence of comprehensive datasets, with the largest existing dataset\\ncontaining merely 16,151 categories. To bridge this critical gap, we introduce\\nMegaHan97K, a mega-category, large-scale dataset covering an unprecedented\\n97,455 categories of Chinese characters. Our work offers three major\\ncontributions: (1) MegaHan97K is the first dataset to fully support the latest\\nGB18030-2022 standard, providing at least six times more categories than\\nexisting datasets; (2) It effectively addresses the long-tail distribution\\nproblem by providing balanced samples across all categories through its three\\ndistinct subsets: handwritten, historical and synthetic subsets; (3)\\nComprehensive benchmarking experiments reveal new challenges in mega-category\\nscenarios, including increased storage demands, morphologically similar\\ncharacter recognition, and zero-shot learning difficulties, while also\\nunlocking substantial opportunities for future research. To the best of our\\nknowledge, the MetaHan97K is likely the dataset with the largest classes not\\nonly in the field of OCR but may also in the broader domain of pattern\\nrecognition. The dataset is available at\\nhttps://github.com/SCUT-DLVCLab/MegaHan97K.', 'upvotes': 2, 'discussionId': '6847c0993ec10bdd8ab4df6f', 'ai_summary': 'MegaHan97K, a large-scale dataset for recognizing over 97,000 Chinese characters, addresses the long-tail distribution problem and reveals new challenges in mega-category OCR.', 'ai_keywords': ['Optical Character Recognition (OCR)', 'mega-category recognition', 'MegaHan97K', 'long-tail distribution', 'zero-shot learning']}, 'publishedAt': '2025-06-05T05:33:06.000Z', 'title': 'MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character\\n  Recognition with over 97K Categories', 'summary': 'Foundational to the Chinese language and culture, Chinese characters\\nencompass extraordinarily extensive and ever-expanding categories, with the\\nlatest Chinese GB18030-2022 standard containing 87,887 categories. The accurate\\nrecognition of this vast number of characters, termed mega-category\\nrecognition, presents a formidable yet crucial challenge for cultural heritage\\npreservation and digital applications. Despite significant advances in Optical\\nCharacter Recognition (OCR), mega-category recognition remains unexplored due\\nto the absence of comprehensive datasets, with the largest existing dataset\\ncontaining merely 16,151 categories. To bridge this critical gap, we introduce\\nMegaHan97K, a mega-category, large-scale dataset covering an unprecedented\\n97,455 categories of Chinese characters. Our work offers three major\\ncontributions: (1) MegaHan97K is the first dataset to fully support the latest\\nGB18030-2022 standard, providing at least six times more categories than\\nexisting datasets; (2) It effectively addresses the long-tail distribution\\nproblem by providing balanced samples across all categories through its three\\ndistinct subsets: handwritten, historical and synthetic subsets; (3)\\nComprehensive benchmarking experiments reveal new challenges in mega-category\\nscenarios, including increased storage demands, morphologically similar\\ncharacter recognition, and zero-shot learning difficulties, while also\\nunlocking substantial opportunities for future research. To the best of our\\nknowledge, the MetaHan97K is likely the dataset with the largest classes not\\nonly in the field of OCR but may also in the broader domain of pattern\\nrecognition. The dataset is available at\\nhttps://github.com/SCUT-DLVCLab/MegaHan97K.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04807.png', 'numComments': 1, 'submittedBy': {'_id': '65fba5700b78c48c9e393a3e', 'avatarUrl': '/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg', 'fullname': 'Yuyi Zhang', 'name': 'ZZXF', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.03690', 'authors': [{'_id': '684664f13ec10bdd8ab4dac0', 'user': {'_id': '64e6c617ecce34cb442cb208', 'avatarUrl': '/avatars/ebc61bf6a043314cb2089b1efd5e6a18.svg', 'isPro': False, 'fullname': 'JieSun', 'user': 'Sunshine279', 'type': 'user'}, 'name': 'Jie Sun', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:11:19.474Z', 'hidden': False}, {'_id': '684664f13ec10bdd8ab4dac1', 'name': 'Junkang Wu', 'hidden': False}, {'_id': '684664f13ec10bdd8ab4dac2', 'name': 'Jiancan Wu', 'hidden': False}, {'_id': '684664f13ec10bdd8ab4dac3', 'name': 'Zhibo Zhu', 'hidden': False}, {'_id': '684664f13ec10bdd8ab4dac4', 'name': 'Xingyu Lu', 'hidden': False}, {'_id': '684664f13ec10bdd8ab4dac5', 'name': 'Jun Zhou', 'hidden': False}, {'_id': '684664f13ec10bdd8ab4dac6', 'name': 'Lintao Ma', 'hidden': False}, {'_id': '684664f13ec10bdd8ab4dac7', 'name': 'Xiang Wang', 'hidden': False}], 'publishedAt': '2025-06-04T08:19:37.000Z', 'submittedOnDailyAt': '2025-06-10T00:51:18.490Z', 'title': 'Robust Preference Optimization via Dynamic Target Margins', 'submittedOnDailyBy': {'_id': '64e6c617ecce34cb442cb208', 'avatarUrl': '/avatars/ebc61bf6a043314cb2089b1efd5e6a18.svg', 'isPro': False, 'fullname': 'JieSun', 'user': 'Sunshine279', 'type': 'user'}, 'summary': 'The alignment of Large Language Models (LLMs) is crucial for ensuring their\\nsafety and reliability in practical applications. Direct Preference\\nOptimization (DPO) has emerged as an efficient method that directly optimizes\\nmodels using preference pairs, significantly reducing resource demands.\\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\\nfrequently compromised by noise. In this work, we propose gamma-PO, a\\ndynamic target margin preference optimization algorithm that adjust reward\\nmargins at the pairwise level. By introducing instance-specific margin\\ncalibration, gamma-PO strategically prioritizes high-confidence pairs (those\\ndemonstrating higher reward margins) while suppressing potential noise from\\nambiguous pairs. Moreover, gamma-PO is a plug-and-play method, compatible\\nwith variants of DPO that rely on reward margin between preference pairs.\\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, gamma-PO achieves an\\naverage 4.4\\\\% improvement over other baselines, setting new benchmarks for\\nstate-of-the-art performance. Additionally, gamma-PO requires minimal code\\nchanges and has a negligible impact on training efficiency, making it a robust\\nsolution for enhancing LLMs alignment. Our codes are available at\\nhttps://github.com/sunjie279/gammaPO{https://github.com/sunjie279/gammaPO}.', 'upvotes': 2, 'discussionId': '684664f13ec10bdd8ab4dac8', 'ai_summary': \"The paper introduces -PO, a dynamic target margin preference optimization algorithm that enhances Large Language Models' alignment by adjusting reward margins at the pairwise level, leading to improved performance with minimal impact on training.\", 'ai_keywords': ['Large Language Models', 'LLMs', 'Direct Preference Optimization', 'DPO', 'preference pairs', '-PO', 'instance-specific margin calibration', 'reward margins', 'AlpacaEval2', 'Arena-Hard', 'state-of-the-art performance']}, 'publishedAt': '2025-06-04T04:19:37.000Z', 'title': 'Robust Preference Optimization via Dynamic Target Margins', 'summary': 'The alignment of Large Language Models (LLMs) is crucial for ensuring their\\nsafety and reliability in practical applications. Direct Preference\\nOptimization (DPO) has emerged as an efficient method that directly optimizes\\nmodels using preference pairs, significantly reducing resource demands.\\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\\nfrequently compromised by noise. In this work, we propose gamma-PO, a\\ndynamic target margin preference optimization algorithm that adjust reward\\nmargins at the pairwise level. By introducing instance-specific margin\\ncalibration, gamma-PO strategically prioritizes high-confidence pairs (those\\ndemonstrating higher reward margins) while suppressing potential noise from\\nambiguous pairs. Moreover, gamma-PO is a plug-and-play method, compatible\\nwith variants of DPO that rely on reward margin between preference pairs.\\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, gamma-PO achieves an\\naverage 4.4\\\\% improvement over other baselines, setting new benchmarks for\\nstate-of-the-art performance. Additionally, gamma-PO requires minimal code\\nchanges and has a negligible impact on training efficiency, making it a robust\\nsolution for enhancing LLMs alignment. Our codes are available at\\nhttps://github.com/sunjie279/gammaPO{https://github.com/sunjie279/gammaPO}.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03690.png', 'numComments': 1, 'submittedBy': {'_id': '64e6c617ecce34cb442cb208', 'avatarUrl': '/avatars/ebc61bf6a043314cb2089b1efd5e6a18.svg', 'fullname': 'JieSun', 'name': 'Sunshine279', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.07645', 'authors': [{'_id': '6847ea583ec10bdd8ab4e05a', 'user': {'_id': '635270e36cfb8f14981312e7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/635270e36cfb8f14981312e7/FpqjdhABEDuxWR8k2zOwA.jpeg', 'isPro': False, 'fullname': 'Maciej Chrabszcz', 'user': 'mchraba', 'type': 'user'}, 'name': 'Maciej Chrabszcz', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:42:31.860Z', 'hidden': False}, {'_id': '6847ea583ec10bdd8ab4e05b', 'user': {'_id': '66dab47f8506f9b6cf5f08ed', 'avatarUrl': '/avatars/e6ba87adbaacdeccf8c4818596c655d0.svg', 'isPro': False, 'fullname': 'LLM Attack', 'user': 'llmAttack', 'type': 'user'}, 'name': 'Katarzyna Lorenc', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-10T08:18:33.059Z', 'hidden': False}, {'_id': '6847ea583ec10bdd8ab4e05c', 'name': 'Karolina Seweryn', 'hidden': False}], 'publishedAt': '2025-06-09T11:09:39.000Z', 'submittedOnDailyAt': '2025-06-10T06:49:10.646Z', 'title': 'Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models', 'submittedOnDailyBy': {'_id': '635270e36cfb8f14981312e7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/635270e36cfb8f14981312e7/FpqjdhABEDuxWR8k2zOwA.jpeg', 'isPro': False, 'fullname': 'Maciej Chrabszcz', 'user': 'mchraba', 'type': 'user'}, 'summary': 'Large language models (LLMs) have demonstrated impressive capabilities across\\nvarious natural language processing (NLP) tasks in recent years. However, their\\nsusceptibility to jailbreaks and perturbations necessitates additional\\nevaluations. Many LLMs are multilingual, but safety-related training data\\ncontains mainly high-resource languages like English. This can leave them\\nvulnerable to perturbations in low-resource languages such as Polish. We show\\nhow surprisingly strong attacks can be cheaply created by altering just a few\\ncharacters and using a small proxy model for word importance calculation. We\\nfind that these character and word-level attacks drastically alter the\\npredictions of different LLMs, suggesting a potential vulnerability that can be\\nused to circumvent their internal safety mechanisms. We validate our attack\\nconstruction methodology on Polish, a low-resource language, and find potential\\nvulnerabilities of LLMs in this language. Additionally, we show how it can be\\nextended to other languages. We release the created datasets and code for\\nfurther research.', 'upvotes': 1, 'discussionId': '6847ea583ec10bdd8ab4e05d', 'ai_summary': 'Character and word-level attacks using a proxy model reveal vulnerabilities in LLMs across languages, particularly in low-resource languages like Polish.', 'ai_keywords': ['large language models', 'natural language processing', 'jailbreaks', 'perturbations', 'multilingual', 'safety-related training data', 'high-resource languages', 'low-resource languages', 'character-level attacks', 'word-level attacks', 'word importance calculation', 'internal safety mechanisms']}, 'publishedAt': '2025-06-09T07:09:39.000Z', 'title': 'Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models', 'summary': 'Large language models (LLMs) have demonstrated impressive capabilities across\\nvarious natural language processing (NLP) tasks in recent years. However, their\\nsusceptibility to jailbreaks and perturbations necessitates additional\\nevaluations. Many LLMs are multilingual, but safety-related training data\\ncontains mainly high-resource languages like English. This can leave them\\nvulnerable to perturbations in low-resource languages such as Polish. We show\\nhow surprisingly strong attacks can be cheaply created by altering just a few\\ncharacters and using a small proxy model for word importance calculation. We\\nfind that these character and word-level attacks drastically alter the\\npredictions of different LLMs, suggesting a potential vulnerability that can be\\nused to circumvent their internal safety mechanisms. We validate our attack\\nconstruction methodology on Polish, a low-resource language, and find potential\\nvulnerabilities of LLMs in this language. Additionally, we show how it can be\\nextended to other languages. We release the created datasets and code for\\nfurther research.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07645.png', 'numComments': 1, 'submittedBy': {'_id': '635270e36cfb8f14981312e7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/635270e36cfb8f14981312e7/FpqjdhABEDuxWR8k2zOwA.jpeg', 'fullname': 'Maciej Chrabszcz', 'name': 'mchraba', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.23473', 'authors': [{'_id': '6847c9693ec10bdd8ab4df91', 'name': 'Xiaorui Wu', 'hidden': False}, {'_id': '6847c9693ec10bdd8ab4df92', 'name': 'Xiaofeng Mao', 'hidden': False}, {'_id': '6847c9693ec10bdd8ab4df93', 'name': 'Xin Zhang', 'hidden': False}, {'_id': '6847c9693ec10bdd8ab4df94', 'name': 'Fei Li', 'hidden': False}, {'_id': '6847c9693ec10bdd8ab4df95', 'name': 'Chong Teng', 'hidden': False}, {'_id': '6847c9693ec10bdd8ab4df96', 'name': 'Yuxiang Peng', 'hidden': False}, {'_id': '6847c9693ec10bdd8ab4df97', 'name': 'Li Zheng', 'hidden': False}, {'_id': '6847c9693ec10bdd8ab4df98', 'name': 'Donghong Ji', 'hidden': False}, {'_id': '6847c9693ec10bdd8ab4df99', 'name': 'Zhuang Li', 'hidden': False}], 'publishedAt': '2025-05-29T14:26:46.000Z', 'submittedOnDailyAt': '2025-06-10T04:28:59.228Z', 'title': 'EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions', 'submittedOnDailyBy': {'_id': '63d159132036e44c44f87a91', 'avatarUrl': '/avatars/6a2c9e5b3b25cf1949277d8c40c0070b.svg', 'isPro': False, 'fullname': 'Zhuang Li', 'user': 'lizhuang144', 'type': 'user'}, 'summary': 'Large language models (LLMs) frequently refuse to respond to pseudo-malicious\\ninstructions: semantically harmless input queries triggering unnecessary LLM\\nrefusals due to conservative safety alignment, significantly impairing user\\nexperience. Collecting such instructions is crucial for evaluating and\\nmitigating over-refusals, but existing instruction curation methods, like\\nmanual creation or instruction rewriting, either lack scalability or fail to\\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\\ngenerates diverse pseudo-malicious instructions consistently eliciting\\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\\nexploring the instruction space in more diverse directions than existing\\nmethods via mutation strategies and recombination, and iteratively evolves seed\\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\\npseudo-malicious instructions that outperforms the next-best benchmark with\\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\\nlexical diversity, and 40.03% improved LLM response confidence scores; and\\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\\nresponses for supervised and preference-based alignment training.\\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\\n14.31% fewer over-refusals than models trained on the second-best alignment\\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\\nmodels trigger over-refusals by overly focusing on sensitive keywords while\\nignoring broader context.', 'upvotes': 1, 'discussionId': '6847c9693ec10bdd8ab4df9a', 'ai_summary': 'EVOREFUSE, an evolutionary algorithm, generates diverse pseudo-malicious instructions to optimize LLM refusal training, improving user experience without compromising safety.', 'ai_keywords': ['large language models', 'pseudo-malicious instructions', 'safety alignment', 'instruction optimization', 'evolutionary algorithm', 'mutation strategies', 'recombination', 'evidence lower bound', 'refusal probability', 'lexical diversity', 'LLM response confidence scores', 'over-refusals', 'supervised fine-tuning', 'preference-based alignment training']}, 'publishedAt': '2025-05-29T10:26:46.000Z', 'title': 'EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions', 'summary': 'Large language models (LLMs) frequently refuse to respond to pseudo-malicious\\ninstructions: semantically harmless input queries triggering unnecessary LLM\\nrefusals due to conservative safety alignment, significantly impairing user\\nexperience. Collecting such instructions is crucial for evaluating and\\nmitigating over-refusals, but existing instruction curation methods, like\\nmanual creation or instruction rewriting, either lack scalability or fail to\\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\\ngenerates diverse pseudo-malicious instructions consistently eliciting\\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\\nexploring the instruction space in more diverse directions than existing\\nmethods via mutation strategies and recombination, and iteratively evolves seed\\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\\npseudo-malicious instructions that outperforms the next-best benchmark with\\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\\nlexical diversity, and 40.03% improved LLM response confidence scores; and\\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\\nresponses for supervised and preference-based alignment training.\\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\\n14.31% fewer over-refusals than models trained on the second-best alignment\\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\\nmodels trigger over-refusals by overly focusing on sensitive keywords while\\nignoring broader context.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23473.png', 'numComments': 1, 'submittedBy': {'_id': '63d159132036e44c44f87a91', 'avatarUrl': '/avatars/6a2c9e5b3b25cf1949277d8c40c0070b.svg', 'fullname': 'Zhuang Li', 'name': 'lizhuang144', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.07833', 'authors': [{'_id': '68480ea5308cb7e626e80e27', 'user': {'_id': '65dd7b2fbdcae7f0e9bc6442', 'avatarUrl': '/avatars/bae0cf61547048f82d071f48f70fe237.svg', 'isPro': False, 'fullname': 'Michael Chen', 'user': 'michaelchenkj', 'type': 'user'}, 'name': 'Michael K. Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T10:59:50.276Z', 'hidden': False}, {'_id': '68480ea5308cb7e626e80e28', 'name': 'Xikun Zhang', 'hidden': False}, {'_id': '68480ea5308cb7e626e80e29', 'name': 'Jiaxing Huang', 'hidden': False}, {'_id': '68480ea5308cb7e626e80e2a', 'name': 'Dacheng Tao', 'hidden': False}], 'publishedAt': '2025-06-09T14:55:00.000Z', 'submittedOnDailyAt': '2025-06-10T09:27:43.828Z', 'title': 'Improving large language models with concept-aware fine-tuning', 'submittedOnDailyBy': {'_id': '65dd7b2fbdcae7f0e9bc6442', 'avatarUrl': '/avatars/bae0cf61547048f82d071f48f70fe237.svg', 'isPro': False, 'fullname': 'Michael Chen', 'user': 'michaelchenkj', 'type': 'user'}, 'summary': 'Large language models (LLMs) have become the cornerstone of modern AI.\\nHowever, the existing paradigm of next-token prediction fundamentally limits\\ntheir ability to form coherent, high-level concepts, making it a critical\\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\\nsequentially, rather than grasping the phrase as a unified, coherent semantic\\nentity. This fragmented representation hinders deeper conceptual understanding\\nand, ultimately, the development of truly intelligent systems. In response, we\\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\\nthat span multiple tokens, this method fosters stronger concept-aware learning.\\nOur experiments demonstrate significant improvements compared to conventional\\nnext-token finetuning methods across diverse tasks, including traditional\\napplications like text summarization and domain-specific ones like de novo\\nprotein design. Multi-token prediction was previously only possible in the\\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\\nto bring the multi-token setting to the post-training phase, thus effectively\\ndemocratizing its benefits for the broader community of practitioners and\\nresearchers. Finally, the unexpected effectiveness of our proposed method\\nsuggests wider implications for the machine learning research community. All\\ncode and data are available at https://github.com/michaelchen-lab/caft-llm', 'upvotes': 0, 'discussionId': '68480ea6308cb7e626e80e2b', 'ai_summary': 'Concept-Aware Fine-Tuning (CAFT) enhances large language models by enabling multi-token learning in the fine-tuning phase, leading to improved coherent understanding and better performance across various tasks.', 'ai_keywords': ['Concept-Aware Fine-Tuning', 'CAFT', 'multi-token training', 'next-token prediction', 'token representation', 'coherent semantic entity', 'concept-aware learning', 'text summarization', 'de novo protein design', 'multi-token prediction', 'pretraining phase', 'post-training phase']}, 'publishedAt': '2025-06-09T10:55:00.000Z', 'title': 'Improving large language models with concept-aware fine-tuning', 'summary': 'Large language models (LLMs) have become the cornerstone of modern AI.\\nHowever, the existing paradigm of next-token prediction fundamentally limits\\ntheir ability to form coherent, high-level concepts, making it a critical\\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\\nsequentially, rather than grasping the phrase as a unified, coherent semantic\\nentity. This fragmented representation hinders deeper conceptual understanding\\nand, ultimately, the development of truly intelligent systems. In response, we\\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\\nthat span multiple tokens, this method fosters stronger concept-aware learning.\\nOur experiments demonstrate significant improvements compared to conventional\\nnext-token finetuning methods across diverse tasks, including traditional\\napplications like text summarization and domain-specific ones like de novo\\nprotein design. Multi-token prediction was previously only possible in the\\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\\nto bring the multi-token setting to the post-training phase, thus effectively\\ndemocratizing its benefits for the broader community of practitioners and\\nresearchers. Finally, the unexpected effectiveness of our proposed method\\nsuggests wider implications for the machine learning research community. All\\ncode and data are available at https://github.com/michaelchen-lab/caft-llm', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07833.png', 'numComments': 1, 'submittedBy': {'_id': '65dd7b2fbdcae7f0e9bc6442', 'avatarUrl': '/avatars/bae0cf61547048f82d071f48f70fe237.svg', 'fullname': 'Michael Chen', 'name': 'michaelchenkj', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.06905', 'authors': [{'_id': '6847d8503ec10bdd8ab4dfd7', 'user': {'_id': '638a5b46002d85749924d5c8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/YA4i44NwKidCTrCANMvMB.png', 'isPro': False, 'fullname': 'Akash Gupta', 'user': 'aksgupta97', 'type': 'user'}, 'name': 'Akash Gupta', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T08:42:36.181Z', 'hidden': False}, {'_id': '6847d8503ec10bdd8ab4dfd8', 'name': 'Amos Storkey', 'hidden': False}, {'_id': '6847d8503ec10bdd8ab4dfd9', 'name': 'Mirella Lapata', 'hidden': False}], 'publishedAt': '2025-06-07T19:37:22.000Z', 'submittedOnDailyAt': '2025-06-10T12:18:08.000Z', 'title': 'Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering', 'submittedOnDailyBy': {'_id': '638a5b46002d85749924d5c8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/YA4i44NwKidCTrCANMvMB.png', 'isPro': False, 'fullname': 'Akash Gupta', 'user': 'aksgupta97', 'type': 'user'}, 'summary': 'Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to\\nperform new tasks with minimal supervision. However, ICL performance,\\nespecially in smaller LMMs, is inconsistent and does not always improve\\nmonotonically with increasing examples. We hypothesize that this occurs due to\\nthe LMM being overwhelmed by additional information present in the image\\nembeddings, which is not required for the downstream task. To address this, we\\npropose a meta-learning approach that provides an alternative for inducing\\nfew-shot capabilities in LMMs, using a fixed set of soft prompts that are\\ndistilled from task-relevant image features and can be adapted at test time\\nusing a few examples. To facilitate this distillation, we introduce an\\nattention-mapper module that can be easily integrated with the popular LLaVA\\nv1.5 architecture and is jointly learned with soft prompts, enabling task\\nadaptation in LMMs under low-data regimes with just a few gradient steps.\\nEvaluation on the VL-ICL Bench shows that our method consistently outperforms\\nICL and related prompt-tuning approaches, even under image perturbations,\\nimproving task induction and reasoning across visual question answering tasks.', 'upvotes': 0, 'discussionId': '6847d8503ec10bdd8ab4dfda', 'ai_summary': 'A meta-learning approach with soft prompts and an attention-mapper module improves few-shot capabilities in LMMs, outperforming ICL and related methods in visual question answering tasks.', 'ai_keywords': ['in-context learning', 'meta-learning', 'soft prompts', 'attention-mapper module', 'LLaVA', 'few-shot capabilities', 'task adaptation', 'low-data regimes', 'gradient steps', 'visual question answering']}, 'publishedAt': '2025-06-07T15:37:22.000Z', 'title': 'Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering', 'summary': 'Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to\\nperform new tasks with minimal supervision. However, ICL performance,\\nespecially in smaller LMMs, is inconsistent and does not always improve\\nmonotonically with increasing examples. We hypothesize that this occurs due to\\nthe LMM being overwhelmed by additional information present in the image\\nembeddings, which is not required for the downstream task. To address this, we\\npropose a meta-learning approach that provides an alternative for inducing\\nfew-shot capabilities in LMMs, using a fixed set of soft prompts that are\\ndistilled from task-relevant image features and can be adapted at test time\\nusing a few examples. To facilitate this distillation, we introduce an\\nattention-mapper module that can be easily integrated with the popular LLaVA\\nv1.5 architecture and is jointly learned with soft prompts, enabling task\\nadaptation in LMMs under low-data regimes with just a few gradient steps.\\nEvaluation on the VL-ICL Bench shows that our method consistently outperforms\\nICL and related prompt-tuning approaches, even under image perturbations,\\nimproving task induction and reasoning across visual question answering tasks.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06905.png', 'numComments': 1, 'submittedBy': {'_id': '638a5b46002d85749924d5c8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/YA4i44NwKidCTrCANMvMB.png', 'fullname': 'Akash Gupta', 'name': 'aksgupta97', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.05904', 'authors': [{'_id': '6847e05a3ec10bdd8ab4e03d', 'user': {'_id': '6369b1d456d1f93498130a8a', 'avatarUrl': '/avatars/8ec228aa6f171715652511f948765db9.svg', 'isPro': False, 'fullname': 'Yichi Zhang', 'user': '594zyc', 'type': 'user'}, 'name': 'Yichi Zhang', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-10T07:35:55.259Z', 'hidden': False}, {'_id': '6847e05a3ec10bdd8ab4e03e', 'name': 'Xin Luna Dong', 'hidden': False}, {'_id': '6847e05a3ec10bdd8ab4e03f', 'name': 'Zhaojiang Lin', 'hidden': False}, {'_id': '6847e05a3ec10bdd8ab4e040', 'name': 'Andrea Madotto', 'hidden': False}, {'_id': '6847e05a3ec10bdd8ab4e041', 'name': 'Anuj Kumar', 'hidden': False}, {'_id': '6847e05a3ec10bdd8ab4e042', 'name': 'Babak Damavandi', 'hidden': False}, {'_id': '6847e05a3ec10bdd8ab4e043', 'name': 'Joyce Chai', 'hidden': False}, {'_id': '6847e05a3ec10bdd8ab4e044', 'name': 'Seungwhan Moon', 'hidden': False}], 'publishedAt': '2025-06-06T09:23:29.000Z', 'submittedOnDailyAt': '2025-06-10T06:07:28.858Z', 'title': 'Proactive Assistant Dialogue Generation from Streaming Egocentric Videos', 'submittedOnDailyBy': {'_id': '6369b1d456d1f93498130a8a', 'avatarUrl': '/avatars/8ec228aa6f171715652511f948765db9.svg', 'isPro': False, 'fullname': 'Yichi Zhang', 'user': '594zyc', 'type': 'user'}, 'summary': 'Recent advances in conversational AI have been substantial, but developing\\nreal-time systems for perceptual task guidance remains challenging. These\\nsystems must provide interactive, proactive assistance based on streaming\\nvisual inputs, yet their development is constrained by the costly and\\nlabor-intensive process of data collection and system evaluation. To address\\nthese limitations, we present a comprehensive framework with three key\\ncontributions. First, we introduce a novel data curation pipeline that\\nsynthesizes dialogues from annotated egocentric videos, resulting in \\\\dataset,\\na large-scale synthetic dialogue dataset spanning multiple domains. Second, we\\ndevelop a suite of automatic evaluation metrics, validated through extensive\\nhuman studies. Third, we propose an end-to-end model that processes streaming\\nvideo inputs to generate contextually appropriate responses, incorporating\\nnovel techniques for handling data imbalance and long-duration videos. This\\nwork lays the foundation for developing real-time, proactive AI assistants\\ncapable of guiding users through diverse tasks. Project page:\\nhttps://pro-assist.github.io/', 'upvotes': 0, 'discussionId': '6847e05a3ec10bdd8ab4e045', 'projectPage': 'https://pro-assist.github.io/', 'ai_summary': 'A framework provides automated data synthesis, evaluation metrics, and an end-to-end model for real-time, proactive conversational AI task guidance using streaming video inputs.', 'ai_keywords': ['data curation pipeline', 'synthetic dialogue dataset', 'automatic evaluation metrics', 'end-to-end model', 'data imbalance', 'long-duration videos']}, 'publishedAt': '2025-06-06T05:23:29.000Z', 'title': 'Proactive Assistant Dialogue Generation from Streaming Egocentric Videos', 'summary': 'Recent advances in conversational AI have been substantial, but developing\\nreal-time systems for perceptual task guidance remains challenging. These\\nsystems must provide interactive, proactive assistance based on streaming\\nvisual inputs, yet their development is constrained by the costly and\\nlabor-intensive process of data collection and system evaluation. To address\\nthese limitations, we present a comprehensive framework with three key\\ncontributions. First, we introduce a novel data curation pipeline that\\nsynthesizes dialogues from annotated egocentric videos, resulting in \\\\dataset,\\na large-scale synthetic dialogue dataset spanning multiple domains. Second, we\\ndevelop a suite of automatic evaluation metrics, validated through extensive\\nhuman studies. Third, we propose an end-to-end model that processes streaming\\nvideo inputs to generate contextually appropriate responses, incorporating\\nnovel techniques for handling data imbalance and long-duration videos. This\\nwork lays the foundation for developing real-time, proactive AI assistants\\ncapable of guiding users through diverse tasks. Project page:\\nhttps://pro-assist.github.io/', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05904.png', 'numComments': 1, 'submittedBy': {'_id': '6369b1d456d1f93498130a8a', 'avatarUrl': '/avatars/8ec228aa6f171715652511f948765db9.svg', 'fullname': 'Yichi Zhang', 'name': '594zyc', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}"
]