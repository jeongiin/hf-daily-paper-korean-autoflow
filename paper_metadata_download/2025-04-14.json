[
    "{'paper': {'id': '2504.08685', 'authors': [{'_id': '67fc6ffc59b22e7c34d64c2e', 'name': 'Team Seawead', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c2f', 'name': 'Ceyuan Yang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c30', 'user': {'_id': '64415957bd0c9726529802f6', 'avatarUrl': '/avatars/1132d1ee68fb58ec635d57c8175caacd.svg', 'isPro': False, 'fullname': 'Zhijie Lin', 'user': 'Ikuinen', 'type': 'user'}, 'name': 'Zhijie Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-04-14T13:17:03.433Z', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c31', 'name': 'Yang Zhao', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c32', 'name': 'Shanchuan Lin', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c33', 'user': {'_id': '645d5a44680734460f9e7742', 'avatarUrl': '/avatars/66687e2c413acce55436d967071f8786.svg', 'isPro': False, 'fullname': 'Zhibei Ma', 'user': 'Brightmzb', 'type': 'user'}, 'name': 'Zhibei Ma', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-04-14T13:19:16.342Z', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c34', 'name': 'Haoyuan Guo', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c35', 'name': 'Hao Chen', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c36', 'name': 'Lu Qi', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c37', 'name': 'Sen Wang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c38', 'name': 'Feng Cheng', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c39', 'name': 'Feilong Zuo Xuejiao Zeng', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c3a', 'user': {'_id': '63c98de85fdc575773c52098', 'avatarUrl': '/avatars/b66675e781695daed7c00a7c802c91f5.svg', 'isPro': False, 'fullname': 'Ziyan Yang', 'user': 'ziyany', 'type': 'user'}, 'name': 'Ziyan Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-04-14T13:20:01.373Z', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c3b', 'name': 'Fangyuan Kong', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c3c', 'name': 'Zhiwu Qing', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c3d', 'name': 'Fei Xiao', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c3e', 'name': 'Meng Wei', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c3f', 'name': 'Tuyen Hoang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c40', 'name': 'Siyu Zhang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c41', 'name': 'Peihao Zhu', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c42', 'name': 'Qi Zhao', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c43', 'name': 'Jiangqiao Yan', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c44', 'name': 'Liangke Gui', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c45', 'name': 'Sheng Bi', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c46', 'name': 'Jiashi Li', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c47', 'name': 'Yuxi Ren', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c48', 'name': 'Rui Wang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c49', 'name': 'Huixia Li', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c4a', 'name': 'Xuefeng Xiao', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c4b', 'name': 'Shu Liu', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c4c', 'user': {'_id': '636a4e4fa55bbbdf8c877667', 'avatarUrl': '/avatars/efdb68c56a4a44fdac52750c07a6cc35.svg', 'isPro': False, 'fullname': 'Ling Feng', 'user': 'lingff', 'type': 'user'}, 'name': 'Feng Ling', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-14T09:46:31.964Z', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c4d', 'name': 'Heng Zhang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c4e', 'name': 'Houmin Wei', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c4f', 'name': 'Huafeng Kuang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c50', 'name': 'Jerry Duncan', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c51', 'name': 'Junda Zhang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c52', 'name': 'Junru Zheng', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c53', 'name': 'Li Sun', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c54', 'name': 'Manlin Zhang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c55', 'name': 'Renfei Sun', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c56', 'name': 'Xiaobin Zhuang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c57', 'name': 'Xiaojie Li', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c58', 'name': 'Xin Xia', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c59', 'name': 'Xuyan Chi', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c5a', 'name': 'Yanghua Peng', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c5b', 'name': 'Yuping Wang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c5c', 'name': 'Yuxuan Wang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c5d', 'name': 'Zhongkai Zhao', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c5e', 'name': 'Zhuo Chen', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c5f', 'name': 'Zuquan Song', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c60', 'user': {'_id': '6421183b69a2c2933882d652', 'avatarUrl': '/avatars/66813a8fa22915087cccd4dbfb945ca7.svg', 'isPro': False, 'fullname': 'Zhenheng Yang', 'user': 'zhenheny', 'type': 'user'}, 'name': 'Zhenheng Yang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-14T09:46:34.053Z', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c61', 'name': 'Jiashi Feng', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c62', 'name': 'Jianchao Yang', 'hidden': False}, {'_id': '67fc6ffc59b22e7c34d64c63', 'name': 'Lu Jiang', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg'], 'publishedAt': '2025-04-11T16:46:20.000Z', 'submittedOnDailyAt': '2025-04-14T00:55:27.428Z', 'title': 'Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model', 'submittedOnDailyBy': {'_id': '64a5cba3bea0116f8f7187a7', 'avatarUrl': '/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg', 'isPro': False, 'fullname': 'Lu Jiang', 'user': 'roadjiang', 'type': 'user'}, 'summary': 'This technical report presents a cost-efficient strategy for training a video\\ngeneration foundation model. We present a mid-sized research model with\\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\\nresources, Seaweed-7B demonstrates highly competitive performance compared to\\ncontemporary video generation models of much larger size. Design choices are\\nespecially crucial in a resource-constrained setting. This technical report\\nhighlights the key design decisions that enhance the performance of the\\nmedium-sized diffusion model. Empirically, we make two observations: (1)\\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\\ntrained on substantially greater GPU resources, and (2) our model, which\\nexhibits strong generalization ability, can be effectively adapted across a\\nwide range of downstream applications either by lightweight fine-tuning or\\ncontinue training. See the project page at https://seaweed.video/', 'upvotes': 70, 'discussionId': '67fc700159b22e7c34d64d78', 'projectPage': 'https://seaweed.video/'}, 'publishedAt': '2025-04-11T12:46:20.000Z', 'title': 'Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model', 'summary': 'This technical report presents a cost-efficient strategy for training a video\\ngeneration foundation model. We present a mid-sized research model with\\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\\nresources, Seaweed-7B demonstrates highly competitive performance compared to\\ncontemporary video generation models of much larger size. Design choices are\\nespecially crucial in a resource-constrained setting. This technical report\\nhighlights the key design decisions that enhance the performance of the\\nmedium-sized diffusion model. Empirically, we make two observations: (1)\\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\\ntrained on substantially greater GPU resources, and (2) our model, which\\nexhibits strong generalization ability, can be effectively adapted across a\\nwide range of downstream applications either by lightweight fine-tuning or\\ncontinue training. See the project page at https://seaweed.video/', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08685.png', 'numComments': 8, 'submittedBy': {'_id': '64a5cba3bea0116f8f7187a7', 'avatarUrl': '/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg', 'fullname': 'Lu Jiang', 'name': 'roadjiang', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.08736', 'authors': [{'_id': '67fc8e37864dfcbd93d3b802', 'user': {'_id': '668125557b50b433cda2a211', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png', 'isPro': False, 'fullname': 'Tianwei Xiong', 'user': 'YuuTennYi', 'type': 'user'}, 'name': 'Tianwei Xiong', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-14T09:46:29.330Z', 'hidden': False}, {'_id': '67fc8e37864dfcbd93d3b803', 'name': 'Jun Hao Liew', 'hidden': False}, {'_id': '67fc8e37864dfcbd93d3b804', 'name': 'Zilong Huang', 'hidden': False}, {'_id': '67fc8e37864dfcbd93d3b805', 'name': 'Jiashi Feng', 'hidden': False}, {'_id': '67fc8e37864dfcbd93d3b806', 'user': {'_id': '65d5ec74cd05bc1eaa125040', 'avatarUrl': '/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg', 'isPro': False, 'fullname': 'Xihui Liu', 'user': 'XihuiLiu', 'type': 'user'}, 'name': 'Xihui Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-14T09:46:26.140Z', 'hidden': False}], 'publishedAt': '2025-04-11T17:59:58.000Z', 'submittedOnDailyAt': '2025-04-14T02:57:04.488Z', 'title': 'GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\\n  Autoregressive Image Generation', 'submittedOnDailyBy': {'_id': '668125557b50b433cda2a211', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png', 'isPro': False, 'fullname': 'Tianwei Xiong', 'user': 'YuuTennYi', 'type': 'user'}, 'summary': 'In autoregressive (AR) image generation, visual tokenizers compress images\\ninto compact discrete latent tokens, enabling efficient training of downstream\\nautoregressive models for visual generation via next-token prediction. While\\nscaling visual tokenizers improves image reconstruction quality, it often\\ndegrades downstream generation quality -- a challenge not adequately addressed\\nin existing literature. To address this, we introduce GigaTok, the first\\napproach to simultaneously improve image reconstruction, generation, and\\nrepresentation learning when scaling visual tokenizers. We identify the growing\\ncomplexity of latent space as the key factor behind the reconstruction vs.\\ngeneration dilemma. To mitigate this, we propose semantic regularization, which\\naligns tokenizer features with semantically consistent features from a\\npre-trained visual encoder. This constraint prevents excessive latent space\\ncomplexity during scaling, yielding consistent improvements in both\\nreconstruction and downstream autoregressive generation. Building on semantic\\nregularization, we explore three key practices for scaling tokenizers:(1) using\\n1D tokenizers for better scalability, (2) prioritizing decoder scaling when\\nexpanding both encoder and decoder, and (3) employing entropy loss to stabilize\\ntraining for billion-scale tokenizers. By scaling to 3 space billion\\nparameters, GigaTok achieves state-of-the-art performance in reconstruction,\\ndownstream AR generation, and downstream AR representation quality.', 'upvotes': 27, 'discussionId': '67fc8e38864dfcbd93d3b836', 'projectPage': 'https://silentview.github.io/GigaTok/', 'githubRepo': 'https://github.com/SilentView/GigaTok'}, 'publishedAt': '2025-04-11T13:59:58.000Z', 'title': 'GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\\n  Autoregressive Image Generation', 'summary': 'In autoregressive (AR) image generation, visual tokenizers compress images\\ninto compact discrete latent tokens, enabling efficient training of downstream\\nautoregressive models for visual generation via next-token prediction. While\\nscaling visual tokenizers improves image reconstruction quality, it often\\ndegrades downstream generation quality -- a challenge not adequately addressed\\nin existing literature. To address this, we introduce GigaTok, the first\\napproach to simultaneously improve image reconstruction, generation, and\\nrepresentation learning when scaling visual tokenizers. We identify the growing\\ncomplexity of latent space as the key factor behind the reconstruction vs.\\ngeneration dilemma. To mitigate this, we propose semantic regularization, which\\naligns tokenizer features with semantically consistent features from a\\npre-trained visual encoder. This constraint prevents excessive latent space\\ncomplexity during scaling, yielding consistent improvements in both\\nreconstruction and downstream autoregressive generation. Building on semantic\\nregularization, we explore three key practices for scaling tokenizers:(1) using\\n1D tokenizers for better scalability, (2) prioritizing decoder scaling when\\nexpanding both encoder and decoder, and (3) employing entropy loss to stabilize\\ntraining for billion-scale tokenizers. By scaling to 3 space billion\\nparameters, GigaTok achieves state-of-the-art performance in reconstruction,\\ndownstream AR generation, and downstream AR representation quality.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08736.png', 'numComments': 1, 'submittedBy': {'_id': '668125557b50b433cda2a211', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png', 'fullname': 'Tianwei Xiong', 'name': 'YuuTennYi', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2504.08388', 'authors': [{'_id': '67fc7367df5f5d1e87c14c6a', 'name': 'Junliang Guo', 'hidden': False}, {'_id': '67fc7367df5f5d1e87c14c6b', 'name': 'Yang Ye', 'hidden': False}, {'_id': '67fc7367df5f5d1e87c14c6c', 'name': 'Tianyu He', 'hidden': False}, {'_id': '67fc7367df5f5d1e87c14c6d', 'name': 'Haoyu Wu', 'hidden': False}, {'_id': '67fc7367df5f5d1e87c14c6e', 'name': 'Yushu Jiang', 'hidden': False}, {'_id': '67fc7367df5f5d1e87c14c6f', 'name': 'Tim Pearce', 'hidden': False}, {'_id': '67fc7367df5f5d1e87c14c70', 'name': 'Jiang Bian', 'hidden': False}], 'publishedAt': '2025-04-11T09:41:04.000Z', 'submittedOnDailyAt': '2025-04-14T02:07:06.500Z', 'title': 'MineWorld: a Real-Time and Open-Source Interactive World Model on\\n  Minecraft', 'submittedOnDailyBy': {'_id': '63468720dd6d90d82ccf3450', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg', 'isPro': False, 'fullname': 'YSH', 'user': 'BestWishYsh', 'type': 'user'}, 'summary': 'World modeling is a crucial task for enabling intelligent agents to\\neffectively interact with humans and operate in dynamic environments. In this\\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\\nan open-ended sandbox game which has been utilized as a common testbed for\\nworld modeling. MineWorld is driven by a visual-action autoregressive\\nTransformer, which takes paired game scenes and corresponding actions as input,\\nand generates consequent new scenes following the actions. Specifically, by\\ntransforming visual game scenes and actions into discrete token ids with an\\nimage tokenizer and an action tokenizer correspondingly, we consist the model\\ninput with the concatenation of the two kinds of ids interleaved. The model is\\nthen trained with next token prediction to learn rich representations of game\\nstates as well as the conditions between states and actions simultaneously. In\\ninference, we develop a novel parallel decoding algorithm that predicts the\\nspatial redundant tokens in each frame at the same time, letting models in\\ndifferent scales generate 4 to 7 frames per second and enabling real-time\\ninteractions with game players. In evaluation, we propose new metrics to assess\\nnot only visual quality but also the action following capacity when generating\\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\\nbased world models significantly. The code and model have been released.', 'upvotes': 16, 'discussionId': '67fc7367df5f5d1e87c14ca6', 'githubRepo': 'https://github.com/microsoft/MineWorld'}, 'publishedAt': '2025-04-11T05:41:04.000Z', 'title': 'MineWorld: a Real-Time and Open-Source Interactive World Model on\\n  Minecraft', 'summary': 'World modeling is a crucial task for enabling intelligent agents to\\neffectively interact with humans and operate in dynamic environments. In this\\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\\nan open-ended sandbox game which has been utilized as a common testbed for\\nworld modeling. MineWorld is driven by a visual-action autoregressive\\nTransformer, which takes paired game scenes and corresponding actions as input,\\nand generates consequent new scenes following the actions. Specifically, by\\ntransforming visual game scenes and actions into discrete token ids with an\\nimage tokenizer and an action tokenizer correspondingly, we consist the model\\ninput with the concatenation of the two kinds of ids interleaved. The model is\\nthen trained with next token prediction to learn rich representations of game\\nstates as well as the conditions between states and actions simultaneously. In\\ninference, we develop a novel parallel decoding algorithm that predicts the\\nspatial redundant tokens in each frame at the same time, letting models in\\ndifferent scales generate 4 to 7 frames per second and enabling real-time\\ninteractions with game players. In evaluation, we propose new metrics to assess\\nnot only visual quality but also the action following capacity when generating\\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\\nbased world models significantly. The code and model have been released.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08388.png', 'numComments': 2, 'submittedBy': {'_id': '63468720dd6d90d82ccf3450', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg', 'fullname': 'YSH', 'name': 'BestWishYsh', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 46}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.08600', 'authors': [{'_id': '67fc9e72b2383c63dc413dcb', 'name': 'Peixian Ma', 'hidden': False}, {'_id': '67fc9e72b2383c63dc413dcc', 'user': {'_id': '6575a625b951d40e7a4d8685', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg', 'isPro': False, 'fullname': 'zhuangxialie', 'user': 'ZhuangXialie', 'type': 'user'}, 'name': 'Xialie Zhuang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-14T09:46:23.810Z', 'hidden': False}, {'_id': '67fc9e72b2383c63dc413dcd', 'name': 'Chengjin Xu', 'hidden': False}, {'_id': '67fc9e72b2383c63dc413dce', 'name': 'Xuhui Jiang', 'hidden': False}, {'_id': '67fc9e72b2383c63dc413dcf', 'name': 'Ran Chen', 'hidden': False}, {'_id': '67fc9e72b2383c63dc413dd0', 'name': 'Jian Guo', 'hidden': False}], 'publishedAt': '2025-04-11T15:01:30.000Z', 'submittedOnDailyAt': '2025-04-14T04:07:17.501Z', 'title': 'SQL-R1: Training Natural Language to SQL Reasoning Model By\\n  Reinforcement Learning', 'submittedOnDailyBy': {'_id': '6575a625b951d40e7a4d8685', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg', 'isPro': False, 'fullname': 'zhuangxialie', 'user': 'ZhuangXialie', 'type': 'user'}, 'summary': 'Natural Language to SQL (NL2SQL) enables intuitive interactions with\\ndatabases by transforming natural language queries into structured SQL\\nstatements. Despite recent advancements in enhancing human-computer interaction\\nwithin database applications, significant challenges persist, particularly\\nregarding the inference performance in complex scenarios involving multi-table\\njoins and nested queries. Current methodologies primarily utilize supervised\\nfine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and\\ninterpretability in new environments (e.g., finance and healthcare). In order\\nto enhance the reasoning performance of the NL2SQL model in the above complex\\nsituations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the\\nreinforcement learning (RL) algorithms. We design a specialized RL-based reward\\nfunction tailored for NL2SQL tasks and discussed the impact of cold start on\\nthe effectiveness of intensive training. In addition, we achieve competitive\\naccuracy using only a tiny amount of synthetic NL2SQL data for augmented\\ntraining and further explore data engineering for RL. In existing experiments,\\nSQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider\\nand BIRD, respectively, only using the 7B base model.', 'upvotes': 9, 'discussionId': '67fc9e73b2383c63dc413e19'}, 'publishedAt': '2025-04-11T11:01:30.000Z', 'title': 'SQL-R1: Training Natural Language to SQL Reasoning Model By\\n  Reinforcement Learning', 'summary': 'Natural Language to SQL (NL2SQL) enables intuitive interactions with\\ndatabases by transforming natural language queries into structured SQL\\nstatements. Despite recent advancements in enhancing human-computer interaction\\nwithin database applications, significant challenges persist, particularly\\nregarding the inference performance in complex scenarios involving multi-table\\njoins and nested queries. Current methodologies primarily utilize supervised\\nfine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and\\ninterpretability in new environments (e.g., finance and healthcare). In order\\nto enhance the reasoning performance of the NL2SQL model in the above complex\\nsituations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the\\nreinforcement learning (RL) algorithms. We design a specialized RL-based reward\\nfunction tailored for NL2SQL tasks and discussed the impact of cold start on\\nthe effectiveness of intensive training. In addition, we achieve competitive\\naccuracy using only a tiny amount of synthetic NL2SQL data for augmented\\ntraining and further explore data engineering for RL. In existing experiments,\\nSQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider\\nand BIRD, respectively, only using the 7B base model.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08600.png', 'numComments': 1, 'submittedBy': {'_id': '6575a625b951d40e7a4d8685', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg', 'fullname': 'zhuangxialie', 'name': 'ZhuangXialie', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.07963', 'authors': [{'_id': '67f86da6ac109135e18e150f', 'name': 'Shoufa Chen', 'hidden': False}, {'_id': '67f86da6ac109135e18e1510', 'name': 'Chongjian Ge', 'hidden': False}, {'_id': '67f86da6ac109135e18e1511', 'name': 'Shilong Zhang', 'hidden': False}, {'_id': '67f86da6ac109135e18e1512', 'name': 'Peize Sun', 'hidden': False}, {'_id': '67f86da6ac109135e18e1513', 'name': 'Ping Luo', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg'], 'publishedAt': '2025-04-10T17:59:56.000Z', 'submittedOnDailyAt': '2025-04-14T04:05:17.975Z', 'title': 'PixelFlow: Pixel-Space Generative Models with Flow', 'submittedOnDailyBy': {'_id': '6412a33900634c4fe9873652', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg', 'isPro': False, 'fullname': 'Shoufa Chen', 'user': 'ShoufaChen', 'type': 'user'}, 'summary': 'We present PixelFlow, a family of image generation models that operate\\ndirectly in the raw pixel space, in contrast to the predominant latent-space\\nmodels. This approach simplifies the image generation process by eliminating\\nthe need for a pre-trained Variational Autoencoder (VAE) and enabling the whole\\nmodel end-to-end trainable. Through efficient cascade flow modeling, PixelFlow\\nachieves affordable computation cost in pixel space. It achieves an FID of 1.98\\non 256times256 ImageNet class-conditional image generation benchmark. The\\nqualitative text-to-image results demonstrate that PixelFlow excels in image\\nquality, artistry, and semantic control. We hope this new paradigm will inspire\\nand open up new opportunities for next-generation visual generation models.\\nCode and models are available at https://github.com/ShoufaChen/PixelFlow.', 'upvotes': 9, 'discussionId': '67f86da7ac109135e18e154b', 'githubRepo': 'https://github.com/ShoufaChen/PixelFlow'}, 'publishedAt': '2025-04-10T13:59:56.000Z', 'title': 'PixelFlow: Pixel-Space Generative Models with Flow', 'summary': 'We present PixelFlow, a family of image generation models that operate\\ndirectly in the raw pixel space, in contrast to the predominant latent-space\\nmodels. This approach simplifies the image generation process by eliminating\\nthe need for a pre-trained Variational Autoencoder (VAE) and enabling the whole\\nmodel end-to-end trainable. Through efficient cascade flow modeling, PixelFlow\\nachieves affordable computation cost in pixel space. It achieves an FID of 1.98\\non 256times256 ImageNet class-conditional image generation benchmark. The\\nqualitative text-to-image results demonstrate that PixelFlow excels in image\\nquality, artistry, and semantic control. We hope this new paradigm will inspire\\nand open up new opportunities for next-generation visual generation models.\\nCode and models are available at https://github.com/ShoufaChen/PixelFlow.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07963.png', 'numComments': 2, 'submittedBy': {'_id': '6412a33900634c4fe9873652', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg', 'fullname': 'Shoufa Chen', 'name': 'ShoufaChen', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 19}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.07405', 'authors': [{'_id': '67fa383909d06d0501a5e34e', 'user': {'_id': '660d844462d63ad0009a9859', 'avatarUrl': '/avatars/6822fc1a82c64dbfd40b88080a5fb1ae.svg', 'isPro': False, 'fullname': 'Linyan Huang', 'user': 'DevLinyan', 'type': 'user'}, 'name': 'Linyan Huang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-13T19:24:47.680Z', 'hidden': False}, {'_id': '67fa383909d06d0501a5e34f', 'name': 'Haonan Lin', 'hidden': False}, {'_id': '67fa383909d06d0501a5e350', 'name': 'Yanning Zhou', 'hidden': False}, {'_id': '67fa383909d06d0501a5e351', 'name': 'Kaiwen Xiao', 'hidden': False}], 'publishedAt': '2025-04-10T02:58:22.000Z', 'submittedOnDailyAt': '2025-04-14T00:54:04.513Z', 'title': 'FlexIP: Dynamic Control of Preservation and Personality for Customized\\n  Image Generation', 'submittedOnDailyBy': {'_id': '63468720dd6d90d82ccf3450', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg', 'isPro': False, 'fullname': 'YSH', 'user': 'BestWishYsh', 'type': 'user'}, 'summary': 'With the rapid advancement of 2D generative models, preserving subject\\nidentity while enabling diverse editing has emerged as a critical research\\nfocus. Existing methods typically face inherent trade-offs between identity\\npreservation and personalized manipulation. We introduce FlexIP, a novel\\nframework that decouples these objectives through two dedicated components: a\\nPersonalization Adapter for stylistic manipulation and a Preservation Adapter\\nfor identity maintenance. By explicitly injecting both control mechanisms into\\nthe generative model, our framework enables flexible parameterized control\\nduring inference through dynamic tuning of the weight adapter. Experimental\\nresults demonstrate that our approach breaks through the performance\\nlimitations of conventional methods, achieving superior identity preservation\\nwhile supporting more diverse personalized generation capabilities (Project\\nPage: https://flexip-tech.github.io/flexip/).', 'upvotes': 7, 'discussionId': '67fa383c09d06d0501a5e3ef', 'projectPage': 'https://flexip-tech.github.io/flexip'}, 'publishedAt': '2025-04-09T22:58:22.000Z', 'title': 'FlexIP: Dynamic Control of Preservation and Personality for Customized\\n  Image Generation', 'summary': 'With the rapid advancement of 2D generative models, preserving subject\\nidentity while enabling diverse editing has emerged as a critical research\\nfocus. Existing methods typically face inherent trade-offs between identity\\npreservation and personalized manipulation. We introduce FlexIP, a novel\\nframework that decouples these objectives through two dedicated components: a\\nPersonalization Adapter for stylistic manipulation and a Preservation Adapter\\nfor identity maintenance. By explicitly injecting both control mechanisms into\\nthe generative model, our framework enables flexible parameterized control\\nduring inference through dynamic tuning of the weight adapter. Experimental\\nresults demonstrate that our approach breaks through the performance\\nlimitations of conventional methods, achieving superior identity preservation\\nwhile supporting more diverse personalized generation capabilities (Project\\nPage: https://flexip-tech.github.io/flexip/).', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07405.png', 'numComments': 1, 'submittedBy': {'_id': '63468720dd6d90d82ccf3450', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg', 'fullname': 'YSH', 'name': 'BestWishYsh', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 46}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.08366', 'authors': [{'_id': '67fc6d8d74c3c0f0d6f24c3b', 'name': 'Sauradip Nag', 'hidden': False}, {'_id': '67fc6d8d74c3c0f0d6f24c3c', 'name': 'Daniel Cohen-Or', 'hidden': False}, {'_id': '67fc6d8d74c3c0f0d6f24c3d', 'name': 'Hao Zhang', 'hidden': False}, {'_id': '67fc6d8d74c3c0f0d6f24c3e', 'name': 'Ali Mahdavi-Amiri', 'hidden': False}], 'publishedAt': '2025-04-11T09:01:09.000Z', 'submittedOnDailyAt': '2025-04-14T00:36:54.457Z', 'title': 'In-2-4D: Inbetweening from Two Single-View Images to 4D Generation', 'submittedOnDailyBy': {'_id': '6399ab3296ce14c5dcf4ccbf', 'avatarUrl': '/avatars/89aeedc96f73d76f7a6da96454f40fd2.svg', 'isPro': False, 'fullname': 'Sauradip Nag', 'user': 'sauradip', 'type': 'user'}, 'summary': 'We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion)\\ninbetweening from a minimalistic input setting: two single-view images\\ncapturing an object in two distinct motion states. Given two images\\nrepresenting the start and end states of an object in motion, our goal is to\\ngenerate and reconstruct the motion in 4D. We utilize a video interpolation\\nmodel to predict the motion, but large frame-to-frame motions can lead to\\nambiguous interpretations. To overcome this, we employ a hierarchical approach\\nto identify keyframes that are visually close to the input states and show\\nsignificant motion, then generate smooth fragments between them. For each\\nfragment, we construct the 3D representation of the keyframe using Gaussian\\nSplatting. The temporal frames within the fragment guide the motion, enabling\\ntheir transformation into dynamic Gaussians through a deformation field. To\\nimprove temporal consistency and refine 3D motion, we expand the self-attention\\nof multi-view diffusion across timesteps and apply rigid transformation\\nregularization. Finally, we merge the independently generated 3D motion\\nsegments by interpolating boundary deformation fields and optimizing them to\\nalign with the guiding video, ensuring smooth and flicker-free transitions.\\nThrough extensive qualitative and quantitiave experiments as well as a user\\nstudy, we show the effectiveness of our method and its components. The project\\npage is available at https://in-2-4d.github.io/', 'upvotes': 4, 'discussionId': '67fc6d9374c3c0f0d6f24e12', 'projectPage': 'https://in-2-4d.github.io/', 'githubRepo': 'https://github.com/sauradip/In-2-4D'}, 'publishedAt': '2025-04-11T05:01:09.000Z', 'title': 'In-2-4D: Inbetweening from Two Single-View Images to 4D Generation', 'summary': 'We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion)\\ninbetweening from a minimalistic input setting: two single-view images\\ncapturing an object in two distinct motion states. Given two images\\nrepresenting the start and end states of an object in motion, our goal is to\\ngenerate and reconstruct the motion in 4D. We utilize a video interpolation\\nmodel to predict the motion, but large frame-to-frame motions can lead to\\nambiguous interpretations. To overcome this, we employ a hierarchical approach\\nto identify keyframes that are visually close to the input states and show\\nsignificant motion, then generate smooth fragments between them. For each\\nfragment, we construct the 3D representation of the keyframe using Gaussian\\nSplatting. The temporal frames within the fragment guide the motion, enabling\\ntheir transformation into dynamic Gaussians through a deformation field. To\\nimprove temporal consistency and refine 3D motion, we expand the self-attention\\nof multi-view diffusion across timesteps and apply rigid transformation\\nregularization. Finally, we merge the independently generated 3D motion\\nsegments by interpolating boundary deformation fields and optimizing them to\\nalign with the guiding video, ensuring smooth and flicker-free transitions.\\nThrough extensive qualitative and quantitiave experiments as well as a user\\nstudy, we show the effectiveness of our method and its components. The project\\npage is available at https://in-2-4d.github.io/', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08366.png', 'numComments': 1, 'submittedBy': {'_id': '6399ab3296ce14c5dcf4ccbf', 'avatarUrl': '/avatars/89aeedc96f73d76f7a6da96454f40fd2.svg', 'fullname': 'Sauradip Nag', 'name': 'sauradip', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.08716', 'authors': [{'_id': '67fca0ca05cd5b5035123b7e', 'name': 'Wissam Antoun', 'hidden': False}, {'_id': '67fca0ca05cd5b5035123b7f', 'name': 'Benoît Sagot', 'hidden': False}, {'_id': '67fca0ca05cd5b5035123b80', 'name': 'Djamé Seddah', 'hidden': False}], 'publishedAt': '2025-04-11T17:29:35.000Z', 'submittedOnDailyAt': '2025-04-14T04:20:01.034Z', 'title': 'ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\\n  Transformer Encoder Models Performance', 'submittedOnDailyBy': {'_id': '5e6a3d4ea9afd5125d9ec064', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg', 'isPro': True, 'fullname': 'Stefan Schweter', 'user': 'stefan-it', 'type': 'user'}, 'summary': \"Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce\\narchitectural advancements aimed at improving efficiency and performance.\\nAlthough the authors of ModernBERT report improved performance over DeBERTaV3\\non several benchmarks, the lack of disclosed training data and the absence of\\ncomparisons using a shared dataset make it difficult to determine whether these\\ngains are due to architectural improvements or differences in training data. In\\nthis work, we conduct a controlled study by pretraining ModernBERT on the same\\ndataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of\\nmodel design. Our results show that the previous model generation remains\\nsuperior in sample efficiency and overall benchmark performance, with\\nModernBERT's primary advantage being faster training and inference speed.\\nHowever, the new proposed model still provides meaningful architectural\\nimprovements compared to earlier models such as BERT and RoBERTa. Additionally,\\nwe observe that high-quality pre-training data accelerates convergence but does\\nnot significantly improve final performance, suggesting potential benchmark\\nsaturation. These findings show the importance of disentangling pretraining\\ndata from architectural innovations when evaluating transformer models.\", 'upvotes': 3, 'discussionId': '67fca0ca05cd5b5035123ba6'}, 'publishedAt': '2025-04-11T13:29:35.000Z', 'title': 'ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\\n  Transformer Encoder Models Performance', 'summary': \"Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce\\narchitectural advancements aimed at improving efficiency and performance.\\nAlthough the authors of ModernBERT report improved performance over DeBERTaV3\\non several benchmarks, the lack of disclosed training data and the absence of\\ncomparisons using a shared dataset make it difficult to determine whether these\\ngains are due to architectural improvements or differences in training data. In\\nthis work, we conduct a controlled study by pretraining ModernBERT on the same\\ndataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of\\nmodel design. Our results show that the previous model generation remains\\nsuperior in sample efficiency and overall benchmark performance, with\\nModernBERT's primary advantage being faster training and inference speed.\\nHowever, the new proposed model still provides meaningful architectural\\nimprovements compared to earlier models such as BERT and RoBERTa. Additionally,\\nwe observe that high-quality pre-training data accelerates convergence but does\\nnot significantly improve final performance, suggesting potential benchmark\\nsaturation. These findings show the importance of disentangling pretraining\\ndata from architectural innovations when evaluating transformer models.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08716.png', 'numComments': 1, 'submittedBy': {'_id': '5e6a3d4ea9afd5125d9ec064', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg', 'fullname': 'Stefan Schweter', 'name': 'stefan-it', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2493}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.07866', 'authors': [{'_id': '67f90407614ff5204ab48c23', 'name': 'Yichun Yin', 'hidden': False}, {'_id': '67f90407614ff5204ab48c24', 'name': 'Wenyong Huang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c25', 'name': 'Kaikai Song', 'hidden': False}, {'_id': '67f90407614ff5204ab48c26', 'name': 'Yehui Tang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c27', 'name': 'Xueyu Wu', 'hidden': False}, {'_id': '67f90407614ff5204ab48c28', 'name': 'Wei Guo', 'hidden': False}, {'_id': '67f90407614ff5204ab48c29', 'name': 'Peng Guo', 'hidden': False}, {'_id': '67f90407614ff5204ab48c2a', 'name': 'Yaoyuan Wang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c2b', 'name': 'Xiaojun Meng', 'hidden': False}, {'_id': '67f90407614ff5204ab48c2c', 'name': 'Yasheng Wang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c2d', 'name': 'Dong Li', 'hidden': False}, {'_id': '67f90407614ff5204ab48c2e', 'name': 'Can Chen', 'hidden': False}, {'_id': '67f90407614ff5204ab48c2f', 'name': 'Dandan Tu', 'hidden': False}, {'_id': '67f90407614ff5204ab48c30', 'name': 'Yin Li', 'hidden': False}, {'_id': '67f90407614ff5204ab48c31', 'name': 'Fisher Yu', 'hidden': False}, {'_id': '67f90407614ff5204ab48c32', 'name': 'Ruiming Tang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c33', 'name': 'Yunhe Wang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c34', 'name': 'Baojun Wang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c35', 'name': 'Bin Wang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c36', 'name': 'Bo Wang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c37', 'name': 'Boxiao Liu', 'hidden': False}, {'_id': '67f90407614ff5204ab48c38', 'name': 'Changzheng Zhang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c39', 'name': 'Duyu Tang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c3a', 'name': 'Fei Mi', 'hidden': False}, {'_id': '67f90407614ff5204ab48c3b', 'name': 'Hui Jin', 'hidden': False}, {'_id': '67f90407614ff5204ab48c3c', 'name': 'Jiansheng Wei', 'hidden': False}, {'_id': '67f90407614ff5204ab48c3d', 'name': 'Jiarui Qin', 'hidden': False}, {'_id': '67f90407614ff5204ab48c3e', 'name': 'Jinpeng Li', 'hidden': False}, {'_id': '67f90407614ff5204ab48c3f', 'name': 'Jun Zhao', 'hidden': False}, {'_id': '67f90407614ff5204ab48c40', 'name': 'Liqun Deng', 'hidden': False}, {'_id': '67f90407614ff5204ab48c41', 'name': 'Lin Li', 'hidden': False}, {'_id': '67f90407614ff5204ab48c42', 'name': 'Minghui Xu', 'hidden': False}, {'_id': '67f90407614ff5204ab48c43', 'name': 'Naifu Zhang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c44', 'name': 'Nianzu Zheng', 'hidden': False}, {'_id': '67f90407614ff5204ab48c45', 'name': 'Qiang Li', 'hidden': False}, {'_id': '67f90407614ff5204ab48c46', 'name': 'Rongju Ruan', 'hidden': False}, {'_id': '67f90407614ff5204ab48c47', 'name': 'Shengjun Cheng', 'hidden': False}, {'_id': '67f90407614ff5204ab48c48', 'name': 'Tianyu Guo', 'hidden': False}, {'_id': '67f90407614ff5204ab48c49', 'name': 'Wei He', 'hidden': False}, {'_id': '67f90407614ff5204ab48c4a', 'name': 'Wei Li', 'hidden': False}, {'_id': '67f90407614ff5204ab48c4b', 'name': 'Weiwen Liu', 'hidden': False}, {'_id': '67f90407614ff5204ab48c4c', 'name': 'Wulong Liu', 'hidden': False}, {'_id': '67f90407614ff5204ab48c4d', 'name': 'Xinyi Dai', 'hidden': False}, {'_id': '67f90407614ff5204ab48c4e', 'name': 'Yonghan Dong', 'hidden': False}, {'_id': '67f90407614ff5204ab48c4f', 'name': 'Yu Pan', 'hidden': False}, {'_id': '67f90407614ff5204ab48c50', 'name': 'Yue Li', 'hidden': False}, {'_id': '67f90407614ff5204ab48c51', 'name': 'Yufei Wang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c52', 'name': 'Yujun Li', 'hidden': False}, {'_id': '67f90407614ff5204ab48c53', 'name': 'Yunsheng Ni', 'hidden': False}, {'_id': '67f90407614ff5204ab48c54', 'name': 'Zhe Liu', 'hidden': False}, {'_id': '67f90407614ff5204ab48c55', 'name': 'Zhenhe Zhang', 'hidden': False}, {'_id': '67f90407614ff5204ab48c56', 'name': 'Zhicheng Liu', 'hidden': False}], 'publishedAt': '2025-04-10T15:41:51.000Z', 'submittedOnDailyAt': '2025-04-14T09:06:50.156Z', 'title': 'Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\\n  NPUs', 'submittedOnDailyBy': {'_id': '63a369d98c0c89dcae3b8329', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png', 'isPro': False, 'fullname': 'Adina Yakefu', 'user': 'AdinaY', 'type': 'user'}, 'summary': 'We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\\nparameters and dense Transformer modules trained on Ascend Neural Processing\\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\\nadvances in pushing the scale and capability of LLM in recent years, training\\nsuch a large-scale model still involves significant optimization and system\\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\\nnormalization, which effectively eliminates loss spikes during the training\\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\\nhigh-quality tokens and further enhance its reasoning capabilities during\\npost-training. To perform such large-scale training efficiently, we utilize\\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\\nmodel structure contains much more parameters. Our exploration demonstrates\\nthat Ascend NPUs are capable of efficiently and effectively training dense\\nmodels with more than 100 billion parameters. Our model and system will be\\navailable for our commercial customers.', 'upvotes': 3, 'discussionId': '67f90409614ff5204ab48cea'}, 'publishedAt': '2025-04-10T11:41:51.000Z', 'title': 'Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\\n  NPUs', 'summary': 'We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\\nparameters and dense Transformer modules trained on Ascend Neural Processing\\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\\nadvances in pushing the scale and capability of LLM in recent years, training\\nsuch a large-scale model still involves significant optimization and system\\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\\nnormalization, which effectively eliminates loss spikes during the training\\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\\nhigh-quality tokens and further enhance its reasoning capabilities during\\npost-training. To perform such large-scale training efficiently, we utilize\\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\\nmodel structure contains much more parameters. Our exploration demonstrates\\nthat Ascend NPUs are capable of efficiently and effectively training dense\\nmodels with more than 100 billion parameters. Our model and system will be\\navailable for our commercial customers.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07866.png', 'numComments': 2, 'submittedBy': {'_id': '63a369d98c0c89dcae3b8329', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png', 'fullname': 'Adina Yakefu', 'name': 'AdinaY', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': False, 'isMod': False, 'followerCount': 569}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.05262', 'authors': [{'_id': '67fcc9a980568c7ef6180dcb', 'user': {'_id': '62cd5917299c0c2e0e435847', 'avatarUrl': '/avatars/b956b4feab86f6866c43cc87a44e25fc.svg', 'isPro': False, 'fullname': 'Yang Yan', 'user': 'kurileo', 'type': 'user'}, 'name': 'Yang Yan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-14T12:52:13.346Z', 'hidden': False}, {'_id': '67fcc9a980568c7ef6180dcc', 'name': 'Yu Lu', 'hidden': False}, {'_id': '67fcc9a980568c7ef6180dcd', 'name': 'Renjun Xu', 'hidden': False}, {'_id': '67fcc9a980568c7ef6180dce', 'name': 'Zhenzhong Lan', 'hidden': False}], 'publishedAt': '2025-04-07T16:57:10.000Z', 'submittedOnDailyAt': '2025-04-14T07:11:56.706Z', 'title': 'Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\\n  vs. Memorization in Large Language Models', 'submittedOnDailyBy': {'_id': '62ce6dd785cfd21c04c7e6f5', 'avatarUrl': '/avatars/89837a5dea6e2d753d59caad142bed4a.svg', 'isPro': False, 'fullname': 'ZhenzhongLan', 'user': 'DannyLan', 'type': 'user'}, 'summary': 'Despite high benchmark scores, Large Language Models (LLMs) often fail simple\\nproblem, raising a critical question: Do LLMs learn mathematical principles or\\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\\nlike recent works, we investigate this using elementary two-integer addition\\n(0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and\\ncompositional generalization (via isomorphic symbolic mappings, e.g., 7\\nrightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\\\\% accuracy on\\nnumerical addition, performance collapses to leq7.5\\\\% under symbolic\\nmapping, indicating failure to generalize learned rules. Non-monotonic\\nperformance scaling with digit count and frequent commutativity violations\\n(over 1,700 cases of A+B neq B+A) further support this. Explicitly providing\\naddition rules degrades performance by 81.2\\\\% on average, while\\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\\nprocessing is misaligned with human-defined principles. Our findings indicate\\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\\narchitectural limitations and the need for new approaches to achieve true\\nmathematical reasoning.', 'upvotes': 3, 'discussionId': '67fcc9aa80568c7ef6180e24'}, 'publishedAt': '2025-04-07T12:57:10.000Z', 'title': 'Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\\n  vs. Memorization in Large Language Models', 'summary': 'Despite high benchmark scores, Large Language Models (LLMs) often fail simple\\nproblem, raising a critical question: Do LLMs learn mathematical principles or\\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\\nlike recent works, we investigate this using elementary two-integer addition\\n(0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and\\ncompositional generalization (via isomorphic symbolic mappings, e.g., 7\\nrightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\\\\% accuracy on\\nnumerical addition, performance collapses to leq7.5\\\\% under symbolic\\nmapping, indicating failure to generalize learned rules. Non-monotonic\\nperformance scaling with digit count and frequent commutativity violations\\n(over 1,700 cases of A+B neq B+A) further support this. Explicitly providing\\naddition rules degrades performance by 81.2\\\\% on average, while\\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\\nprocessing is misaligned with human-defined principles. Our findings indicate\\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\\narchitectural limitations and the need for new approaches to achieve true\\nmathematical reasoning.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05262.png', 'numComments': 3, 'submittedBy': {'_id': '62ce6dd785cfd21c04c7e6f5', 'avatarUrl': '/avatars/89837a5dea6e2d753d59caad142bed4a.svg', 'fullname': 'ZhenzhongLan', 'name': 'DannyLan', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2504.08727', 'authors': [{'_id': '67fd083b3bf3cc207a879c3d', 'user': {'_id': '66d3896c6839bd191d0c5ad3', 'avatarUrl': '/avatars/34acf042c946b5e7144ce3c002c9939f.svg', 'isPro': False, 'fullname': 'Boyang Deng', 'user': 'bydeng', 'type': 'user'}, 'name': 'Boyang Deng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-14T13:17:32.224Z', 'hidden': False}, {'_id': '67fd083b3bf3cc207a879c3e', 'name': 'Songyou Peng', 'hidden': False}, {'_id': '67fd083b3bf3cc207a879c3f', 'name': 'Kyle Genova', 'hidden': False}, {'_id': '67fd083b3bf3cc207a879c40', 'name': 'Gordon Wetzstein', 'hidden': False}, {'_id': '67fd083b3bf3cc207a879c41', 'name': 'Noah Snavely', 'hidden': False}, {'_id': '67fd083b3bf3cc207a879c42', 'name': 'Leonidas Guibas', 'hidden': False}, {'_id': '67fd083b3bf3cc207a879c43', 'name': 'Thomas Funkhouser', 'hidden': False}], 'publishedAt': '2025-04-11T17:55:45.000Z', 'submittedOnDailyAt': '2025-04-14T11:38:13.588Z', 'title': 'Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\\n  of Images', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': 'We present a system using Multimodal LLMs (MLLMs) to analyze a large database\\nwith tens of millions of images captured at different times, with the aim of\\ndiscovering patterns in temporal changes. Specifically, we aim to capture\\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\\n\"what are the frequent types of changes in the city?\") without any\\npredetermined target subjects or training labels. These properties cast prior\\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\\nas context. So we introduce a bottom-up procedure that decomposes the massive\\nvisual analysis problem into more tractable sub-problems. We carefully design\\nMLLM-based solutions to each sub-problem. During experiments and ablation\\nstudies with our system, we find it significantly outperforms baselines and is\\nable to discover interesting trends from images captured in large cities (e.g.,\\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.', 'upvotes': 2, 'discussionId': '67fd08403bf3cc207a879d63'}, 'publishedAt': '2025-04-11T13:55:45.000Z', 'title': 'Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\\n  of Images', 'summary': 'We present a system using Multimodal LLMs (MLLMs) to analyze a large database\\nwith tens of millions of images captured at different times, with the aim of\\ndiscovering patterns in temporal changes. Specifically, we aim to capture\\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\\n\"what are the frequent types of changes in the city?\") without any\\npredetermined target subjects or training labels. These properties cast prior\\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\\nas context. So we introduce a bottom-up procedure that decomposes the massive\\nvisual analysis problem into more tractable sub-problems. We carefully design\\nMLLM-based solutions to each sub-problem. During experiments and ablation\\nstudies with our system, we find it significantly outperforms baselines and is\\nable to discover interesting trends from images captured in large cities (e.g.,\\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08727.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6647}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.08192', 'authors': [{'_id': '67fcb3584a92187863e732d5', 'name': 'Aashiq Muhamed', 'hidden': False}, {'_id': '67fcb3584a92187863e732d6', 'name': 'Jacopo Bonato', 'hidden': False}, {'_id': '67fcb3584a92187863e732d7', 'name': 'Mona Diab', 'hidden': False}, {'_id': '67fcb3584a92187863e732d8', 'name': 'Virginia Smith', 'hidden': False}], 'publishedAt': '2025-04-11T01:24:03.000Z', 'submittedOnDailyAt': '2025-04-14T05:34:15.388Z', 'title': 'SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\\n  Guardrails for Precision Unlearning in LLMs', 'submittedOnDailyBy': {'_id': '64755a83e0b188d3cb2579d8', 'avatarUrl': '/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg', 'isPro': False, 'fullname': 'Aashiq Muhamed', 'user': 'aashiqmuhamed', 'type': 'user'}, 'summary': 'Machine unlearning is a promising approach to improve LLM safety by removing\\nunwanted knowledge from the model. However, prevailing gradient-based\\nunlearning methods suffer from issues such as high computational costs,\\nhyperparameter instability, poor sequential unlearning capability,\\nvulnerability to relearning attacks, low data efficiency, and lack of\\ninterpretability. While Sparse Autoencoders are well-suited to improve these\\naspects by enabling targeted activation-based unlearning, prior approaches\\nunderperform gradient-based methods. This work demonstrates that, contrary to\\nthese earlier findings, SAEs can significantly improve unlearning when employed\\ndynamically. We introduce Dynamic DAE Guardrails (DSG), a novel\\nmethod for precision unlearning that leverages principled feature selection and\\na dynamic classifier. Our experiments show DSG substantially outperforms\\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\\nenhanced computational efficiency and stability, robust performance in\\nsequential unlearning, stronger resistance to relearning attacks, better data\\nefficiency including zero-shot settings, and more interpretable unlearning.', 'upvotes': 2, 'discussionId': '67fcb3594a92187863e732fa'}, 'publishedAt': '2025-04-10T21:24:03.000Z', 'title': 'SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\\n  Guardrails for Precision Unlearning in LLMs', 'summary': 'Machine unlearning is a promising approach to improve LLM safety by removing\\nunwanted knowledge from the model. However, prevailing gradient-based\\nunlearning methods suffer from issues such as high computational costs,\\nhyperparameter instability, poor sequential unlearning capability,\\nvulnerability to relearning attacks, low data efficiency, and lack of\\ninterpretability. While Sparse Autoencoders are well-suited to improve these\\naspects by enabling targeted activation-based unlearning, prior approaches\\nunderperform gradient-based methods. This work demonstrates that, contrary to\\nthese earlier findings, SAEs can significantly improve unlearning when employed\\ndynamically. We introduce Dynamic DAE Guardrails (DSG), a novel\\nmethod for precision unlearning that leverages principled feature selection and\\na dynamic classifier. Our experiments show DSG substantially outperforms\\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\\nenhanced computational efficiency and stability, robust performance in\\nsequential unlearning, stronger resistance to relearning attacks, better data\\nefficiency including zero-shot settings, and more interpretable unlearning.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08192.png', 'numComments': 1, 'submittedBy': {'_id': '64755a83e0b188d3cb2579d8', 'avatarUrl': '/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg', 'fullname': 'Aashiq Muhamed', 'name': 'aashiqmuhamed', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.07615', 'authors': [{'_id': '67fc52b190075b82590d75db', 'name': 'Haozhan Shen', 'hidden': False}, {'_id': '67fc52b190075b82590d75dc', 'name': 'Peng Liu', 'hidden': False}, {'_id': '67fc52b190075b82590d75dd', 'name': 'Jingcheng Li', 'hidden': False}, {'_id': '67fc52b190075b82590d75de', 'name': 'Chunxin Fang', 'hidden': False}, {'_id': '67fc52b190075b82590d75df', 'name': 'Yibo Ma', 'hidden': False}, {'_id': '67fc52b190075b82590d75e0', 'name': 'Jiajia Liao', 'hidden': False}, {'_id': '67fc52b190075b82590d75e1', 'name': 'Qiaoli Shen', 'hidden': False}, {'_id': '67fc52b190075b82590d75e2', 'name': 'Zilun Zhang', 'hidden': False}, {'_id': '67fc52b190075b82590d75e3', 'name': 'Kangjia Zhao', 'hidden': False}, {'_id': '67fc52b190075b82590d75e4', 'name': 'Qianqian Zhang', 'hidden': False}, {'_id': '67fc52b190075b82590d75e5', 'name': 'Ruochen Xu', 'hidden': False}, {'_id': '67fc52b190075b82590d75e6', 'name': 'Tiancheng Zhao', 'hidden': False}], 'publishedAt': '2025-04-10T10:05:15.000Z', 'submittedOnDailyAt': '2025-04-14T13:37:56.110Z', 'title': 'VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model', 'submittedOnDailyBy': {'_id': '5f0de36419cb630495b8153c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1658676776546-5f0de36419cb630495b8153c.jpeg', 'isPro': False, 'fullname': 'Tony Zhao', 'user': 'tianchez', 'type': 'user'}, 'summary': 'Recently DeepSeek R1 has shown that reinforcement learning (RL) can\\nsubstantially improve the reasoning capabilities of Large Language Models\\n(LLMs) through a simple yet effective design. The core of R1 lies in its\\nrule-based reward formulation, which leverages tasks with deterministic\\nground-truth answers to enable precise and stable reward computation. In the\\nvisual domain, we similarly observe that a wide range of visual understanding\\ntasks are inherently equipped with well-defined ground-truth annotations. This\\nproperty makes them naturally compatible with rule-based reward mechanisms.\\nMotivated by this observation, we investigate the extension of R1-style\\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\\ndedicated framework designed to harness RL for improving VLMs\\' performance on\\ngeneral vision-language tasks. Using this framework, we further explore the\\nfeasibility of applying RL to visual domain. Experimental results indicate that\\nthe RL-based model not only delivers competitive performance on visual\\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\\nthat uncover a series of noteworthy insights, including the presence of reward\\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\\nof training data quality, and the scaling behavior of RL across different model\\nsizes. Through these analyses, we aim to deepen the understanding of how\\nreinforcement learning enhances the capabilities of vision-language models, and\\nwe hope our findings and open-source contributions will support continued\\nprogress in the vision-language RL community. Our code and model are available\\nat https://github.com/om-ai-lab/VLM-R1', 'upvotes': 2, 'discussionId': '67fc52b390075b82590d7634', 'githubRepo': 'https://github.com/om-ai-lab/VLM-R1'}, 'publishedAt': '2025-04-10T06:05:15.000Z', 'title': 'VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model', 'summary': 'Recently DeepSeek R1 has shown that reinforcement learning (RL) can\\nsubstantially improve the reasoning capabilities of Large Language Models\\n(LLMs) through a simple yet effective design. The core of R1 lies in its\\nrule-based reward formulation, which leverages tasks with deterministic\\nground-truth answers to enable precise and stable reward computation. In the\\nvisual domain, we similarly observe that a wide range of visual understanding\\ntasks are inherently equipped with well-defined ground-truth annotations. This\\nproperty makes them naturally compatible with rule-based reward mechanisms.\\nMotivated by this observation, we investigate the extension of R1-style\\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\\ndedicated framework designed to harness RL for improving VLMs\\' performance on\\ngeneral vision-language tasks. Using this framework, we further explore the\\nfeasibility of applying RL to visual domain. Experimental results indicate that\\nthe RL-based model not only delivers competitive performance on visual\\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\\nthat uncover a series of noteworthy insights, including the presence of reward\\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\\nof training data quality, and the scaling behavior of RL across different model\\nsizes. Through these analyses, we aim to deepen the understanding of how\\nreinforcement learning enhances the capabilities of vision-language models, and\\nwe hope our findings and open-source contributions will support continued\\nprogress in the vision-language RL community. Our code and model are available\\nat https://github.com/om-ai-lab/VLM-R1', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07615.png', 'numComments': 1, 'submittedBy': {'_id': '5f0de36419cb630495b8153c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1658676776546-5f0de36419cb630495b8153c.jpeg', 'fullname': 'Tony Zhao', 'name': 'tianchez', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 19}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.01883', 'authors': [{'_id': '67fcb50ea69150c25fb4b645', 'name': 'Aashiq Muhamed', 'hidden': False}, {'_id': '67fcb50ea69150c25fb4b646', 'name': 'Mona Diab', 'hidden': False}, {'_id': '67fcb50ea69150c25fb4b647', 'name': 'Virginia Smith', 'hidden': False}], 'publishedAt': '2025-04-02T16:40:43.000Z', 'submittedOnDailyAt': '2025-04-14T05:41:34.796Z', 'title': 'CoRAG: Collaborative Retrieval-Augmented Generation', 'submittedOnDailyBy': {'_id': '64755a83e0b188d3cb2579d8', 'avatarUrl': '/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg', 'isPro': False, 'fullname': 'Aashiq Muhamed', 'user': 'aashiqmuhamed', 'type': 'user'}, 'summary': 'Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive\\ntasks, especially under few-shot learning constraints. We introduce CoRAG, a\\nframework extending RAG to collaborative settings, where clients jointly train\\na shared model using a collaborative passage store. To evaluate CoRAG, we\\nintroduce CRAB, a benchmark for collaborative homogeneous open-domain question\\nanswering. Our experiments demonstrate that CoRAG consistently outperforms both\\nparametric collaborative learning methods and locally trained RAG models in\\nlow-resource scenarios. Further analysis reveals the critical importance of\\nrelevant passages within the shared store, the surprising benefits of\\nincorporating irrelevant passages, and the potential for hard negatives to\\nnegatively impact performance. This introduces a novel consideration in\\ncollaborative RAG: the trade-off between leveraging a collectively enriched\\nknowledge base and the potential risk of incorporating detrimental passages\\nfrom other clients. Our findings underscore the viability of CoRAG, while also\\nhighlighting key design challenges and promising avenues for future research.', 'upvotes': 2, 'discussionId': '67fcb510a69150c25fb4b6b1'}, 'publishedAt': '2025-04-02T12:40:43.000Z', 'title': 'CoRAG: Collaborative Retrieval-Augmented Generation', 'summary': 'Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive\\ntasks, especially under few-shot learning constraints. We introduce CoRAG, a\\nframework extending RAG to collaborative settings, where clients jointly train\\na shared model using a collaborative passage store. To evaluate CoRAG, we\\nintroduce CRAB, a benchmark for collaborative homogeneous open-domain question\\nanswering. Our experiments demonstrate that CoRAG consistently outperforms both\\nparametric collaborative learning methods and locally trained RAG models in\\nlow-resource scenarios. Further analysis reveals the critical importance of\\nrelevant passages within the shared store, the surprising benefits of\\nincorporating irrelevant passages, and the potential for hard negatives to\\nnegatively impact performance. This introduces a novel consideration in\\ncollaborative RAG: the trade-off between leveraging a collectively enriched\\nknowledge base and the potential risk of incorporating detrimental passages\\nfrom other clients. Our findings underscore the viability of CoRAG, while also\\nhighlighting key design challenges and promising avenues for future research.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01883.png', 'numComments': 1, 'submittedBy': {'_id': '64755a83e0b188d3cb2579d8', 'avatarUrl': '/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg', 'fullname': 'Aashiq Muhamed', 'name': 'aashiqmuhamed', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2504.08635', 'authors': [{'_id': '67fd028b27381665ef977a14', 'user': {'_id': '664de98d04ee28f9f944af1c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-HAg0eaZv4PXRfEcBWqhL.png', 'isPro': False, 'fullname': 'Gabriele Lozupone', 'user': 'gabrielelozupone98', 'type': 'user'}, 'name': 'Gabriele Lozupone', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-04-14T12:51:10.674Z', 'hidden': False}, {'_id': '67fd028b27381665ef977a15', 'name': 'Alessandro Bria', 'hidden': False}, {'_id': '67fd028b27381665ef977a16', 'name': 'Francesco Fontanella', 'hidden': False}, {'_id': '67fd028b27381665ef977a17', 'name': 'Frederick J. A. Meijer', 'hidden': False}, {'_id': '67fd028b27381665ef977a18', 'name': 'Claudio De Stefano', 'hidden': False}, {'_id': '67fd028b27381665ef977a19', 'name': 'Henkjan Huisman', 'hidden': False}], 'publishedAt': '2025-04-11T15:37:46.000Z', 'submittedOnDailyAt': '2025-04-14T11:16:25.493Z', 'title': 'Latent Diffusion Autoencoders: Toward Efficient and Meaningful\\n  Unsupervised Representation Learning in Medical Imaging', 'submittedOnDailyBy': {'_id': '664de98d04ee28f9f944af1c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-HAg0eaZv4PXRfEcBWqhL.png', 'isPro': False, 'fullname': 'Gabriele Lozupone', 'user': 'gabrielelozupone98', 'type': 'user'}, 'summary': 'This study presents Latent Diffusion Autoencoder (LDAE), a novel\\nencoder-decoder diffusion-based framework for efficient and meaningful\\nunsupervised learning in medical imaging, focusing on Alzheimer disease (AD)\\nusing brain MR from the ADNI database as a case study. Unlike conventional\\ndiffusion autoencoders operating in image space, LDAE applies the diffusion\\nprocess in a compressed latent representation, improving computational\\nefficiency and making 3D medical imaging representation learning tractable. To\\nvalidate the proposed approach, we explore two key hypotheses: (i) LDAE\\neffectively captures meaningful semantic representations on 3D brain MR\\nassociated with AD and ageing, and (ii) LDAE achieves high-quality image\\ngeneration and reconstruction while being computationally efficient.\\nExperimental results support both hypotheses: (i) linear-probe evaluations\\ndemonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%)\\nand age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic\\nrepresentations enable attribute manipulation, yielding anatomically plausible\\nmodifications; (iii) semantic interpolation experiments show strong\\nreconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month\\ngap. Even for longer gaps (24 months), the model maintains robust performance\\n(SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal\\nprogression trends; (iv) compared to conventional diffusion autoencoders, LDAE\\nsignificantly increases inference throughput (20x faster) while also enhancing\\nreconstruction quality. These findings position LDAE as a promising framework\\nfor scalable medical imaging applications, with the potential to serve as a\\nfoundation model for medical image analysis. Code available at\\nhttps://github.com/GabrieleLozupone/LDAE', 'upvotes': 0, 'discussionId': '67fd028c27381665ef977a8f', 'githubRepo': 'https://github.com/GabrieleLozupone/LDAE'}, 'publishedAt': '2025-04-11T11:37:46.000Z', 'title': 'Latent Diffusion Autoencoders: Toward Efficient and Meaningful\\n  Unsupervised Representation Learning in Medical Imaging', 'summary': 'This study presents Latent Diffusion Autoencoder (LDAE), a novel\\nencoder-decoder diffusion-based framework for efficient and meaningful\\nunsupervised learning in medical imaging, focusing on Alzheimer disease (AD)\\nusing brain MR from the ADNI database as a case study. Unlike conventional\\ndiffusion autoencoders operating in image space, LDAE applies the diffusion\\nprocess in a compressed latent representation, improving computational\\nefficiency and making 3D medical imaging representation learning tractable. To\\nvalidate the proposed approach, we explore two key hypotheses: (i) LDAE\\neffectively captures meaningful semantic representations on 3D brain MR\\nassociated with AD and ageing, and (ii) LDAE achieves high-quality image\\ngeneration and reconstruction while being computationally efficient.\\nExperimental results support both hypotheses: (i) linear-probe evaluations\\ndemonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%)\\nand age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic\\nrepresentations enable attribute manipulation, yielding anatomically plausible\\nmodifications; (iii) semantic interpolation experiments show strong\\nreconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month\\ngap. Even for longer gaps (24 months), the model maintains robust performance\\n(SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal\\nprogression trends; (iv) compared to conventional diffusion autoencoders, LDAE\\nsignificantly increases inference throughput (20x faster) while also enhancing\\nreconstruction quality. These findings position LDAE as a promising framework\\nfor scalable medical imaging applications, with the potential to serve as a\\nfoundation model for medical image analysis. Code available at\\nhttps://github.com/GabrieleLozupone/LDAE', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08635.png', 'numComments': 1, 'submittedBy': {'_id': '664de98d04ee28f9f944af1c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-HAg0eaZv4PXRfEcBWqhL.png', 'fullname': 'Gabriele Lozupone', 'name': 'gabrielelozupone98', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2504.05303', 'authors': [{'_id': '67fcbbe8daf0cf6803943949', 'user': {'_id': '6492bf9681d93008eb33f167', 'avatarUrl': '/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg', 'isPro': False, 'fullname': 'Sai Kumar Dwivedi', 'user': 'saidwivedi', 'type': 'user'}, 'name': 'Sai Kumar Dwivedi', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-04-14T09:46:12.532Z', 'hidden': False}, {'_id': '67fcbbe8daf0cf680394394a', 'name': 'Dimitrije Antić', 'hidden': False}, {'_id': '67fcbbe8daf0cf680394394b', 'name': 'Shashank Tripathi', 'hidden': False}, {'_id': '67fcbbe8daf0cf680394394c', 'name': 'Omid Taheri', 'hidden': False}, {'_id': '67fcbbe8daf0cf680394394d', 'name': 'Cordelia Schmid', 'hidden': False}, {'_id': '67fcbbe8daf0cf680394394e', 'name': 'Michael J. Black', 'hidden': False}, {'_id': '67fcbbe8daf0cf680394394f', 'name': 'Dimitrios Tzionas', 'hidden': False}], 'publishedAt': '2025-04-07T17:59:33.000Z', 'submittedOnDailyAt': '2025-04-14T06:14:23.936Z', 'title': 'InteractVLM: 3D Interaction Reasoning from 2D Foundational Models', 'submittedOnDailyBy': {'_id': '6492bf9681d93008eb33f167', 'avatarUrl': '/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg', 'isPro': False, 'fullname': 'Sai Kumar Dwivedi', 'user': 'saidwivedi', 'type': 'user'}, 'summary': 'We introduce InteractVLM, a novel method to estimate 3D contact points on\\nhuman bodies and objects from single in-the-wild images, enabling accurate\\nhuman-object joint reconstruction in 3D. This is challenging due to occlusions,\\ndepth ambiguities, and widely varying object shapes. Existing methods rely on\\n3D contact annotations collected via expensive motion-capture systems or\\ntedious manual labeling, limiting scalability and generalization. To overcome\\nthis, InteractVLM harnesses the broad visual knowledge of large Vision-Language\\nModels (VLMs), fine-tuned with limited 3D contact data. However, directly\\napplying these models is non-trivial, as they reason only in 2D, while\\nhuman-object contact is inherently 3D. Thus we introduce a novel\\nRender-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D\\nspace via multi-view rendering, (2) trains a novel multi-view localization\\nmodel (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D.\\nAdditionally, we propose a new task called Semantic Human Contact estimation,\\nwhere human contact predictions are conditioned explicitly on object semantics,\\nenabling richer interaction modeling. InteractVLM outperforms existing work on\\ncontact estimation and also facilitates 3D reconstruction from an in-the wild\\nimage. Code and models are available at https://interactvlm.is.tue.mpg.de.', 'upvotes': 0, 'discussionId': '67fcbbeadaf0cf68039439b9', 'projectPage': 'https://interactvlm.is.tue.mpg.de/', 'githubRepo': 'https://github.com/saidwivedi/InteractVLM'}, 'publishedAt': '2025-04-07T13:59:33.000Z', 'title': 'InteractVLM: 3D Interaction Reasoning from 2D Foundational Models', 'summary': 'We introduce InteractVLM, a novel method to estimate 3D contact points on\\nhuman bodies and objects from single in-the-wild images, enabling accurate\\nhuman-object joint reconstruction in 3D. This is challenging due to occlusions,\\ndepth ambiguities, and widely varying object shapes. Existing methods rely on\\n3D contact annotations collected via expensive motion-capture systems or\\ntedious manual labeling, limiting scalability and generalization. To overcome\\nthis, InteractVLM harnesses the broad visual knowledge of large Vision-Language\\nModels (VLMs), fine-tuned with limited 3D contact data. However, directly\\napplying these models is non-trivial, as they reason only in 2D, while\\nhuman-object contact is inherently 3D. Thus we introduce a novel\\nRender-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D\\nspace via multi-view rendering, (2) trains a novel multi-view localization\\nmodel (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D.\\nAdditionally, we propose a new task called Semantic Human Contact estimation,\\nwhere human contact predictions are conditioned explicitly on object semantics,\\nenabling richer interaction modeling. InteractVLM outperforms existing work on\\ncontact estimation and also facilitates 3D reconstruction from an in-the wild\\nimage. Code and models are available at https://interactvlm.is.tue.mpg.de.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05303.png', 'numComments': 1, 'submittedBy': {'_id': '6492bf9681d93008eb33f167', 'avatarUrl': '/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg', 'fullname': 'Sai Kumar Dwivedi', 'name': 'saidwivedi', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}"
]