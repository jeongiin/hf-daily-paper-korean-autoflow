[
    "{'paper': {'id': '2505.04410', 'authors': [{'_id': '681d615fbd89ba9ceb5e94bc', 'user': {'_id': '64a385281cbf675203fbb7df', 'avatarUrl': '/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg', 'isPro': False, 'fullname': 'Junjie Wang', 'user': 'xiaomoguhzz', 'type': 'user'}, 'name': 'Junjie Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-09T07:21:48.630Z', 'hidden': False}, {'_id': '681d615fbd89ba9ceb5e94bd', 'user': {'_id': '647d70d136e109abce415e0e', 'avatarUrl': '/avatars/c89864c663cbf3256e34785be85561bc.svg', 'isPro': False, 'fullname': 'Bin Chen', 'user': 'BinChen68', 'type': 'user'}, 'name': 'Bin Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:47:00.478Z', 'hidden': False}, {'_id': '681d615fbd89ba9ceb5e94be', 'name': 'Yulin Li', 'hidden': False}, {'_id': '681d615fbd89ba9ceb5e94bf', 'user': {'_id': '67069836f95c7ec727df2806', 'avatarUrl': '/avatars/fbe87f41e680362595b5864e690b62b4.svg', 'isPro': False, 'fullname': 'bin kang', 'user': 'tygeer', 'type': 'user'}, 'name': 'Bin Kang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:47:31.037Z', 'hidden': False}, {'_id': '681d615fbd89ba9ceb5e94c0', 'name': 'Yichi Chen', 'hidden': False}, {'_id': '681d615fbd89ba9ceb5e94c1', 'user': {'_id': '64d104a37a7305c5895bd720', 'avatarUrl': '/avatars/2d9eff3a2dff6d02e45ae8964fb91f27.svg', 'isPro': False, 'fullname': 'zt tian', 'user': 'tianzhuotao', 'type': 'user'}, 'name': 'Zhuotao Tian', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:48:12.084Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64a385281cbf675203fbb7df/MpXgXINxg1RmSFnfeqXqU.mp4'], 'publishedAt': '2025-05-07T13:46:34.000Z', 'submittedOnDailyAt': '2025-05-15T06:24:33.810Z', 'title': 'DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception', 'submittedOnDailyBy': {'_id': '64a385281cbf675203fbb7df', 'avatarUrl': '/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg', 'isPro': False, 'fullname': 'Junjie Wang', 'user': 'xiaomoguhzz', 'type': 'user'}, 'summary': \"Dense visual prediction tasks have been constrained by their reliance on\\npredefined categories, limiting their applicability in real-world scenarios\\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\\nCLIP have shown promise in open-vocabulary tasks, their direct application to\\ndense prediction often leads to suboptimal performance due to limitations in\\nlocal feature representation. In this work, we present our observation that\\nCLIP's image tokens struggle to effectively aggregate information from\\nspatially or semantically related regions, resulting in features that lack\\nlocal discriminability and spatial consistency. To address this issue, we\\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\\nself-attention module to obtain ``content'' and ``context'' features\\nrespectively. The ``content'' features are aligned with image crop\\nrepresentations to improve local discriminability, while ``context'' features\\nlearn to retain the spatial correlations under the guidance of vision\\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\\nsignificantly outperforms existing methods across multiple open-vocabulary\\ndense prediction tasks, including object detection and semantic segmentation.\\nCode is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.\", 'upvotes': 33, 'discussionId': '681d6161bd89ba9ceb5e9571', 'githubRepo': 'https://github.com/xiaomoguhz/DeCLIP', 'ai_keywords': ['Vision-Language Models (VLMs)', 'CLIP', 'dense prediction', 'predefined categories', 'open-vocabulary tasks', 'spatially related regions', 'semantically related regions', 'local discriminability', 'spatial consistency', 'self-attention module', 'content features', 'context features', 'image crop representations', 'vision foundation models', 'DINO', 'object detection', 'semantic segmentation']}, 'publishedAt': '2025-05-07T09:46:34.000Z', 'title': 'DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception', 'summary': \"Dense visual prediction tasks have been constrained by their reliance on\\npredefined categories, limiting their applicability in real-world scenarios\\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\\nCLIP have shown promise in open-vocabulary tasks, their direct application to\\ndense prediction often leads to suboptimal performance due to limitations in\\nlocal feature representation. In this work, we present our observation that\\nCLIP's image tokens struggle to effectively aggregate information from\\nspatially or semantically related regions, resulting in features that lack\\nlocal discriminability and spatial consistency. To address this issue, we\\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\\nself-attention module to obtain ``content'' and ``context'' features\\nrespectively. The ``content'' features are aligned with image crop\\nrepresentations to improve local discriminability, while ``context'' features\\nlearn to retain the spatial correlations under the guidance of vision\\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\\nsignificantly outperforms existing methods across multiple open-vocabulary\\ndense prediction tasks, including object detection and semantic segmentation.\\nCode is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64a385281cbf675203fbb7df/MpXgXINxg1RmSFnfeqXqU.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04410.png', 'numComments': 1, 'submittedBy': {'_id': '64a385281cbf675203fbb7df', 'avatarUrl': '/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg', 'fullname': 'Junjie Wang', 'name': 'xiaomoguhzz', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.09568', 'authors': [{'_id': '68254419181d43c25d829239', 'user': {'_id': '6393847e3e30234ae798b7be', 'avatarUrl': '/avatars/daeb8c37dff4432d837a69b87c196521.svg', 'isPro': True, 'fullname': 'JiuhaiChen', 'user': 'jiuhai', 'type': 'user'}, 'name': 'Jiuhai Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:48:28.916Z', 'hidden': False}, {'_id': '68254419181d43c25d82923a', 'user': {'_id': '64b6c686cf5117d7962d8f62', 'avatarUrl': '/avatars/96ed7a9602aa4c21b3a3d89608e76dc8.svg', 'isPro': False, 'fullname': 'Zhiyang Xu', 'user': 'Zhiyang03', 'type': 'user'}, 'name': 'Zhiyang Xu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:48:51.984Z', 'hidden': False}, {'_id': '68254419181d43c25d82923b', 'user': {'_id': '63172831c92fd6fee3181f50', 'avatarUrl': '/avatars/0f57068a138cb181e9451bfc1ed3d1c0.svg', 'isPro': False, 'fullname': 'Xichen Pan', 'user': 'xcpan', 'type': 'user'}, 'name': 'Xichen Pan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-15T10:31:43.038Z', 'hidden': False}, {'_id': '68254419181d43c25d82923c', 'user': {'_id': '62b1474bdcbad6848a91a54e', 'avatarUrl': '/avatars/d7308899b46232cad4a48a0e876449a8.svg', 'isPro': False, 'fullname': 'Yushi Hu', 'user': 'yushihu', 'type': 'user'}, 'name': 'Yushi Hu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:49:05.178Z', 'hidden': False}, {'_id': '68254419181d43c25d82923d', 'name': 'Can Qin', 'hidden': False}, {'_id': '68254419181d43c25d82923e', 'user': {'_id': '6381ca7d65dc156aba0b933d', 'avatarUrl': '/avatars/84dfdca8e1cd6fbf50d6fb2a6f1b488d.svg', 'isPro': False, 'fullname': 'Tom Goldstein', 'user': 'tomgoldstein', 'type': 'user'}, 'name': 'Tom Goldstein', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:49:13.762Z', 'hidden': False}, {'_id': '68254419181d43c25d82923f', 'name': 'Lifu Huang', 'hidden': False}, {'_id': '68254419181d43c25d829240', 'user': {'_id': '647f5af5b0e96764589f3b2a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg', 'isPro': False, 'fullname': 'Tianyi Zhou', 'user': 'zhoutianyi', 'type': 'user'}, 'name': 'Tianyi Zhou', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-15T10:32:05.507Z', 'hidden': False}, {'_id': '68254419181d43c25d829241', 'user': {'_id': '6596422646624a86ff3b3bda', 'avatarUrl': '/avatars/216e12b77e45ac5f1fa20932f5745411.svg', 'isPro': False, 'fullname': 'Saining Xie', 'user': 'sainx', 'type': 'user'}, 'name': 'Saining Xie', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:49:28.644Z', 'hidden': False}, {'_id': '68254419181d43c25d829242', 'user': {'_id': '67d5674bbc03ef961e733ddd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/3EUXNd-mKvsXFDlr1FETh.png', 'isPro': False, 'fullname': 'Silvio Savarese', 'user': 'SilvioSav8', 'type': 'user'}, 'name': 'Silvio Savarese', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:49:36.341Z', 'hidden': False}, {'_id': '68254419181d43c25d829243', 'user': {'_id': '63dd73e7422ca8d7f7e3698c', 'avatarUrl': '/avatars/7b0f8419f6941230b81dbbbb4f273edf.svg', 'isPro': False, 'fullname': 'Le Xue', 'user': 'SFXX', 'type': 'user'}, 'name': 'Le Xue', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:49:58.945Z', 'hidden': False}, {'_id': '68254419181d43c25d829244', 'user': {'_id': '649dbcc4e0fff1ed099dc80a', 'avatarUrl': '/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg', 'isPro': False, 'fullname': 'Caiming Xiong', 'user': 'cxiong', 'type': 'user'}, 'name': 'Caiming Xiong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:49:42.774Z', 'hidden': False}, {'_id': '68254419181d43c25d829245', 'user': {'_id': '6465c4c863e7e09dd02e3e1b', 'avatarUrl': '/avatars/200b029184d2616f98296a2c212f0785.svg', 'isPro': False, 'fullname': 'Ran Xu', 'user': 'xurantju', 'type': 'user'}, 'name': 'Ran Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-15T10:31:39.465Z', 'hidden': False}], 'publishedAt': '2025-05-14T17:11:07.000Z', 'submittedOnDailyAt': '2025-05-15T00:07:05.564Z', 'title': 'BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\\n  Training and Dataset', 'submittedOnDailyBy': {'_id': '6393847e3e30234ae798b7be', 'avatarUrl': '/avatars/daeb8c37dff4432d837a69b87c196521.svg', 'isPro': True, 'fullname': 'JiuhaiChen', 'user': 'jiuhai', 'type': 'user'}, 'summary': 'Unifying image understanding and generation has gained growing attention in\\nrecent research on multimodal models. Although design choices for image\\nunderstanding have been extensively studied, the optimal model architecture and\\ntraining recipe for a unified framework with image generation remain\\nunderexplored. Motivated by the strong potential of autoregressive and\\ndiffusion models for high-quality generation and scalability, we conduct a\\ncomprehensive study of their use in unified multimodal settings, with emphasis\\non image representations, modeling objectives, and training strategies.\\nGrounded in these investigations, we introduce a novel approach that employs a\\ndiffusion transformer to generate semantically rich CLIP image features, in\\ncontrast to conventional VAE-based representations. This design yields both\\nhigher training efficiency and improved generative quality. Furthermore, we\\ndemonstrate that a sequential pretraining strategy for unified models-first\\ntraining on image understanding and subsequently on image generation-offers\\npractical advantages by preserving image understanding capability while\\ndeveloping strong image generation ability. Finally, we carefully curate a\\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\\nprompting GPT-4o with a diverse set of captions covering various scenes,\\nobjects, human gestures, and more. Building on our innovative model design,\\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\\nunified multimodal models. BLIP3-o achieves superior performance across most of\\nthe popular benchmarks spanning both image understanding and generation tasks.\\nTo facilitate future research, we fully open-source our models, including code,\\nmodel weights, training scripts, and pretraining and instruction tuning\\ndatasets.', 'upvotes': 28, 'discussionId': '6825441a181d43c25d82927a', 'ai_keywords': ['autoregressive models', 'diffusion models', 'semantically rich CLIP image features', 'diffusion transformer', 'VAE-based representations', 'sequential pretraining strategy', 'image understanding', 'image generation', 'instruction-tuning dataset', 'GPT-4o', 'state-of-the-art unified multimodal models']}, 'publishedAt': '2025-05-14T13:11:07.000Z', 'title': 'BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\\n  Training and Dataset', 'summary': 'Unifying image understanding and generation has gained growing attention in\\nrecent research on multimodal models. Although design choices for image\\nunderstanding have been extensively studied, the optimal model architecture and\\ntraining recipe for a unified framework with image generation remain\\nunderexplored. Motivated by the strong potential of autoregressive and\\ndiffusion models for high-quality generation and scalability, we conduct a\\ncomprehensive study of their use in unified multimodal settings, with emphasis\\non image representations, modeling objectives, and training strategies.\\nGrounded in these investigations, we introduce a novel approach that employs a\\ndiffusion transformer to generate semantically rich CLIP image features, in\\ncontrast to conventional VAE-based representations. This design yields both\\nhigher training efficiency and improved generative quality. Furthermore, we\\ndemonstrate that a sequential pretraining strategy for unified models-first\\ntraining on image understanding and subsequently on image generation-offers\\npractical advantages by preserving image understanding capability while\\ndeveloping strong image generation ability. Finally, we carefully curate a\\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\\nprompting GPT-4o with a diverse set of captions covering various scenes,\\nobjects, human gestures, and more. Building on our innovative model design,\\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\\nunified multimodal models. BLIP3-o achieves superior performance across most of\\nthe popular benchmarks spanning both image understanding and generation tasks.\\nTo facilitate future research, we fully open-source our models, including code,\\nmodel weights, training scripts, and pretraining and instruction tuning\\ndatasets.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09568.png', 'numComments': 1, 'submittedBy': {'_id': '6393847e3e30234ae798b7be', 'avatarUrl': '/avatars/daeb8c37dff4432d837a69b87c196521.svg', 'fullname': 'JiuhaiChen', 'name': 'jiuhai', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 27}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.09343', 'authors': [{'_id': '682578ca1b93095c061429ff', 'user': {'_id': '66053b1f9e3555d648b21c3d', 'avatarUrl': '/avatars/c8b33e7f702c4edb17add47f0eafe5e6.svg', 'isPro': False, 'fullname': 'Chenggang Zhao', 'user': 'LyricZ', 'type': 'user'}, 'name': 'Chenggang Zhao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:50:09.165Z', 'hidden': False}, {'_id': '682578ca1b93095c06142a00', 'name': 'Chengqi Deng', 'hidden': False}, {'_id': '682578ca1b93095c06142a01', 'user': {'_id': '6398203609f12714ed1935c2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6398203609f12714ed1935c2/uXgl0LgKnFYjq1Wz39-a6.jpeg', 'isPro': False, 'fullname': 'Chong Ruan', 'user': 'Chester111', 'type': 'user'}, 'name': 'Chong Ruan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:50:30.400Z', 'hidden': False}, {'_id': '682578ca1b93095c06142a02', 'user': {'_id': '659389f8de82e1ef7b9a8b13', 'avatarUrl': '/avatars/896ed9f4cdbd317493b303d070b7e12a.svg', 'isPro': False, 'fullname': 'Damai Dai', 'user': 'DeepSeekDDM', 'type': 'user'}, 'name': 'Damai Dai', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:50:51.345Z', 'hidden': False}, {'_id': '682578ca1b93095c06142a03', 'user': {'_id': '64e370be59aa5366642ac329', 'avatarUrl': '/avatars/0fa1eb6ac6c1aeff3e65bc86a6617f64.svg', 'isPro': False, 'fullname': 'Huazuo Gao', 'user': 'gaohuazuo', 'type': 'user'}, 'name': 'Huazuo Gao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:51:03.129Z', 'hidden': False}, {'_id': '682578ca1b93095c06142a04', 'user': {'_id': '64fca5f28d50404bc42ca78a', 'avatarUrl': '/avatars/ae01ac0296d6ce1277dacb6894f570b8.svg', 'isPro': False, 'fullname': 'Jiashi Li', 'user': 'Beginlner', 'type': 'user'}, 'name': 'Jiashi Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:51:09.237Z', 'hidden': False}, {'_id': '682578ca1b93095c06142a05', 'user': {'_id': '67367647517b82b436d74930', 'avatarUrl': '/avatars/34c1f894a3da9f38816d0b30bfdc6d50.svg', 'isPro': False, 'fullname': 'Liyue Zhang', 'user': 'Lyriccc', 'type': 'user'}, 'name': 'Liyue Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:51:15.494Z', 'hidden': False}, {'_id': '682578ca1b93095c06142a06', 'name': 'Panpan Huang', 'hidden': False}, {'_id': '682578ca1b93095c06142a07', 'user': {'_id': '654453e19b639f21e1d77d16', 'avatarUrl': '/avatars/079ec500c2ca7a31f6cb754b8c7ef065.svg', 'isPro': False, 'fullname': 'Shangyan Zhou', 'user': 'syzhou', 'type': 'user'}, 'name': 'Shangyan Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:51:29.946Z', 'hidden': False}, {'_id': '682578ca1b93095c06142a08', 'user': {'_id': '6482e57a04f67f5f6056a61b', 'avatarUrl': '/avatars/b26faf19ba1493b91102ac7978ab3230.svg', 'isPro': False, 'fullname': 'Shirong Ma', 'user': 'msr2000', 'type': 'user'}, 'name': 'Shirong Ma', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:51:50.612Z', 'hidden': False}, {'_id': '682578ca1b93095c06142a09', 'name': 'Wenfeng Liang', 'hidden': False}, {'_id': '682578ca1b93095c06142a0a', 'name': 'Ying He', 'hidden': False}, {'_id': '682578ca1b93095c06142a0b', 'user': {'_id': '63ea23b9dedfeebe54d02bdf', 'avatarUrl': '/avatars/4d9f9a546aa8c63e277161ea700075c4.svg', 'isPro': False, 'fullname': 'Yuqing Wang', 'user': 'Epiphqny', 'type': 'user'}, 'name': 'Yuqing Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:52:24.404Z', 'hidden': False}, {'_id': '682578ca1b93095c06142a0c', 'name': 'Yuxuan Liu', 'hidden': False}, {'_id': '682578ca1b93095c06142a0d', 'name': 'Y. X. Wei', 'hidden': False}], 'publishedAt': '2025-05-14T12:39:03.000Z', 'submittedOnDailyAt': '2025-05-15T05:22:39.526Z', 'title': 'Insights into DeepSeek-V3: Scaling Challenges and Reflections on\\n  Hardware for AI Architectures', 'submittedOnDailyBy': {'_id': '5f1158120c833276f61f1a84', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg', 'isPro': False, 'fullname': 'Niels Rogge', 'user': 'nielsr', 'type': 'user'}, 'summary': \"The rapid scaling of large language models (LLMs) has unveiled critical\\nlimitations in current hardware architectures, including constraints in memory\\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\\nco-design can effectively address these challenges, enabling cost-efficient\\ntraining and inference at scale. This paper presents an in-depth analysis of\\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\\nefficiency, Mixture of Experts (MoE) architectures for optimized\\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\\nto minimize cluster-level network overhead. Building on the hardware\\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\\nbroader discussion with academic and industry peers on potential future\\nhardware directions, including precise low-precision computation units,\\nscale-up and scale-out convergence, and innovations in low-latency\\ncommunication fabrics. These insights underscore the critical role of hardware\\nand model co-design in meeting the escalating demands of AI workloads, offering\\na practical blueprint for innovation in next-generation AI systems.\", 'upvotes': 19, 'discussionId': '682578cb1b93095c06142a55', 'ai_keywords': ['Multi-head Latent Attention (MLA)', 'Mixture of Experts (MoE)', 'FP8 mixed-precision training', 'Multi-Plane Network Topology', 'low-precision computation units', 'scale-up and scale-out convergence', 'low-latency communication fabrics']}, 'publishedAt': '2025-05-14T08:39:03.000Z', 'title': 'Insights into DeepSeek-V3: Scaling Challenges and Reflections on\\n  Hardware for AI Architectures', 'summary': \"The rapid scaling of large language models (LLMs) has unveiled critical\\nlimitations in current hardware architectures, including constraints in memory\\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\\nco-design can effectively address these challenges, enabling cost-efficient\\ntraining and inference at scale. This paper presents an in-depth analysis of\\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\\nefficiency, Mixture of Experts (MoE) architectures for optimized\\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\\nto minimize cluster-level network overhead. Building on the hardware\\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\\nbroader discussion with academic and industry peers on potential future\\nhardware directions, including precise low-precision computation units,\\nscale-up and scale-out convergence, and innovations in low-latency\\ncommunication fabrics. These insights underscore the critical role of hardware\\nand model co-design in meeting the escalating demands of AI workloads, offering\\na practical blueprint for innovation in next-generation AI systems.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09343.png', 'numComments': 1, 'submittedBy': {'_id': '5f1158120c833276f61f1a84', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg', 'fullname': 'Niels Rogge', 'name': 'nielsr', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': False, 'isMod': False, 'followerCount': 864}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2505.09358', 'authors': [{'_id': '6825c0e6bfd4908da7747c41', 'user': {'_id': '63d90391da4f72339244c2a8', 'avatarUrl': '/avatars/eb0e0259c391d59739c1a205c36bb539.svg', 'isPro': False, 'fullname': 'Bingxin Ke', 'user': 'Bingxin', 'type': 'user'}, 'name': 'Bingxin Ke', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-15T13:55:42.142Z', 'hidden': False}, {'_id': '6825c0e6bfd4908da7747c42', 'name': 'Kevin Qu', 'hidden': False}, {'_id': '6825c0e6bfd4908da7747c43', 'name': 'Tianfu Wang', 'hidden': False}, {'_id': '6825c0e6bfd4908da7747c44', 'user': {'_id': '63a5785a8fb23d08bb2d0291', 'avatarUrl': '/avatars/758b06dae06e9eee6fced10ce682aef1.svg', 'isPro': False, 'fullname': 'Nando Metzger', 'user': 'nandometzger', 'type': 'user'}, 'name': 'Nando Metzger', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-15T13:45:09.533Z', 'hidden': False}, {'_id': '6825c0e6bfd4908da7747c45', 'name': 'Shengyu Huang', 'hidden': False}, {'_id': '6825c0e6bfd4908da7747c46', 'name': 'Bo Li', 'hidden': False}, {'_id': '6825c0e6bfd4908da7747c47', 'user': {'_id': '62f93abbc4817cfc0756b6f8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62f93abbc4817cfc0756b6f8/rGYLaq-rmoJJYotkC1VXk.jpeg', 'isPro': True, 'fullname': 'Anton Obukhov', 'user': 'toshas', 'type': 'user'}, 'name': 'Anton Obukhov', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-15T13:45:11.907Z', 'hidden': False}, {'_id': '6825c0e6bfd4908da7747c48', 'user': {'_id': '6750649b7edd6a98a1bbcd06', 'avatarUrl': '/avatars/ba27f12d0333cf2d400d4405af7efe97.svg', 'isPro': False, 'fullname': 'Konrad Schindler', 'user': 'konradschindler', 'type': 'user'}, 'name': 'Konrad Schindler', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-05-15T10:53:52.126Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62f93abbc4817cfc0756b6f8/fDCIA8Ghqly2s-qQZj64D.mp4'], 'publishedAt': '2025-05-14T13:07:03.000Z', 'submittedOnDailyAt': '2025-05-15T09:14:40.835Z', 'title': 'Marigold: Affordable Adaptation of Diffusion-Based Image Generators for\\n  Image Analysis', 'submittedOnDailyBy': {'_id': '62f93abbc4817cfc0756b6f8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62f93abbc4817cfc0756b6f8/rGYLaq-rmoJJYotkC1VXk.jpeg', 'isPro': True, 'fullname': 'Anton Obukhov', 'user': 'toshas', 'type': 'user'}, 'summary': \"The success of deep learning in computer vision over the past decade has\\nhinged on large labeled datasets and strong pretrained models. In data-scarce\\nsettings, the quality of these pretrained models becomes crucial for effective\\ntransfer learning. Image classification and self-supervised learning have\\ntraditionally been the primary methods for pretraining CNNs and\\ntransformer-based architectures. Recently, the rise of text-to-image generative\\nmodels, particularly those using denoising diffusion in a latent space, has\\nintroduced a new class of foundational models trained on massive, captioned\\nimage datasets. These models' ability to generate realistic images of unseen\\ncontent suggests they possess a deep understanding of the visual world. In this\\nwork, we present Marigold, a family of conditional generative models and a\\nfine-tuning protocol that extracts the knowledge from pretrained latent\\ndiffusion models like Stable Diffusion and adapts them for dense image analysis\\ntasks, including monocular depth estimation, surface normals prediction, and\\nintrinsic decomposition. Marigold requires minimal modification of the\\npre-trained latent diffusion model's architecture, trains with small synthetic\\ndatasets on a single GPU over a few days, and demonstrates state-of-the-art\\nzero-shot generalization. Project page:\\nhttps://marigoldcomputervision.github.io\", 'upvotes': 11, 'discussionId': '6825c0ebbfd4908da7747d67', 'ai_keywords': ['denoising diffusion', 'latent space', 'generative models', 'pretrained latent diffusion models', 'Stable Diffusion', 'dense image analysis tasks', 'monocular depth estimation', 'surface normals prediction', 'intrinsic decomposition', 'fine-tuning protocol', 'zero-shot generalization']}, 'publishedAt': '2025-05-14T09:07:03.000Z', 'title': 'Marigold: Affordable Adaptation of Diffusion-Based Image Generators for\\n  Image Analysis', 'summary': \"The success of deep learning in computer vision over the past decade has\\nhinged on large labeled datasets and strong pretrained models. In data-scarce\\nsettings, the quality of these pretrained models becomes crucial for effective\\ntransfer learning. Image classification and self-supervised learning have\\ntraditionally been the primary methods for pretraining CNNs and\\ntransformer-based architectures. Recently, the rise of text-to-image generative\\nmodels, particularly those using denoising diffusion in a latent space, has\\nintroduced a new class of foundational models trained on massive, captioned\\nimage datasets. These models' ability to generate realistic images of unseen\\ncontent suggests they possess a deep understanding of the visual world. In this\\nwork, we present Marigold, a family of conditional generative models and a\\nfine-tuning protocol that extracts the knowledge from pretrained latent\\ndiffusion models like Stable Diffusion and adapts them for dense image analysis\\ntasks, including monocular depth estimation, surface normals prediction, and\\nintrinsic decomposition. Marigold requires minimal modification of the\\npre-trained latent diffusion model's architecture, trains with small synthetic\\ndatasets on a single GPU over a few days, and demonstrates state-of-the-art\\nzero-shot generalization. Project page:\\nhttps://marigoldcomputervision.github.io\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62f93abbc4817cfc0756b6f8/fDCIA8Ghqly2s-qQZj64D.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09358.png', 'numComments': 1, 'submittedBy': {'_id': '62f93abbc4817cfc0756b6f8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62f93abbc4817cfc0756b6f8/rGYLaq-rmoJJYotkC1VXk.jpeg', 'fullname': 'Anton Obukhov', 'name': 'toshas', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 75}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.08787', 'authors': [{'_id': '6823f20eb9e5c4a5c866a634', 'user': {'_id': '66f21f9e14fcea0aa11361f5', 'avatarUrl': '/avatars/fb8261fb9da98f47d9102d68762ac821.svg', 'isPro': False, 'fullname': 'Hanjung Kim', 'user': 'HanjungKim', 'type': 'user'}, 'name': 'Hanjung Kim', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-15T10:32:41.278Z', 'hidden': False}, {'_id': '6823f20eb9e5c4a5c866a635', 'name': 'Jaehyun Kang', 'hidden': False}, {'_id': '6823f20eb9e5c4a5c866a636', 'name': 'Hyolim Kang', 'hidden': False}, {'_id': '6823f20eb9e5c4a5c866a637', 'name': 'Meedeum Cho', 'hidden': False}, {'_id': '6823f20eb9e5c4a5c866a638', 'name': 'Seon Joo Kim', 'hidden': False}, {'_id': '6823f20eb9e5c4a5c866a639', 'name': 'Youngwoon Lee', 'hidden': False}], 'publishedAt': '2025-05-13T17:59:22.000Z', 'submittedOnDailyAt': '2025-05-15T11:39:27.106Z', 'title': 'UniSkill: Imitating Human Videos via Cross-Embodiment Skill\\n  Representations', 'submittedOnDailyBy': {'_id': '66f21f9e14fcea0aa11361f5', 'avatarUrl': '/avatars/fb8261fb9da98f47d9102d68762ac821.svg', 'isPro': False, 'fullname': 'Hanjung Kim', 'user': 'HanjungKim', 'type': 'user'}, 'summary': 'Mimicry is a fundamental learning mechanism in humans, enabling individuals\\nto learn new tasks by observing and imitating experts. However, applying this\\nability to robots presents significant challenges due to the inherent\\ndifferences between human and robot embodiments in both their visual appearance\\nand physical capabilities. While previous methods bridge this gap using\\ncross-embodiment datasets with shared scenes and tasks, collecting such aligned\\ndata between humans and robots at scale is not trivial. In this paper, we\\npropose UniSkill, a novel framework that learns embodiment-agnostic skill\\nrepresentations from large-scale cross-embodiment video data without any\\nlabels, enabling skills extracted from human video prompts to effectively\\ntransfer to robot policies trained only on robot data. Our experiments in both\\nsimulation and real-world environments show that our cross-embodiment skills\\nsuccessfully guide robots in selecting appropriate actions, even with unseen\\nvideo prompts. The project website can be found at:\\nhttps://kimhanjung.github.io/UniSkill.', 'upvotes': 10, 'discussionId': '6823f210b9e5c4a5c866a6c5', 'projectPage': 'https://kimhanjung.github.io/UniSkill/', 'githubRepo': 'https://github.com/KimHanjung/UniSkill', 'ai_keywords': ['embodiment-agnostic', 'skill representations', 'cross-embodiment video data', 'robot policies', 'action selection', 'video prompts', 'UniSkill']}, 'publishedAt': '2025-05-13T13:59:22.000Z', 'title': 'UniSkill: Imitating Human Videos via Cross-Embodiment Skill\\n  Representations', 'summary': 'Mimicry is a fundamental learning mechanism in humans, enabling individuals\\nto learn new tasks by observing and imitating experts. However, applying this\\nability to robots presents significant challenges due to the inherent\\ndifferences between human and robot embodiments in both their visual appearance\\nand physical capabilities. While previous methods bridge this gap using\\ncross-embodiment datasets with shared scenes and tasks, collecting such aligned\\ndata between humans and robots at scale is not trivial. In this paper, we\\npropose UniSkill, a novel framework that learns embodiment-agnostic skill\\nrepresentations from large-scale cross-embodiment video data without any\\nlabels, enabling skills extracted from human video prompts to effectively\\ntransfer to robot policies trained only on robot data. Our experiments in both\\nsimulation and real-world environments show that our cross-embodiment skills\\nsuccessfully guide robots in selecting appropriate actions, even with unseen\\nvideo prompts. The project website can be found at:\\nhttps://kimhanjung.github.io/UniSkill.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08787.png', 'numComments': 1, 'submittedBy': {'_id': '66f21f9e14fcea0aa11361f5', 'avatarUrl': '/avatars/fb8261fb9da98f47d9102d68762ac821.svg', 'fullname': 'Hanjung Kim', 'name': 'HanjungKim', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.07849', 'authors': [{'_id': '6825c9c7c6f06669c8fa6813', 'name': 'Revanth Gangi Reddy', 'hidden': False}, {'_id': '6825c9c7c6f06669c8fa6814', 'name': 'Tarun Suresh', 'hidden': False}, {'_id': '6825c9c7c6f06669c8fa6815', 'name': 'JaeHyeok Doo', 'hidden': False}, {'_id': '6825c9c7c6f06669c8fa6816', 'name': 'Ye Liu', 'hidden': False}, {'_id': '6825c9c7c6f06669c8fa6817', 'name': 'Xuan Phi Nguyen', 'hidden': False}, {'_id': '6825c9c7c6f06669c8fa6818', 'name': 'Yingbo Zhou', 'hidden': False}, {'_id': '6825c9c7c6f06669c8fa6819', 'name': 'Semih Yavuz', 'hidden': False}, {'_id': '6825c9c7c6f06669c8fa681a', 'name': 'Caiming Xiong', 'hidden': False}, {'_id': '6825c9c7c6f06669c8fa681b', 'name': 'Heng Ji', 'hidden': False}, {'_id': '6825c9c7c6f06669c8fa681c', 'name': 'Shafiq Joty', 'hidden': False}], 'publishedAt': '2025-05-07T19:44:09.000Z', 'submittedOnDailyAt': '2025-05-15T09:34:37.521Z', 'title': 'SweRank: Software Issue Localization with Code Ranking', 'submittedOnDailyBy': {'_id': '65e7bb35e5e78134ab049942', 'avatarUrl': '/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg', 'isPro': False, 'fullname': 'Tarun Suresh', 'user': 'tarsur909', 'type': 'user'}, 'summary': \"Software issue localization, the task of identifying the precise code\\nlocations (files, classes, or functions) relevant to a natural language issue\\ndescription (e.g., bug report, feature request), is a critical yet\\ntime-consuming aspect of software development. While recent LLM-based agentic\\napproaches demonstrate promise, they often incur significant latency and cost\\ndue to complex multi-step reasoning and relying on closed-source LLMs.\\nAlternatively, traditional code ranking models, typically optimized for\\nquery-to-code or code-to-code retrieval, struggle with the verbose and\\nfailure-descriptive nature of issue localization queries. To bridge this gap,\\nwe introduce SweRank, an efficient and effective retrieve-and-rerank framework\\nfor software issue localization. To facilitate training, we construct SweLoc, a\\nlarge-scale dataset curated from public GitHub repositories, featuring\\nreal-world issue descriptions paired with corresponding code modifications.\\nEmpirical results on SWE-Bench-Lite and LocBench show that SweRank achieves\\nstate-of-the-art performance, outperforming both prior ranking models and\\ncostly agent-based systems using closed-source LLMs like Claude-3.5. Further,\\nwe demonstrate SweLoc's utility in enhancing various existing retriever and\\nreranker models for issue localization, establishing the dataset as a valuable\\nresource for the community.\", 'upvotes': 4, 'discussionId': '6825c9c8c6f06669c8fa685e', 'ai_keywords': ['SweRank', 'SweLoc', 'SWE-Bench-Lite', 'LocBench', 'LLM-based agentic approaches', 'closed-source LLMs', 'code ranking models', 'query-to-code', 'code-to-code retrieval', 'issue localization queries', 'retrieve-and-rerank framework', 'public GitHub repositories', 'issue descriptions', 'code modifications', 'state-of-the-art performance']}, 'publishedAt': '2025-05-07T15:44:09.000Z', 'title': 'SweRank: Software Issue Localization with Code Ranking', 'summary': \"Software issue localization, the task of identifying the precise code\\nlocations (files, classes, or functions) relevant to a natural language issue\\ndescription (e.g., bug report, feature request), is a critical yet\\ntime-consuming aspect of software development. While recent LLM-based agentic\\napproaches demonstrate promise, they often incur significant latency and cost\\ndue to complex multi-step reasoning and relying on closed-source LLMs.\\nAlternatively, traditional code ranking models, typically optimized for\\nquery-to-code or code-to-code retrieval, struggle with the verbose and\\nfailure-descriptive nature of issue localization queries. To bridge this gap,\\nwe introduce SweRank, an efficient and effective retrieve-and-rerank framework\\nfor software issue localization. To facilitate training, we construct SweLoc, a\\nlarge-scale dataset curated from public GitHub repositories, featuring\\nreal-world issue descriptions paired with corresponding code modifications.\\nEmpirical results on SWE-Bench-Lite and LocBench show that SweRank achieves\\nstate-of-the-art performance, outperforming both prior ranking models and\\ncostly agent-based systems using closed-source LLMs like Claude-3.5. Further,\\nwe demonstrate SweLoc's utility in enhancing various existing retriever and\\nreranker models for issue localization, establishing the dataset as a valuable\\nresource for the community.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07849.png', 'numComments': 1, 'submittedBy': {'_id': '65e7bb35e5e78134ab049942', 'avatarUrl': '/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg', 'fullname': 'Tarun Suresh', 'name': 'tarsur909', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2505.08455', 'authors': [{'_id': '6824176351679cbc704daa88', 'user': {'_id': '6483b3d52193a1768c00c5ff', 'avatarUrl': '/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg', 'isPro': False, 'fullname': 'Pritam Sarkar', 'user': 'pritamqu', 'type': 'user'}, 'name': 'Pritam Sarkar', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-14T04:09:09.283Z', 'hidden': False}, {'_id': '6824176351679cbc704daa89', 'name': 'Ali Etemad', 'hidden': False}], 'publishedAt': '2025-05-13T11:35:58.000Z', 'submittedOnDailyAt': '2025-05-15T04:58:35.875Z', 'title': 'VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\\n  Video Language Models', 'submittedOnDailyBy': {'_id': '6483b3d52193a1768c00c5ff', 'avatarUrl': '/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg', 'isPro': False, 'fullname': 'Pritam Sarkar', 'user': 'pritamqu', 'type': 'user'}, 'summary': 'Despite recent advances in video understanding, the capabilities of Large\\nVideo Language Models (LVLMs) to perform video-based causal reasoning remains\\nunderexplored, largely due to the absence of relevant and dedicated benchmarks\\nfor evaluating causal reasoning in visually grounded and goal-driven settings.\\nTo fill this gap, we introduce a novel benchmark named Video-based long-form\\nCausal Reasoning (VCRBench). We create VCRBench using procedural videos of\\nsimple everyday activities, where the steps are deliberately shuffled with each\\nclip capturing a key causal event, to test whether LVLMs can identify, reason\\nabout, and correctly sequence the events needed to accomplish a specific goal.\\nMoreover, the benchmark is carefully designed to prevent LVLMs from exploiting\\nlinguistic shortcuts, as seen in multiple-choice or binary QA formats, while\\nalso avoiding the challenges associated with evaluating open-ended QA. Our\\nevaluation of state-of-the-art LVLMs on VCRBench suggests that these models\\nstruggle with video-based long-form causal reasoning, primarily due to their\\ndifficulty in modeling long-range causal dependencies directly from visual\\nobservations. As a simple step toward enabling such capabilities, we propose\\nRecognition-Reasoning Decomposition (RRD), a modular approach that breaks\\nvideo-based causal reasoning into two sub-tasks of video recognition and causal\\nreasoning. Our experiments on VCRBench show that RRD significantly boosts\\naccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis\\nreveals interesting insights, for instance, that LVLMs primarily rely on\\nlanguage knowledge for complex video-based long-form causal reasoning tasks.', 'upvotes': 3, 'discussionId': '6824176551679cbc704daafb', 'projectPage': 'https://pritamsarkar.com/VCRBench/', 'githubRepo': 'https://github.com/pritamqu/VCRBench', 'ai_keywords': ['Large Video Language Models (LVLMs)', 'causal reasoning', 'benchmarks', 'procedural videos', 'causal dependencies', 'video-based long-form causal reasoning', 'VCRBench', 'video recognition', 'Recognition-Reasoning Decomposition (RRD)']}, 'publishedAt': '2025-05-13T07:35:58.000Z', 'title': 'VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\\n  Video Language Models', 'summary': 'Despite recent advances in video understanding, the capabilities of Large\\nVideo Language Models (LVLMs) to perform video-based causal reasoning remains\\nunderexplored, largely due to the absence of relevant and dedicated benchmarks\\nfor evaluating causal reasoning in visually grounded and goal-driven settings.\\nTo fill this gap, we introduce a novel benchmark named Video-based long-form\\nCausal Reasoning (VCRBench). We create VCRBench using procedural videos of\\nsimple everyday activities, where the steps are deliberately shuffled with each\\nclip capturing a key causal event, to test whether LVLMs can identify, reason\\nabout, and correctly sequence the events needed to accomplish a specific goal.\\nMoreover, the benchmark is carefully designed to prevent LVLMs from exploiting\\nlinguistic shortcuts, as seen in multiple-choice or binary QA formats, while\\nalso avoiding the challenges associated with evaluating open-ended QA. Our\\nevaluation of state-of-the-art LVLMs on VCRBench suggests that these models\\nstruggle with video-based long-form causal reasoning, primarily due to their\\ndifficulty in modeling long-range causal dependencies directly from visual\\nobservations. As a simple step toward enabling such capabilities, we propose\\nRecognition-Reasoning Decomposition (RRD), a modular approach that breaks\\nvideo-based causal reasoning into two sub-tasks of video recognition and causal\\nreasoning. Our experiments on VCRBench show that RRD significantly boosts\\naccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis\\nreveals interesting insights, for instance, that LVLMs primarily rely on\\nlanguage knowledge for complex video-based long-form causal reasoning tasks.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08455.png', 'numComments': 1, 'submittedBy': {'_id': '6483b3d52193a1768c00c5ff', 'avatarUrl': '/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg', 'fullname': 'Pritam Sarkar', 'name': 'pritamqu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.12894', 'authors': [{'_id': '67b60a5c92649eb787234701', 'name': 'Kaixin Yao', 'hidden': False}, {'_id': '67b60a5c92649eb787234702', 'name': 'Longwen Zhang', 'hidden': False}, {'_id': '67b60a5c92649eb787234703', 'name': 'Xinhao Yan', 'hidden': False}, {'_id': '67b60a5c92649eb787234704', 'name': 'Yan Zeng', 'hidden': False}, {'_id': '67b60a5c92649eb787234705', 'name': 'Qixuan Zhang', 'hidden': False}, {'_id': '67b60a5c92649eb787234706', 'name': 'Lan Xu', 'hidden': False}, {'_id': '67b60a5c92649eb787234707', 'name': 'Wei Yang', 'hidden': False}, {'_id': '67b60a5c92649eb787234708', 'name': 'Jiayuan Gu', 'hidden': False}, {'_id': '67b60a5c92649eb787234709', 'name': 'Jingyi Yu', 'hidden': False}], 'publishedAt': '2025-02-18T14:29:52.000Z', 'submittedOnDailyAt': '2025-05-15T11:19:14.934Z', 'title': 'CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': \"Recovering high-quality 3D scenes from a single RGB image is a challenging\\ntask in computer graphics. Current methods often struggle with domain-specific\\nlimitations or low-quality object generation. To address these, we propose CAST\\n(Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel\\nmethod for 3D scene reconstruction and recovery. CAST starts by extracting\\nobject-level 2D segmentation and relative depth information from the input\\nimage, followed by using a GPT-based model to analyze inter-object spatial\\nrelationships. This enables the understanding of how objects relate to each\\nother within the scene, ensuring more coherent reconstruction. CAST then\\nemploys an occlusion-aware large-scale 3D generation model to independently\\ngenerate each object's full geometry, using MAE and point cloud conditioning to\\nmitigate the effects of occlusions and partial object information, ensuring\\naccurate alignment with the source image's geometry and texture. To align each\\nobject with the scene, the alignment generation model computes the necessary\\ntransformations, allowing the generated meshes to be accurately placed and\\nintegrated into the scene's point cloud. Finally, CAST incorporates a\\nphysics-aware correction step that leverages a fine-grained relation graph to\\ngenerate a constraint graph. This graph guides the optimization of object\\nposes, ensuring physical consistency and spatial coherence. By utilizing Signed\\nDistance Fields (SDF), the model effectively addresses issues such as\\nocclusions, object penetration, and floating objects, ensuring that the\\ngenerated scene accurately reflects real-world physical interactions. CAST can\\nbe leveraged in robotics, enabling efficient real-to-simulation workflows and\\nproviding realistic, scalable simulation environments for robotic systems.\", 'upvotes': 3, 'discussionId': '67b60a6192649eb7872347d8', 'ai_keywords': ['GPT-based model', 'spatial relationships', 'occlusion-aware', 'MAE', 'point cloud conditioning', 'object geometry', 'texture alignment', 'transformation computation', 'physics-aware correction', 'relation graph', 'Signed Distance Fields (SDF)', 'object poses', 'physical consistency', 'spatial coherence', 'Signed Distance Fields (SDF)']}, 'publishedAt': '2025-02-18T09:29:52.000Z', 'title': 'CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image', 'summary': \"Recovering high-quality 3D scenes from a single RGB image is a challenging\\ntask in computer graphics. Current methods often struggle with domain-specific\\nlimitations or low-quality object generation. To address these, we propose CAST\\n(Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel\\nmethod for 3D scene reconstruction and recovery. CAST starts by extracting\\nobject-level 2D segmentation and relative depth information from the input\\nimage, followed by using a GPT-based model to analyze inter-object spatial\\nrelationships. This enables the understanding of how objects relate to each\\nother within the scene, ensuring more coherent reconstruction. CAST then\\nemploys an occlusion-aware large-scale 3D generation model to independently\\ngenerate each object's full geometry, using MAE and point cloud conditioning to\\nmitigate the effects of occlusions and partial object information, ensuring\\naccurate alignment with the source image's geometry and texture. To align each\\nobject with the scene, the alignment generation model computes the necessary\\ntransformations, allowing the generated meshes to be accurately placed and\\nintegrated into the scene's point cloud. Finally, CAST incorporates a\\nphysics-aware correction step that leverages a fine-grained relation graph to\\ngenerate a constraint graph. This graph guides the optimization of object\\nposes, ensuring physical consistency and spatial coherence. By utilizing Signed\\nDistance Fields (SDF), the model effectively addresses issues such as\\nocclusions, object penetration, and floating objects, ensuring that the\\ngenerated scene accurately reflects real-world physical interactions. CAST can\\nbe leveraged in robotics, enabling efficient real-to-simulation workflows and\\nproviding realistic, scalable simulation environments for robotic systems.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12894.png', 'numComments': 2, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6845}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2505.09439', 'authors': [{'_id': '6825cfe4658e0204a1b492c7', 'user': {'_id': '6253482bde8d23c3ce8816d0', 'avatarUrl': '/avatars/44475ac0e514a4e88d148ab43d0fd489.svg', 'isPro': False, 'fullname': 'Andrew Rouditchenko', 'user': 'h9LtLSb', 'type': 'user'}, 'name': 'Andrew Rouditchenko', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-05-15T11:29:56.341Z', 'hidden': False}, {'_id': '6825cfe4658e0204a1b492c8', 'name': 'Saurabhchand Bhati', 'hidden': False}, {'_id': '6825cfe4658e0204a1b492c9', 'name': 'Edson Araujo', 'hidden': False}, {'_id': '6825cfe4658e0204a1b492ca', 'name': 'Samuel Thomas', 'hidden': False}, {'_id': '6825cfe4658e0204a1b492cb', 'name': 'Hilde Kuehne', 'hidden': False}, {'_id': '6825cfe4658e0204a1b492cc', 'name': 'Rogerio Feris', 'hidden': False}, {'_id': '6825cfe4658e0204a1b492cd', 'name': 'James Glass', 'hidden': False}], 'publishedAt': '2025-05-14T14:47:16.000Z', 'submittedOnDailyAt': '2025-05-15T09:59:19.074Z', 'title': 'Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?', 'submittedOnDailyBy': {'_id': '6253482bde8d23c3ce8816d0', 'avatarUrl': '/avatars/44475ac0e514a4e88d148ab43d0fd489.svg', 'isPro': False, 'fullname': 'Andrew Rouditchenko', 'user': 'h9LtLSb', 'type': 'user'}, 'summary': 'We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni,\\non an audio question answering dataset with the reinforcement learning method\\nGRPO. This leads to new State-of-the-Art performance on the recent MMAU\\nbenchmark. Omni-R1 achieves the highest accuracies on the sounds, music,\\nspeech, and overall average categories, both on the Test-mini and Test-full\\nsplits. To understand the performance improvement, we tested models both with\\nand without audio and found that much of the performance improvement from GRPO\\ncould be attributed to better text-based reasoning. We also made a surprising\\ndiscovery that fine-tuning without audio on a text-only dataset was effective\\nat improving the audio-based performance.', 'upvotes': 2, 'discussionId': '6825cfe4658e0204a1b492eb', 'ai_keywords': ['multi-modal LLM', 'Qwen2.5-Omni', 'audio question answering', 'reinforcement learning', 'GRPO', 'MMAU benchmark', 'text-based reasoning']}, 'publishedAt': '2025-05-14T10:47:16.000Z', 'title': 'Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?', 'summary': 'We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni,\\non an audio question answering dataset with the reinforcement learning method\\nGRPO. This leads to new State-of-the-Art performance on the recent MMAU\\nbenchmark. Omni-R1 achieves the highest accuracies on the sounds, music,\\nspeech, and overall average categories, both on the Test-mini and Test-full\\nsplits. To understand the performance improvement, we tested models both with\\nand without audio and found that much of the performance improvement from GRPO\\ncould be attributed to better text-based reasoning. We also made a surprising\\ndiscovery that fine-tuning without audio on a text-only dataset was effective\\nat improving the audio-based performance.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09439.png', 'numComments': 1, 'submittedBy': {'_id': '6253482bde8d23c3ce8816d0', 'avatarUrl': '/avatars/44475ac0e514a4e88d148ab43d0fd489.svg', 'fullname': 'Andrew Rouditchenko', 'name': 'h9LtLSb', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.08084', 'authors': [{'_id': '6824bf46b4f1e881646d2dda', 'user': {'_id': '65a84987c5ffe1d019d7ac02', 'avatarUrl': '/avatars/1e54a4d99e98da142f0d6441df3d2d4b.svg', 'isPro': False, 'fullname': 'Yu Cheng', 'user': 'JadeCheng', 'type': 'user'}, 'name': 'Yu Cheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-15T10:32:11.536Z', 'hidden': False}, {'_id': '6824bf46b4f1e881646d2ddb', 'user': {'_id': '65aa4c2ea92a64ef5b2f2ff9', 'avatarUrl': '/avatars/13d894fe51c011620d094c2f1a114aaf.svg', 'isPro': False, 'fullname': 'Arushi Goel', 'user': 'goarushi', 'type': 'user'}, 'name': 'Arushi Goel', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-14T16:05:27.452Z', 'hidden': False}, {'_id': '6824bf46b4f1e881646d2ddc', 'name': 'Hakan Bilen', 'hidden': False}], 'publishedAt': '2025-05-12T21:37:06.000Z', 'submittedOnDailyAt': '2025-05-15T11:25:30.602Z', 'title': 'Visually Interpretable Subtask Reasoning for Visual Question Answering', 'submittedOnDailyBy': {'_id': '65a84987c5ffe1d019d7ac02', 'avatarUrl': '/avatars/1e54a4d99e98da142f0d6441df3d2d4b.svg', 'isPro': False, 'fullname': 'Yu Cheng', 'user': 'JadeCheng', 'type': 'user'}, 'summary': \"Answering complex visual questions like `Which red furniture can be used for\\nsitting?' requires multi-step reasoning, including object recognition,\\nattribute filtering, and relational understanding. Recent work improves\\ninterpretability in multimodal large language models (MLLMs) by decomposing\\ntasks into sub-task programs, but these methods are computationally expensive\\nand less accurate due to poor adaptation to target data. To address this, we\\nintroduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a\\nsubtask-driven training framework that enhances both interpretability and\\nreasoning by generating textual and visual explanations within MLLMs. Instead\\nof relying on external models, VISTAR fine-tunes MLLMs to produce structured\\nSubtask-of-Thought rationales (step-by-step reasoning sequences). Experiments\\non two benchmarks show that VISTAR consistently improves reasoning accuracy\\nwhile maintaining interpretability. Our code and dataset will be available at\\nhttps://github.com/ChengJade/VISTAR.\", 'upvotes': 1, 'discussionId': '6824bf47b4f1e881646d2e0b', 'githubRepo': 'https://github.com/ChengJade/VISTAR.git', 'ai_keywords': ['VISTAR (Visually Interpretable Subtask-Aware Reasoning Model)', 'multimodal large language models (MLLMs)', 'subtask-driven training framework', 'Subtask-of-Thought rationales (step-by-step reasoning sequences)']}, 'publishedAt': '2025-05-12T17:37:06.000Z', 'title': 'Visually Interpretable Subtask Reasoning for Visual Question Answering', 'summary': \"Answering complex visual questions like `Which red furniture can be used for\\nsitting?' requires multi-step reasoning, including object recognition,\\nattribute filtering, and relational understanding. Recent work improves\\ninterpretability in multimodal large language models (MLLMs) by decomposing\\ntasks into sub-task programs, but these methods are computationally expensive\\nand less accurate due to poor adaptation to target data. To address this, we\\nintroduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a\\nsubtask-driven training framework that enhances both interpretability and\\nreasoning by generating textual and visual explanations within MLLMs. Instead\\nof relying on external models, VISTAR fine-tunes MLLMs to produce structured\\nSubtask-of-Thought rationales (step-by-step reasoning sequences). Experiments\\non two benchmarks show that VISTAR consistently improves reasoning accuracy\\nwhile maintaining interpretability. Our code and dataset will be available at\\nhttps://github.com/ChengJade/VISTAR.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08084.png', 'numComments': 1, 'submittedBy': {'_id': '65a84987c5ffe1d019d7ac02', 'avatarUrl': '/avatars/1e54a4d99e98da142f0d6441df3d2d4b.svg', 'fullname': 'Yu Cheng', 'name': 'JadeCheng', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.04793', 'authors': [{'_id': '6825d678d5a62682c1a581ac', 'user': {'_id': '61c334dc77d54877f7643294', 'avatarUrl': '/avatars/7c68821762bcfd844c8d780b9d1276f4.svg', 'isPro': False, 'fullname': 'Hambarde', 'user': 'kailassrt', 'type': 'user'}, 'name': 'Kailash A. Hambarde', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-15T13:45:06.526Z', 'hidden': False}, {'_id': '6825d678d5a62682c1a581ad', 'name': 'Nzakiese Mbongo', 'hidden': False}, {'_id': '6825d678d5a62682c1a581ae', 'name': 'Pavan Kumar MP', 'hidden': False}, {'_id': '6825d678d5a62682c1a581af', 'name': 'Satish Mekewad', 'hidden': False}, {'_id': '6825d678d5a62682c1a581b0', 'name': 'Carolina Fernandes', 'hidden': False}, {'_id': '6825d678d5a62682c1a581b1', 'name': 'Gökhan Silahtaroğlu', 'hidden': False}, {'_id': '6825d678d5a62682c1a581b2', 'name': 'Alice Nithya', 'hidden': False}, {'_id': '6825d678d5a62682c1a581b3', 'name': 'Pawan Wasnik', 'hidden': False}, {'_id': '6825d678d5a62682c1a581b4', 'name': 'MD. Rashidunnabi', 'hidden': False}, {'_id': '6825d678d5a62682c1a581b5', 'name': 'Pranita Samale', 'hidden': False}, {'_id': '6825d678d5a62682c1a581b6', 'name': 'Hugo Proença', 'hidden': False}], 'publishedAt': '2025-05-07T20:41:06.000Z', 'submittedOnDailyAt': '2025-05-15T10:27:35.006Z', 'title': 'DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person\\n  Recognition', 'submittedOnDailyBy': {'_id': '61c334dc77d54877f7643294', 'avatarUrl': '/avatars/7c68821762bcfd844c8d780b9d1276f4.svg', 'isPro': False, 'fullname': 'Hambarde', 'user': 'kailassrt', 'type': 'user'}, 'summary': 'Person reidentification (ReID) technology has been considered to perform\\nrelatively well under controlled, ground-level conditions, but it breaks down\\nwhen deployed in challenging real-world settings. Evidently, this is due to\\nextreme data variability factors such as resolution, viewpoint changes, scale\\nvariations, occlusions, and appearance shifts from clothing or session drifts.\\nMoreover, the publicly available data sets do not realistically incorporate\\nsuch kinds and magnitudes of variability, which limits the progress of this\\ntechnology. This paper introduces DetReIDX, a large-scale aerial-ground person\\ndataset, that was explicitly designed as a stress test to ReID under real-world\\nconditions. DetReIDX is a multi-session set that includes over 13 million\\nbounding boxes from 509 identities, collected in seven university campuses from\\nthree continents, with drone altitudes between 5.8 and 120 meters. More\\nimportant, as a key novelty, DetReIDX subjects were recorded in (at least) two\\nsessions on different days, with changes in clothing, daylight and location,\\nmaking it suitable to actually evaluate long-term person ReID. Plus, data were\\nannotated from 16 soft biometric attributes and multitask labels for detection,\\ntracking, ReID, and action recognition. In order to provide empirical evidence\\nof DetReIDX usefulness, we considered the specific tasks of human detection and\\nReID, where SOTA methods catastrophically degrade performance (up to 80% in\\ndetection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs\\nconditions. The dataset, annotations, and official evaluation protocols are\\npublicly available at https://www.it.ubi.pt/DetReIDX/', 'upvotes': 1, 'discussionId': '6825d67cd5a62682c1a582d3'}, 'publishedAt': '2025-05-07T16:41:06.000Z', 'title': 'DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person\\n  Recognition', 'summary': 'Person reidentification (ReID) technology has been considered to perform\\nrelatively well under controlled, ground-level conditions, but it breaks down\\nwhen deployed in challenging real-world settings. Evidently, this is due to\\nextreme data variability factors such as resolution, viewpoint changes, scale\\nvariations, occlusions, and appearance shifts from clothing or session drifts.\\nMoreover, the publicly available data sets do not realistically incorporate\\nsuch kinds and magnitudes of variability, which limits the progress of this\\ntechnology. This paper introduces DetReIDX, a large-scale aerial-ground person\\ndataset, that was explicitly designed as a stress test to ReID under real-world\\nconditions. DetReIDX is a multi-session set that includes over 13 million\\nbounding boxes from 509 identities, collected in seven university campuses from\\nthree continents, with drone altitudes between 5.8 and 120 meters. More\\nimportant, as a key novelty, DetReIDX subjects were recorded in (at least) two\\nsessions on different days, with changes in clothing, daylight and location,\\nmaking it suitable to actually evaluate long-term person ReID. Plus, data were\\nannotated from 16 soft biometric attributes and multitask labels for detection,\\ntracking, ReID, and action recognition. In order to provide empirical evidence\\nof DetReIDX usefulness, we considered the specific tasks of human detection and\\nReID, where SOTA methods catastrophically degrade performance (up to 80% in\\ndetection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs\\nconditions. The dataset, annotations, and official evaluation protocols are\\npublicly available at https://www.it.ubi.pt/DetReIDX/', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04793.png', 'numComments': 1, 'submittedBy': {'_id': '61c334dc77d54877f7643294', 'avatarUrl': '/avatars/7c68821762bcfd844c8d780b9d1276f4.svg', 'fullname': 'Hambarde', 'name': 'kailassrt', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}"
]