[
    "{'paper': {'id': '2502.13923', 'authors': [{'_id': '67b6b0688b56622e70b9e83e', 'name': 'Shuai Bai', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e83f', 'name': 'Keqin Chen', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e840', 'name': 'Xuejing Liu', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e841', 'name': 'Jialin Wang', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e842', 'name': 'Wenbin Ge', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e843', 'name': 'Sibo Song', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e844', 'name': 'Kai Dang', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e845', 'name': 'Peng Wang', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e846', 'name': 'Shijie Wang', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e847', 'name': 'Jun Tang', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e848', 'name': 'Humen Zhong', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e849', 'name': 'Yuanzhi Zhu', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e84a', 'user': {'_id': '6417fa211f1f3b0fa811edc0', 'avatarUrl': '/avatars/fa9e1ef1472a736c2ceebe12b77d6c89.svg', 'isPro': False, 'fullname': 'Mingkun Yang', 'user': 'ayumiymk', 'type': 'user'}, 'name': 'Mingkun Yang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T09:35:44.878Z', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e84b', 'name': 'Zhaohai Li', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e84c', 'name': 'Jianqiang Wan', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e84d', 'name': 'Pengfei Wang', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e84e', 'name': 'Wei Ding', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e84f', 'user': {'_id': '63ee22e75f1300034ddaaf54', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1676550873969-noauth.jpeg', 'isPro': False, 'fullname': 'Zheren Fu', 'user': 'darkpromise', 'type': 'user'}, 'name': 'Zheren Fu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T10:49:47.484Z', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e850', 'name': 'Yiheng Xu', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e851', 'name': 'Jiabo Ye', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e852', 'name': 'Xi Zhang', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e853', 'name': 'Tianbao Xie', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e854', 'name': 'Zesen Cheng', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e855', 'name': 'Hang Zhang', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e856', 'name': 'Zhibo Yang', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e857', 'user': {'_id': '645b10e80c73ea27d13f7aca', 'avatarUrl': '/avatars/95e565306472a15067440b5b43e07a6f.svg', 'isPro': False, 'fullname': 'xuhaiyang', 'user': 'xhyandwyy', 'type': 'user'}, 'name': 'Haiyang Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T09:35:42.372Z', 'hidden': False}, {'_id': '67b6b0688b56622e70b9e858', 'name': 'Junyang Lin', 'hidden': False}], 'publishedAt': '2025-02-19T18:00:14.000Z', 'title': 'Qwen2.5-VL Technical Report', 'summary': 'We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language\\nseries, which demonstrates significant advancements in both foundational\\ncapabilities and innovative functionalities. Qwen2.5-VL achieves a major leap\\nforward in understanding and interacting with the world through enhanced visual\\nrecognition, precise object localization, robust document parsing, and\\nlong-video comprehension. A standout feature of Qwen2.5-VL is its ability to\\nlocalize objects using bounding boxes or points accurately. It provides robust\\nstructured data extraction from invoices, forms, and tables, as well as\\ndetailed analysis of charts, diagrams, and layouts. To handle complex inputs,\\nQwen2.5-VL introduces dynamic resolution processing and absolute time encoding,\\nenabling it to process images of varying sizes and videos of extended durations\\n(up to hours) with second-level event localization. This allows the model to\\nnatively perceive spatial scales and temporal dynamics without relying on\\ntraditional normalization techniques. By training a native dynamic-resolution\\nVision Transformer (ViT) from scratch and incorporating Window Attention, we\\nreduce computational overhead while maintaining native resolution. As a result,\\nQwen2.5-VL excels not only in static image and document understanding but also\\nas an interactive visual agent capable of reasoning, tool usage, and task\\nexecution in real-world scenarios such as operating computers and mobile\\ndevices. Qwen2.5-VL is available in three sizes, addressing diverse use cases\\nfrom edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model\\nmatches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly\\nexcelling in document and diagram understanding. Additionally, Qwen2.5-VL\\nmaintains robust linguistic performance, preserving the core language\\ncompetencies of the Qwen2.5 LLM.', 'upvotes': 70, 'discussionId': '67b6b0688b56622e70b9e875'}, 'publishedAt': '2025-02-19T23:35:06.194Z', 'title': 'Qwen2.5-VL Technical Report', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13923.png', 'numComments': 1, 'submittedBy': {'_id': '63451cf0a05b51f7ded25505', 'avatarUrl': '/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg', 'fullname': 'shuai bai', 'name': 'bluelike', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 11}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.13144', 'authors': [{'_id': '67b55c7fba22c1ddbb8d5746', 'user': {'_id': '6536187bd34e9f02b9df1c3b', 'avatarUrl': '/avatars/0b34d62868b93053b0a05062a018b5bd.svg', 'isPro': False, 'fullname': 'Hao Gao', 'user': 'Hao605', 'type': 'user'}, 'name': 'Hao Gao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-19T09:00:48.944Z', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d5747', 'name': 'Shaoyu Chen', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d5748', 'name': 'Bo Jiang', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d5749', 'name': 'Bencheng Liao', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d574a', 'name': 'Yiang Shi', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d574b', 'name': 'Xiaoyang Guo', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d574c', 'name': 'Yuechuan Pu', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d574d', 'name': 'Haoran Yin', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d574e', 'name': 'Xiangyu Li', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d574f', 'name': 'Xinbang Zhang', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d5750', 'name': 'Ying Zhang', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d5751', 'name': 'Wenyu Liu', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d5752', 'name': 'Qian Zhang', 'hidden': False}, {'_id': '67b55c7fba22c1ddbb8d5753', 'name': 'Xinggang Wang', 'hidden': False}], 'publishedAt': '2025-02-18T18:59:21.000Z', 'title': 'RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based\\n  Reinforcement Learning', 'summary': 'Existing end-to-end autonomous driving (AD) algorithms typically follow the\\nImitation Learning (IL) paradigm, which faces challenges such as causal\\nconfusion and the open-loop gap. In this work, we establish a 3DGS-based\\nclosed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS\\ntechniques, we construct a photorealistic digital replica of the real physical\\nworld, enabling the AD policy to extensively explore the state space and learn\\nto handle out-of-distribution scenarios through large-scale trial and error. To\\nenhance safety, we design specialized rewards that guide the policy to\\neffectively respond to safety-critical events and understand real-world causal\\nrelationships. For better alignment with human driving behavior, IL is\\nincorporated into RL training as a regularization term. We introduce a\\nclosed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS\\nenvironments. Compared to IL-based methods, RAD achieves stronger performance\\nin most closed-loop metrics, especially 3x lower collision rate. Abundant\\nclosed-loop results are presented at https://hgao-cv.github.io/RAD.', 'upvotes': 27, 'discussionId': '67b55c80ba22c1ddbb8d579c'}, 'publishedAt': '2025-02-19T22:13:49.764Z', 'title': 'RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13144.png', 'numComments': 1, 'submittedBy': {'_id': '6536187bd34e9f02b9df1c3b', 'avatarUrl': '/avatars/0b34d62868b93053b0a05062a018b5bd.svg', 'fullname': 'Hao Gao', 'name': 'Hao605', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.13128', 'authors': [{'_id': '67b6c696e9b901edeaf320d5', 'name': 'Zihan Liu', 'hidden': False}, {'_id': '67b6c696e9b901edeaf320d6', 'name': 'Shuangrui Ding', 'hidden': False}, {'_id': '67b6c696e9b901edeaf320d7', 'name': 'Zhixiong Zhang', 'hidden': False}, {'_id': '67b6c696e9b901edeaf320d8', 'name': 'Xiaoyi Dong', 'hidden': False}, {'_id': '67b6c696e9b901edeaf320d9', 'name': 'Pan Zhang', 'hidden': False}, {'_id': '67b6c696e9b901edeaf320da', 'name': 'Yuhang Zang', 'hidden': False}, {'_id': '67b6c696e9b901edeaf320db', 'name': 'Yuhang Cao', 'hidden': False}, {'_id': '67b6c696e9b901edeaf320dc', 'name': 'Dahua Lin', 'hidden': False}, {'_id': '67b6c696e9b901edeaf320dd', 'name': 'Jiaqi Wang', 'hidden': False}], 'publishedAt': '2025-02-18T18:52:21.000Z', 'title': 'SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song\\n  Generation', 'summary': 'Text-to-song generation, the task of creating vocals and accompaniment from\\ntextual inputs, poses significant challenges due to domain complexity and data\\nscarcity. Existing approaches often employ multi-stage generation procedures,\\nresulting in cumbersome training and inference pipelines. In this paper, we\\npropose SongGen, a fully open-source, single-stage auto-regressive transformer\\ndesigned for controllable song generation. The proposed model facilitates\\nfine-grained control over diverse musical attributes, including lyrics and\\ntextual descriptions of instrumentation, genre, mood, and timbre, while also\\noffering an optional three-second reference clip for voice cloning. Within a\\nunified auto-regressive framework, SongGen supports two output modes: mixed\\nmode, which generates a mixture of vocals and accompaniment directly, and\\ndual-track mode, which synthesizes them separately for greater flexibility in\\ndownstream applications. We explore diverse token pattern strategies for each\\nmode, leading to notable improvements and valuable insights. Furthermore, we\\ndesign an automated data preprocessing pipeline with effective quality control.\\nTo foster community engagement and future research, we will release our model\\nweights, training code, annotated data, and preprocessing pipeline. The\\ngenerated samples are showcased on our project page at\\nhttps://liuzh-19.github.io/SongGen/ , and the code will be available at\\nhttps://github.com/LiuZH-19/SongGen .', 'upvotes': 24, 'discussionId': '67b6c698e9b901edeaf321a7'}, 'publishedAt': '2025-02-20T01:07:44.785Z', 'title': 'SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13128.png', 'numComments': 1, 'submittedBy': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'fullname': 'Jiaqi Wang', 'name': 'myownskyW7', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 16}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.13685', 'authors': [{'_id': '67b6dc1ba7567156c6547880', 'name': 'Jusen Du', 'hidden': False}, {'_id': '67b6dc1ba7567156c6547881', 'user': {'_id': '6246bb33da617c00b48e4d92', 'avatarUrl': '/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg', 'isPro': False, 'fullname': 'Weigao Sun', 'user': 'weigao266', 'type': 'user'}, 'name': 'Weigao Sun', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-02-20T07:39:08.547Z', 'hidden': False}, {'_id': '67b6dc1ba7567156c6547882', 'name': 'Disen Lan', 'hidden': False}, {'_id': '67b6dc1ba7567156c6547883', 'name': 'Jiaxi Hu', 'hidden': False}, {'_id': '67b6dc1ba7567156c6547884', 'name': 'Yu Cheng', 'hidden': False}], 'publishedAt': '2025-02-19T12:53:55.000Z', 'title': 'MoM: Linear Sequence Modeling with Mixture-of-Memories', 'summary': 'Linear sequence modeling methods, such as linear attention, state space\\nmodeling, and linear RNNs, offer significant efficiency improvements by\\nreducing the complexity of training and inference. However, these methods\\ntypically compress the entire input sequence into a single fixed-size memory\\nstate, which leads to suboptimal performance on recall-intensive downstream\\ntasks. Drawing inspiration from neuroscience, particularly the brain\\'s ability\\nto maintain robust long-term memory while mitigating \"memory interference\", we\\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\\nmultiple independent memory states, with a router network directing input\\ntokens to specific memory states. This approach greatly enhances the overall\\nmemory capacity while minimizing memory interference. As a result, MoM performs\\nexceptionally well on recall-intensive tasks, surpassing existing linear\\nsequence modeling techniques. Despite incorporating multiple memory states, the\\ncomputation of each memory state remains linear in complexity, allowing MoM to\\nretain the linear-complexity advantage during training, while\\nconstant-complexity during inference. Our experimental results show that MoM\\nsignificantly outperforms current linear sequence models on downstream language\\ntasks, particularly recall-intensive tasks, and even achieves performance\\ncomparable to Transformer models. The code is released at\\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\\nhttps://github.com/OpenSparseLLMs/Linear-MoE.', 'upvotes': 19, 'discussionId': '67b6dc1ca7567156c65478b8'}, 'publishedAt': '2025-02-20T02:40:09.567Z', 'title': 'MoM: Linear Sequence Modeling with Mixture-of-Memories', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13685.png', 'numComments': 1, 'submittedBy': {'_id': '6246bb33da617c00b48e4d92', 'avatarUrl': '/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg', 'fullname': 'Weigao Sun', 'name': 'weigao266', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.13347', 'authors': [{'_id': '67b6a7e83ef3656c48f149b9', 'user': {'_id': '6135eeeb5bc6ecdf86b60f0d', 'avatarUrl': '/avatars/43cedcf20ab6b0801a662787400e1384.svg', 'isPro': False, 'fullname': 'Shi Yu', 'user': 'yushi', 'type': 'user'}, 'name': 'Shi Yu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T09:35:47.487Z', 'hidden': False}, {'_id': '67b6a7e83ef3656c48f149ba', 'name': 'Zhiyuan Liu', 'hidden': False}, {'_id': '67b6a7e83ef3656c48f149bb', 'name': 'Chenyan Xiong', 'hidden': False}], 'publishedAt': '2025-02-19T00:31:43.000Z', 'title': 'Craw4LLM: Efficient Web Crawling for LLM Pretraining', 'summary': \"Web crawl is a main source of large language models' (LLMs) pretraining data,\\nbut the majority of crawled web pages are discarded in pretraining due to low\\ndata quality. This paper presents Crawl4LLM, an efficient web crawling method\\nthat explores the web graph based on the preference of LLM pretraining.\\nSpecifically, it leverages the influence of a webpage in LLM pretraining as the\\npriority score of the web crawler's scheduler, replacing the standard graph\\nconnectivity based priority. Our experiments on a web graph containing 900\\nmillion webpages from a commercial search engine's index demonstrate the\\nefficiency of Crawl4LLM in obtaining high-quality pretraining data. With just\\n21% URLs crawled, LLMs pretrained on Crawl4LLM data reach the same downstream\\nperformances of previous crawls, significantly reducing the crawling waste and\\nalleviating the burdens on websites. Our code is publicly available at\\nhttps://github.com/cxcscmu/Crawl4LLM.\", 'upvotes': 18, 'discussionId': '67b6a7e93ef3656c48f149f1'}, 'publishedAt': '2025-02-19T22:57:23.298Z', 'title': 'Craw4LLM: Efficient Web Crawling for LLM Pretraining', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13347.png', 'numComments': 1, 'submittedBy': {'_id': '6135eeeb5bc6ecdf86b60f0d', 'avatarUrl': '/avatars/43cedcf20ab6b0801a662787400e1384.svg', 'fullname': 'Shi Yu', 'name': 'yushi', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.13922', 'authors': [{'_id': '67b6948dbef24bad725b5d4b', 'name': 'Guanzheng Chen', 'hidden': False}, {'_id': '67b6948dbef24bad725b5d4c', 'name': 'Xin Li', 'hidden': False}, {'_id': '67b6948dbef24bad725b5d4d', 'name': 'Michael Qizhe Shieh', 'hidden': False}, {'_id': '67b6948dbef24bad725b5d4e', 'name': 'Lidong Bing', 'hidden': False}], 'publishedAt': '2025-02-19T17:59:03.000Z', 'title': 'LongPO: Long Context Self-Evolution of Large Language Models through\\n  Short-to-Long Preference Optimization', 'summary': 'Large Language Models (LLMs) have demonstrated remarkable capabilities\\nthrough pretraining and alignment. However, superior short-context LLMs may\\nunderperform in long-context scenarios due to insufficient long-context\\nalignment. This alignment process remains challenging due to the impracticality\\nof human annotation for extended contexts and the difficulty in balancing\\nshort- and long-context performance. To address these challenges, we introduce\\nLongPO, that enables short-context LLMs to self-evolve to excel on long-context\\ntasks by internally transferring short-context capabilities. LongPO harnesses\\nLLMs to learn from self-generated short-to-long preference data, comprising\\npaired responses generated for identical instructions with long-context inputs\\nand their compressed short-context counterparts, respectively. This preference\\nreveals capabilities and potentials of LLMs cultivated during short-context\\nalignment that may be diminished in under-aligned long-context scenarios.\\nAdditionally, LongPO incorporates a short-to-long KL constraint to mitigate\\nshort-context performance decline during long-context alignment. When applied\\nto Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully\\nretains short-context performance and largely outperforms naive SFT and DPO in\\nboth long- and short-context tasks. Specifically, \\\\ourMethod-trained models can\\nachieve results on long-context benchmarks comparable to, or even surpassing,\\nthose of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context\\nannotation and larger parameter scales.', 'upvotes': 18, 'discussionId': '67b6948ebef24bad725b5d84'}, 'publishedAt': '2025-02-19T21:35:20.931Z', 'title': 'LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13922.png', 'numComments': 1, 'submittedBy': {'_id': '645475e2548f22be59847604', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645475e2548f22be59847604/EhSurrZ25u31qQ2TVXQXt.jpeg', 'fullname': 'Chen', 'name': 'Guanzheng', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.12143', 'authors': [{'_id': '67b4d05a9f8a8ab661450397', 'name': 'Yuetai Li', 'hidden': False}, {'_id': '67b4d05a9f8a8ab661450398', 'name': 'Xiang Yue', 'hidden': False}, {'_id': '67b4d05a9f8a8ab661450399', 'user': {'_id': '653df1323479e9ebbe3eb6cc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg', 'isPro': True, 'fullname': 'Zhangchen Xu', 'user': 'flydust', 'type': 'user'}, 'name': 'Zhangchen Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T09:37:32.715Z', 'hidden': False}, {'_id': '67b4d05a9f8a8ab66145039a', 'name': 'Fengqing Jiang', 'hidden': False}, {'_id': '67b4d05a9f8a8ab66145039b', 'name': 'Luyao Niu', 'hidden': False}, {'_id': '67b4d05a9f8a8ab66145039c', 'name': 'Bill Yuchen Lin', 'hidden': False}, {'_id': '67b4d05a9f8a8ab66145039d', 'name': 'Bhaskar Ramasubramanian', 'hidden': False}, {'_id': '67b4d05a9f8a8ab66145039e', 'name': 'Radha Poovendran', 'hidden': False}], 'publishedAt': '2025-02-17T18:56:15.000Z', 'title': 'Small Models Struggle to Learn from Strong Reasoners', 'summary': 'Large language models (LLMs) excel in complex reasoning tasks, and distilling\\ntheir reasoning capabilities into smaller models has shown promise. However, we\\nuncover an interesting phenomenon, which we term the Small Model Learnability\\nGap: small models (leq3B parameters) do not consistently benefit from long\\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\\nbetter align with their intrinsic learning capacity. To address this, we\\npropose Mix Distillation, a simple yet effective strategy that balances\\nreasoning complexity by combining long and short CoT examples or reasoning from\\nboth larger and smaller models. Our experiments demonstrate that Mix\\nDistillation significantly improves small model reasoning performance compared\\nto training on either data alone. These findings highlight the limitations of\\ndirect strong model distillation and underscore the importance of adapting\\nreasoning complexity for effective reasoning capability transfer.', 'upvotes': 15, 'discussionId': '67b4d05b9f8a8ab6614503cb'}, 'publishedAt': '2025-02-19T21:38:13.468Z', 'title': 'Small Models Struggle to Learn from Strong Reasoners', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12143.png', 'numComments': 1, 'submittedBy': {'_id': '653df1323479e9ebbe3eb6cc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg', 'fullname': 'Zhangchen Xu', 'name': 'flydust', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 8}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.13965', 'authors': [{'_id': '67b6a3fa09841367596a1db5', 'name': 'Michael Luo', 'hidden': False}, {'_id': '67b6a3fa09841367596a1db6', 'name': 'Xiaoxiang Shi', 'hidden': False}, {'_id': '67b6a3fa09841367596a1db7', 'name': 'Colin Cai', 'hidden': False}, {'_id': '67b6a3fa09841367596a1db8', 'name': 'Tianjun Zhang', 'hidden': False}, {'_id': '67b6a3fa09841367596a1db9', 'name': 'Justin Wong', 'hidden': False}, {'_id': '67b6a3fa09841367596a1dba', 'user': {'_id': '626e3449e7914f0d5ea78ad1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/626e3449e7914f0d5ea78ad1/pVzdmdPMpNcxuj94qiIvB.jpeg', 'isPro': False, 'fullname': 'Yichuan', 'user': 'Chrisyichuan', 'type': 'user'}, 'name': 'Yichuan Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T09:35:50.487Z', 'hidden': False}, {'_id': '67b6a3fa09841367596a1dbb', 'name': 'Chi Wang', 'hidden': False}, {'_id': '67b6a3fa09841367596a1dbc', 'name': 'Yanping Huang', 'hidden': False}, {'_id': '67b6a3fa09841367596a1dbd', 'name': 'Zhifeng Chen', 'hidden': False}, {'_id': '67b6a3fa09841367596a1dbe', 'name': 'Joseph E. Gonzalez', 'hidden': False}, {'_id': '67b6a3fa09841367596a1dbf', 'name': 'Ion Stoica', 'hidden': False}], 'publishedAt': '2025-02-19T18:59:30.000Z', 'title': 'Autellix: An Efficient Serving Engine for LLM Agents as General Programs', 'summary': \"Large language model (LLM) applications are evolving beyond simple chatbots\\ninto dynamic, general-purpose agentic programs, which scale LLM calls and\\noutput tokens to help AI agents reason, explore, and solve complex tasks.\\nHowever, existing LLM serving systems ignore dependencies between programs and\\ncalls, missing significant opportunities for optimization. Our analysis reveals\\nthat programs submitted to LLM serving engines experience long cumulative wait\\ntimes, primarily due to head-of-line blocking at both the individual LLM\\nrequest and the program. To address this, we introduce Autellix, an LLM serving\\nsystem that treats programs as first-class citizens to minimize their\\nend-to-end latencies. Autellix intercepts LLM calls submitted by programs,\\nenriching schedulers with program-level context. We propose two scheduling\\nalgorithms-for single-threaded and distributed programs-that preempt and\\nprioritize LLM calls based on their programs' previously completed calls. Our\\nevaluation demonstrates that across diverse LLMs and agentic workloads,\\nAutellix improves throughput of programs by 4-15x at the same latency compared\\nto state-of-the-art systems, such as vLLM.\", 'upvotes': 12, 'discussionId': '67b6a3fb09841367596a1e06'}, 'publishedAt': '2025-02-19T22:42:06.502Z', 'title': 'Autellix: An Efficient Serving Engine for LLM Agents as General Programs', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13965.png', 'numComments': 1, 'submittedBy': {'_id': '654037be97949fd2304aab7f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/654037be97949fd2304aab7f/2cSME81gcwYa2OTeVlq5Q.jpeg', 'fullname': 'Michael Luo', 'name': 'michaelzhiluo', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.11995', 'authors': [{'_id': '67b65bbe0d878eff1a6b111d', 'name': 'Siddhesh Pawar', 'hidden': False}, {'_id': '67b65bbe0d878eff1a6b111e', 'name': 'Arnav Arora', 'hidden': False}, {'_id': '67b65bbe0d878eff1a6b111f', 'name': 'Lucie-Aim√©e Kaffee', 'hidden': False}, {'_id': '67b65bbe0d878eff1a6b1120', 'user': {'_id': '608918b7df398c3b285ce960', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1621507769190-608918b7df398c3b285ce960.jpeg', 'isPro': False, 'fullname': 'Isabelle Augenstein', 'user': 'IAugenstein', 'type': 'user'}, 'name': 'Isabelle Augenstein', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T09:36:32.278Z', 'hidden': False}], 'publishedAt': '2025-02-17T16:35:15.000Z', 'title': 'Presumed Cultural Identity: How Names Shape LLM Responses', 'summary': 'Names are deeply tied to human identity. They can serve as markers of\\nindividuality, cultural heritage, and personal history. However, using names as\\na core indicator of identity can lead to over-simplification of complex\\nidentities. When interacting with LLMs, user names are an important point of\\ninformation for personalisation. Names can enter chatbot conversations through\\ndirect user input (requested by chatbots), as part of task contexts such as CV\\nreviews, or as built-in memory features that store user information for\\npersonalisation. We study biases associated with names by measuring cultural\\npresumptions in the responses generated by LLMs when presented with common\\nsuggestion-seeking queries, which might involve making assumptions about the\\nuser. Our analyses demonstrate strong assumptions about cultural identity\\nassociated with names present in LLM generations across multiple cultures. Our\\nwork has implications for designing more nuanced personalisation systems that\\navoid reinforcing stereotypes while maintaining meaningful customisation.', 'upvotes': 9, 'discussionId': '67b65bbf0d878eff1a6b1174'}, 'publishedAt': '2025-02-20T01:20:46.431Z', 'title': 'Presumed Cultural Identity: How Names Shape LLM Responses', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11995.png', 'numComments': 1, 'submittedBy': {'_id': '60c50f18754747f54fa37114', 'avatarUrl': '/avatars/648ae58b81806dbd93a68546666047e3.svg', 'fullname': 'Siddhesh', 'name': 'sidicity', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.13946', 'authors': [{'_id': '67b6b416b4ad845374143c31', 'name': 'Chak Tou Leong', 'hidden': False}, {'_id': '67b6b416b4ad845374143c32', 'name': 'Qingyu Yin', 'hidden': False}, {'_id': '67b6b416b4ad845374143c33', 'name': 'Jian Wang', 'hidden': False}, {'_id': '67b6b416b4ad845374143c34', 'name': 'Wenjie Li', 'hidden': False}], 'publishedAt': '2025-02-19T18:42:45.000Z', 'title': \"Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety\\n  Mechanisms Tend to Be Anchored in The Template Region\", 'summary': \"The safety alignment of large language models (LLMs) remains vulnerable, as\\ntheir initial behavior can be easily jailbroken by even relatively simple\\nattacks. Since infilling a fixed template between the input instruction and\\ninitial model output is a common practice for existing LLMs, we hypothesize\\nthat this template is a key factor behind their vulnerabilities: LLMs'\\nsafety-related decision-making overly relies on the aggregated information from\\nthe template region, which largely influences these models' safety behavior. We\\nrefer to this issue as template-anchored safety alignment. In this paper, we\\nconduct extensive experiments and verify that template-anchored safety\\nalignment is widespread across various aligned LLMs. Our mechanistic analyses\\ndemonstrate how it leads to models' susceptibility when encountering\\ninference-time jailbreak attacks. Furthermore, we show that detaching safety\\nmechanisms from the template region is promising in mitigating vulnerabilities\\nto jailbreak attacks. We encourage future research to develop more robust\\nsafety alignment techniques that reduce reliance on the template region.\", 'upvotes': 8, 'discussionId': '67b6b416b4ad845374143c5b'}, 'publishedAt': '2025-02-19T23:54:57.669Z', 'title': \"Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13946.png', 'numComments': 1, 'submittedBy': {'_id': '631326d6289cf15634c52369', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/631326d6289cf15634c52369/lmPWGHLsQ36H556cqcXjT.jpeg', 'fullname': 'Cooper Leong', 'name': 'cooperleong00', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.13233', 'authors': [{'_id': '67b689aeba514d2c2c969289', 'user': {'_id': '64beb6b6140491ca9f803ebf', 'avatarUrl': '/avatars/0daa2e813a13668b8b708cd8c12763d9.svg', 'isPro': False, 'fullname': 'Yucheng SHi', 'user': 'YuchengShi', 'type': 'user'}, 'name': 'Yucheng Shi', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T09:36:18.925Z', 'hidden': False}, {'_id': '67b689aeba514d2c2c96928a', 'name': 'Tianze Yang', 'hidden': False}, {'_id': '67b689aeba514d2c2c96928b', 'name': 'Canyu Chen', 'hidden': False}, {'_id': '67b689aeba514d2c2c96928c', 'name': 'Quanzheng Li', 'hidden': False}, {'_id': '67b689aeba514d2c2c96928d', 'name': 'Tianming Liu', 'hidden': False}, {'_id': '67b689aeba514d2c2c96928e', 'name': 'Xiang Li', 'hidden': False}, {'_id': '67b689aeba514d2c2c96928f', 'name': 'Ninghao Liu', 'hidden': False}], 'publishedAt': '2025-02-18T19:12:15.000Z', 'title': 'SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question\\n  Answering?', 'summary': \"Large Language Models (LLMs) have shown remarkable capabilities in general\\ndomains but often struggle with tasks requiring specialized knowledge.\\nConventional Retrieval-Augmented Generation (RAG) techniques typically retrieve\\nexternal information from static knowledge bases, which can be outdated or\\nincomplete, missing fine-grained clinical details essential for accurate\\nmedical question answering. In this work, we propose SearchRAG, a novel\\nframework that overcomes these limitations by leveraging real-time search\\nengines. Our method employs synthetic query generation to convert complex\\nmedical questions into search-engine-friendly queries and utilizes\\nuncertainty-based knowledge selection to filter and incorporate the most\\nrelevant and informative medical knowledge into the LLM's input. Experimental\\nresults demonstrate that our method significantly improves response accuracy in\\nmedical question answering tasks, particularly for complex questions requiring\\ndetailed and up-to-date knowledge.\", 'upvotes': 8, 'discussionId': '67b689aeba514d2c2c9692b9'}, 'publishedAt': '2025-02-19T22:27:22.403Z', 'title': 'SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13233.png', 'numComments': 1, 'submittedBy': {'_id': '64beb6b6140491ca9f803ebf', 'avatarUrl': '/avatars/0daa2e813a13668b8b708cd8c12763d9.svg', 'fullname': 'Yucheng SHi', 'name': 'YuchengShi', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.13173', 'authors': [{'_id': '67b6b014f7e569081326494f', 'name': 'Wang Yang', 'hidden': False}, {'_id': '67b6b014f7e5690813264950', 'name': 'Hongye Jin', 'hidden': False}, {'_id': '67b6b014f7e5690813264951', 'name': 'Jingfeng Yang', 'hidden': False}, {'_id': '67b6b014f7e5690813264952', 'name': 'Vipin Chaudhary', 'hidden': False}, {'_id': '67b6b014f7e5690813264953', 'name': 'Xiaotian Han', 'hidden': False}], 'publishedAt': '2025-02-17T19:56:21.000Z', 'title': 'Thinking Preference Optimization', 'summary': \"Supervised Fine-Tuning (SFT) has been a go-to and effective method for\\nenhancing long chain-of-thought (CoT) reasoning in relatively small LLMs by\\nfine-tuning them with long CoT responses from larger LLMs. To continually\\nimprove reasoning abilities, we can either collect new high-quality long CoT\\nreasoning SFT data or repeatedly train on existing SFT datasets. However,\\nacquiring new long CoT SFT data is costly and limited, while repeated training\\noften results in a performance plateau or decline. To further boost the\\nperformance with the SFT data, we propose Thinking Preference Optimization\\n(ThinkPO), a simple yet effective post-SFT method that enhances long CoT\\nreasoning without requiring new long CoT responses. Instead, ThinkPO utilizes\\nreadily available or easily obtainable short CoT reasoning responses as\\nrejected answers and long CoT responses as chosen answers for the same\\nquestion. It then applies direct preference optimization to encourage the model\\nto favor longer reasoning outputs. Experiments show that ThinkPO further\\nimproves the reasoning performance of SFT-ed models, e.g. it increases math\\nreasoning accuracy of SFT-ed models by 8.6% and output length by 25.9%.\\nNotably, ThinkPO is capable of continually boosting the performance of the\\npublicly distilled SFT model, e.g., increasing the official\\nDeepSeek-R1-Distill-Qwen-7B's performance on MATH500 from 87.4% to 91.2%.\", 'upvotes': 7, 'discussionId': '67b6b015f7e56908132649a0'}, 'publishedAt': '2025-02-19T23:31:36.410Z', 'title': 'Thinking Preference Optimization', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13173.png', 'numComments': 2, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 6150}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.13962', 'authors': [{'_id': '67b691751f861500916ecd5d', 'user': {'_id': '6372bc95c4267fd7cd77f4d0', 'avatarUrl': '/avatars/17a24af68f45487e601687d777b352b6.svg', 'isPro': False, 'fullname': 'William Jurayj', 'user': 'wjurayj', 'type': 'user'}, 'name': 'William Jurayj', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T09:36:09.674Z', 'hidden': False}, {'_id': '67b691751f861500916ecd5e', 'name': 'Jeffrey Cheng', 'hidden': False}, {'_id': '67b691751f861500916ecd5f', 'name': 'Benjamin Van Durme', 'hidden': False}], 'publishedAt': '2025-02-19T18:58:31.000Z', 'title': 'Is That Your Final Answer? Test-Time Scaling Improves Selective Question\\n  Answering', 'summary': 'Scaling the test-time compute of large language models has demonstrated\\nimpressive performance on reasoning benchmarks. However, existing evaluations\\nof test-time scaling make the strong assumption that a reasoning system should\\nalways give an answer to any question provided. This overlooks concerns about\\nwhether a model is confident in its answer, and whether it is appropriate to\\nalways provide a response. To address these concerns, we extract confidence\\nscores during reasoning for thresholding model responses. We find that\\nincreasing compute budget at inference time not only helps models answer more\\nquestions correctly, but also increases confidence in correct responses. We\\nthen extend the current paradigm of zero-risk responses during evaluation by\\nconsidering settings with non-zero levels of response risk, and suggest a\\nrecipe for reporting evaluations under these settings.', 'upvotes': 6, 'discussionId': '67b691761f861500916ecd8e'}, 'publishedAt': '2025-02-19T23:34:43.424Z', 'title': 'Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13962.png', 'numComments': 2, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 6150}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.13943', 'authors': [{'_id': '67b6a9a7c721bee91cac2888', 'name': 'Yuliang Liu', 'hidden': False}, {'_id': '67b6a9a7c721bee91cac2889', 'name': 'Junjie Lu', 'hidden': False}, {'_id': '67b6a9a7c721bee91cac288a', 'name': 'Zhaoling Chen', 'hidden': False}, {'_id': '67b6a9a7c721bee91cac288b', 'name': 'Chaofeng Qu', 'hidden': False}, {'_id': '67b6a9a7c721bee91cac288c', 'name': 'Jason Klein Liu', 'hidden': False}, {'_id': '67b6a9a7c721bee91cac288d', 'name': 'Chonghan Liu', 'hidden': False}, {'_id': '67b6a9a7c721bee91cac288e', 'name': 'Zefan Cai', 'hidden': False}, {'_id': '67b6a9a7c721bee91cac288f', 'name': 'Yunhui Xia', 'hidden': False}, {'_id': '67b6a9a7c721bee91cac2890', 'name': 'Li Zhao', 'hidden': False}, {'_id': '67b6a9a7c721bee91cac2891', 'name': 'Jiang Bian', 'hidden': False}, {'_id': '67b6a9a7c721bee91cac2892', 'name': 'Chuheng Zhang', 'hidden': False}, {'_id': '67b6a9a7c721bee91cac2893', 'name': 'Wei Shen', 'hidden': False}, {'_id': '67b6a9a7c721bee91cac2894', 'name': 'Zhouhan Lin', 'hidden': False}], 'publishedAt': '2025-02-19T18:35:55.000Z', 'title': 'AdaptiveStep: Automatically Dividing Reasoning Step through Model\\n  Confidence', 'summary': \"Current approaches for training Process Reward Models (PRMs) often involve\\nbreaking down responses into multiple reasoning steps using rule-based\\ntechniques, such as using predefined placeholder tokens or setting the\\nreasoning step's length into a fixed size. These approaches overlook the fact\\nthat specific words do not typically mark true decision points in a text. To\\naddress this, we propose AdaptiveStep, a method that divides reasoning steps\\nbased on the model's confidence in predicting the next word. This division\\nmethod provides more decision-making information at each step, enhancing\\ndownstream tasks, such as reward model learning. Moreover, our method does not\\nrequire manual annotation. We demonstrate its effectiveness through experiments\\nwith AdaptiveStep-trained PRMs in mathematical reasoning and code generation\\ntasks. Experimental results indicate that the outcome PRM achieves\\nstate-of-the-art Best-of-N performance, surpassing greedy search strategy with\\ntoken-level value-guided decoding, while also reducing construction costs by\\nover 30% compared to existing open-source PRMs. In addition, we provide a\\nthorough analysis and case study on the PRM's performance, transferability, and\\ngeneralization capabilities.\", 'upvotes': 5, 'discussionId': '67b6a9a8c721bee91cac28e7'}, 'publishedAt': '2025-02-19T23:07:01.367Z', 'title': 'AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13943.png', 'numComments': 1, 'submittedBy': {'_id': '6529f79e802e3d1a4f8ec662', 'avatarUrl': '/avatars/d05320c370a6497d8792ef5acb563dd5.svg', 'fullname': 'Yuliang Liu', 'name': 'yuliang03181', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.13622', 'authors': [{'_id': '67b69cf4573aa8417aec103c', 'user': {'_id': '6540fbf9cb7fffd683942b43', 'avatarUrl': '/avatars/d4a64fbde511d0949e1c339179586850.svg', 'isPro': False, 'fullname': 'DongGeon Lee', 'user': 'oneonlee', 'type': 'user'}, 'name': 'DongGeon Lee', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T09:35:55.480Z', 'hidden': False}, {'_id': '67b69cf4573aa8417aec103d', 'name': 'Hwanjo Yu', 'hidden': False}], 'publishedAt': '2025-02-19T10:59:05.000Z', 'title': 'REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large\\n  Language Models', 'summary': 'Hallucinations in large language model (LLM) outputs severely limit their\\nreliability in knowledge-intensive tasks such as question answering. To address\\nthis challenge, we introduce REFIND (Retrieval-augmented Factuality\\nhallucINation Detection), a novel framework that detects hallucinated spans\\nwithin LLM outputs by directly leveraging retrieved documents. As part of the\\nREFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that\\nquantifies the sensitivity of LLM outputs to retrieved evidence. This\\ninnovative approach enables REFIND to efficiently and accurately detect\\nhallucinations, setting it apart from existing methods. In the evaluation,\\nREFIND demonstrated robustness across nine languages, including low-resource\\nsettings, and significantly outperformed baseline models, achieving superior\\nIoU scores in identifying hallucinated spans. This work highlights the\\neffectiveness of quantifying context sensitivity for hallucination detection,\\nthereby paving the way for more reliable and trustworthy LLM applications\\nacross diverse languages.', 'upvotes': 3, 'discussionId': '67b69cf7573aa8417aec10bf'}, 'publishedAt': '2025-02-20T07:25:12.795Z', 'title': 'REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13622.png', 'numComments': 1, 'submittedBy': {'_id': '6540fbf9cb7fffd683942b43', 'avatarUrl': '/avatars/d4a64fbde511d0949e1c339179586850.svg', 'fullname': 'DongGeon Lee', 'name': 'oneonlee', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.12638', 'authors': [{'_id': '67b6acdb3a3df2f965e7af0b', 'name': 'Zhiyuan Liu', 'hidden': False}, {'_id': '67b6acdb3a3df2f965e7af0c', 'name': 'Yanchen Luo', 'hidden': False}, {'_id': '67b6acdb3a3df2f965e7af0d', 'name': 'Han Huang', 'hidden': False}, {'_id': '67b6acdb3a3df2f965e7af0e', 'name': 'Enzhi Zhang', 'hidden': False}, {'_id': '67b6acdb3a3df2f965e7af0f', 'name': 'Sihang Li', 'hidden': False}, {'_id': '67b6acdb3a3df2f965e7af10', 'name': 'Junfeng Fang', 'hidden': False}, {'_id': '67b6acdb3a3df2f965e7af11', 'name': 'Yaorui Shi', 'hidden': False}, {'_id': '67b6acdb3a3df2f965e7af12', 'user': {'_id': '65fca775fa59bdf4737b1a84', 'avatarUrl': '/avatars/a161b510bde8f57e7686cbb0b4aa6a52.svg', 'isPro': False, 'fullname': 'Xiang Wang', 'user': 'xiangwang1223', 'type': 'user'}, 'name': 'Xiang Wang', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-02-20T04:17:33.860Z', 'hidden': False}, {'_id': '67b6acdb3a3df2f965e7af13', 'name': 'Kenji Kawaguchi', 'hidden': False}, {'_id': '67b6acdb3a3df2f965e7af14', 'name': 'Tat-Seng Chua', 'hidden': False}], 'publishedAt': '2025-02-18T08:40:13.000Z', 'title': 'NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule\\n  Generation', 'summary': \"3D molecule generation is crucial for drug discovery and material design.\\nWhile prior efforts focus on 3D diffusion models for their benefits in modeling\\ncontinuous 3D conformers, they overlook the advantages of 1D SELFIES-based\\nLanguage Models (LMs), which can generate 100% valid molecules and leverage the\\nbillion-scale 1D molecule datasets. To combine these advantages for 3D molecule\\ngeneration, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D\\nLanguage Modeling for 3D Molecule Generation. NExT-Mol uses an extensively\\npretrained molecule LM for 1D molecule generation, and subsequently predicts\\nthe generated molecule's 3D conformers with a 3D diffusion model. We enhance\\nNExT-Mol's performance by scaling up the LM's model size, refining the\\ndiffusion neural architecture, and applying 1D to 3D transfer learning.\\nNotably, our 1D molecule LM significantly outperforms baselines in\\ndistributional similarity while ensuring validity, and our 3D diffusion model\\nachieves leading performances in conformer prediction. Given these improvements\\nin 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD\\nfor de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for\\nconditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are\\navailable at https://github.com/acharkq/NExT-Mol.\", 'upvotes': 3, 'discussionId': '67b6acdd3a3df2f965e7af85'}, 'publishedAt': '2025-02-19T23:18:32.647Z', 'title': 'NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12638.png', 'numComments': 1, 'submittedBy': {'_id': '6310a3cd531cc21f9e06de6a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg', 'fullname': 'Zhiyuan Liu', 'name': 'acharkq', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.13533', 'authors': [{'_id': '67b68f883cd5860d8597eace', 'user': {'_id': '63fcb42c987f631186e554f2', 'avatarUrl': '/avatars/5cf87e9fa21c088c0bd8577d651d01f6.svg', 'isPro': False, 'fullname': 'Jun Zhang', 'user': 'junzhang98', 'type': 'user'}, 'name': 'Jun Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T09:36:13.757Z', 'hidden': False}, {'_id': '67b68f883cd5860d8597eacf', 'name': 'Jue Wang', 'hidden': False}, {'_id': '67b68f883cd5860d8597ead0', 'name': 'Huan Li', 'hidden': False}, {'_id': '67b68f883cd5860d8597ead1', 'name': 'Lidan Shou', 'hidden': False}, {'_id': '67b68f883cd5860d8597ead2', 'name': 'Ke Chen', 'hidden': False}, {'_id': '67b68f883cd5860d8597ead3', 'name': 'Yang You', 'hidden': False}, {'_id': '67b68f883cd5860d8597ead4', 'name': 'Guiming Xie', 'hidden': False}, {'_id': '67b68f883cd5860d8597ead5', 'name': 'Xuejian Gong', 'hidden': False}, {'_id': '67b68f883cd5860d8597ead6', 'name': 'Kunlong Zhou', 'hidden': False}], 'publishedAt': '2025-02-19T08:39:15.000Z', 'title': 'Train Small, Infer Large: Memory-Efficient LoRA Training for Large\\n  Language Models', 'summary': 'Large Language Models (LLMs) have significantly advanced natural language\\nprocessing with exceptional task generalization capabilities. Low-Rank Adaption\\n(LoRA) offers a cost-effective fine-tuning solution, freezing the original\\nmodel parameters and training only lightweight, low-rank adapter matrices.\\nHowever, the memory footprint of LoRA is largely dominated by the original\\nmodel parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA\\ntraining scheme founded on the intuition that many neurons in\\nover-parameterized LLMs have low training utility but are essential for\\ninference. LoRAM presents a unique twist: it trains on a pruned (small) model\\nto obtain pruned low-rank matrices, which are then recovered and utilized with\\nthe original (large) model for inference. Additionally, minimal-cost continual\\npre-training, performed by the model publishers in advance, aligns the\\nknowledge discrepancy between pruned and original models. Our extensive\\nexperiments demonstrate the efficacy of LoRAM across various pruning strategies\\nand downstream tasks. For a model with 70 billion parameters, LoRAM enables\\ntraining on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA\\ntraining and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by\\nstructured pruning combined with 4-bit quantization, for LLaMA-3.1-70B\\n(LLaMA-2-70B), reduces the parameter storage cost that dominates the memory\\nusage in low-rank matrix training by 15.81times (16.95times), while\\nachieving dominant performance gains over both the original LLaMA-3.1-70B\\n(LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).', 'upvotes': 2, 'discussionId': '67b68f8b3cd5860d8597eb97'}, 'publishedAt': '2025-02-20T06:45:40.507Z', 'title': 'Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13533.png', 'numComments': 1, 'submittedBy': {'_id': '63fcb42c987f631186e554f2', 'avatarUrl': '/avatars/5cf87e9fa21c088c0bd8577d651d01f6.svg', 'fullname': 'Jun Zhang', 'name': 'junzhang98', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.13766', 'authors': [{'_id': '67b6faf5a96bf2b8ff8c235c', 'user': {'_id': '62dfd54798815401141c47fe', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62dfd54798815401141c47fe/ct2OA_K0Wwpshy8DCswxy.png', 'isPro': False, 'fullname': 'Flo Schneider', 'user': 'floschne', 'type': 'user'}, 'name': 'Florian Schneider', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T10:49:37.443Z', 'hidden': False}, {'_id': '67b6faf5a96bf2b8ff8c235d', 'name': 'Carolin Holtermann', 'hidden': False}, {'_id': '67b6faf5a96bf2b8ff8c235e', 'name': 'Chris Biemann', 'hidden': False}, {'_id': '67b6faf5a96bf2b8ff8c235f', 'name': 'Anne Lauscher', 'hidden': False}], 'publishedAt': '2025-02-19T14:27:40.000Z', 'title': 'GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge\\n  Benchmarking', 'summary': 'Large Vision-Language Models (LVLMs) have recently gained attention due to\\ntheir distinctive performance and broad applicability. While it has been\\npreviously shown that their efficacy in usage scenarios involving non-Western\\ncontexts falls short, existing studies are limited in scope, covering just a\\nnarrow range of cultures, focusing exclusively on a small number of cultural\\naspects, or evaluating a limited selection of models on a single task only.\\nTowards globally inclusive LVLM research, we introduce GIMMICK, an extensive\\nmultimodal benchmark designed to assess a broad spectrum of cultural knowledge\\nacross 144 countries representing six global macro-regions. GIMMICK comprises\\nsix tasks built upon three new datasets that span 728 unique cultural events or\\nfacets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary\\nand 26 open-weight models of all sizes. We systematically examine (1) regional\\ncultural biases, (2) the influence of model size, (3) input modalities, and (4)\\nexternal cues. Our analyses reveal strong biases toward Western cultures across\\nmodels and tasks and highlight strong correlations between model size and\\nperformance, as well as the effectiveness of multimodal input and external\\ngeographic cues. We further find that models have more knowledge of tangible\\nthan intangible aspects (e.g., food vs. rituals) and that they excel in\\nrecognizing broad cultural origins but struggle with a more nuanced\\nunderstanding.', 'upvotes': 2, 'discussionId': '67b6faf8a96bf2b8ff8c2422'}, 'publishedAt': '2025-02-20T05:19:11.890Z', 'title': 'GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge Benchmarking', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13766.png', 'numComments': 1, 'submittedBy': {'_id': '62dfd54798815401141c47fe', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62dfd54798815401141c47fe/ct2OA_K0Wwpshy8DCswxy.png', 'fullname': 'Flo Schneider', 'name': 'floschne', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.13581', 'authors': [{'_id': '67b6ee04412c9eccae5151f5', 'user': {'_id': '64a62c2f500beb50968e5c9c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wfL3ojJmXqyzGUmCblPf4.jpeg', 'isPro': False, 'fullname': 'Yupeng Hou', 'user': 'hyp1231', 'type': 'user'}, 'name': 'Yupeng Hou', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-20T09:35:14.498Z', 'hidden': False}, {'_id': '67b6ee04412c9eccae5151f6', 'name': 'Jianmo Ni', 'hidden': False}, {'_id': '67b6ee04412c9eccae5151f7', 'name': 'Zhankui He', 'hidden': False}, {'_id': '67b6ee04412c9eccae5151f8', 'name': 'Noveen Sachdeva', 'hidden': False}, {'_id': '67b6ee04412c9eccae5151f9', 'name': 'Wang-Cheng Kang', 'hidden': False}, {'_id': '67b6ee04412c9eccae5151fa', 'name': 'Ed H. Chi', 'hidden': False}, {'_id': '67b6ee04412c9eccae5151fb', 'name': 'Julian McAuley', 'hidden': False}, {'_id': '67b6ee04412c9eccae5151fc', 'name': 'Derek Zhiyuan Cheng', 'hidden': False}], 'publishedAt': '2025-02-19T09:45:29.000Z', 'title': 'ActionPiece: Contextually Tokenizing Action Sequences for Generative\\n  Recommendation', 'summary': 'Generative recommendation (GR) is an emerging paradigm where user actions are\\ntokenized into discrete token patterns and autoregressively generated as\\npredictions. However, existing GR models tokenize each action independently,\\nassigning the same fixed tokens to identical actions across all sequences\\nwithout considering contextual relationships. This lack of context-awareness\\ncan lead to suboptimal performance, as the same action may hold different\\nmeanings depending on its surrounding context. To address this issue, we\\npropose ActionPiece to explicitly incorporate context when tokenizing action\\nsequences. In ActionPiece, each action is represented as a set of item\\nfeatures, which serve as the initial tokens. Given the action sequence corpora,\\nwe construct the vocabulary by merging feature patterns as new tokens, based on\\ntheir co-occurrence frequency both within individual sets and across adjacent\\nsets. Considering the unordered nature of feature sets, we further introduce\\nset permutation regularization, which produces multiple segmentations of action\\nsequences with the same semantics. Experiments on public datasets demonstrate\\nthat ActionPiece consistently outperforms existing action tokenization methods,\\nimproving NDCG@10 by 6.00% to 12.82%.', 'upvotes': 2, 'discussionId': '67b6ee04412c9eccae515223'}, 'publishedAt': '2025-02-20T03:56:54.121Z', 'title': 'ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13581.png', 'numComments': 1, 'submittedBy': {'_id': '64a62c2f500beb50968e5c9c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wfL3ojJmXqyzGUmCblPf4.jpeg', 'fullname': 'Yupeng Hou', 'name': 'hyp1231', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.11573', 'authors': [{'_id': '67b6f629d9da6999328e38f5', 'name': 'Congkai Xie', 'hidden': False}, {'_id': '67b6f629d9da6999328e38f6', 'name': 'Shuo Cai', 'hidden': False}, {'_id': '67b6f629d9da6999328e38f7', 'name': 'Wenjun Wang', 'hidden': False}, {'_id': '67b6f629d9da6999328e38f8', 'name': 'Pengxiang Li', 'hidden': False}, {'_id': '67b6f629d9da6999328e38f9', 'name': 'Zhijie Sang', 'hidden': False}, {'_id': '67b6f629d9da6999328e38fa', 'name': 'Kejing Yang', 'hidden': False}, {'_id': '67b6f629d9da6999328e38fb', 'name': 'Yiming Zhang', 'hidden': False}, {'_id': '67b6f629d9da6999328e38fc', 'name': 'Zhen Li', 'hidden': False}, {'_id': '67b6f629d9da6999328e38fd', 'name': 'Guanghao Zhu', 'hidden': False}, {'_id': '67b6f629d9da6999328e38fe', 'name': 'Zeyu Liu', 'hidden': False}, {'_id': '67b6f629d9da6999328e38ff', 'name': 'Yang Yu', 'hidden': False}, {'_id': '67b6f629d9da6999328e3900', 'name': 'Yuhang Liu', 'hidden': False}, {'_id': '67b6f629d9da6999328e3901', 'name': 'Su Lu', 'hidden': False}, {'_id': '67b6f629d9da6999328e3902', 'name': 'Baoyi He', 'hidden': False}, {'_id': '67b6f629d9da6999328e3903', 'name': 'Qi Zhou', 'hidden': False}, {'_id': '67b6f629d9da6999328e3904', 'name': 'Xiaotian Han', 'hidden': False}, {'_id': '67b6f629d9da6999328e3905', 'name': 'Jianbo Yuan', 'hidden': False}, {'_id': '67b6f629d9da6999328e3906', 'name': 'Shengyu Zhang', 'hidden': False}, {'_id': '67b6f629d9da6999328e3907', 'name': 'Fei Wu', 'hidden': False}, {'_id': '67b6f629d9da6999328e3908', 'name': 'Hongxia Yang', 'hidden': False}], 'publishedAt': '2025-02-17T09:07:32.000Z', 'title': 'InfiR : Crafting Effective Small Language Models and Multimodal Small\\n  Language Models in Reasoning', 'summary': 'Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\\nhave made significant advancements in reasoning capabilities. However, they\\nstill face challenges such as high computational demands and privacy concerns.\\nThis paper focuses on developing efficient Small Language Models (SLMs) and\\nMultimodal Small Language Models (MSLMs) that retain competitive reasoning\\nabilities. We introduce a novel training pipeline that enhances reasoning\\ncapabilities and facilitates deployment on edge devices, achieving\\nstate-of-the-art performance while minimizing development costs. \\\\InfR~ aims to\\nadvance AI systems by improving reasoning, reducing adoption barriers, and\\naddressing privacy concerns through smaller model sizes. Resources are\\navailable at https://github. com/Reallm-Labs/InfiR.', 'upvotes': 1, 'discussionId': '67b6f62ad9da6999328e3955'}, 'publishedAt': '2025-02-20T04:32:22.011Z', 'title': 'InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11573.png', 'numComments': 1, 'submittedBy': {'_id': '618c1ad1c74578e0a4a4d074', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/618c1ad1c74578e0a4a4d074/8u_AkeHt4d6xtQ8hzaffU.jpeg', 'fullname': 'Drishti Sharma', 'name': 'DrishtiSharma', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 60}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.13573', 'authors': [{'_id': '67b70459ea22340afaaf416f', 'user': {'_id': '668bb3b14c25c09b01815a55', 'avatarUrl': '/avatars/5d46301dd5d7641e3da05b0ad560efee.svg', 'isPro': False, 'fullname': 'Yuan Yao', 'user': 'yyyaoyuan', 'type': 'user'}, 'name': 'Yuan Yao', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-02-20T10:30:51.477Z', 'hidden': False}, {'_id': '67b70459ea22340afaaf4170', 'name': 'Xiaopu Zhang', 'hidden': False}, {'_id': '67b70459ea22340afaaf4171', 'name': 'Yu Zhang', 'hidden': False}, {'_id': '67b70459ea22340afaaf4172', 'name': 'Jian Jin', 'hidden': False}, {'_id': '67b70459ea22340afaaf4173', 'name': 'Qiang Yang', 'hidden': False}], 'publishedAt': '2025-02-19T09:27:03.000Z', 'title': 'Noise May Contain Transferable Knowledge: Understanding Semi-supervised\\n  Heterogeneous Domain Adaptation from an Empirical Perspective', 'summary': 'Semi-supervised heterogeneous domain adaptation (SHDA) addresses learning\\nacross domains with distinct feature representations and distributions, where\\nsource samples are labeled while most target samples are unlabeled, with only a\\nsmall fraction labeled. Moreover, there is no one-to-one correspondence between\\nsource and target samples. Although various SHDA methods have been developed to\\ntackle this problem, the nature of the knowledge transferred across\\nheterogeneous domains remains unclear. This paper delves into this question\\nfrom an empirical perspective. We conduct extensive experiments on about 330\\nSHDA tasks, employing two supervised learning methods and seven representative\\nSHDA methods. Surprisingly, our observations indicate that both the category\\nand feature information of source samples do not significantly impact the\\nperformance of the target domain. Additionally, noise drawn from simple\\ndistributions, when used as source samples, may contain transferable knowledge.\\nBased on this insight, we perform a series of experiments to uncover the\\nunderlying principles of transferable knowledge in SHDA. Specifically, we\\ndesign a unified Knowledge Transfer Framework (KTF) for SHDA. Based on the KTF,\\nwe find that the transferable knowledge in SHDA primarily stems from the\\ntransferability and discriminability of the source domain. Consequently,\\nensuring those properties in source samples, regardless of their origin (e.g.,\\nimage, text, noise), can enhance the effectiveness of knowledge transfer in\\nSHDA tasks. The codes and datasets are available at\\nhttps://github.com/yyyaoyuan/SHDA.', 'upvotes': 0, 'discussionId': '67b7045bea22340afaaf41fd'}, 'publishedAt': '2025-02-20T05:38:39.430Z', 'title': 'Noise May Contain Transferable Knowledge: Understanding Semi-supervised Heterogeneous Domain Adaptation from an Empirical Perspective', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13573.png', 'numComments': 1, 'submittedBy': {'_id': '668bb3b14c25c09b01815a55', 'avatarUrl': '/avatars/5d46301dd5d7641e3da05b0ad560efee.svg', 'fullname': 'Yuan Yao', 'name': 'yyyaoyuan', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}"
]