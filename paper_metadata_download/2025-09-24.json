[
    "{'paper': {'id': '2509.18174', 'authors': [{'_id': '68d38bec0e215259d193b388', 'user': {'_id': '65276c7911a8a521c91bc10f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg', 'isPro': False, 'fullname': 'Khalil Hennara', 'user': 'Hennara', 'type': 'user'}, 'name': 'Khalil Hennara', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:52:41.543Z', 'hidden': False}, {'_id': '68d38bec0e215259d193b389', 'user': {'_id': '6496df4b3c64d75523a11973', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6496df4b3c64d75523a11973/I_Qn5-3Czngle-NsGmabO.jpeg', 'isPro': False, 'fullname': 'Muhammad Hreden', 'user': 'muhammad0-0hreden', 'type': 'user'}, 'name': 'Muhammad Hreden', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:52:34.689Z', 'hidden': False}, {'_id': '68d38bec0e215259d193b38a', 'user': {'_id': '63aa7667769a10efc404fbbc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63aa7667769a10efc404fbbc/tn8ZxUmTEMS0Gze7_F7JL.jpeg', 'isPro': False, 'fullname': 'Mohamed Motasim Hamed', 'user': 'Moatasem444', 'type': 'user'}, 'name': 'Mohamed Motasim Hamed', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:52:31.754Z', 'hidden': False}, {'_id': '68d38bec0e215259d193b38b', 'name': 'Ahmad Bastati', 'hidden': False}, {'_id': '68d38bec0e215259d193b38c', 'user': {'_id': '65704741e1cfce1764ce652e', 'avatarUrl': '/avatars/9189aaf417426af4ebe381ed364a6c0e.svg', 'isPro': False, 'fullname': 'Zeina Aldallal', 'user': 'ZeinaD', 'type': 'user'}, 'name': 'Zeina Aldallal', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:52:37.383Z', 'hidden': False}, {'_id': '68d38bec0e215259d193b38d', 'name': 'Sara Chrouf', 'hidden': False}, {'_id': '68d38bec0e215259d193b38e', 'name': 'Safwan AlModhayan', 'hidden': False}], 'publishedAt': '2025-09-17T15:07:29.000Z', 'submittedOnDailyAt': '2025-09-24T04:44:39.971Z', 'title': 'Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR', 'submittedOnDailyBy': {'_id': '65276c7911a8a521c91bc10f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg', 'isPro': False, 'fullname': 'Khalil Hennara', 'user': 'Hennara', 'type': 'user'}, 'summary': \"Arabic document OCR remains a challenging task due to the language's cursive\\nscript, diverse fonts, diacritics, and right-to-left orientation. While modern\\nMultimodal Large Language Models (MLLMs) have advanced document understanding\\nfor high-resource languages, their performance on Arabic remains limited. In\\nthis work, we introduce Baseer, a vision-language model fine- tuned\\nspecifically for Arabic document OCR. Leveraging a large-scale dataset\\ncombining synthetic and real-world documents, Baseer is trained using a\\ndecoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving\\ngeneral visual features. We also present Misraj-DocOCR, a high-quality,\\nexpert-verified benchmark designed for rigorous evaluation of Arabic OCR\\nsystems. Our experiments show that Baseer significantly outperforms existing\\nopen-source and commercial solutions, achieving a WER of 0.25 and establishing\\na new state-of-the-art in the domain of Arabic document OCR. Our results\\nhighlight the benefits of domain-specific adaptation of general-purpose MLLMs\\nand establish a strong baseline for high-accuracy OCR on morphologically rich\\nlanguages like Arabic.\", 'upvotes': 76, 'discussionId': '68d38bec0e215259d193b38f', 'ai_summary': 'Baseer, a vision-language model fine-tuned for Arabic document OCR, achieves state-of-the-art performance using a decoder-only strategy and a large-scale dataset, outperforming existing solutions with a WER of 0.25.', 'ai_keywords': ['Multimodal Large Language Models', 'vision-language model', 'decoder-only fine-tuning', 'Misraj-DocOCR', 'WER', 'Arabic document OCR']}, 'publishedAt': '2025-09-17T11:07:29.000Z', 'title': 'Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR', 'summary': \"Arabic document OCR remains a challenging task due to the language's cursive\\nscript, diverse fonts, diacritics, and right-to-left orientation. While modern\\nMultimodal Large Language Models (MLLMs) have advanced document understanding\\nfor high-resource languages, their performance on Arabic remains limited. In\\nthis work, we introduce Baseer, a vision-language model fine- tuned\\nspecifically for Arabic document OCR. Leveraging a large-scale dataset\\ncombining synthetic and real-world documents, Baseer is trained using a\\ndecoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving\\ngeneral visual features. We also present Misraj-DocOCR, a high-quality,\\nexpert-verified benchmark designed for rigorous evaluation of Arabic OCR\\nsystems. Our experiments show that Baseer significantly outperforms existing\\nopen-source and commercial solutions, achieving a WER of 0.25 and establishing\\na new state-of-the-art in the domain of Arabic document OCR. Our results\\nhighlight the benefits of domain-specific adaptation of general-purpose MLLMs\\nand establish a strong baseline for high-accuracy OCR on morphologically rich\\nlanguages like Arabic.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18174.png', 'numComments': 1, 'submittedBy': {'_id': '65276c7911a8a521c91bc10f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg', 'fullname': 'Khalil Hennara', 'name': 'Hennara', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 19}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.19249', 'authors': [{'_id': '68d352680e215259d193b1fa', 'user': {'_id': '66d45a8de5837f38ce3b73f7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66d45a8de5837f38ce3b73f7/3omslNRb8wV_c1xbrCmQC.jpeg', 'isPro': False, 'fullname': 'SihengLi', 'user': 'Siheng99', 'type': 'user'}, 'name': 'Siheng Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:53:23.476Z', 'hidden': False}, {'_id': '68d352680e215259d193b1fb', 'name': 'Kejiao Li', 'hidden': False}, {'_id': '68d352680e215259d193b1fc', 'user': {'_id': '66dd5a7468f47ec63e502794', 'avatarUrl': '/avatars/6497fd124e6009777167feef2558f058.svg', 'isPro': False, 'fullname': 'xu', 'user': 'xavier-z', 'type': 'user'}, 'name': 'Zenan Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:53:18.104Z', 'hidden': False}, {'_id': '68d352680e215259d193b1fd', 'name': 'Guanhua Huang', 'hidden': False}, {'_id': '68d352680e215259d193b1fe', 'name': 'Evander Yang', 'hidden': False}, {'_id': '68d352680e215259d193b1ff', 'name': 'Kun Li', 'hidden': False}, {'_id': '68d352680e215259d193b200', 'user': {'_id': '6445e7b1b272430bdbf64e80', 'avatarUrl': '/avatars/d3e59a3b488f8539966c944bb16f7b90.svg', 'isPro': False, 'fullname': 'Haoyuan WU', 'user': 'hywu', 'type': 'user'}, 'name': 'Haoyuan Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:53:15.202Z', 'hidden': False}, {'_id': '68d352680e215259d193b201', 'name': 'Jiajia Wu', 'hidden': False}, {'_id': '68d352680e215259d193b202', 'name': 'Zihao Zheng', 'hidden': False}, {'_id': '68d352680e215259d193b203', 'name': 'Chenchen Zhang', 'hidden': False}, {'_id': '68d352680e215259d193b204', 'name': 'Kun Shi', 'hidden': False}, {'_id': '68d352680e215259d193b205', 'name': 'Kyrierl Deng', 'hidden': False}, {'_id': '68d352680e215259d193b206', 'name': 'Qi Yi', 'hidden': False}, {'_id': '68d352680e215259d193b207', 'name': 'Ruibin Xiong', 'hidden': False}, {'_id': '68d352680e215259d193b208', 'name': 'Tingqiang Xu', 'hidden': False}, {'_id': '68d352680e215259d193b209', 'name': 'Yuhao Jiang', 'hidden': False}, {'_id': '68d352680e215259d193b20a', 'name': 'Jianfeng Yan', 'hidden': False}, {'_id': '68d352680e215259d193b20b', 'name': 'Yuyuan Zeng', 'hidden': False}, {'_id': '68d352680e215259d193b20c', 'name': 'Guanghui Xu', 'hidden': False}, {'_id': '68d352680e215259d193b20d', 'name': 'Jinbao Xue', 'hidden': False}, {'_id': '68d352680e215259d193b20e', 'name': 'Zhijiang Xu', 'hidden': False}, {'_id': '68d352680e215259d193b20f', 'name': 'Zheng Fang', 'hidden': False}, {'_id': '68d352680e215259d193b210', 'user': {'_id': '6525511b3f8d02205bf1e9ef', 'avatarUrl': '/avatars/7dfc3020337f3c403ecf133fa312b5d1.svg', 'isPro': False, 'fullname': 'Shuai Li', 'user': 'DiveBlue', 'type': 'user'}, 'name': 'Shuai Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:53:20.582Z', 'hidden': False}, {'_id': '68d352680e215259d193b211', 'name': 'Qibin Liu', 'hidden': False}, {'_id': '68d352680e215259d193b212', 'name': 'Xiaoxue Li', 'hidden': False}, {'_id': '68d352680e215259d193b213', 'name': 'Zhuoyu Li', 'hidden': False}, {'_id': '68d352680e215259d193b214', 'name': 'Yangyu Tao', 'hidden': False}, {'_id': '68d352680e215259d193b215', 'name': 'Fei Gao', 'hidden': False}, {'_id': '68d352680e215259d193b216', 'name': 'Cheng Jiang', 'hidden': False}, {'_id': '68d352680e215259d193b217', 'name': 'Bo Chao Wang', 'hidden': False}, {'_id': '68d352680e215259d193b218', 'name': 'Kai Liu', 'hidden': False}, {'_id': '68d352680e215259d193b219', 'name': 'Jianchen Zhu', 'hidden': False}, {'_id': '68d352680e215259d193b21a', 'name': 'Wai Lam', 'hidden': False}, {'_id': '68d352680e215259d193b21b', 'name': 'Wayyt Wang', 'hidden': False}, {'_id': '68d352680e215259d193b21c', 'name': 'Bo Zhou', 'hidden': False}, {'_id': '68d352680e215259d193b21d', 'name': 'Di Wang', 'hidden': False}], 'publishedAt': '2025-09-23T17:10:40.000Z', 'submittedOnDailyAt': '2025-09-24T00:37:49.095Z', 'title': 'Reinforcement Learning on Pre-Training Data', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'The growing disparity between the exponential scaling of computational\\nresources and the finite growth of high-quality text data now constrains\\nconventional scaling approaches for large language models (LLMs). To address\\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\\nto prior approaches that scale training primarily through supervised learning,\\nRLPT enables the policy to autonomously explore meaningful trajectories to\\nlearn from pre-training data and improve its capability through reinforcement\\nlearning (RL). While existing RL strategies such as reinforcement learning from\\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\\nrely on human annotation for reward construction, RLPT eliminates this\\ndependency by deriving reward signals directly from pre-training data.\\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\\npolicy for accurately predicting subsequent text segments conditioned on the\\npreceding context. This formulation allows RL to be scaled on pre-training\\ndata, encouraging the exploration of richer trajectories across broader\\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\\nexperiments on both general-domain and mathematical reasoning benchmarks across\\nmultiple models validate the effectiveness of RLPT. For example, when applied\\nto Qwen3-4B-Base, RLPT yields absolute improvements of 3.0, 5.1, 8.1,\\n6.0, 6.6, and 5.3 on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\\nAIME25, respectively. The results further demonstrate favorable scaling\\nbehavior, suggesting strong potential for continued gains with more compute. In\\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\\nof LLMs and enhancing RLVR performance.', 'upvotes': 39, 'discussionId': '68d352680e215259d193b21e', 'ai_summary': 'Reinforcement Learning on Pre-Training data (RLPT) optimizes large language models by autonomously exploring meaningful trajectories in pre-training data, improving generalizable reasoning skills without human annotation.', 'ai_keywords': ['Reinforcement Learning on Pre-Training data', 'RLPT', 'large language models', 'LLMs', 'reinforcement learning', 'RL', 'reinforcement learning from human feedback', 'RLHF', 'reinforcement learning with verifiable rewards', 'RLVR', 'next-segment reasoning objective', 'MMLU', 'MMLU-Pro', 'GPQA-Diamond', 'KOR-Bench', 'AIME24', 'AIME25']}, 'publishedAt': '2025-09-23T13:10:40.000Z', 'title': 'Reinforcement Learning on Pre-Training Data', 'summary': 'The growing disparity between the exponential scaling of computational\\nresources and the finite growth of high-quality text data now constrains\\nconventional scaling approaches for large language models (LLMs). To address\\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\\nto prior approaches that scale training primarily through supervised learning,\\nRLPT enables the policy to autonomously explore meaningful trajectories to\\nlearn from pre-training data and improve its capability through reinforcement\\nlearning (RL). While existing RL strategies such as reinforcement learning from\\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\\nrely on human annotation for reward construction, RLPT eliminates this\\ndependency by deriving reward signals directly from pre-training data.\\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\\npolicy for accurately predicting subsequent text segments conditioned on the\\npreceding context. This formulation allows RL to be scaled on pre-training\\ndata, encouraging the exploration of richer trajectories across broader\\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\\nexperiments on both general-domain and mathematical reasoning benchmarks across\\nmultiple models validate the effectiveness of RLPT. For example, when applied\\nto Qwen3-4B-Base, RLPT yields absolute improvements of 3.0, 5.1, 8.1,\\n6.0, 6.6, and 5.3 on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\\nAIME25, respectively. The results further demonstrate favorable scaling\\nbehavior, suggesting strong potential for continued gains with more compute. In\\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\\nof LLMs and enhancing RLVR performance.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19249.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 109}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.18644', 'authors': [{'_id': '68d387b50e215259d193b364', 'user': {'_id': '63217b064b3a874743797fa5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63217b064b3a874743797fa5/FBaU2EiZFO2R_6alDcox8.png', 'isPro': False, 'fullname': 'JT Zhao', 'user': 'JTZhaoSJTU', 'type': 'user'}, 'name': 'Juntu Zhao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:52:57.974Z', 'hidden': False}, {'_id': '68d387b50e215259d193b365', 'name': 'Wenbo Lu', 'hidden': False}, {'_id': '68d387b50e215259d193b366', 'name': 'Di Zhang', 'hidden': False}, {'_id': '68d387b50e215259d193b367', 'user': {'_id': '65d72c946a36b5b354f80cf8', 'avatarUrl': '/avatars/2cfc99ccd4f52f4a60c7fa40fe4313b7.svg', 'isPro': False, 'fullname': 'lyfeng', 'user': 'lyfeng001', 'type': 'user'}, 'name': 'Yufeng Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:52:54.957Z', 'hidden': False}, {'_id': '68d387b50e215259d193b368', 'name': 'Yushen Liang', 'hidden': False}, {'_id': '68d387b50e215259d193b369', 'name': 'Tianluo Zhang', 'hidden': False}, {'_id': '68d387b50e215259d193b36a', 'name': 'Yifeng Cao', 'hidden': False}, {'_id': '68d387b50e215259d193b36b', 'name': 'Junyuan Xie', 'hidden': False}, {'_id': '68d387b50e215259d193b36c', 'name': 'Yingdong Hu', 'hidden': False}, {'_id': '68d387b50e215259d193b36d', 'name': 'Shengjie Wang', 'hidden': False}, {'_id': '68d387b50e215259d193b36e', 'name': 'Junliang Guo', 'hidden': False}, {'_id': '68d387b50e215259d193b36f', 'user': {'_id': '61ad24836da53246bd6ac410', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61ad24836da53246bd6ac410/o-FL-C6B77iB94wyAtTuO.png', 'isPro': False, 'fullname': 'Dequan Wang', 'user': 'dqwang', 'type': 'user'}, 'name': 'Dequan Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:52:50.291Z', 'hidden': False}, {'_id': '68d387b50e215259d193b370', 'name': 'Yang Gao', 'hidden': False}], 'publishedAt': '2025-09-23T04:56:59.000Z', 'submittedOnDailyAt': '2025-09-24T04:41:47.105Z', 'title': 'Do You Need Proprioceptive States in Visuomotor Policies?', 'submittedOnDailyBy': {'_id': '66d01e4401f2a6b4cd93ad87', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png', 'isPro': False, 'fullname': 'Mohan Jiang', 'user': 'mhjiang0408', 'type': 'user'}, 'summary': 'Imitation-learning-based visuomotor policies have been widely used in robot\\nmanipulation, where both visual observations and proprioceptive states are\\ntypically adopted together for precise control. However, in this study, we find\\nthat this common practice makes the policy overly reliant on the proprioceptive\\nstate input, which causes overfitting to the training trajectories and results\\nin poor spatial generalization. On the contrary, we propose the State-free\\nPolicy, removing the proprioceptive state input and predicting actions only\\nconditioned on visual observations. The State-free Policy is built in the\\nrelative end-effector action space, and should ensure the full task-relevant\\nvisual observations, here provided by dual wide-angle wrist cameras. Empirical\\nresults demonstrate that the State-free policy achieves significantly stronger\\nspatial generalization than the state-based policy: in real-world tasks such as\\npick-and-place, challenging shirt-folding, and complex whole-body manipulation,\\nspanning multiple robot embodiments, the average success rate improves from 0\\\\%\\nto 85\\\\% in height generalization and from 6\\\\% to 64\\\\% in horizontal\\ngeneralization. Furthermore, they also show advantages in data efficiency and\\ncross-embodiment adaptation, enhancing their practicality for real-world\\ndeployment.', 'upvotes': 39, 'discussionId': '68d387b50e215259d193b371', 'projectPage': 'https://statefreepolicy.github.io', 'ai_summary': 'A state-free policy using only visual observations achieves better spatial generalization and data efficiency in robot manipulation tasks compared to state-based policies.', 'ai_keywords': ['imitation-learning-based visuomotor policies', 'proprioceptive state input', 'overfitting', 'spatial generalization', 'relative end-effector action space', 'dual wide-angle wrist cameras', 'pick-and-place', 'shirt-folding', 'whole-body manipulation', 'cross-embodiment adaptation']}, 'publishedAt': '2025-09-23T00:56:59.000Z', 'title': 'Do You Need Proprioceptive States in Visuomotor Policies?', 'summary': 'Imitation-learning-based visuomotor policies have been widely used in robot\\nmanipulation, where both visual observations and proprioceptive states are\\ntypically adopted together for precise control. However, in this study, we find\\nthat this common practice makes the policy overly reliant on the proprioceptive\\nstate input, which causes overfitting to the training trajectories and results\\nin poor spatial generalization. On the contrary, we propose the State-free\\nPolicy, removing the proprioceptive state input and predicting actions only\\nconditioned on visual observations. The State-free Policy is built in the\\nrelative end-effector action space, and should ensure the full task-relevant\\nvisual observations, here provided by dual wide-angle wrist cameras. Empirical\\nresults demonstrate that the State-free policy achieves significantly stronger\\nspatial generalization than the state-based policy: in real-world tasks such as\\npick-and-place, challenging shirt-folding, and complex whole-body manipulation,\\nspanning multiple robot embodiments, the average success rate improves from 0\\\\%\\nto 85\\\\% in height generalization and from 6\\\\% to 64\\\\% in horizontal\\ngeneralization. Furthermore, they also show advantages in data efficiency and\\ncross-embodiment adaptation, enhancing their practicality for real-world\\ndeployment.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18644.png', 'numComments': 1, 'submittedBy': {'_id': '66d01e4401f2a6b4cd93ad87', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png', 'fullname': 'Mohan Jiang', 'name': 'mhjiang0408', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.18154', 'authors': [{'_id': '68d351d70e215259d193b1d6', 'name': 'Tianyu Yu', 'hidden': False}, {'_id': '68d351d70e215259d193b1d7', 'name': 'Zefan Wang', 'hidden': False}, {'_id': '68d351d70e215259d193b1d8', 'user': {'_id': '6350fa8385bdb764f6a9aa82', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6350fa8385bdb764f6a9aa82/QBp92D7x9_XImYTigBHsu.jpeg', 'isPro': False, 'fullname': 'chongyi', 'user': 'yuzaa', 'type': 'user'}, 'name': 'Chongyi Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:53:26.793Z', 'hidden': False}, {'_id': '68d351d70e215259d193b1d9', 'name': 'Fuwei Huang', 'hidden': False}, {'_id': '68d351d70e215259d193b1da', 'name': 'Wenshuo Ma', 'hidden': False}, {'_id': '68d351d70e215259d193b1db', 'name': 'Zhihui He', 'hidden': False}, {'_id': '68d351d70e215259d193b1dc', 'name': 'Tianchi Cai', 'hidden': False}, {'_id': '68d351d70e215259d193b1dd', 'name': 'Weize Chen', 'hidden': False}, {'_id': '68d351d70e215259d193b1de', 'name': 'Yuxiang Huang', 'hidden': False}, {'_id': '68d351d70e215259d193b1df', 'name': 'Yuanqian Zhao', 'hidden': False}, {'_id': '68d351d70e215259d193b1e0', 'name': 'Bokai Xu', 'hidden': False}, {'_id': '68d351d70e215259d193b1e1', 'name': 'Junbo Cui', 'hidden': False}, {'_id': '68d351d70e215259d193b1e2', 'name': 'Yingjing Xu', 'hidden': False}, {'_id': '68d351d70e215259d193b1e3', 'name': 'Liqing Ruan', 'hidden': False}, {'_id': '68d351d70e215259d193b1e4', 'name': 'Luoyuan Zhang', 'hidden': False}, {'_id': '68d351d70e215259d193b1e5', 'name': 'Hanyu Liu', 'hidden': False}, {'_id': '68d351d70e215259d193b1e6', 'name': 'Jingkun Tang', 'hidden': False}, {'_id': '68d351d70e215259d193b1e7', 'name': 'Hongyuan Liu', 'hidden': False}, {'_id': '68d351d70e215259d193b1e8', 'name': 'Qining Guo', 'hidden': False}, {'_id': '68d351d70e215259d193b1e9', 'name': 'Wenhao Hu', 'hidden': False}, {'_id': '68d351d70e215259d193b1ea', 'name': 'Bingxiang He', 'hidden': False}, {'_id': '68d351d70e215259d193b1eb', 'name': 'Jie Zhou', 'hidden': False}, {'_id': '68d351d70e215259d193b1ec', 'name': 'Jie Cai', 'hidden': False}, {'_id': '68d351d70e215259d193b1ed', 'name': 'Ji Qi', 'hidden': False}, {'_id': '68d351d70e215259d193b1ee', 'name': 'Zonghao Guo', 'hidden': False}, {'_id': '68d351d70e215259d193b1ef', 'name': 'Chi Chen', 'hidden': False}, {'_id': '68d351d70e215259d193b1f0', 'name': 'Guoyang Zeng', 'hidden': False}, {'_id': '68d351d70e215259d193b1f1', 'name': 'Yuxuan Li', 'hidden': False}, {'_id': '68d351d70e215259d193b1f2', 'name': 'Ganqu Cui', 'hidden': False}, {'_id': '68d351d70e215259d193b1f3', 'name': 'Ning Ding', 'hidden': False}, {'_id': '68d351d70e215259d193b1f4', 'name': 'Xu Han', 'hidden': False}, {'_id': '68d351d70e215259d193b1f5', 'name': 'Yuan Yao', 'hidden': False}, {'_id': '68d351d70e215259d193b1f6', 'name': 'Zhiyuan Liu', 'hidden': False}, {'_id': '68d351d70e215259d193b1f7', 'name': 'Maosong Sun', 'hidden': False}], 'publishedAt': '2025-09-16T19:41:48.000Z', 'submittedOnDailyAt': '2025-09-24T00:35:18.789Z', 'title': 'MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\\n  Training Recipe', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\\nrepresent the frontier of AI development. However, their training and inference\\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\\nparameter model designed for high efficiency and strong performance. We\\nintroduce three core improvements in model architecture, data strategy and\\ntraining method: a unified 3D-Resampler model architecture for highly compact\\nencoding over images and videos, a unified learning paradigm for document\\nknowledge and text recognition without heavy data engineering, and a hybrid\\nreinforcement learning strategy for proficiency in both short and long\\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\\n72B. Notably, the strong performance is achieved with remarkable efficiency.\\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\\nstate-of-the-art performance among models under 30B size, using just 46.7\\\\% GPU\\nmemory cost and 8.7\\\\% inference time of Qwen2.5-VL 7B.', 'upvotes': 27, 'discussionId': '68d351d80e215259d193b1f8', 'githubRepo': 'https://github.com/OpenBMB/MiniCPM-V', 'ai_summary': 'MiniCPM-V 4.5, a 8B parameter multimodal large language model, achieves high performance and efficiency through a unified 3D-Resampler architecture, a unified learning paradigm, and a hybrid reinforcement learning strategy.', 'ai_keywords': ['3D-Resampler', 'unified learning paradigm', 'hybrid reinforcement learning strategy', 'multimodal large language models', 'OpenCompass evaluation', 'VideoMME benchmark'], 'githubStars': 21944}, 'publishedAt': '2025-09-16T15:41:48.000Z', 'title': 'MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\\n  Training Recipe', 'summary': 'Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\\nrepresent the frontier of AI development. However, their training and inference\\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\\nparameter model designed for high efficiency and strong performance. We\\nintroduce three core improvements in model architecture, data strategy and\\ntraining method: a unified 3D-Resampler model architecture for highly compact\\nencoding over images and videos, a unified learning paradigm for document\\nknowledge and text recognition without heavy data engineering, and a hybrid\\nreinforcement learning strategy for proficiency in both short and long\\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\\n72B. Notably, the strong performance is achieved with remarkable efficiency.\\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\\nstate-of-the-art performance among models under 30B size, using just 46.7\\\\% GPU\\nmemory cost and 8.7\\\\% inference time of Qwen2.5-VL 7B.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18154.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 109}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.18824', 'authors': [{'_id': '68d350cf0e215259d193b1c4', 'user': {'_id': '6614cbd40bbea65e71db4e1f', 'avatarUrl': '/avatars/ca8ff74887bbf8eb3f5b04ae9bb6d05b.svg', 'isPro': False, 'fullname': 'Yanzuo Lu', 'user': 'oliveryanzuolu', 'type': 'user'}, 'name': 'Yanzuo Lu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:53:29.233Z', 'hidden': False}, {'_id': '68d350cf0e215259d193b1c5', 'user': {'_id': '63089a78ff78e2aead8d10e7', 'avatarUrl': '/avatars/f326fd08abee7e31599a78923be30003.svg', 'isPro': False, 'fullname': 'XinXia', 'user': 'XiaXin-Aloys', 'type': 'user'}, 'name': 'Xin Xia', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:53:32.477Z', 'hidden': False}, {'_id': '68d350cf0e215259d193b1c6', 'name': 'Manlin Zhang', 'hidden': False}, {'_id': '68d350cf0e215259d193b1c7', 'name': 'Huafeng Kuang', 'hidden': False}, {'_id': '68d350cf0e215259d193b1c8', 'name': 'Jianbin Zheng', 'hidden': False}, {'_id': '68d350cf0e215259d193b1c9', 'name': 'Yuxi Ren', 'hidden': False}, {'_id': '68d350cf0e215259d193b1ca', 'name': 'Xuefeng Xiao', 'hidden': False}], 'publishedAt': '2025-09-23T09:12:46.000Z', 'submittedOnDailyAt': '2025-09-24T00:30:55.687Z', 'title': 'Hyper-Bagel: A Unified Acceleration Framework for Multimodal\\n  Understanding and Generation', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'Unified multimodal models have recently attracted considerable attention for\\ntheir remarkable abilities in jointly understanding and generating diverse\\ncontent. However, as contexts integrate increasingly numerous interleaved\\nmultimodal tokens, the iterative processes of diffusion denoising and\\nautoregressive decoding impose significant computational overhead. To address\\nthis, we propose Hyper-Bagel, a unified acceleration framework designed to\\nsimultaneously speed up both multimodal understanding and generation tasks. Our\\napproach uses a divide-and-conquer strategy, employing speculative decoding for\\nnext-token prediction and a multi-stage distillation process for diffusion\\ndenoising. The framework delivers substantial performance gains, achieving over\\na 2x speedup in multimodal understanding. For generative tasks, our resulting\\nlossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a\\n22x speedup in image editing, all while preserving the high-quality output of\\nthe original model. We further develop a highly efficient 1-NFE model that\\nenables near real-time interactive editing and generation. By combining\\nadvanced adversarial distillation with human feedback learning, this model\\nachieves ultimate cost-effectiveness and responsiveness, making complex\\nmultimodal interactions seamless and instantaneous.', 'upvotes': 16, 'discussionId': '68d350cf0e215259d193b1cb', 'projectPage': 'https://hyper-bagel.github.io/', 'ai_summary': 'Hyper-Bagel accelerates multimodal understanding and generation tasks using speculative decoding and multi-stage distillation, achieving significant speedups while maintaining high-quality outputs.', 'ai_keywords': ['diffusion denoising', 'autoregressive decoding', 'speculative decoding', 'multi-stage distillation', 'text-to-image generation', 'image editing', 'adversarial distillation', 'human feedback learning']}, 'publishedAt': '2025-09-23T05:12:46.000Z', 'title': 'Hyper-Bagel: A Unified Acceleration Framework for Multimodal\\n  Understanding and Generation', 'summary': 'Unified multimodal models have recently attracted considerable attention for\\ntheir remarkable abilities in jointly understanding and generating diverse\\ncontent. However, as contexts integrate increasingly numerous interleaved\\nmultimodal tokens, the iterative processes of diffusion denoising and\\nautoregressive decoding impose significant computational overhead. To address\\nthis, we propose Hyper-Bagel, a unified acceleration framework designed to\\nsimultaneously speed up both multimodal understanding and generation tasks. Our\\napproach uses a divide-and-conquer strategy, employing speculative decoding for\\nnext-token prediction and a multi-stage distillation process for diffusion\\ndenoising. The framework delivers substantial performance gains, achieving over\\na 2x speedup in multimodal understanding. For generative tasks, our resulting\\nlossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a\\n22x speedup in image editing, all while preserving the high-quality output of\\nthe original model. We further develop a highly efficient 1-NFE model that\\nenables near real-time interactive editing and generation. By combining\\nadvanced adversarial distillation with human feedback learning, this model\\nachieves ultimate cost-effectiveness and responsiveness, making complex\\nmultimodal interactions seamless and instantaneous.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18824.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 109}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.18849', 'authors': [{'_id': '68d363d20e215259d193b257', 'user': {'_id': '66b1dd6b93121096ffcfdab1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66b1dd6b93121096ffcfdab1/sOrbqdZhbp3ZP_cUGjhlN.jpeg', 'isPro': False, 'fullname': 'Wenke Huang', 'user': 'WilliamHuang91', 'type': 'user'}, 'name': 'Wenke Huang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:53:06.262Z', 'hidden': False}, {'_id': '68d363d20e215259d193b258', 'name': 'Quan Zhang', 'hidden': False}, {'_id': '68d363d20e215259d193b259', 'name': 'Yiyang Fang', 'hidden': False}, {'_id': '68d363d20e215259d193b25a', 'name': 'Jian Liang', 'hidden': False}, {'_id': '68d363d20e215259d193b25b', 'name': 'Xuankun Rong', 'hidden': False}, {'_id': '68d363d20e215259d193b25c', 'name': 'Huanjin Yao', 'hidden': False}, {'_id': '68d363d20e215259d193b25d', 'name': 'Guancheng Wan', 'hidden': False}, {'_id': '68d363d20e215259d193b25e', 'name': 'Ke Liang', 'hidden': False}, {'_id': '68d363d20e215259d193b25f', 'name': 'Wenwen He', 'hidden': False}, {'_id': '68d363d20e215259d193b260', 'name': 'Mingjun Li', 'hidden': False}, {'_id': '68d363d20e215259d193b261', 'name': 'Leszek Rutkowski', 'hidden': False}, {'_id': '68d363d20e215259d193b262', 'name': 'Mang Ye', 'hidden': False}, {'_id': '68d363d20e215259d193b263', 'name': 'Bo Du', 'hidden': False}, {'_id': '68d363d20e215259d193b264', 'name': 'Dacheng Tao', 'hidden': False}], 'publishedAt': '2025-09-23T09:37:16.000Z', 'submittedOnDailyAt': '2025-09-24T01:52:30.743Z', 'title': 'MAPO: Mixed Advantage Policy Optimization', 'submittedOnDailyBy': {'_id': '66b1dd6b93121096ffcfdab1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66b1dd6b93121096ffcfdab1/sOrbqdZhbp3ZP_cUGjhlN.jpeg', 'isPro': False, 'fullname': 'Wenke Huang', 'user': 'WilliamHuang91', 'type': 'user'}, 'summary': 'Recent advances in reinforcement learning for foundation models, such as\\nGroup Relative Policy Optimization (GRPO), have significantly improved the\\nperformance of foundation models on reasoning tasks. Notably, the advantage\\nfunction serves as a central mechanism in GRPO for ranking the trajectory\\nimportance. However, existing explorations encounter both advantage reversion\\nand advantage mirror problems, which hinder the reasonable advantage allocation\\nacross different query samples. In this work, we propose an easy but effective\\nGRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the\\ntrajectory appears with different certainty and propose the advantage percent\\ndeviation for samples with high-certainty trajectories. Furthermore, we\\ndynamically reweight the advantage function for samples with varying trajectory\\ncertainty, thereby adaptively configuring the advantage function to account for\\nsample-specific characteristics. Comparison with related state-of-the-art\\nmethods, along with ablation studies on different advantage variants, validates\\nthe effectiveness of our approach.', 'upvotes': 14, 'discussionId': '68d363d30e215259d193b265', 'githubRepo': 'https://github.com/WenkeHuang/MAPO', 'ai_summary': 'Mixed Advantage Policy Optimization (MAPO) dynamically reweights the advantage function to improve trajectory ranking in reinforcement learning for foundation models.', 'ai_keywords': ['Group Relative Policy Optimization (GRPO)', 'advantage function', 'trajectory importance', 'advantage reversion', 'advantage mirror problems', 'Mixed Advantage Policy Optimization (MAPO)', 'trajectory certainty', 'advantage percent deviation', 'sample-specific characteristics'], 'githubStars': 24}, 'publishedAt': '2025-09-23T05:37:16.000Z', 'title': 'MAPO: Mixed Advantage Policy Optimization', 'summary': 'Recent advances in reinforcement learning for foundation models, such as\\nGroup Relative Policy Optimization (GRPO), have significantly improved the\\nperformance of foundation models on reasoning tasks. Notably, the advantage\\nfunction serves as a central mechanism in GRPO for ranking the trajectory\\nimportance. However, existing explorations encounter both advantage reversion\\nand advantage mirror problems, which hinder the reasonable advantage allocation\\nacross different query samples. In this work, we propose an easy but effective\\nGRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the\\ntrajectory appears with different certainty and propose the advantage percent\\ndeviation for samples with high-certainty trajectories. Furthermore, we\\ndynamically reweight the advantage function for samples with varying trajectory\\ncertainty, thereby adaptively configuring the advantage function to account for\\nsample-specific characteristics. Comparison with related state-of-the-art\\nmethods, along with ablation studies on different advantage variants, validates\\nthe effectiveness of our approach.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18849.png', 'numComments': 1, 'submittedBy': {'_id': '66b1dd6b93121096ffcfdab1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66b1dd6b93121096ffcfdab1/sOrbqdZhbp3ZP_cUGjhlN.jpeg', 'fullname': 'Wenke Huang', 'name': 'WilliamHuang91', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.19297', 'authors': [{'_id': '68d360650e215259d193b245', 'user': {'_id': '66699aa8a33847217b5a49c7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png', 'isPro': False, 'fullname': 'Weijie Wang', 'user': 'lhmd', 'type': 'user'}, 'name': 'Weijie Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:53:11.784Z', 'hidden': False}, {'_id': '68d360650e215259d193b246', 'name': 'Yeqing Chen', 'hidden': False}, {'_id': '68d360650e215259d193b247', 'user': {'_id': '64ec877bb93654d4ca5c92e9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg', 'isPro': True, 'fullname': 'Zeyu Zhang', 'user': 'SteveZeyuZhang', 'type': 'user'}, 'name': 'Zeyu Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:53:09.030Z', 'hidden': False}, {'_id': '68d360650e215259d193b248', 'name': 'Hengyu Liu', 'hidden': False}, {'_id': '68d360650e215259d193b249', 'name': 'Haoxiao Wang', 'hidden': False}, {'_id': '68d360650e215259d193b24a', 'name': 'Zhiyuan Feng', 'hidden': False}, {'_id': '68d360650e215259d193b24b', 'name': 'Wenkang Qin', 'hidden': False}, {'_id': '68d360650e215259d193b24c', 'name': 'Zheng Zhu', 'hidden': False}, {'_id': '68d360650e215259d193b24d', 'name': 'Donny Y. Chen', 'hidden': False}, {'_id': '68d360650e215259d193b24e', 'name': 'Bohan Zhuang', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/66699aa8a33847217b5a49c7/6Nma90jNFIDRa6TqQnV9o.mp4'], 'publishedAt': '2025-09-23T17:59:02.000Z', 'submittedOnDailyAt': '2025-09-24T01:46:40.615Z', 'title': 'VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with\\n  Voxel-Aligned Prediction', 'submittedOnDailyBy': {'_id': '66699aa8a33847217b5a49c7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png', 'isPro': False, 'fullname': 'Weijie Wang', 'user': 'lhmd', 'type': 'user'}, 'summary': \"Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective\\nsolution for novel view synthesis. Existing methods predominantly rely on a\\npixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a\\n3D Gaussian. We rethink this widely adopted formulation and identify several\\ninherent limitations: it renders the reconstructed 3D models heavily dependent\\non the number of input views, leads to view-biased density distributions, and\\nintroduces alignment errors, particularly when source views contain occlusions\\nor low texture. To address these challenges, we introduce VolSplat, a new\\nmulti-view feed-forward paradigm that replaces pixel alignment with\\nvoxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D\\nvoxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature\\nmatching, ensuring robust multi-view consistency. Furthermore, it enables\\nadaptive control over Gaussian density based on 3D scene complexity, yielding\\nmore faithful Gaussian point clouds, improved geometric consistency, and\\nenhanced novel-view rendering quality. Experiments on widely used benchmarks\\nincluding RealEstate10K and ScanNet demonstrate that VolSplat achieves\\nstate-of-the-art performance while producing more plausible and view-consistent\\nGaussian reconstructions. In addition to superior results, our approach\\nestablishes a more scalable framework for feed-forward 3D reconstruction with\\ndenser and more robust representations, paving the way for further research in\\nwider communities. The video results, code and trained models are available on\\nour project page: https://lhmd.top/volsplat.\", 'upvotes': 13, 'discussionId': '68d360650e215259d193b24f', 'projectPage': 'https://lhmd.top/volsplat/', 'githubRepo': 'https://github.com/ziplab/VolSplat', 'ai_summary': 'VolSplat, a voxel-aligned Gaussian prediction method, improves novel view synthesis by overcoming pixel alignment limitations and enhancing 3D reconstruction quality.', 'ai_keywords': ['feed-forward 3D Gaussian Splatting', 'pixel-aligned Gaussian prediction', 'voxel-aligned Gaussians', '3D voxel grid', 'Gaussian point clouds', 'geometric consistency', 'novel-view rendering quality', 'RealEstate10K', 'ScanNet'], 'githubStars': 29}, 'publishedAt': '2025-09-23T13:59:02.000Z', 'title': 'VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with\\n  Voxel-Aligned Prediction', 'summary': \"Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective\\nsolution for novel view synthesis. Existing methods predominantly rely on a\\npixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a\\n3D Gaussian. We rethink this widely adopted formulation and identify several\\ninherent limitations: it renders the reconstructed 3D models heavily dependent\\non the number of input views, leads to view-biased density distributions, and\\nintroduces alignment errors, particularly when source views contain occlusions\\nor low texture. To address these challenges, we introduce VolSplat, a new\\nmulti-view feed-forward paradigm that replaces pixel alignment with\\nvoxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D\\nvoxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature\\nmatching, ensuring robust multi-view consistency. Furthermore, it enables\\nadaptive control over Gaussian density based on 3D scene complexity, yielding\\nmore faithful Gaussian point clouds, improved geometric consistency, and\\nenhanced novel-view rendering quality. Experiments on widely used benchmarks\\nincluding RealEstate10K and ScanNet demonstrate that VolSplat achieves\\nstate-of-the-art performance while producing more plausible and view-consistent\\nGaussian reconstructions. In addition to superior results, our approach\\nestablishes a more scalable framework for feed-forward 3D reconstruction with\\ndenser and more robust representations, paving the way for further research in\\nwider communities. The video results, code and trained models are available on\\nour project page: https://lhmd.top/volsplat.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/66699aa8a33847217b5a49c7/6Nma90jNFIDRa6TqQnV9o.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19297.png', 'numComments': 3, 'submittedBy': {'_id': '66699aa8a33847217b5a49c7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png', 'fullname': 'Weijie Wang', 'name': 'lhmd', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.19296', 'authors': [{'_id': '68d34f1a0e215259d193b19c', 'name': 'Sherwin Bahmani', 'hidden': False}, {'_id': '68d34f1a0e215259d193b19d', 'name': 'Tianchang Shen', 'hidden': False}, {'_id': '68d34f1a0e215259d193b19e', 'name': 'Jiawei Ren', 'hidden': False}, {'_id': '68d34f1a0e215259d193b19f', 'name': 'Jiahui Huang', 'hidden': False}, {'_id': '68d34f1a0e215259d193b1a0', 'name': 'Yifeng Jiang', 'hidden': False}, {'_id': '68d34f1a0e215259d193b1a1', 'name': 'Haithem Turki', 'hidden': False}, {'_id': '68d34f1a0e215259d193b1a2', 'name': 'Andrea Tagliasacchi', 'hidden': False}, {'_id': '68d34f1a0e215259d193b1a3', 'name': 'David B. Lindell', 'hidden': False}, {'_id': '68d34f1a0e215259d193b1a4', 'name': 'Zan Gojcic', 'hidden': False}, {'_id': '68d34f1a0e215259d193b1a5', 'name': 'Sanja Fidler', 'hidden': False}, {'_id': '68d34f1a0e215259d193b1a6', 'name': 'Huan Ling', 'hidden': False}, {'_id': '68d34f1a0e215259d193b1a7', 'name': 'Jun Gao', 'hidden': False}, {'_id': '68d34f1a0e215259d193b1a8', 'name': 'Xuanchi Ren', 'hidden': False}], 'publishedAt': '2025-09-23T17:58:01.000Z', 'submittedOnDailyAt': '2025-09-24T00:23:39.732Z', 'title': 'Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\\n  Self-Distillation', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'The ability to generate virtual environments is crucial for applications\\nranging from gaming to physical AI domains such as robotics, autonomous\\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\\nrely on the availability of captured real-world multi-view data, which is not\\nalways readily available. Recent advancements in video diffusion models have\\nshown remarkable imagination capabilities, yet their 2D nature limits the\\napplications to simulation where a robot needs to navigate and interact with\\nthe environment. In this paper, we propose a self-distillation framework that\\naims to distill the implicit 3D knowledge in the video diffusion models into an\\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\\nmulti-view training data. Specifically, we augment the typical RGB decoder with\\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\\napproach, the 3DGS decoder can be purely trained with synthetic data generated\\nby video diffusion models. At inference time, our model can synthesize 3D\\nscenes from either a text prompt or a single image for real-time rendering. Our\\nframework further extends to dynamic 3D scene generation from a monocular input\\nvideo. Experimental results show that our framework achieves state-of-the-art\\nperformance in static and dynamic 3D scene generation.', 'upvotes': 9, 'discussionId': '68d34f1b0e215259d193b1a9', 'projectPage': 'https://research.nvidia.com/labs/toronto-ai/lyra/', 'githubRepo': 'https://github.com/nv-tlabs/lyra', 'ai_summary': 'A self-distillation framework converts implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting representation, enabling 3D scene generation from text or images.', 'ai_keywords': ['video diffusion models', '3D Gaussian Splatting', '3DGS', 'RGB decoder', '3D scene generation', 'text prompt', 'monocular input video', 'dynamic 3D scene generation'], 'githubStars': 88}, 'publishedAt': '2025-09-23T13:58:01.000Z', 'title': 'Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\\n  Self-Distillation', 'summary': 'The ability to generate virtual environments is crucial for applications\\nranging from gaming to physical AI domains such as robotics, autonomous\\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\\nrely on the availability of captured real-world multi-view data, which is not\\nalways readily available. Recent advancements in video diffusion models have\\nshown remarkable imagination capabilities, yet their 2D nature limits the\\napplications to simulation where a robot needs to navigate and interact with\\nthe environment. In this paper, we propose a self-distillation framework that\\naims to distill the implicit 3D knowledge in the video diffusion models into an\\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\\nmulti-view training data. Specifically, we augment the typical RGB decoder with\\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\\napproach, the 3DGS decoder can be purely trained with synthetic data generated\\nby video diffusion models. At inference time, our model can synthesize 3D\\nscenes from either a text prompt or a single image for real-time rendering. Our\\nframework further extends to dynamic 3D scene generation from a monocular input\\nvideo. Experimental results show that our framework achieves state-of-the-art\\nperformance in static and dynamic 3D scene generation.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19296.png', 'numComments': 3, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 109}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.19284', 'authors': [{'_id': '68d3538e0e215259d193b220', 'name': 'Yunzhen Feng', 'hidden': False}, {'_id': '68d3538e0e215259d193b221', 'name': 'Julia Kempe', 'hidden': False}, {'_id': '68d3538e0e215259d193b222', 'name': 'Cheng Zhang', 'hidden': False}, {'_id': '68d3538e0e215259d193b223', 'name': 'Parag Jain', 'hidden': False}, {'_id': '68d3538e0e215259d193b224', 'name': 'Anthony Hartshorn', 'hidden': False}], 'publishedAt': '2025-09-23T17:50:54.000Z', 'submittedOnDailyAt': '2025-09-24T00:43:19.278Z', 'title': 'What Characterizes Effective Reasoning? Revisiting Length, Review, and\\n  Structure of CoT', 'submittedOnDailyBy': {'_id': '65cbfa6c968742be942e6cba', 'avatarUrl': '/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg', 'isPro': False, 'fullname': 'Feng', 'user': 'Yunzhen', 'type': 'user'}, 'summary': 'Large reasoning models (LRMs) spend substantial test-time compute on long\\nchain-of-thought (CoT) traces, but what *characterizes* an effective CoT\\nremains unclear. While prior work reports gains from lengthening CoTs and\\nincreasing review (revisiting earlier steps) via appended *wait* tokens, recent\\nstudies suggest that shorter thinking can outperform longer traces. We\\ntherefore conduct a systematic evaluation across ten LRMs on math and\\nscientific reasoning. Contrary to the \"longer-is-better\" narrative, we find\\nthat both naive CoT lengthening and increased review are associated with\\n*lower* accuracy.\\n  As CoT unfolds step by step, token-level metrics can conflate verbosity with\\nprocess quality. We introduce a graph view of CoT to extract structure and\\nidentify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of\\nsteps in abandoned branches-that consistently outpredicts length and review\\nratio for correctness across models. To probe causality, we design two\\ninterventions. First, we rank candidate CoTs by each metric at test time, where\\nFSF yields the largest pass@1 gains; second, we edit CoTs to remove failed\\nbranches, which significantly improves accuracy, indicating that failed\\nbranches bias subsequent reasoning. Taken together, these results characterize\\neffective CoTs as those that *fail less* and support *structure-aware*\\ntest-time scaling over indiscriminately generating long CoT.', 'upvotes': 8, 'discussionId': '68d3538e0e215259d193b225', 'ai_summary': 'Effective chain-of-thoughts in large reasoning models are characterized by fewer failed steps and better structural quality, not necessarily by length or review.', 'ai_keywords': ['chain-of-thought', 'CoT', 'large reasoning models', 'LRMs', 'wait tokens', 'Failed-Step Fraction', 'FSF', 'structure-aware', 'test-time scaling']}, 'publishedAt': '2025-09-23T13:50:54.000Z', 'title': 'What Characterizes Effective Reasoning? Revisiting Length, Review, and\\n  Structure of CoT', 'summary': 'Large reasoning models (LRMs) spend substantial test-time compute on long\\nchain-of-thought (CoT) traces, but what *characterizes* an effective CoT\\nremains unclear. While prior work reports gains from lengthening CoTs and\\nincreasing review (revisiting earlier steps) via appended *wait* tokens, recent\\nstudies suggest that shorter thinking can outperform longer traces. We\\ntherefore conduct a systematic evaluation across ten LRMs on math and\\nscientific reasoning. Contrary to the \"longer-is-better\" narrative, we find\\nthat both naive CoT lengthening and increased review are associated with\\n*lower* accuracy.\\n  As CoT unfolds step by step, token-level metrics can conflate verbosity with\\nprocess quality. We introduce a graph view of CoT to extract structure and\\nidentify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of\\nsteps in abandoned branches-that consistently outpredicts length and review\\nratio for correctness across models. To probe causality, we design two\\ninterventions. First, we rank candidate CoTs by each metric at test time, where\\nFSF yields the largest pass@1 gains; second, we edit CoTs to remove failed\\nbranches, which significantly improves accuracy, indicating that failed\\nbranches bias subsequent reasoning. Taken together, these results characterize\\neffective CoTs as those that *fail less* and support *structure-aware*\\ntest-time scaling over indiscriminately generating long CoT.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19284.png', 'numComments': 1, 'submittedBy': {'_id': '65cbfa6c968742be942e6cba', 'avatarUrl': '/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg', 'fullname': 'Feng', 'name': 'Yunzhen', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.13835', 'authors': [{'_id': '68d3913f0e215259d193b3c1', 'user': {'_id': '65e0e6fa4394fc3d1b59627a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65e0e6fa4394fc3d1b59627a/rlKw_UdH3MpmpgUcchMgB.jpeg', 'isPro': False, 'fullname': 'Minh Duc Bui', 'user': 'MinhDucBui', 'type': 'user'}, 'name': 'Minh Duc Bui', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:47:16.416Z', 'hidden': False}, {'_id': '68d3913f0e215259d193b3c2', 'name': 'Carolin Holtermann', 'hidden': False}, {'_id': '68d3913f0e215259d193b3c3', 'name': 'Valentin Hofmann', 'hidden': False}, {'_id': '68d3913f0e215259d193b3c4', 'name': 'Anne Lauscher', 'hidden': False}, {'_id': '68d3913f0e215259d193b3c5', 'name': 'Katharina von der Wense', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65e0e6fa4394fc3d1b59627a/28_BpeIrXFYdBsMeu6h1G.png'], 'publishedAt': '2025-09-17T09:05:37.000Z', 'submittedOnDailyAt': '2025-09-24T05:08:18.551Z', 'title': 'Large Language Models Discriminate Against Speakers of German Dialects', 'submittedOnDailyBy': {'_id': '65e0e6fa4394fc3d1b59627a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65e0e6fa4394fc3d1b59627a/rlKw_UdH3MpmpgUcchMgB.jpeg', 'isPro': False, 'fullname': 'Minh Duc Bui', 'user': 'MinhDucBui', 'type': 'user'}, 'summary': \"Dialects represent a significant component of human culture and are found\\nacross all regions of the world. In Germany, more than 40% of the population\\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\\nimportance, individuals speaking dialects often face negative societal\\nstereotypes. We examine whether such stereotypes are mirrored by large language\\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\\nto analyze traits commonly associated with dialect speakers. Based on these\\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\\nLLMs in two tasks: an association task and a decision task. To assess a model's\\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\\nstandard German counterparts. We find that: (1) in the association task, all\\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\\nagainst German dialect speakers, reflected in negative adjective associations;\\n(2) all models reproduce these dialect naming and dialect usage biases in their\\ndecision making; and (3) contrary to prior work showing minimal bias with\\nexplicit demographic mentions, we find that explicitly labeling linguistic\\ndemographics--German dialect speakers--amplifies bias more than implicit cues\\nlike dialect usage.\", 'upvotes': 4, 'discussionId': '68d3913f0e215259d193b3c6', 'ai_summary': 'Large language models exhibit significant dialect naming and usage bias against German dialect speakers, which is amplified when linguistic demographics are explicitly labeled.', 'ai_keywords': ['large language models', 'dialect naming bias', 'dialect usage bias', 'association task', 'decision task', 'regional German dialects', 'standard German', 'negative adjective associations']}, 'publishedAt': '2025-09-17T05:05:37.000Z', 'title': 'Large Language Models Discriminate Against Speakers of German Dialects', 'summary': \"Dialects represent a significant component of human culture and are found\\nacross all regions of the world. In Germany, more than 40% of the population\\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\\nimportance, individuals speaking dialects often face negative societal\\nstereotypes. We examine whether such stereotypes are mirrored by large language\\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\\nto analyze traits commonly associated with dialect speakers. Based on these\\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\\nLLMs in two tasks: an association task and a decision task. To assess a model's\\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\\nstandard German counterparts. We find that: (1) in the association task, all\\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\\nagainst German dialect speakers, reflected in negative adjective associations;\\n(2) all models reproduce these dialect naming and dialect usage biases in their\\ndecision making; and (3) contrary to prior work showing minimal bias with\\nexplicit demographic mentions, we find that explicitly labeling linguistic\\ndemographics--German dialect speakers--amplifies bias more than implicit cues\\nlike dialect usage.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65e0e6fa4394fc3d1b59627a/28_BpeIrXFYdBsMeu6h1G.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13835.png', 'numComments': 1, 'submittedBy': {'_id': '65e0e6fa4394fc3d1b59627a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65e0e6fa4394fc3d1b59627a/rlKw_UdH3MpmpgUcchMgB.jpeg', 'fullname': 'Minh Duc Bui', 'name': 'MinhDucBui', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.17321', 'authors': [{'_id': '68d2cb3b0e215259d193b0db', 'name': 'Pawe Budzianowski', 'hidden': False}, {'_id': '68d2cb3b0e215259d193b0dc', 'user': {'_id': '669fd3b34d95f13eb6e9dbd5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/669fd3b34d95f13eb6e9dbd5/01VPhZO7bqgvCCaFFZbP3.jpeg', 'isPro': True, 'fullname': 'Emilia Winios', 'user': 'emilia-wisnios', 'type': 'user'}, 'name': 'Emilia Winios', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:54:14.638Z', 'hidden': False}, {'_id': '68d2cb3b0e215259d193b0dd', 'name': 'Gracjan Gral', 'hidden': False}, {'_id': '68d2cb3b0e215259d193b0de', 'name': 'Igor Kulakov', 'hidden': False}, {'_id': '68d2cb3b0e215259d193b0df', 'name': 'Viktor Petrenko', 'hidden': False}, {'_id': '68d2cb3b0e215259d193b0e0', 'name': 'Krzysztof Walas', 'hidden': False}], 'publishedAt': '2025-09-22T02:52:55.000Z', 'submittedOnDailyAt': '2025-09-24T04:02:10.466Z', 'title': 'OpenGVL - Benchmarking Visual Temporal Progress for Data Curation', 'submittedOnDailyBy': {'_id': '669fd3b34d95f13eb6e9dbd5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/669fd3b34d95f13eb6e9dbd5/01VPhZO7bqgvCCaFFZbP3.jpeg', 'isPro': True, 'fullname': 'Emilia Winios', 'user': 'emilia-wisnios', 'type': 'user'}, 'summary': 'Data scarcity remains one of the most limiting factors in driving progress in\\nrobotics. However, the amount of available robotics data in the wild is growing\\nexponentially, creating new opportunities for large-scale data utilization.\\nReliable temporal task completion prediction could help automatically annotate\\nand curate this data at scale. The Generative Value Learning (GVL) approach was\\nrecently proposed, leveraging the knowledge embedded in vision-language models\\n(VLMs) to predict task progress from visual observations. Building upon GVL, we\\npropose OpenGVL, a comprehensive benchmark for estimating task progress across\\ndiverse challenging manipulation tasks involving both robotic and human\\nembodiments. We evaluate the capabilities of publicly available open-source\\nfoundation models, showing that open-source model families significantly\\nunderperform closed-source counterparts, achieving only approximately 70% of\\ntheir performance on temporal progress prediction tasks. Furthermore, we\\ndemonstrate how OpenGVL can serve as a practical tool for automated data\\ncuration and filtering, enabling efficient quality assessment of large-scale\\nrobotics datasets. We release the benchmark along with the complete codebase at\\ngithub.com/budzianowski/opengvl{OpenGVL}.', 'upvotes': 3, 'discussionId': '68d2cb3b0e215259d193b0e1', 'ai_summary': 'OpenGVL is a benchmark for task progress prediction in robotics using vision-language models, showing open-source models underperform compared to closed-source ones and enabling automated data curation.', 'ai_keywords': ['Generative Value Learning', 'GVL', 'vision-language models', 'VLMs', 'task progress prediction', 'manipulation tasks', 'open-source foundation models', 'closed-source counterparts', 'automated data curation', 'quality assessment', 'robotics datasets']}, 'publishedAt': '2025-09-21T22:52:55.000Z', 'title': 'OpenGVL - Benchmarking Visual Temporal Progress for Data Curation', 'summary': 'Data scarcity remains one of the most limiting factors in driving progress in\\nrobotics. However, the amount of available robotics data in the wild is growing\\nexponentially, creating new opportunities for large-scale data utilization.\\nReliable temporal task completion prediction could help automatically annotate\\nand curate this data at scale. The Generative Value Learning (GVL) approach was\\nrecently proposed, leveraging the knowledge embedded in vision-language models\\n(VLMs) to predict task progress from visual observations. Building upon GVL, we\\npropose OpenGVL, a comprehensive benchmark for estimating task progress across\\ndiverse challenging manipulation tasks involving both robotic and human\\nembodiments. We evaluate the capabilities of publicly available open-source\\nfoundation models, showing that open-source model families significantly\\nunderperform closed-source counterparts, achieving only approximately 70% of\\ntheir performance on temporal progress prediction tasks. Furthermore, we\\ndemonstrate how OpenGVL can serve as a practical tool for automated data\\ncuration and filtering, enabling efficient quality assessment of large-scale\\nrobotics datasets. We release the benchmark along with the complete codebase at\\ngithub.com/budzianowski/opengvl{OpenGVL}.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17321.png', 'numComments': 1, 'submittedBy': {'_id': '669fd3b34d95f13eb6e9dbd5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/669fd3b34d95f13eb6e9dbd5/01VPhZO7bqgvCCaFFZbP3.jpeg', 'fullname': 'Emilia Winios', 'name': 'emilia-wisnios', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.17083', 'authors': [{'_id': '68d211241ca7156988a8ed80', 'user': {'_id': '65d31df8e3667040af7bc945', 'avatarUrl': '/avatars/abe1137fccc45da0e1bd41ef0c172899.svg', 'isPro': False, 'fullname': 'Zipeng Wang', 'user': 'ZipW', 'type': 'user'}, 'name': 'Zipeng Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-23T10:05:49.538Z', 'hidden': False}, {'_id': '68d211241ca7156988a8ed81', 'user': {'_id': '66feab48651e00e22f33222e', 'avatarUrl': '/avatars/7344377e2c796c7ec85194bb2fc78521.svg', 'isPro': False, 'fullname': 'Dan Xu', 'user': 'danxuhk', 'type': 'user'}, 'name': 'Dan Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:55:37.112Z', 'hidden': False}], 'publishedAt': '2025-09-21T13:59:26.000Z', 'submittedOnDailyAt': '2025-09-24T01:06:44.788Z', 'title': 'HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel\\n  View Synthesis', 'submittedOnDailyBy': {'_id': '65d31df8e3667040af7bc945', 'avatarUrl': '/avatars/abe1137fccc45da0e1bd41ef0c172899.svg', 'isPro': False, 'fullname': 'Zipeng Wang', 'user': 'ZipW', 'type': 'user'}, 'summary': 'Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative\\nto NeRF-based approaches, enabling real-time, high-quality novel view synthesis\\nthrough explicit, optimizable 3D Gaussians. However, 3DGS suffers from\\nsignificant memory overhead due to its reliance on per-Gaussian parameters to\\nmodel view-dependent effects and anisotropic shapes. While recent works propose\\ncompressing 3DGS with neural fields, these methods struggle to capture\\nhigh-frequency spatial variations in Gaussian properties, leading to degraded\\nreconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a\\nnovel scene representation that combines the strengths of explicit Gaussians\\nand neural fields. HyRF decomposes the scene into (1) a compact set of explicit\\nGaussians storing only critical high-frequency parameters and (2) grid-based\\nneural fields that predict remaining properties. To enhance representational\\ncapacity, we introduce a decoupled neural field architecture, separately\\nmodeling geometry (scale, opacity, rotation) and view-dependent color.\\nAdditionally, we propose a hybrid rendering scheme that composites Gaussian\\nsplatting with a neural field-predicted background, addressing limitations in\\ndistant scene representation. Experiments demonstrate that HyRF achieves\\nstate-of-the-art rendering quality while reducing model size by over 20 times\\ncompared to 3DGS and maintaining real-time performance. Our project page is\\navailable at https://wzpscott.github.io/hyrf/.', 'upvotes': 3, 'discussionId': '68d211241ca7156988a8ed82', 'projectPage': 'https://wzpscott.github.io/hyrf/', 'githubRepo': 'https://github.com/wzpscott/hybrid-radiance-fields', 'ai_summary': 'Hybrid Radiance Fields combine explicit Gaussians and neural fields to achieve high-quality rendering with reduced memory usage and real-time performance.', 'ai_keywords': ['3D Gaussian Splatting', 'NeRF', 'explicit Gaussians', 'neural fields', 'high-frequency parameters', 'grid-based neural fields', 'decoupled neural field architecture', 'hybrid rendering scheme', 'Gaussian splatting', 'neural field-predicted background'], 'githubStars': 23}, 'publishedAt': '2025-09-21T09:59:26.000Z', 'title': 'HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel\\n  View Synthesis', 'summary': 'Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative\\nto NeRF-based approaches, enabling real-time, high-quality novel view synthesis\\nthrough explicit, optimizable 3D Gaussians. However, 3DGS suffers from\\nsignificant memory overhead due to its reliance on per-Gaussian parameters to\\nmodel view-dependent effects and anisotropic shapes. While recent works propose\\ncompressing 3DGS with neural fields, these methods struggle to capture\\nhigh-frequency spatial variations in Gaussian properties, leading to degraded\\nreconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a\\nnovel scene representation that combines the strengths of explicit Gaussians\\nand neural fields. HyRF decomposes the scene into (1) a compact set of explicit\\nGaussians storing only critical high-frequency parameters and (2) grid-based\\nneural fields that predict remaining properties. To enhance representational\\ncapacity, we introduce a decoupled neural field architecture, separately\\nmodeling geometry (scale, opacity, rotation) and view-dependent color.\\nAdditionally, we propose a hybrid rendering scheme that composites Gaussian\\nsplatting with a neural field-predicted background, addressing limitations in\\ndistant scene representation. Experiments demonstrate that HyRF achieves\\nstate-of-the-art rendering quality while reducing model size by over 20 times\\ncompared to 3DGS and maintaining real-time performance. Our project page is\\navailable at https://wzpscott.github.io/hyrf/.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17083.png', 'numComments': 1, 'submittedBy': {'_id': '65d31df8e3667040af7bc945', 'avatarUrl': '/avatars/abe1137fccc45da0e1bd41ef0c172899.svg', 'fullname': 'Zipeng Wang', 'name': 'ZipW', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.19300', 'authors': [{'_id': '68d34ffd0e215259d193b1ab', 'name': 'Chen Chen', 'hidden': False}, {'_id': '68d34ffd0e215259d193b1ac', 'name': 'Pengsheng Guo', 'hidden': False}, {'_id': '68d34ffd0e215259d193b1ad', 'name': 'Liangchen Song', 'hidden': False}, {'_id': '68d34ffd0e215259d193b1ae', 'name': 'Jiasen Lu', 'hidden': False}, {'_id': '68d34ffd0e215259d193b1af', 'name': 'Rui Qian', 'hidden': False}, {'_id': '68d34ffd0e215259d193b1b0', 'name': 'Xinze Wang', 'hidden': False}, {'_id': '68d34ffd0e215259d193b1b1', 'name': 'Tsu-Jui Fu', 'hidden': False}, {'_id': '68d34ffd0e215259d193b1b2', 'name': 'Wei Liu', 'hidden': False}, {'_id': '68d34ffd0e215259d193b1b3', 'name': 'Yinfei Yang', 'hidden': False}, {'_id': '68d34ffd0e215259d193b1b4', 'name': 'Alex Schwing', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65409e53d07194a81ecdd427/Esx1jPSPMVHM3uXjKVzR6.png'], 'publishedAt': '2025-09-23T17:59:31.000Z', 'submittedOnDailyAt': '2025-09-24T03:03:54.232Z', 'title': 'CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target\\n  for Better Flow Matching', 'submittedOnDailyBy': {'_id': '65409e53d07194a81ecdd427', 'avatarUrl': '/avatars/73a88ffe7f78f05b6fc8eb57e9f1e587.svg', 'isPro': False, 'fullname': 'Chen Chen', 'user': 'ultra7chen', 'type': 'user'}, 'summary': 'Conditional generative modeling aims to learn a conditional data distribution\\nfrom samples containing data-condition pairs. For this, diffusion and\\nflow-based methods have attained compelling results. These methods use a\\nlearned (flow) model to transport an initial standard Gaussian noise that\\nignores the condition to the conditional data distribution. The model is hence\\nrequired to learn both mass transport and conditional injection. To ease the\\ndemand on the model, we propose Condition-Aware Reparameterization for Flow\\nMatching (CAR-Flow) -- a lightweight, learned shift that conditions the source,\\nthe target, or both distributions. By relocating these distributions, CAR-Flow\\nshortens the probability path the model must learn, leading to faster training\\nin practice. On low-dimensional synthetic data, we visualize and quantify the\\neffects of CAR. On higher-dimensional natural image data (ImageNet-256),\\nequipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while\\nintroducing less than 0.6% additional parameters.', 'upvotes': 2, 'discussionId': '68d34ffd0e215259d193b1b5', 'ai_summary': 'Condition-Aware Reparameterization for Flow Matching (CAR-Flow) enhances conditional generative modeling by repositioning distributions, leading to faster training and improved performance on image data.', 'ai_keywords': ['conditional generative modeling', 'diffusion', 'flow-based methods', 'learned shift', 'source distribution', 'target distribution', 'probability path', 'SiT-XL/2', 'FID']}, 'publishedAt': '2025-09-23T13:59:31.000Z', 'title': 'CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target\\n  for Better Flow Matching', 'summary': 'Conditional generative modeling aims to learn a conditional data distribution\\nfrom samples containing data-condition pairs. For this, diffusion and\\nflow-based methods have attained compelling results. These methods use a\\nlearned (flow) model to transport an initial standard Gaussian noise that\\nignores the condition to the conditional data distribution. The model is hence\\nrequired to learn both mass transport and conditional injection. To ease the\\ndemand on the model, we propose Condition-Aware Reparameterization for Flow\\nMatching (CAR-Flow) -- a lightweight, learned shift that conditions the source,\\nthe target, or both distributions. By relocating these distributions, CAR-Flow\\nshortens the probability path the model must learn, leading to faster training\\nin practice. On low-dimensional synthetic data, we visualize and quantify the\\neffects of CAR. On higher-dimensional natural image data (ImageNet-256),\\nequipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while\\nintroducing less than 0.6% additional parameters.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65409e53d07194a81ecdd427/Esx1jPSPMVHM3uXjKVzR6.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19300.png', 'numComments': 1, 'submittedBy': {'_id': '65409e53d07194a81ecdd427', 'avatarUrl': '/avatars/73a88ffe7f78f05b6fc8eb57e9f1e587.svg', 'fullname': 'Chen Chen', 'name': 'ultra7chen', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.19087', 'authors': [{'_id': '68d350120e215259d193b1b7', 'name': 'Ganesh Mallya', 'hidden': False}, {'_id': '68d350120e215259d193b1b8', 'name': 'Yotam Gigi', 'hidden': False}, {'_id': '68d350120e215259d193b1b9', 'name': 'Dahun Kim', 'hidden': False}, {'_id': '68d350120e215259d193b1ba', 'name': 'Maxim Neumann', 'hidden': False}, {'_id': '68d350120e215259d193b1bb', 'name': 'Genady Beryozkin', 'hidden': False}, {'_id': '68d350120e215259d193b1bc', 'name': 'Tomer Shekel', 'hidden': False}, {'_id': '68d350120e215259d193b1bd', 'name': 'Anelia Angelova', 'hidden': False}], 'publishedAt': '2025-09-23T14:40:52.000Z', 'submittedOnDailyAt': '2025-09-24T00:27:49.250Z', 'title': 'Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal\\n  Gemini 2.5 Model for Remote Sensing Applications', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': \"Multi-spectral imagery plays a crucial role in diverse Remote Sensing\\napplications including land-use classification, environmental monitoring and\\nurban planning. These images are widely adopted because their additional\\nspectral bands correlate strongly with physical materials on the ground, such\\nas ice, water, and vegetation. This allows for more accurate identification,\\nand their public availability from missions, such as Sentinel-2 and Landsat,\\nonly adds to their value. Currently, the automatic analysis of such data is\\npredominantly managed through machine learning models specifically trained for\\nmulti-spectral input, which are costly to train and support. Furthermore,\\nalthough providing a lot of utility for Remote Sensing, such additional inputs\\ncannot be used with powerful generalist large multimodal models, which are\\ncapable of solving many visual problems, but are not able to understand\\nspecialized multi-spectral signals.\\n  To address this, we propose a training-free approach which introduces new\\nmulti-spectral data in a Zero-Shot-only mode, as inputs to generalist\\nmultimodal models, trained on RGB-only inputs. Our approach leverages the\\nmultimodal models' understanding of the visual space, and proposes to adapt to\\ninputs to that space, and to inject domain-specific information as instructions\\ninto the model. We exemplify this idea with the Gemini2.5 model and observe\\nstrong Zero-Shot performance gains of the approach on popular Remote Sensing\\nbenchmarks for land cover and land use classification and demonstrate the easy\\nadaptability of Gemini2.5 to new inputs. These results highlight the potential\\nfor geospatial professionals, working with non-standard specialized inputs, to\\neasily leverage powerful multimodal models, such as Gemini2.5, to accelerate\\ntheir work, benefiting from their rich reasoning and contextual capabilities,\\ngrounded in the specialized sensor data.\", 'upvotes': 1, 'discussionId': '68d350120e215259d193b1be', 'ai_summary': 'A training-free method enables generalist multimodal models to process multi-spectral imagery in a zero-shot manner, enhancing performance on remote sensing tasks.', 'ai_keywords': ['multi-spectral imagery', 'remote sensing', 'land-use classification', 'environmental monitoring', 'urban planning', 'machine learning models', 'multimodal models', 'zero-shot learning', 'Gemini2.5', 'land cover', 'land use classification']}, 'publishedAt': '2025-09-23T10:40:52.000Z', 'title': 'Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal\\n  Gemini 2.5 Model for Remote Sensing Applications', 'summary': \"Multi-spectral imagery plays a crucial role in diverse Remote Sensing\\napplications including land-use classification, environmental monitoring and\\nurban planning. These images are widely adopted because their additional\\nspectral bands correlate strongly with physical materials on the ground, such\\nas ice, water, and vegetation. This allows for more accurate identification,\\nand their public availability from missions, such as Sentinel-2 and Landsat,\\nonly adds to their value. Currently, the automatic analysis of such data is\\npredominantly managed through machine learning models specifically trained for\\nmulti-spectral input, which are costly to train and support. Furthermore,\\nalthough providing a lot of utility for Remote Sensing, such additional inputs\\ncannot be used with powerful generalist large multimodal models, which are\\ncapable of solving many visual problems, but are not able to understand\\nspecialized multi-spectral signals.\\n  To address this, we propose a training-free approach which introduces new\\nmulti-spectral data in a Zero-Shot-only mode, as inputs to generalist\\nmultimodal models, trained on RGB-only inputs. Our approach leverages the\\nmultimodal models' understanding of the visual space, and proposes to adapt to\\ninputs to that space, and to inject domain-specific information as instructions\\ninto the model. We exemplify this idea with the Gemini2.5 model and observe\\nstrong Zero-Shot performance gains of the approach on popular Remote Sensing\\nbenchmarks for land cover and land use classification and demonstrate the easy\\nadaptability of Gemini2.5 to new inputs. These results highlight the potential\\nfor geospatial professionals, working with non-standard specialized inputs, to\\neasily leverage powerful multimodal models, such as Gemini2.5, to accelerate\\ntheir work, benefiting from their rich reasoning and contextual capabilities,\\ngrounded in the specialized sensor data.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19087.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 109}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.19002', 'authors': [{'_id': '68d3903b0e215259d193b3ad', 'user': {'_id': '625cfd862f4600c018fcb8a3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1650261345491-noauth.jpeg', 'isPro': False, 'fullname': 'Hao Wang', 'user': 'conan1024hao', 'type': 'user'}, 'name': 'Hao Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:47:21.213Z', 'hidden': False}, {'_id': '68d3903b0e215259d193b3ae', 'name': 'Eiki Murata', 'hidden': False}, {'_id': '68d3903b0e215259d193b3af', 'name': 'Lingfang Zhang', 'hidden': False}, {'_id': '68d3903b0e215259d193b3b0', 'name': 'Ayako Sato', 'hidden': False}, {'_id': '68d3903b0e215259d193b3b1', 'name': 'So Fukuda', 'hidden': False}, {'_id': '68d3903b0e215259d193b3b2', 'name': 'Ziqi Yin', 'hidden': False}, {'_id': '68d3903b0e215259d193b3b3', 'name': 'Wentao Hu', 'hidden': False}, {'_id': '68d3903b0e215259d193b3b4', 'name': 'Keisuke Nakao', 'hidden': False}, {'_id': '68d3903b0e215259d193b3b5', 'name': 'Yusuke Nakamura', 'hidden': False}, {'_id': '68d3903b0e215259d193b3b6', 'name': 'Sebastian Zwirner', 'hidden': False}, {'_id': '68d3903b0e215259d193b3b7', 'name': 'Yi-Chia Chen', 'hidden': False}, {'_id': '68d3903b0e215259d193b3b8', 'name': 'Hiroyuki Otomo', 'hidden': False}, {'_id': '68d3903b0e215259d193b3b9', 'name': 'Hiroki Ouchi', 'hidden': False}, {'_id': '68d3903b0e215259d193b3ba', 'name': 'Daisuke Kawahara', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/625cfd862f4600c018fcb8a3/zkmvSAO48wifsIQajtJFL.png'], 'publishedAt': '2025-09-23T13:46:31.000Z', 'submittedOnDailyAt': '2025-09-24T05:02:27.764Z', 'title': 'VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via\\n  Travel Video Itinerary Reconstruction', 'submittedOnDailyBy': {'_id': '625cfd862f4600c018fcb8a3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1650261345491-noauth.jpeg', 'isPro': False, 'fullname': 'Hao Wang', 'user': 'conan1024hao', 'type': 'user'}, 'summary': \"Recent advances in multimodal large language models (MLLMs) have\\nsignificantly enhanced video understanding capabilities, opening new\\npossibilities for practical applications. Yet current video benchmarks focus\\nlargely on indoor scenes or short-range outdoor activities, leaving the\\nchallenges associated with long-distance travel largely unexplored. Mastering\\nextended geospatial-temporal trajectories is critical for next-generation\\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\\nconsisting of 200 travel videos that frames itinerary reconstruction as a\\nchallenging task designed to evaluate and push forward MLLMs'\\ngeospatial-temporal intelligence. Experimental results reveal that\\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\\nscores, underscoring the difficulty of handling videos that span extended\\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\\nwhich we develop a prototype travel-planning agent that leverages the insights\\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\\nverify that our evaluation protocol not only benchmarks models effectively but\\nalso translates into concrete performance gains in user-facing applications.\", 'upvotes': 1, 'discussionId': '68d3903c0e215259d193b3bb', 'githubRepo': 'https://github.com/nlp-waseda/VIR-Bench', 'ai_summary': \"VIR-Bench, a new benchmark for travel videos, evaluates and enhances MLLMs' geospatial-temporal intelligence, improving itinerary recommendations in real-world applications.\", 'ai_keywords': ['multimodal large language models', 'MLLMs', 'video understanding', 'video benchmarks', 'indoor scenes', 'outdoor activities', 'long-distance travel', 'geospatial-temporal trajectories', 'embodied-AI planning', 'navigation', 'itinerary reconstruction', 'geospatial-temporal intelligence', 'travel-planning agent', 'itinerary recommendations'], 'githubStars': 1}, 'publishedAt': '2025-09-23T09:46:31.000Z', 'title': 'VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via\\n  Travel Video Itinerary Reconstruction', 'summary': \"Recent advances in multimodal large language models (MLLMs) have\\nsignificantly enhanced video understanding capabilities, opening new\\npossibilities for practical applications. Yet current video benchmarks focus\\nlargely on indoor scenes or short-range outdoor activities, leaving the\\nchallenges associated with long-distance travel largely unexplored. Mastering\\nextended geospatial-temporal trajectories is critical for next-generation\\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\\nconsisting of 200 travel videos that frames itinerary reconstruction as a\\nchallenging task designed to evaluate and push forward MLLMs'\\ngeospatial-temporal intelligence. Experimental results reveal that\\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\\nscores, underscoring the difficulty of handling videos that span extended\\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\\nwhich we develop a prototype travel-planning agent that leverages the insights\\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\\nverify that our evaluation protocol not only benchmarks models effectively but\\nalso translates into concrete performance gains in user-facing applications.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/625cfd862f4600c018fcb8a3/zkmvSAO48wifsIQajtJFL.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19002.png', 'numComments': 1, 'submittedBy': {'_id': '625cfd862f4600c018fcb8a3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1650261345491-noauth.jpeg', 'fullname': 'Hao Wang', 'name': 'conan1024hao', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.18090', 'authors': [{'_id': '68d3e32c4402066be56e1a90', 'name': 'Jiahe Li', 'hidden': False}, {'_id': '68d3e32c4402066be56e1a91', 'name': 'Jiawei Zhang', 'hidden': False}, {'_id': '68d3e32c4402066be56e1a92', 'name': 'Youmin Zhang', 'hidden': False}, {'_id': '68d3e32c4402066be56e1a93', 'name': 'Xiao Bai', 'hidden': False}, {'_id': '68d3e32c4402066be56e1a94', 'name': 'Jin Zheng', 'hidden': False}, {'_id': '68d3e32c4402066be56e1a95', 'name': 'Xiaohan Yu', 'hidden': False}, {'_id': '68d3e32c4402066be56e1a96', 'name': 'Lin Gu', 'hidden': False}], 'publishedAt': '2025-09-22T17:58:48.000Z', 'submittedOnDailyAt': '2025-09-24T10:57:10.183Z', 'title': 'GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface\\n  Reconstruction', 'submittedOnDailyBy': {'_id': '66eec3889cbe309858abb830', 'avatarUrl': '/avatars/aeaee6ee34c867d246b1c9334048351d.svg', 'isPro': False, 'fullname': 'Jiahe Li', 'user': 'Fictionary', 'type': 'user'}, 'summary': 'Reconstructing accurate surfaces with radiance fields has achieved remarkable\\nprogress in recent years. However, prevailing approaches, primarily based on\\nGaussian Splatting, are increasingly constrained by representational\\nbottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based\\nframework that explores and extends the under-investigated potential of sparse\\nvoxels for achieving accurate, detailed, and complete surface reconstruction.\\nAs strengths, sparse voxels support preserving the coverage completeness and\\ngeometric clarity, while corresponding challenges also arise from absent scene\\nconstraints and locality in surface refinement. To ensure correct scene\\nconvergence, we first propose a Voxel-Uncertainty Depth Constraint that\\nmaximizes the effect of monocular depth cues while presenting a voxel-oriented\\nuncertainty to avoid quality degradation, enabling effective and robust scene\\nconstraints yet preserving highly accurate geometries. Subsequently, Sparse\\nVoxel Surface Regularization is designed to enhance geometric consistency for\\ntiny voxels and facilitate the voxel-based formation of sharp and accurate\\nsurfaces. Extensive experiments demonstrate our superior performance compared\\nto existing methods across diverse challenging scenarios, excelling in\\ngeometric accuracy, detail preservation, and reconstruction completeness while\\nmaintaining high efficiency. Code is available at\\nhttps://github.com/Fictionarry/GeoSVR.', 'upvotes': 1, 'discussionId': '68d3e32c4402066be56e1a97', 'projectPage': 'https://fictionarry.github.io/GeoSVR-project/', 'githubRepo': 'https://github.com/Fictionarry/GeoSVR', 'ai_summary': 'GeoSVR, a voxel-based framework, improves surface reconstruction accuracy and detail using sparse voxels with depth constraints and surface regularization.', 'ai_keywords': ['Gaussian Splatting', 'GeoSVR', 'voxel-based framework', 'sparse voxels', 'Voxel-Uncertainty Depth Constraint', 'Sparse Voxel Surface Regularization', 'geometric accuracy', 'detail preservation', 'reconstruction completeness'], 'githubStars': 24}, 'publishedAt': '2025-09-22T13:58:48.000Z', 'title': 'GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface\\n  Reconstruction', 'summary': 'Reconstructing accurate surfaces with radiance fields has achieved remarkable\\nprogress in recent years. However, prevailing approaches, primarily based on\\nGaussian Splatting, are increasingly constrained by representational\\nbottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based\\nframework that explores and extends the under-investigated potential of sparse\\nvoxels for achieving accurate, detailed, and complete surface reconstruction.\\nAs strengths, sparse voxels support preserving the coverage completeness and\\ngeometric clarity, while corresponding challenges also arise from absent scene\\nconstraints and locality in surface refinement. To ensure correct scene\\nconvergence, we first propose a Voxel-Uncertainty Depth Constraint that\\nmaximizes the effect of monocular depth cues while presenting a voxel-oriented\\nuncertainty to avoid quality degradation, enabling effective and robust scene\\nconstraints yet preserving highly accurate geometries. Subsequently, Sparse\\nVoxel Surface Regularization is designed to enhance geometric consistency for\\ntiny voxels and facilitate the voxel-based formation of sharp and accurate\\nsurfaces. Extensive experiments demonstrate our superior performance compared\\nto existing methods across diverse challenging scenarios, excelling in\\ngeometric accuracy, detail preservation, and reconstruction completeness while\\nmaintaining high efficiency. Code is available at\\nhttps://github.com/Fictionarry/GeoSVR.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18090.png', 'numComments': 1, 'submittedBy': {'_id': '66eec3889cbe309858abb830', 'avatarUrl': '/avatars/aeaee6ee34c867d246b1c9334048351d.svg', 'fullname': 'Jiahe Li', 'name': 'Fictionary', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.17349', 'authors': [{'_id': '68d2871f0e215259d193b041', 'name': 'Peter Polk', 'hidden': False}, {'_id': '68d2871f0e215259d193b042', 'user': {'_id': '66309b3833ccd9e68c5d5171', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg', 'isPro': False, 'fullname': 'Sara Papi', 'user': 'spapi', 'type': 'user'}, 'name': 'Sara Papi', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-24T13:54:25.376Z', 'hidden': False}, {'_id': '68d2871f0e215259d193b043', 'name': 'Luisa Bentivogli', 'hidden': False}, {'_id': '68d2871f0e215259d193b044', 'name': 'Ondej Bojar', 'hidden': False}], 'publishedAt': '2025-09-22T04:21:19.000Z', 'submittedOnDailyAt': '2025-09-24T11:54:02.143Z', 'title': 'Better Late Than Never: Evaluation of Latency Metrics for Simultaneous\\n  Speech-to-Text Translation', 'submittedOnDailyBy': {'_id': '66309b3833ccd9e68c5d5171', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg', 'isPro': False, 'fullname': 'Sara Papi', 'user': 'spapi', 'type': 'user'}, 'summary': 'Simultaneous speech-to-text translation (SimulST) systems have to balance\\ntranslation quality with latency--the delay between speech input and the\\ntranslated output. While quality evaluation is well established, accurate\\nlatency measurement remains a challenge. Existing metrics often produce\\ninconsistent or misleading results, especially in the widely used short-form\\nsetting, where speech is artificially presegmented. In this paper, we present\\nthe first comprehensive analysis of SimulST latency metrics across language\\npairs, systems, and both short- and long-form regimes. We uncover a structural\\nbias in current metrics related to segmentation that undermines fair and\\nmeaningful comparisons. To address this, we introduce YAAL (Yet Another Average\\nLagging), a refined latency metric that delivers more accurate evaluations in\\nthe short-form regime. We extend YAAL to LongYAAL for unsegmented audio and\\npropose SoftSegmenter, a novel resegmentation tool based on word-level\\nalignment. Our experiments show that YAAL and LongYAAL outperform popular\\nlatency metrics, while SoftSegmenter enhances alignment quality in long-form\\nevaluation, together enabling more reliable assessments of SimulST systems.', 'upvotes': 1, 'discussionId': '68d2871f0e215259d193b045', 'ai_summary': 'The paper analyzes SimulST latency metrics, identifies segmentation bias, and introduces YAAL and LongYAAL for more accurate latency evaluation, along with SoftSegmenter for improved alignment quality.', 'ai_keywords': ['SimulST', 'latency metrics', 'short-form', 'long-form', 'segmentation bias', 'YAAL', 'LongYAAL', 'SoftSegmenter', 'word-level alignment']}, 'publishedAt': '2025-09-22T00:21:19.000Z', 'title': 'Better Late Than Never: Evaluation of Latency Metrics for Simultaneous\\n  Speech-to-Text Translation', 'summary': 'Simultaneous speech-to-text translation (SimulST) systems have to balance\\ntranslation quality with latency--the delay between speech input and the\\ntranslated output. While quality evaluation is well established, accurate\\nlatency measurement remains a challenge. Existing metrics often produce\\ninconsistent or misleading results, especially in the widely used short-form\\nsetting, where speech is artificially presegmented. In this paper, we present\\nthe first comprehensive analysis of SimulST latency metrics across language\\npairs, systems, and both short- and long-form regimes. We uncover a structural\\nbias in current metrics related to segmentation that undermines fair and\\nmeaningful comparisons. To address this, we introduce YAAL (Yet Another Average\\nLagging), a refined latency metric that delivers more accurate evaluations in\\nthe short-form regime. We extend YAAL to LongYAAL for unsegmented audio and\\npropose SoftSegmenter, a novel resegmentation tool based on word-level\\nalignment. Our experiments show that YAAL and LongYAAL outperform popular\\nlatency metrics, while SoftSegmenter enhances alignment quality in long-form\\nevaluation, together enabling more reliable assessments of SimulST systems.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17349.png', 'numComments': 1, 'submittedBy': {'_id': '66309b3833ccd9e68c5d5171', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg', 'fullname': 'Sara Papi', 'name': 'spapi', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 11}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.19274', 'authors': [{'_id': '68d3e8ae4402066be56e1a99', 'name': 'Arijit Maji', 'hidden': False}, {'_id': '68d3e8ae4402066be56e1a9a', 'name': 'Raghvendra Kumar', 'hidden': False}, {'_id': '68d3e8ae4402066be56e1a9b', 'name': 'Akash Ghosh', 'hidden': False}, {'_id': '68d3e8ae4402066be56e1a9c', 'name': 'Anushka', 'hidden': False}, {'_id': '68d3e8ae4402066be56e1a9d', 'name': 'Nemil Shah', 'hidden': False}, {'_id': '68d3e8ae4402066be56e1a9e', 'name': 'Abhilekh Borah', 'hidden': False}, {'_id': '68d3e8ae4402066be56e1a9f', 'name': 'Vanshika Shah', 'hidden': False}, {'_id': '68d3e8ae4402066be56e1aa0', 'name': 'Nishant Mishra', 'hidden': False}, {'_id': '68d3e8ae4402066be56e1aa1', 'name': 'Sriparna Saha', 'hidden': False}], 'publishedAt': '2025-09-23T17:40:43.000Z', 'submittedOnDailyAt': '2025-09-24T11:22:22.925Z', 'title': \"DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language\\n  Models' Understanding on Indian Culture\", 'submittedOnDailyBy': {'_id': '65425237ea69bcb6203c8d76', 'avatarUrl': '/avatars/42953b27288faac8eb1397f194cecc66.svg', 'isPro': False, 'fullname': 'Abhilekh Borah', 'user': 'abhilekhborah', 'type': 'user'}, 'summary': \"We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual\\nbenchmark centered exclusively on Indian culture, designed to evaluate the\\ncultural understanding of generative AI systems. Unlike existing benchmarks\\nwith a generic or global scope, DRISHTIKON offers deep, fine-grained coverage\\nacross India's diverse regions, spanning 15 languages, covering all states and\\nunion territories, and incorporating over 64,000 aligned text-image pairs. The\\ndataset captures rich cultural themes including festivals, attire, cuisines,\\nart forms, and historical heritage amongst many more. We evaluate a wide range\\nof vision-language models (VLMs), including open-source small and large models,\\nproprietary systems, reasoning-specialized VLMs, and Indic-focused models,\\nacross zero-shot and chain-of-thought settings. Our results expose key\\nlimitations in current models' ability to reason over culturally grounded,\\nmultimodal inputs, particularly for low-resource languages and less-documented\\ntraditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a\\nrobust testbed to advance culturally aware, multimodally competent language\\ntechnologies.\", 'upvotes': 0, 'discussionId': '68d3e8ae4402066be56e1aa2', 'ai_summary': \"DRISHTIKON is a multimodal and multilingual benchmark for evaluating generative AI systems' cultural understanding across India's diverse regions and languages.\", 'ai_keywords': ['multimodal', 'multilingual', 'benchmark', 'cultural understanding', 'generative AI systems', 'text-image pairs', 'vision-language models', 'zero-shot', 'chain-of-thought', 'culturally grounded', 'low-resource languages', 'inclusive AI research', 'culturally aware', 'multimodally competent language technologies']}, 'publishedAt': '2025-09-23T13:40:43.000Z', 'title': \"DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language\\n  Models' Understanding on Indian Culture\", 'summary': \"We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual\\nbenchmark centered exclusively on Indian culture, designed to evaluate the\\ncultural understanding of generative AI systems. Unlike existing benchmarks\\nwith a generic or global scope, DRISHTIKON offers deep, fine-grained coverage\\nacross India's diverse regions, spanning 15 languages, covering all states and\\nunion territories, and incorporating over 64,000 aligned text-image pairs. The\\ndataset captures rich cultural themes including festivals, attire, cuisines,\\nart forms, and historical heritage amongst many more. We evaluate a wide range\\nof vision-language models (VLMs), including open-source small and large models,\\nproprietary systems, reasoning-specialized VLMs, and Indic-focused models,\\nacross zero-shot and chain-of-thought settings. Our results expose key\\nlimitations in current models' ability to reason over culturally grounded,\\nmultimodal inputs, particularly for low-resource languages and less-documented\\ntraditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a\\nrobust testbed to advance culturally aware, multimodally competent language\\ntechnologies.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19274.png', 'numComments': 1, 'submittedBy': {'_id': '65425237ea69bcb6203c8d76', 'avatarUrl': '/avatars/42953b27288faac8eb1397f194cecc66.svg', 'fullname': 'Abhilekh Borah', 'name': 'abhilekhborah', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}"
]