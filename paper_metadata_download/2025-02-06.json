[
    "{'paper': {'id': '2502.02737', 'authors': [{'_id': '67a446a9430e358f5d5ac4c3', 'user': {'_id': '61c141342aac764ce1654e43', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg', 'isPro': False, 'fullname': 'Loubna Ben Allal', 'user': 'loubnabnl', 'type': 'user'}, 'name': 'Loubna Ben Allal', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:14:56.506Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4c4', 'user': {'_id': '602e6dee60e3dd96631c906e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1613655355830-noauth.png', 'isPro': False, 'fullname': 'Anton Lozhkov', 'user': 'anton-l', 'type': 'user'}, 'name': 'Anton Lozhkov', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:14:39.237Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4c5', 'user': {'_id': '651e96991b97c9f33d26bde6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg', 'isPro': False, 'fullname': 'Elie Bakouch', 'user': 'eliebak', 'type': 'user'}, 'name': 'Elie Bakouch', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:14:45.734Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4c6', 'user': {'_id': '60f2fc91b92afccb7c34b8ed', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60f2fc91b92afccb7c34b8ed/W2-Nay12Ef4Ltyaf8EKE9.jpeg', 'isPro': False, 'fullname': 'Gabriel Martín Blázquez', 'user': 'gabrielmbmb', 'type': 'user'}, 'name': 'Gabriel Martín Blázquez', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:14:43.746Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4c7', 'user': {'_id': '62596f9e1c0a084224b93e00', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62596f9e1c0a084224b93e00/X2aLkJ0ofhkXwAg7lXvxD.jpeg', 'isPro': False, 'fullname': 'Guilherme Penedo', 'user': 'guipenedo', 'type': 'user'}, 'name': 'Guilherme Penedo', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:14:50.085Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4c8', 'user': {'_id': '5f0c746619cb630495b814fd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1594651707950-noauth.jpeg', 'isPro': True, 'fullname': 'Lewis Tunstall', 'user': 'lewtun', 'type': 'user'}, 'name': 'Lewis Tunstall', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:16:30.456Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4c9', 'user': {'_id': '65d66b494bbd0d92b641cdbb', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg', 'isPro': False, 'fullname': 'Andres Marafioti', 'user': 'andito', 'type': 'user'}, 'name': 'Andrés Marafioti', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:16:37.742Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4ca', 'user': {'_id': '626ede24d2fa9e7d598c8709', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/626ede24d2fa9e7d598c8709/JKS8-Y2Jw87EgNQZBRswq.jpeg', 'isPro': True, 'fullname': 'Hynek Kydlicek', 'user': 'hynky', 'type': 'user'}, 'name': 'Hynek Kydlíček', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:16:43.590Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4cb', 'user': {'_id': '6435d564a4bd75c62cc03701', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6435d564a4bd75c62cc03701/7P2G_wVNB6MISp2Phh427.jpeg', 'isPro': False, 'fullname': 'Agustín Piqueres Lajarín', 'user': 'plaguss', 'type': 'user'}, 'name': 'Agustín Piqueres Lajarín', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:16:49.324Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4cc', 'user': {'_id': '61b85ce86eb1f2c5e6233736', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1655385361868-61b85ce86eb1f2c5e6233736.jpeg', 'isPro': True, 'fullname': 'Vaibhav Srivastav', 'user': 'reach-vb', 'type': 'user'}, 'name': 'Vaibhav Srivastav', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:14:52.239Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4cd', 'user': {'_id': '61b253b7ac5ecaae3d1efe0c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png', 'isPro': False, 'fullname': 'Joshua', 'user': 'Xenova', 'type': 'user'}, 'name': 'Joshua Lochner', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:14:36.878Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4ce', 'user': {'_id': '648a374f00f7a3374ee64b99', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/648a374f00f7a3374ee64b99/YPwSOrronoozwHbJchPn3.jpeg', 'isPro': True, 'fullname': 'Caleb Fahlgren', 'user': 'cfahlgren1', 'type': 'user'}, 'name': 'Caleb Fahlgren', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:16:56.849Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4cf', 'user': {'_id': '63ca214abedad7e2bf1d1517', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674191139776-noauth.png', 'isPro': False, 'fullname': 'Xuan Son NGUYEN', 'user': 'ngxson', 'type': 'user'}, 'name': 'Xuan-Son Nguyen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:17:02.477Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4d0', 'user': {'_id': '6202a599216215a22221dea9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1644340617257-noauth.png', 'isPro': False, 'fullname': 'Clémentine Fourrier', 'user': 'clefourrier', 'type': 'user'}, 'name': 'Clémentine Fourrier', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:14:54.591Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4d1', 'user': {'_id': '62d648291fa3e4e7ae3fa6e8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62d648291fa3e4e7ae3fa6e8/oatOwf8Xqe5eDbCSuYqCd.png', 'isPro': False, 'fullname': 'ben burtenshaw', 'user': 'burtenshaw', 'type': 'user'}, 'name': 'Ben Burtenshaw', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:14:41.918Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4d2', 'user': {'_id': '641cc77c92cd25302998b740', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/641cc77c92cd25302998b740/5A81W5s3ecLaLXFir52Rw.jpeg', 'isPro': False, 'fullname': 'Hugo Larcher', 'user': 'hlarcher', 'type': 'user'}, 'name': 'Hugo Larcher', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:17:14.767Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4d3', 'user': {'_id': '660ed80b1889bf2cd53cab7f', 'avatarUrl': '/avatars/93ee6ff00668c2698ad8b6fa6f072b92.svg', 'isPro': False, 'fullname': 'Haojun Zhao', 'user': 'zzhhjjj', 'type': 'user'}, 'name': 'Haojun Zhao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:17:33.798Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4d4', 'user': {'_id': '66ba71a4447411b9c0e19d71', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4f93ZrYdaKfK3F53IB51x.jpeg', 'isPro': False, 'fullname': 'Cyril', 'user': 'cyrilzakka', 'type': 'user'}, 'name': 'Cyril Zakka', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:17:43.679Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4d5', 'user': {'_id': '664d7d1e4f54c9372970e121', 'avatarUrl': '/avatars/695a209d6951a4623eceedcd2eed3a68.svg', 'isPro': False, 'fullname': 'Mathieu Morlon', 'user': 'glutamatt', 'type': 'user'}, 'name': 'Mathieu Morlon', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:17:50.199Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4d6', 'user': {'_id': '6079c29765b9d0165cb18392', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1618592397610-noauth.jpeg', 'isPro': False, 'fullname': 'Colin Raffel', 'user': 'craffel', 'type': 'user'}, 'name': 'Colin Raffel', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:17:57.936Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4d7', 'user': {'_id': '5e48005437cb5b49818287a5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/5e48005437cb5b49818287a5/4uCXGGui-9QifAT4qelxU.png', 'isPro': False, 'fullname': 'Leandro von Werra', 'user': 'lvwerra', 'type': 'user'}, 'name': 'Leandro von Werra', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T11:03:02.572Z', 'hidden': False}, {'_id': '67a446a9430e358f5d5ac4d8', 'user': {'_id': '5df7e9e5da6d0311fd3d53f9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg', 'isPro': True, 'fullname': 'Thomas Wolf', 'user': 'thomwolf', 'type': 'user'}, 'name': 'Thomas Wolf', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:14:14.159Z', 'hidden': False}], 'publishedAt': '2025-02-04T21:43:16.000Z', 'title': 'SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language\\n  Model', 'summary': 'While large language models have facilitated breakthroughs in many\\napplications of artificial intelligence, their inherent largeness makes them\\ncomputationally expensive and challenging to deploy in resource-constrained\\nsettings. In this paper, we document the development of SmolLM2, a\\nstate-of-the-art \"small\" (1.7 billion parameter) language model (LM). To attain\\nstrong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a\\nmulti-stage training process that mixes web text with specialized math, code,\\nand instruction-following data. We additionally introduce new specialized\\ndatasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing\\ndatasets to be problematically small or low-quality. To inform our design\\ndecisions, we perform both small-scale ablations as well as a manual refinement\\nprocess that updates the dataset mixing rates at each stage based on the\\nperformance at the previous stage. Ultimately, we demonstrate that SmolLM2\\noutperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To\\nfacilitate future research on LM development as well as applications of small\\nLMs, we release both SmolLM2 as well as all of the datasets we prepared in the\\ncourse of this project.', 'upvotes': 61, 'discussionId': '67a446a9430e358f5d5ac4f8'}, 'publishedAt': '2025-02-06T00:20:51.704Z', 'title': 'SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02737.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5960}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.01506', 'authors': [{'_id': '67a4214f12b90b15dc5a648e', 'user': {'_id': '63f622c69cbd6730302783eb', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63f622c69cbd6730302783eb/9cb96JVKiOm_JhF-shbFw.jpeg', 'isPro': False, 'fullname': 'Yuzhe Yang', 'user': 'TobyYang7', 'type': 'user'}, 'name': 'Yuzhe Yang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:15:03.582Z', 'hidden': False}, {'_id': '67a4214f12b90b15dc5a648f', 'user': {'_id': '643c047326f177a3e41627b6', 'avatarUrl': '/avatars/ade75cebd049daf080ba80a80d516240.svg', 'isPro': False, 'fullname': 'Yifei Zhang', 'user': 'amstrongzyf', 'type': 'user'}, 'name': 'Yifei Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:15:05.578Z', 'hidden': False}, {'_id': '67a4214f12b90b15dc5a6490', 'user': {'_id': '62d4bf8c97ab9eb08762a975', 'avatarUrl': '/avatars/73c6228e317cf37b4e3c3e7a4b3d8ae8.svg', 'isPro': False, 'fullname': 'Minghao Wu', 'user': 'minghaowu', 'type': 'user'}, 'name': 'Minghao Wu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:18:20.423Z', 'hidden': False}, {'_id': '67a4214f12b90b15dc5a6491', 'name': 'Kaidi Zhang', 'hidden': False}, {'_id': '67a4214f12b90b15dc5a6492', 'name': 'Yunmiao Zhang', 'hidden': False}, {'_id': '67a4214f12b90b15dc5a6493', 'name': 'Honghai Yu', 'hidden': False}, {'_id': '67a4214f12b90b15dc5a6494', 'name': 'Yan Hu', 'hidden': False}, {'_id': '67a4214f12b90b15dc5a6495', 'user': {'_id': '637c6703ca8542a0ba900ccb', 'avatarUrl': '/avatars/288ed63a1efa566c3f01e850c6ba5dd5.svg', 'isPro': False, 'fullname': 'Wang', 'user': 'Benyou', 'type': 'user'}, 'name': 'Benyou Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:34:30.901Z', 'hidden': False}], 'publishedAt': '2025-02-03T16:39:48.000Z', 'title': 'TwinMarket: A Scalable Behavioral and Social Simulation for Financial\\n  Markets', 'summary': 'The study of social emergence has long been a central focus in social\\nscience. Traditional modeling approaches, such as rule-based Agent-Based Models\\n(ABMs), struggle to capture the diversity and complexity of human behavior,\\nparticularly the irrational factors emphasized in behavioral economics.\\nRecently, large language model (LLM) agents have gained traction as simulation\\ntools for modeling human behavior in social science and role-playing\\napplications. Studies suggest that LLMs can account for cognitive biases,\\nemotional fluctuations, and other non-rational influences, enabling more\\nrealistic simulations of socio-economic dynamics. In this work, we introduce\\nTwinMarket, a novel multi-agent framework that leverages LLMs to simulate\\nsocio-economic systems. Specifically, we examine how individual behaviors,\\nthrough interactions and feedback mechanisms, give rise to collective dynamics\\nand emergent phenomena. Through experiments in a simulated stock market\\nenvironment, we demonstrate how individual actions can trigger group behaviors,\\nleading to emergent outcomes such as financial bubbles and recessions. Our\\napproach provides valuable insights into the complex interplay between\\nindividual decision-making and collective socio-economic patterns.', 'upvotes': 26, 'discussionId': '67a4215212b90b15dc5a650a'}, 'publishedAt': '2025-02-05T21:44:36.248Z', 'title': 'TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01506.png', 'numComments': 2, 'submittedBy': {'_id': '643c047326f177a3e41627b6', 'avatarUrl': '/avatars/ade75cebd049daf080ba80a80d516240.svg', 'fullname': 'Yifei Zhang', 'name': 'amstrongzyf', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.03373', 'authors': [{'_id': '67a42c079a4fb11b11cc4f6f', 'name': 'Edward Yeo', 'hidden': False}, {'_id': '67a42c079a4fb11b11cc4f70', 'user': {'_id': '6448e1fbe988635a3d6aa97d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eG4R9-3hgrimttP7ep3dN.jpeg', 'isPro': False, 'fullname': 'Shawn/Yuxuan Tong', 'user': 'tongyx361', 'type': 'user'}, 'name': 'Yuxuan Tong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:34:50.960Z', 'hidden': False}, {'_id': '67a42c079a4fb11b11cc4f71', 'user': {'_id': '65bb14f139c4e7087640a91c', 'avatarUrl': '/avatars/dbf75dd161d22b4511e9fccff6afc515.svg', 'isPro': False, 'fullname': 'Morry Niu', 'user': 'bl1ndbot', 'type': 'user'}, 'name': 'Morry Niu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:34:57.424Z', 'hidden': False}, {'_id': '67a42c079a4fb11b11cc4f72', 'user': {'_id': '60de14638bedd2315529d43f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1625166923504-noauth.png', 'isPro': False, 'fullname': 'Graham Neubig', 'user': 'gneubig', 'type': 'user'}, 'name': 'Graham Neubig', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:35:04.994Z', 'hidden': False}, {'_id': '67a42c079a4fb11b11cc4f73', 'user': {'_id': '6230d750d93e84e233882dbc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg', 'isPro': False, 'fullname': 'Xiang Yue', 'user': 'yuexiang96', 'type': 'user'}, 'name': 'Xiang Yue', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:35:19.222Z', 'hidden': False}], 'publishedAt': '2025-02-05T17:13:32.000Z', 'title': 'Demystifying Long Chain-of-Thought Reasoning in LLMs', 'summary': 'Scaling inference compute enhances reasoning in large language models (LLMs),\\nwith long chains-of-thought (CoTs) enabling strategies like backtracking and\\nerror correction. Reinforcement learning (RL) has emerged as a crucial method\\nfor developing these capabilities, yet the conditions under which long CoTs\\nemerge remain unclear, and RL training requires careful design choices. In this\\nstudy, we systematically investigate the mechanics of long CoT reasoning,\\nidentifying the key factors that enable models to generate long CoT\\ntrajectories. Through extensive supervised fine-tuning (SFT) and RL\\nexperiments, we present four main findings: (1) While SFT is not strictly\\nnecessary, it simplifies training and improves efficiency; (2) Reasoning\\ncapabilities tend to emerge with increased training compute, but their\\ndevelopment is not guaranteed, making reward shaping crucial for stabilizing\\nCoT length growth; (3) Scaling verifiable reward signals is critical for RL. We\\nfind that leveraging noisy, web-extracted solutions with filtering mechanisms\\nshows strong potential, particularly for out-of-distribution (OOD) tasks such\\nas STEM reasoning; and (4) Core abilities like error correction are inherently\\npresent in base models, but incentivizing these skills effectively for complex\\ntasks via RL demands significant compute, and measuring their emergence\\nrequires a nuanced approach. These insights provide practical guidance for\\noptimizing training strategies to enhance long CoT reasoning in LLMs. Our code\\nis available at: https://github.com/eddycmu/demystify-long-cot.', 'upvotes': 18, 'discussionId': '67a42c089a4fb11b11cc4fae'}, 'publishedAt': '2025-02-05T22:27:48.348Z', 'title': 'Demystifying Long Chain-of-Thought Reasoning in LLMs', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03373.png', 'numComments': 2, 'submittedBy': {'_id': '6230d750d93e84e233882dbc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg', 'fullname': 'Xiang Yue', 'name': 'yuexiang96', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 26}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.03387', 'authors': [{'_id': '67a445ccbdd74b63b4e52a7d', 'name': 'Yixin Ye', 'hidden': False}, {'_id': '67a445ccbdd74b63b4e52a7e', 'user': {'_id': '643581a4f3b08e267d990499', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/643581a4f3b08e267d990499/KRhB-48W4IPuB0bX16Ahj.png', 'isPro': False, 'fullname': 'Zhen Huang', 'user': 'ZhenHuang', 'type': 'user'}, 'name': 'Zhen Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:37:03.643Z', 'hidden': False}, {'_id': '67a445ccbdd74b63b4e52a7f', 'name': 'Yang Xiao', 'hidden': False}, {'_id': '67a445ccbdd74b63b4e52a80', 'user': {'_id': '64bb5f9d8e051085bace4d1e', 'avatarUrl': '/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg', 'isPro': True, 'fullname': 'Ethan Chern', 'user': 'ethanchern', 'type': 'user'}, 'name': 'Ethan Chern', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:36:45.045Z', 'hidden': False}, {'_id': '67a445ccbdd74b63b4e52a81', 'user': {'_id': '65900d4ff5a209eeac08b463', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65900d4ff5a209eeac08b463/PJNNBRJIk1qR24oaRLTex.jpeg', 'isPro': False, 'fullname': 'shijie xia', 'user': 'seven-cat', 'type': 'user'}, 'name': 'Shijie Xia', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:14:59.334Z', 'hidden': False}, {'_id': '67a445ccbdd74b63b4e52a82', 'user': {'_id': '6144a0c4ff1146bbd84d9865', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1661715958139-6144a0c4ff1146bbd84d9865.png', 'isPro': True, 'fullname': 'Pengfei Liu', 'user': 'Pengfei', 'type': 'user'}, 'name': 'Pengfei Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:36:38.049Z', 'hidden': False}], 'publishedAt': '2025-02-05T17:23:45.000Z', 'title': 'LIMO: Less is More for Reasoning', 'summary': 'We present a fundamental discovery that challenges our understanding of how\\ncomplex reasoning emerges in large language models. While conventional wisdom\\nsuggests that sophisticated reasoning tasks demand extensive training data\\n(>100,000 examples), we demonstrate that complex mathematical reasoning\\nabilities can be effectively elicited with surprisingly few examples. Through\\ncomprehensive experiments, our proposed model LIMO demonstrates unprecedented\\nperformance in mathematical reasoning. With merely 817 curated training\\nsamples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from\\nprevious SFT-based models\\' 6.5% and 59.2% respectively, while only using 1% of\\nthe training data required by previous approaches. LIMO demonstrates\\nexceptional out-of-distribution generalization, achieving 40.5% absolute\\nimprovement across 10 diverse benchmarks, outperforming models trained on 100x\\nmore data, challenging the notion that SFT leads to memorization rather than\\ngeneralization. Based on these results, we propose the Less-Is-More Reasoning\\nHypothesis (LIMO Hypothesis): In foundation models where domain knowledge has\\nbeen comprehensively encoded during pre-training, sophisticated reasoning\\ncapabilities can emerge through minimal but precisely orchestrated\\ndemonstrations of cognitive processes. This hypothesis posits that the\\nelicitation threshold for complex reasoning is determined by two key factors:\\n(1) the completeness of the model\\'s encoded knowledge foundation during\\npre-training, and (2) the effectiveness of post-training examples as \"cognitive\\ntemplates\" that show the model how to utilize its knowledge base to solve\\ncomplex reasoning tasks. To facilitate reproducibility and future research in\\ndata-efficient reasoning, we release LIMO as a comprehensive open-source suite\\nat https://github.com/GAIR-NLP/LIMO.', 'upvotes': 15, 'discussionId': '67a445cdbdd74b63b4e52af7'}, 'publishedAt': '2025-02-06T00:26:02.483Z', 'title': 'LIMO: Less is More for Reasoning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03387.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5960}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.02339', 'authors': [{'_id': '67a3262873bdaf626f1e9eab', 'user': {'_id': '6747de57f8cab58c22ec94a2', 'avatarUrl': '/avatars/5bae0341862fac24564781c0fa32aac5.svg', 'isPro': False, 'fullname': 'Jinyang Wu', 'user': 'Jinyang23', 'type': 'user'}, 'name': 'Jinyang Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:15:16.426Z', 'hidden': False}, {'_id': '67a3262873bdaf626f1e9eac', 'user': {'_id': '660d13b85e00095e45ee28e0', 'avatarUrl': '/avatars/8f06c01edc2a791266feadc775acb901.svg', 'isPro': False, 'fullname': 'FengMingkuan', 'user': 'fmk345', 'type': 'user'}, 'name': 'Mingkuan Feng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:37:21.287Z', 'hidden': False}, {'_id': '67a3262873bdaf626f1e9ead', 'name': 'Shuai Zhang', 'hidden': False}, {'_id': '67a3262873bdaf626f1e9eae', 'user': {'_id': '64be16d8ef8c0e42bf3d27f6', 'avatarUrl': '/avatars/6ae308088f6f196d9f470655dae0c14d.svg', 'isPro': False, 'fullname': 'Ruihan Jin', 'user': 'RuihanJin', 'type': 'user'}, 'name': 'Ruihan Jin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:37:48.323Z', 'hidden': False}, {'_id': '67a3262873bdaf626f1e9eaf', 'user': {'_id': '63ef2de81e695b35aa4813a2', 'avatarUrl': '/avatars/6abd1918c1b94d927c7c976054e16322.svg', 'isPro': False, 'fullname': 'feihu', 'user': 'feihuchen', 'type': 'user'}, 'name': 'Feihu Che', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-06T14:37:55.510Z', 'hidden': False}, {'_id': '67a3262873bdaf626f1e9eb0', 'name': 'Zengqi Wen', 'hidden': False}, {'_id': '67a3262873bdaf626f1e9eb1', 'name': 'Jianhua Tao', 'hidden': False}], 'publishedAt': '2025-02-04T14:18:29.000Z', 'title': 'Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking', 'summary': \"Multimodal large language models (MLLMs) exhibit impressive capabilities but\\nstill face challenges in complex visual reasoning. While recent efforts attempt\\nto enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking\\nthrough explicit search structures or teacher-guided distillation, they often\\nstruggle to balance performance and efficiency. A critical limitation is their\\nheavy reliance on extensive data and search spaces, resulting in low-efficiency\\nimplicit insight extraction and data utilization. To address this, we propose\\nAStar, an Automated Structured thinking paradigm for multimodal reasoning via\\nMonte Carlo Tree Search (MCTS). AStar automatically derives high-level\\ncognitive reasoning patterns from limited data using MCTS-powered hierarchical\\nstructures. Building on these explicit patterns, we design a unified reasoning\\nframework that seamlessly integrates models' internal reasoning capabilities\\nand external reasoning guidelines, enabling efficient inference with minimal\\ntree iterations. This novel paradigm strikes a compelling balance between\\nperformance and efficiency. Extensive experiments demonstrate AStar's\\neffectiveness, achieving superior accuracy (54.0%) on the MathVerse\\nbenchmark with a 7B backbone, surpassing GPT-4o (50.2%) while maintaining\\nsubstantial data and computational efficiency.\", 'upvotes': 9, 'discussionId': '67a3262973bdaf626f1e9edb'}, 'publishedAt': '2025-02-05T21:45:32.304Z', 'title': 'Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02339.png', 'numComments': 1, 'submittedBy': {'_id': '6747de57f8cab58c22ec94a2', 'avatarUrl': '/avatars/5bae0341862fac24564781c0fa32aac5.svg', 'fullname': 'Jinyang Wu', 'name': 'Jinyang23', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.01105', 'authors': [{'_id': '67a45c85e73ad243c0b9529e', 'name': 'Yiren Song', 'hidden': False}, {'_id': '67a45c85e73ad243c0b9529f', 'name': 'Danze Chen', 'hidden': False}, {'_id': '67a45c85e73ad243c0b952a0', 'user': {'_id': '63a55320ce5763e06f78519c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1671779060549-noauth.jpeg', 'isPro': False, 'fullname': 'Mike Shou', 'user': 'mikeshou', 'type': 'user'}, 'name': 'Mike Zheng Shou', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-02-06T06:54:02.195Z', 'hidden': False}], 'publishedAt': '2025-02-03T06:49:58.000Z', 'title': 'LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion\\n  Transformer', 'summary': \"Generating cognitive-aligned layered SVGs remains challenging due to existing\\nmethods' tendencies toward either oversimplified single-layer outputs or\\noptimization-induced shape redundancies. We propose LayerTracer, a diffusion\\ntransformer based framework that bridges this gap by learning designers'\\nlayered SVG creation processes from a novel dataset of sequential design\\noperations. Our approach operates in two phases: First, a text-conditioned DiT\\ngenerates multi-phase rasterized construction blueprints that simulate human\\ndesign workflows. Second, layer-wise vectorization with path deduplication\\nproduces clean, editable SVGs. For image vectorization, we introduce a\\nconditional diffusion mechanism that encodes reference images into latent\\ntokens, guiding hierarchical reconstruction while preserving structural\\nintegrity. Extensive experiments demonstrate LayerTracer's superior performance\\nagainst optimization-based and neural baselines in both generation quality and\\neditability, effectively aligning AI-generated vectors with professional design\\ncognition.\", 'upvotes': 5, 'discussionId': '67a45c8ae73ad243c0b953ea'}, 'publishedAt': '2025-02-06T01:55:37.207Z', 'title': 'LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01105.png', 'numComments': 1, 'submittedBy': {'_id': '64311a95034ecbefddd141ef', 'avatarUrl': '/avatars/b6dc5ca373bedbaa368208517954c375.svg', 'fullname': 'Yiren Song', 'name': 'yiren98', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.01618', 'authors': [{'_id': '67a438d26bb8caaab06f5a5e', 'user': {'_id': '64c2abe8c43875b438efef25', 'avatarUrl': '/avatars/6efda081f52cf56db2d29a5ec05cb557.svg', 'isPro': False, 'fullname': 'isha', 'user': 'ishapuri-mit', 'type': 'user'}, 'name': 'Isha Puri', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-02-06T04:21:39.202Z', 'hidden': False}, {'_id': '67a438d26bb8caaab06f5a5f', 'name': 'Shivchander Sudalairaj', 'hidden': False}, {'_id': '67a438d26bb8caaab06f5a60', 'name': 'Guangxuan Xu', 'hidden': False}, {'_id': '67a438d26bb8caaab06f5a61', 'name': 'Kai Xu', 'hidden': False}, {'_id': '67a438d26bb8caaab06f5a62', 'name': 'Akash Srivastava', 'hidden': False}], 'publishedAt': '2025-02-03T18:50:50.000Z', 'title': 'A Probabilistic Inference Approach to Inference-Time Scaling of LLMs\\n  using Particle-Based Monte Carlo Methods', 'summary': 'Large language models (LLMs) have achieved significant performance gains via\\nscaling up model sizes and/or data. However, recent evidence suggests\\ndiminishing returns from such approaches, motivating scaling the computation\\nspent at inference time. Existing inference-time scaling methods, usually with\\nreward models, cast the task as a search problem, which tends to be vulnerable\\nto reward hacking as a consequence of approximation errors in reward models. In\\nthis paper, we instead cast inference-time scaling as a probabilistic inference\\ntask and leverage sampling-based techniques to explore the typical set of the\\nstate distribution of a state-space model with an approximate likelihood,\\nrather than optimize for its mode directly. We propose a novel inference-time\\nscaling approach by adapting particle-based Monte Carlo methods to this task.\\nOur empirical evaluation demonstrates that our methods have a 4-16x better\\nscaling rate over our deterministic search counterparts on various challenging\\nmathematical reasoning tasks. Using our approach, we show that\\nQwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts,\\nwhile Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.\\nOur work not only presents an effective method to inference-time scaling, but\\nalso connects the rich literature in probabilistic inference with\\ninference-time scaling of LLMs to develop more robust algorithms in future\\nwork. Code and further information is available at\\nhttps://probabilistic-inference-scaling.github.io.', 'upvotes': 4, 'discussionId': '67a438d36bb8caaab06f5a87'}, 'publishedAt': '2025-02-05T23:23:08.428Z', 'title': 'A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/648b3f3208c4a9d807a90a99/gwgJD14Bd0fdz7xpcHdHe.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/648b3f3208c4a9d807a90a99/KHcaqxZL3wiloAm7x-7nA.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01618.png', 'numComments': 2, 'submittedBy': {'_id': '648b3f3208c4a9d807a90a99', 'avatarUrl': '/avatars/03634b4e7f8afe9b589a2d7370e29960.svg', 'fullname': 'Akash Srivastava', 'name': 'akashsri', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 8}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.01154', 'authors': [{'_id': '67a4609af2e553c1d0da914d', 'name': 'Yu-Ling Hsu', 'hidden': False}, {'_id': '67a4609af2e553c1d0da914e', 'user': {'_id': '608abf1272b50b02c4b02865', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg', 'isPro': False, 'fullname': 'Hsuan Su', 'user': 'jacksukk', 'type': 'user'}, 'name': 'Hsuan Su', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:14:29.721Z', 'hidden': False}, {'_id': '67a4609af2e553c1d0da914f', 'name': 'Shang-Tse Chen', 'hidden': False}], 'publishedAt': '2025-02-03T08:44:24.000Z', 'title': 'Jailbreaking with Universal Multi-Prompts', 'summary': 'Large language models (LLMs) have seen rapid development in recent years,\\nrevolutionizing various applications and significantly enhancing convenience\\nand productivity. However, alongside their impressive capabilities, ethical\\nconcerns and new types of attacks, such as jailbreaking, have emerged. While\\nmost prompting techniques focus on optimizing adversarial inputs for individual\\ncases, resulting in higher computational costs when dealing with large\\ndatasets. Less research has addressed the more general setting of training a\\nuniversal attacker that can transfer to unseen tasks. In this paper, we\\nintroduce JUMP, a prompt-based method designed to jailbreak LLMs using\\nuniversal multi-prompts. We also adapt our approach for defense, which we term\\nDUMP. Experimental results demonstrate that our method for optimizing universal\\nmulti-prompts outperforms existing techniques.', 'upvotes': 3, 'discussionId': '67a4609bf2e553c1d0da9181'}, 'publishedAt': '2025-02-06T02:11:41.374Z', 'title': 'Jailbreaking with Universal Multi-Prompts', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01154.png', 'numComments': 1, 'submittedBy': {'_id': '608abf1272b50b02c4b02865', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg', 'fullname': 'Hsuan Su', 'name': 'jacksukk', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.03275', 'authors': [{'_id': '67a448b69ca42c642a723a7d', 'name': 'DiJia Su', 'hidden': False}, {'_id': '67a448b69ca42c642a723a7e', 'name': 'Hanlin Zhu', 'hidden': False}, {'_id': '67a448b69ca42c642a723a7f', 'name': 'Yingchen Xu', 'hidden': False}, {'_id': '67a448b69ca42c642a723a80', 'name': 'Jiantao Jiao', 'hidden': False}, {'_id': '67a448b69ca42c642a723a81', 'name': 'Yuandong Tian', 'hidden': False}, {'_id': '67a448b69ca42c642a723a82', 'name': 'Qinqing Zheng', 'hidden': False}], 'publishedAt': '2025-02-05T15:33:00.000Z', 'title': 'Token Assorted: Mixing Latent and Text Tokens for Improved Language\\n  Model Reasoning', 'summary': 'Large Language Models (LLMs) excel at reasoning and planning when trained on\\nchainof-thought (CoT) data, where the step-by-step thought process is\\nexplicitly outlined by text tokens. However, this results in lengthy inputs\\nwhere many words support textual coherence rather than core reasoning\\ninformation, and processing these inputs consumes substantial computation\\nresources. In this work, we propose a hybrid representation of the reasoning\\nprocess, where we partially abstract away the initial reasoning steps using\\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\\nof reasoning traces. We explore the use of latent trace abstractions in two\\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\\nincluding unseen latent tokens, for both logical and mathematical reasoning\\nproblems. To facilitate effective learning, we introduce a simple training\\nprocedure that randomly mixes latent and text tokens, which enables fast\\nadaptation to new latent tokens. Our approach consistently outperforms the\\nbaselines methods in various benchmarks.', 'upvotes': 3, 'discussionId': '67a448b89ca42c642a723ac6'}, 'publishedAt': '2025-02-06T00:29:44.686Z', 'title': 'Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03275.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5960}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.02671', 'authors': [{'_id': '67a495ce0f2d0f0303a3af71', 'user': {'_id': '6262880c5eb4fa93219f0064', 'avatarUrl': '/avatars/13539e63e3df7d94e91ed3fa8988e39a.svg', 'isPro': False, 'fullname': 'Daniil Tiapkin', 'user': 'dtiapkin', 'type': 'user'}, 'name': 'Daniil Tiapkin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:14:19.738Z', 'hidden': False}, {'_id': '67a495ce0f2d0f0303a3af72', 'name': 'Daniele Calandriello', 'hidden': False}, {'_id': '67a495ce0f2d0f0303a3af73', 'name': 'Johan Ferret', 'hidden': False}, {'_id': '67a495ce0f2d0f0303a3af74', 'name': 'Sarah Perrin', 'hidden': False}, {'_id': '67a495ce0f2d0f0303a3af75', 'name': 'Nino Vieillard', 'hidden': False}, {'_id': '67a495ce0f2d0f0303a3af76', 'name': 'Alexandre Ramé', 'hidden': False}, {'_id': '67a495ce0f2d0f0303a3af77', 'name': 'Mathieu Blondel', 'hidden': False}], 'publishedAt': '2025-02-04T19:26:28.000Z', 'title': 'On Teacher Hacking in Language Model Distillation', 'summary': \"Post-training of language models (LMs) increasingly relies on the following\\ntwo stages: (i) knowledge distillation, where the LM is trained to imitate a\\nlarger teacher LM, and (ii) reinforcement learning from human feedback (RLHF),\\nwhere the LM is aligned by optimizing a reward model. In the second RLHF stage,\\na well-known challenge is reward hacking, where the LM over-optimizes the\\nreward model. Such phenomenon is in line with Goodhart's law and can lead to\\ndegraded performance on the true objective. In this paper, we investigate\\nwhether a similar phenomenon, that we call teacher hacking, can occur during\\nknowledge distillation. This could arise because the teacher LM is itself an\\nimperfect approximation of the true distribution. To study this, we propose a\\ncontrolled experimental setup involving: (i) an oracle LM representing the\\nground-truth distribution, (ii) a teacher LM distilled from the oracle, and\\n(iii) a student LM distilled from the teacher. Our experiments reveal the\\nfollowing insights. When using a fixed offline dataset for distillation,\\nteacher hacking occurs; moreover, we can detect it by observing when the\\noptimization process deviates from polynomial convergence laws. In contrast,\\nemploying online data generation techniques effectively mitigates teacher\\nhacking. More precisely, we identify data diversity as the key factor in\\npreventing hacking. Overall, our findings provide a deeper understanding of the\\nbenefits and limitations of distillation for building robust and efficient LMs.\", 'upvotes': 2, 'discussionId': '67a495d00f2d0f0303a3afde'}, 'publishedAt': '2025-02-06T06:15:51.159Z', 'title': 'On Teacher Hacking in Language Model Distillation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02671.png', 'numComments': 1, 'submittedBy': {'_id': '6262880c5eb4fa93219f0064', 'avatarUrl': '/avatars/13539e63e3df7d94e91ed3fa8988e39a.svg', 'fullname': 'Daniil Tiapkin', 'name': 'dtiapkin', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.00306', 'authors': [{'_id': '67a4d341784a1ad88b6110a0', 'name': 'Ali Naseh', 'hidden': False}, {'_id': '67a4d341784a1ad88b6110a1', 'name': 'Yuefeng Peng', 'hidden': False}, {'_id': '67a4d341784a1ad88b6110a2', 'name': 'Anshuman Suri', 'hidden': False}, {'_id': '67a4d341784a1ad88b6110a3', 'name': 'Harsh Chaudhari', 'hidden': False}, {'_id': '67a4d341784a1ad88b6110a4', 'name': 'Alina Oprea', 'hidden': False}, {'_id': '67a4d341784a1ad88b6110a5', 'name': 'Amir Houmansadr', 'hidden': False}], 'publishedAt': '2025-02-01T04:01:18.000Z', 'title': 'Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented\\n  Generation', 'summary': \"Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\\ngenerate grounded responses by leveraging external knowledge databases without\\naltering model parameters. Although the absence of weight tuning prevents\\nleakage via model parameters, it introduces the risk of inference adversaries\\nexploiting retrieved documents in the model's context. Existing methods for\\nmembership inference and data extraction often rely on jailbreaking or\\ncarefully crafted unnatural queries, which can be easily detected or thwarted\\nwith query rewriting techniques common in RAG systems. In this work, we present\\nInterrogation Attack (IA), a membership inference technique targeting documents\\nin the RAG datastore. By crafting natural-text queries that are answerable only\\nwith the target document's presence, our approach demonstrates successful\\ninference with just 30 queries while remaining stealthy; straightforward\\ndetectors identify adversarial prompts from existing methods up to ~76x more\\nfrequently than those generated by our attack. We observe a 2x improvement in\\nTPR@1%FPR over prior inference attacks across diverse RAG configurations, all\\nwhile costing less than $0.02 per document inference.\", 'upvotes': 0, 'discussionId': '67a4d342784a1ad88b6110d9'}, 'publishedAt': '2025-02-06T10:25:05.958Z', 'title': 'Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00306.png', 'numComments': 1, 'submittedBy': {'_id': '647a1010ffe1b559f5418534', 'avatarUrl': '/avatars/fed1a8dbd1090d8f48dc6c2d321a6212.svg', 'fullname': 'Anshuman Suri', 'name': 'iamgroot42', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.02928', 'authors': [{'_id': '67a4213c54bfb820ffb26f4a', 'user': {'_id': '65eef9ce7443c09267513796', 'avatarUrl': '/avatars/62547f99130557f54093b2ff4d6c9c24.svg', 'isPro': False, 'fullname': 'Muntasir Adnan', 'user': 'adnaan525', 'type': 'user'}, 'name': 'Muntasir Adnan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-06T14:15:07.849Z', 'hidden': False}, {'_id': '67a4213c54bfb820ffb26f4b', 'name': 'Zhiwei Xu', 'hidden': False}, {'_id': '67a4213c54bfb820ffb26f4c', 'name': 'Carlos C. N. Kuhn', 'hidden': False}], 'publishedAt': '2025-02-05T06:43:40.000Z', 'title': 'Large Language Model Guided Self-Debugging Code Generation', 'summary': 'Automated code generation is gaining significant importance in intelligent\\ncomputer programming and system deployment. However, current approaches often\\nface challenges in computational efficiency and lack robust mechanisms for code\\nparsing and error correction. In this work, we propose a novel framework,\\nPyCapsule, with a simple yet effective two-agent pipeline and efficient\\nself-debugging modules for Python code generation. PyCapsule features\\nsophisticated prompt inference, iterative error handling, and case testing,\\nensuring high generation stability, safety, and correctness. Empirically,\\nPyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3%\\non HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art\\nmethods. We also observe a decrease in normalized success rate given more\\nself-debugging attempts, potentially affected by limited and noisy error\\nfeedback in retention. PyCapsule demonstrates broader impacts on advancing\\nlightweight and efficient code generation for artificial intelligence systems.', 'upvotes': 0, 'discussionId': '67a4213d54bfb820ffb26f75'}, 'publishedAt': '2025-02-06T09:35:37.969Z', 'title': 'Large Language Model Guided Self-Debugging Code Generation', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65eef9ce7443c09267513796/TfB30iULajOm37wTzZDER.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02928.png', 'numComments': 1, 'submittedBy': {'_id': '65eef9ce7443c09267513796', 'avatarUrl': '/avatars/62547f99130557f54093b2ff4d6c9c24.svg', 'fullname': 'Muntasir Adnan', 'name': 'adnaan525', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}"
]