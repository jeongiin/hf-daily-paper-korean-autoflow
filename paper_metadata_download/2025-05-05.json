[
    "{'paper': {'id': '2504.20438', 'authors': [{'_id': '6814e35a19162d7749852c4b', 'user': {'_id': '633d4630e1aec4b8b33ad5b8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/633d4630e1aec4b8b33ad5b8/owFqnyCMW8yuo9VcjA1hx.jpeg', 'isPro': False, 'fullname': 'Ziyang Xu', 'user': 'Uyoung', 'type': 'user'}, 'name': 'Ziyang Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-04T10:04:40.638Z', 'hidden': False}, {'_id': '6814e35a19162d7749852c4c', 'name': 'Kangsheng Duan', 'hidden': False}, {'_id': '6814e35a19162d7749852c4d', 'user': {'_id': '627a34dac488a8ce15a2dc4a', 'avatarUrl': '/avatars/61aecef507dea6620fe5574493f83595.svg', 'isPro': False, 'fullname': 'ShenXiaolei', 'user': 'SmileTAT', 'type': 'user'}, 'name': 'Xiaolei Shen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-05T07:32:16.185Z', 'hidden': False}, {'_id': '6814e35a19162d7749852c4e', 'name': 'Zhifeng Ding', 'hidden': False}, {'_id': '6814e35a19162d7749852c4f', 'user': {'_id': '66c2e7fc934e2f07753542ac', 'avatarUrl': '/avatars/f6fa3f94435cf1c1d06daa6c925d07d0.svg', 'isPro': False, 'fullname': 'LWY', 'user': 'wenyuliu', 'type': 'user'}, 'name': 'Wenyu Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-05T07:32:31.210Z', 'hidden': False}, {'_id': '6814e35a19162d7749852c50', 'name': 'Xiaohu Ruan', 'hidden': False}, {'_id': '6814e35a19162d7749852c51', 'user': {'_id': '65389a669c474315d7425f96', 'avatarUrl': '/avatars/2fa3828ca489cfe1948129a0eccf264f.svg', 'isPro': False, 'fullname': 'chenxiaoxin', 'user': 'steelozazala', 'type': 'user'}, 'name': 'Xiaoxin Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-05T07:32:55.012Z', 'hidden': False}, {'_id': '6814e35a19162d7749852c52', 'user': {'_id': '62600de6d47e3dbae32ce1ce', 'avatarUrl': '/avatars/a536417cfec6e10ac415091bd1829426.svg', 'isPro': False, 'fullname': 'Xinggang Wang', 'user': 'xinggangw', 'type': 'user'}, 'name': 'Xinggang Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-05T07:33:01.396Z', 'hidden': False}], 'publishedAt': '2025-04-29T05:28:36.000Z', 'submittedOnDailyAt': '2025-05-05T05:42:06.841Z', 'title': 'PixelHacker: Image Inpainting with Structural and Semantic Consistency', 'submittedOnDailyBy': {'_id': '633d4630e1aec4b8b33ad5b8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/633d4630e1aec4b8b33ad5b8/owFqnyCMW8yuo9VcjA1hx.jpeg', 'isPro': False, 'fullname': 'Ziyang Xu', 'user': 'Uyoung', 'type': 'user'}, 'summary': 'Image inpainting is a fundamental research area between image editing and\\nimage generation. Recent state-of-the-art (SOTA) methods have explored novel\\nattention mechanisms, lightweight architectures, and context-aware modeling,\\ndemonstrating impressive performance. However, they often struggle with complex\\nstructure (e.g., texture, shape, spatial relations) and semantics (e.g., color\\nconsistency, object restoration, and logical correctness), leading to artifacts\\nand inappropriate generation. To address this challenge, we design a simple yet\\neffective inpainting paradigm called latent categories guidance, and further\\npropose a diffusion-based model named PixelHacker. Specifically, we first\\nconstruct a large dataset containing 14 million image-mask pairs by annotating\\nforeground and background (potential 116 and 21 categories, respectively).\\nThen, we encode potential foreground and background representations separately\\nthrough two fixed-size embeddings, and intermittently inject these features\\ninto the denoising process via linear attention. Finally, by pre-training on\\nour dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.\\nExtensive experiments show that PixelHacker comprehensively outperforms the\\nSOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits\\nremarkable consistency in both structure and semantics. Project page at\\nhttps://hustvl.github.io/PixelHacker.', 'upvotes': 23, 'discussionId': '6814e35c19162d7749852caa', 'projectPage': 'https://hustvl.github.io/PixelHacker', 'githubRepo': 'https://github.com/hustvl/PixelHacker', 'ai_keywords': ['latent categories guidance', 'diffusion-based model', 'PixelHacker', 'image-mask pairs', 'fixed-size embeddings', 'linear attention', 'pre-training', 'fine-tuning', 'Places2', 'CelebA-HQ', 'FFHQ']}, 'publishedAt': '2025-04-29T01:28:36.000Z', 'title': 'PixelHacker: Image Inpainting with Structural and Semantic Consistency', 'summary': 'Image inpainting is a fundamental research area between image editing and\\nimage generation. Recent state-of-the-art (SOTA) methods have explored novel\\nattention mechanisms, lightweight architectures, and context-aware modeling,\\ndemonstrating impressive performance. However, they often struggle with complex\\nstructure (e.g., texture, shape, spatial relations) and semantics (e.g., color\\nconsistency, object restoration, and logical correctness), leading to artifacts\\nand inappropriate generation. To address this challenge, we design a simple yet\\neffective inpainting paradigm called latent categories guidance, and further\\npropose a diffusion-based model named PixelHacker. Specifically, we first\\nconstruct a large dataset containing 14 million image-mask pairs by annotating\\nforeground and background (potential 116 and 21 categories, respectively).\\nThen, we encode potential foreground and background representations separately\\nthrough two fixed-size embeddings, and intermittently inject these features\\ninto the denoising process via linear attention. Finally, by pre-training on\\nour dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.\\nExtensive experiments show that PixelHacker comprehensively outperforms the\\nSOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits\\nremarkable consistency in both structure and semantics. Project page at\\nhttps://hustvl.github.io/PixelHacker.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20438.png', 'numComments': 4, 'submittedBy': {'_id': '633d4630e1aec4b8b33ad5b8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/633d4630e1aec4b8b33ad5b8/owFqnyCMW8yuo9VcjA1hx.jpeg', 'fullname': 'Ziyang Xu', 'name': 'Uyoung', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.01079', 'authors': [{'_id': '68183d9ae65ec5d5716c6d94', 'user': {'_id': '636b20591340f879a2eb98d0', 'avatarUrl': '/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg', 'isPro': False, 'fullname': 'Daneul Kim', 'user': 'carpedkm', 'type': 'user'}, 'name': 'Daneul Kim', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-05T07:30:39.678Z', 'hidden': False}, {'_id': '68183d9ae65ec5d5716c6d95', 'name': 'Jaeah Lee', 'hidden': False}, {'_id': '68183d9ae65ec5d5716c6d96', 'name': 'Jaesik Park', 'hidden': False}], 'publishedAt': '2025-05-02T07:36:49.000Z', 'submittedOnDailyAt': '2025-05-05T02:56:01.277Z', 'title': 'Improving Editability in Image Generation with Layer-wise Memory', 'submittedOnDailyBy': {'_id': '636b20591340f879a2eb98d0', 'avatarUrl': '/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg', 'isPro': False, 'fullname': 'Daneul Kim', 'user': 'carpedkm', 'type': 'user'}, 'summary': 'Most real-world image editing tasks require multiple sequential edits to\\nachieve desired results. Current editing approaches, primarily designed for\\nsingle-object modifications, struggle with sequential editing: especially with\\nmaintaining previous edits along with adapting new objects naturally into the\\nexisting content. These limitations significantly hinder complex editing\\nscenarios where multiple objects need to be modified while preserving their\\ncontextual relationships. We address this fundamental challenge through two key\\nproposals: enabling rough mask inputs that preserve existing content while\\nnaturally integrating new elements and supporting consistent editing across\\nmultiple modifications. Our framework achieves this through layer-wise memory,\\nwhich stores latent representations and prompt embeddings from previous edits.\\nWe propose Background Consistency Guidance that leverages memorized latents to\\nmaintain scene coherence and Multi-Query Disentanglement in cross-attention\\nthat ensures natural adaptation to existing content. To evaluate our method, we\\npresent a new benchmark dataset incorporating semantic alignment metrics and\\ninteractive editing scenarios. Through comprehensive experiments, we\\ndemonstrate superior performance in iterative image editing tasks with minimal\\nuser effort, requiring only rough masks while maintaining high-quality results\\nthroughout multiple editing steps.', 'upvotes': 17, 'discussionId': '68183d9de65ec5d5716c6e78', 'projectPage': 'https://carpedkm.github.io/projects/improving_edit/index.html', 'githubRepo': 'https://github.com/carpedkm/improving-editability', 'ai_keywords': ['layer-wise memory', 'latent representations', 'prompt embeddings', 'Background Consistency Guidance', 'Multi-Query Disentanglement', 'cross-attention', 'semantic alignment metrics', 'interactive editing scenarios', 'iterative image editing']}, 'publishedAt': '2025-05-02T03:36:49.000Z', 'title': 'Improving Editability in Image Generation with Layer-wise Memory', 'summary': 'Most real-world image editing tasks require multiple sequential edits to\\nachieve desired results. Current editing approaches, primarily designed for\\nsingle-object modifications, struggle with sequential editing: especially with\\nmaintaining previous edits along with adapting new objects naturally into the\\nexisting content. These limitations significantly hinder complex editing\\nscenarios where multiple objects need to be modified while preserving their\\ncontextual relationships. We address this fundamental challenge through two key\\nproposals: enabling rough mask inputs that preserve existing content while\\nnaturally integrating new elements and supporting consistent editing across\\nmultiple modifications. Our framework achieves this through layer-wise memory,\\nwhich stores latent representations and prompt embeddings from previous edits.\\nWe propose Background Consistency Guidance that leverages memorized latents to\\nmaintain scene coherence and Multi-Query Disentanglement in cross-attention\\nthat ensures natural adaptation to existing content. To evaluate our method, we\\npresent a new benchmark dataset incorporating semantic alignment metrics and\\ninteractive editing scenarios. Through comprehensive experiments, we\\ndemonstrate superior performance in iterative image editing tasks with minimal\\nuser effort, requiring only rough masks while maintaining high-quality results\\nthroughout multiple editing steps.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01079.png', 'numComments': 1, 'submittedBy': {'_id': '636b20591340f879a2eb98d0', 'avatarUrl': '/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg', 'fullname': 'Daneul Kim', 'name': 'carpedkm', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2504.21117', 'authors': [{'_id': '681889891bcdcf80a49d9ef8', 'user': {'_id': '6648d6fff7e9d961cfb57260', 'avatarUrl': '/avatars/37acf522f2383f7cdab7d3f2dc73f628.svg', 'isPro': False, 'fullname': 'Hanhua Hong', 'user': 'kou199024', 'type': 'user'}, 'name': 'Hanhua Hong', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-05T14:07:10.803Z', 'hidden': False}, {'_id': '681889891bcdcf80a49d9ef9', 'name': 'Chenghao Xiao', 'hidden': False}, {'_id': '681889891bcdcf80a49d9efa', 'user': {'_id': '60f313f4adf471cbdf8bb66a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60f313f4adf471cbdf8bb66a/5NJFqnldE_0fdE_mEvz9V.jpeg', 'isPro': False, 'fullname': 'Yang Wang', 'user': 'yangwang825', 'type': 'user'}, 'name': 'Yang Wang', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-05T09:48:58.639Z', 'hidden': False}, {'_id': '681889891bcdcf80a49d9efb', 'user': {'_id': '6399fecbdfc6c5bb8f164073', 'avatarUrl': '/avatars/e63ac1a4de53d1fe3e039d0e9dff5c25.svg', 'isPro': False, 'fullname': 'Yiqi Liu', 'user': 'Yliu566', 'type': 'user'}, 'name': 'Yiqi Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-05T14:07:07.758Z', 'hidden': False}, {'_id': '681889891bcdcf80a49d9efc', 'name': 'Wenge Rong', 'hidden': False}, {'_id': '681889891bcdcf80a49d9efd', 'name': 'Chenghua Lin', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63108cc834c7d77420b0fd68/6pjQShgH8JjsuyXvDdwv1.png'], 'publishedAt': '2025-04-29T18:56:12.000Z', 'submittedOnDailyAt': '2025-05-05T08:29:07.779Z', 'title': 'Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\\n  Evaluation Prompts', 'submittedOnDailyBy': {'_id': '63108cc834c7d77420b0fd68', 'avatarUrl': '/avatars/2721e573a417a8ec0b81ee048c4b42ba.svg', 'isPro': False, 'fullname': 'chenghao xiao', 'user': 'gowitheflow', 'type': 'user'}, 'summary': 'Evaluating natural language generation (NLG) systems is challenging due to\\nthe diversity of valid outputs. While human evaluation is the gold standard, it\\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\\nlimiting reproducibility. LLM-based evaluation offers a scalable alternative\\nbut is highly sensitive to prompt design, where small variations can lead to\\nsignificant discrepancies. In this work, we propose an inversion learning\\nmethod that learns effective reverse mappings from model outputs back to their\\ninput instructions, enabling the automatic generation of highly effective,\\nmodel-specific evaluation prompts. Our method requires only a single evaluation\\nsample and eliminates the need for time-consuming manual prompt engineering,\\nthereby improving both efficiency and robustness. Our work contributes toward a\\nnew direction for more robust and efficient LLM-based evaluation.', 'upvotes': 11, 'discussionId': '6818898a1bcdcf80a49d9f28', 'ai_keywords': ['natural language generation (NLG)', 'human evaluation', 'LLM-based evaluation', 'prompt design', 'inversion learning', 'reverse mappings', 'model outputs', 'input instructions', 'automatic generation', 'model-specific evaluation prompts', 'evaluation sample', 'manual prompt engineering']}, 'publishedAt': '2025-04-29T14:56:12.000Z', 'title': 'Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\\n  Evaluation Prompts', 'summary': 'Evaluating natural language generation (NLG) systems is challenging due to\\nthe diversity of valid outputs. While human evaluation is the gold standard, it\\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\\nlimiting reproducibility. LLM-based evaluation offers a scalable alternative\\nbut is highly sensitive to prompt design, where small variations can lead to\\nsignificant discrepancies. In this work, we propose an inversion learning\\nmethod that learns effective reverse mappings from model outputs back to their\\ninput instructions, enabling the automatic generation of highly effective,\\nmodel-specific evaluation prompts. Our method requires only a single evaluation\\nsample and eliminates the need for time-consuming manual prompt engineering,\\nthereby improving both efficiency and robustness. Our work contributes toward a\\nnew direction for more robust and efficient LLM-based evaluation.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63108cc834c7d77420b0fd68/6pjQShgH8JjsuyXvDdwv1.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21117.png', 'numComments': 2, 'submittedBy': {'_id': '63108cc834c7d77420b0fd68', 'avatarUrl': '/avatars/2721e573a417a8ec0b81ee048c4b42ba.svg', 'fullname': 'chenghao xiao', 'name': 'gowitheflow', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 17}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.00949', 'authors': [{'_id': '681885e585df02e13b44d3f1', 'name': 'Akhiad Bercovich', 'hidden': False}, {'_id': '681885e585df02e13b44d3f2', 'name': 'Itay Levy', 'hidden': False}, {'_id': '681885e585df02e13b44d3f3', 'name': 'Izik Golan', 'hidden': False}, {'_id': '681885e585df02e13b44d3f4', 'name': 'Mohammad Dabbah', 'hidden': False}, {'_id': '681885e585df02e13b44d3f5', 'name': 'Ran El-Yaniv', 'hidden': False}, {'_id': '681885e585df02e13b44d3f6', 'name': 'Omri Puny', 'hidden': False}, {'_id': '681885e585df02e13b44d3f7', 'name': 'Ido Galil', 'hidden': False}, {'_id': '681885e585df02e13b44d3f8', 'name': 'Zach Moshe', 'hidden': False}, {'_id': '681885e585df02e13b44d3f9', 'name': 'Tomer Ronen', 'hidden': False}, {'_id': '681885e585df02e13b44d3fa', 'name': 'Najeeb Nabwani', 'hidden': False}, {'_id': '681885e585df02e13b44d3fb', 'name': 'Ido Shahaf', 'hidden': False}, {'_id': '681885e585df02e13b44d3fc', 'name': 'Oren Tropp', 'hidden': False}, {'_id': '681885e585df02e13b44d3fd', 'name': 'Ehud Karpas', 'hidden': False}, {'_id': '681885e585df02e13b44d3fe', 'name': 'Ran Zilberstein', 'hidden': False}, {'_id': '681885e585df02e13b44d3ff', 'name': 'Jiaqi Zeng', 'hidden': False}, {'_id': '681885e585df02e13b44d400', 'name': 'Soumye Singhal', 'hidden': False}, {'_id': '681885e585df02e13b44d401', 'name': 'Alexander Bukharin', 'hidden': False}, {'_id': '681885e585df02e13b44d402', 'name': 'Yian Zhang', 'hidden': False}, {'_id': '681885e585df02e13b44d403', 'name': 'Tugrul Konuk', 'hidden': False}, {'_id': '681885e585df02e13b44d404', 'name': 'Gerald Shen', 'hidden': False}, {'_id': '681885e585df02e13b44d405', 'name': 'Ameya Sunil Mahabaleshwarkar', 'hidden': False}, {'_id': '681885e585df02e13b44d406', 'name': 'Bilal Kartal', 'hidden': False}, {'_id': '681885e585df02e13b44d407', 'name': 'Yoshi Suhara', 'hidden': False}, {'_id': '681885e585df02e13b44d408', 'name': 'Olivier Delalleau', 'hidden': False}, {'_id': '681885e585df02e13b44d409', 'name': 'Zijia Chen', 'hidden': False}, {'_id': '681885e585df02e13b44d40a', 'name': 'Zhilin Wang', 'hidden': False}, {'_id': '681885e585df02e13b44d40b', 'name': 'David Mosallanezhad', 'hidden': False}, {'_id': '681885e585df02e13b44d40c', 'name': 'Adi Renduchintala', 'hidden': False}, {'_id': '681885e585df02e13b44d40d', 'name': 'Haifeng Qian', 'hidden': False}, {'_id': '681885e585df02e13b44d40e', 'name': 'Dima Rekesh', 'hidden': False}, {'_id': '681885e585df02e13b44d40f', 'name': 'Fei Jia', 'hidden': False}, {'_id': '681885e585df02e13b44d410', 'name': 'Somshubra Majumdar', 'hidden': False}, {'_id': '681885e585df02e13b44d411', 'name': 'Vahid Noroozi', 'hidden': False}, {'_id': '681885e585df02e13b44d412', 'name': 'Wasi Uddin Ahmad', 'hidden': False}, {'_id': '681885e585df02e13b44d413', 'name': 'Sean Narenthiran', 'hidden': False}, {'_id': '681885e585df02e13b44d414', 'name': 'Aleksander Ficek', 'hidden': False}, {'_id': '681885e585df02e13b44d415', 'name': 'Mehrzad Samadi', 'hidden': False}, {'_id': '681885e585df02e13b44d416', 'name': 'Jocelyn Huang', 'hidden': False}, {'_id': '681885e585df02e13b44d417', 'name': 'Siddhartha Jain', 'hidden': False}, {'_id': '681885e585df02e13b44d418', 'name': 'Igor Gitman', 'hidden': False}, {'_id': '681885e585df02e13b44d419', 'name': 'Ivan Moshkov', 'hidden': False}, {'_id': '681885e585df02e13b44d41a', 'name': 'Wei Du', 'hidden': False}, {'_id': '681885e585df02e13b44d41b', 'name': 'Shubham Toshniwal', 'hidden': False}, {'_id': '681885e585df02e13b44d41c', 'name': 'George Armstrong', 'hidden': False}, {'_id': '681885e585df02e13b44d41d', 'name': 'Branislav Kisacanin', 'hidden': False}, {'_id': '681885e585df02e13b44d41e', 'name': 'Matvei Novikov', 'hidden': False}, {'_id': '681885e585df02e13b44d41f', 'name': 'Daria Gitman', 'hidden': False}, {'_id': '681885e585df02e13b44d420', 'name': 'Evelina Bakhturina', 'hidden': False}, {'_id': '681885e585df02e13b44d421', 'name': 'Jane Polak Scowcroft', 'hidden': False}, {'_id': '681885e585df02e13b44d422', 'name': 'John Kamalu', 'hidden': False}, {'_id': '681885e585df02e13b44d423', 'name': 'Dan Su', 'hidden': False}, {'_id': '681885e585df02e13b44d424', 'name': 'Kezhi Kong', 'hidden': False}, {'_id': '681885e585df02e13b44d425', 'name': 'Markus Kliegl', 'hidden': False}, {'_id': '681885e585df02e13b44d426', 'name': 'Rabeeh Karimi', 'hidden': False}, {'_id': '681885e585df02e13b44d427', 'name': 'Ying Lin', 'hidden': False}, {'_id': '681885e585df02e13b44d428', 'name': 'Sanjeev Satheesh', 'hidden': False}, {'_id': '681885e585df02e13b44d429', 'name': 'Jupinder Parmar', 'hidden': False}, {'_id': '681885e585df02e13b44d42a', 'name': 'Pritam Gundecha', 'hidden': False}, {'_id': '681885e585df02e13b44d42b', 'name': 'Brandon Norick', 'hidden': False}, {'_id': '681885e585df02e13b44d42c', 'name': 'Joseph Jennings', 'hidden': False}, {'_id': '681885e585df02e13b44d42d', 'name': 'Shrimai Prabhumoye', 'hidden': False}, {'_id': '681885e585df02e13b44d42e', 'name': 'Syeda Nahida Akter', 'hidden': False}, {'_id': '681885e585df02e13b44d42f', 'name': 'Mostofa Patwary', 'hidden': False}, {'_id': '681885e585df02e13b44d430', 'name': 'Abhinav Khattar', 'hidden': False}, {'_id': '681885e585df02e13b44d431', 'name': 'Deepak Narayanan', 'hidden': False}, {'_id': '681885e585df02e13b44d432', 'name': 'Roger Waleffe', 'hidden': False}, {'_id': '681885e585df02e13b44d433', 'name': 'Jimmy Zhang', 'hidden': False}, {'_id': '681885e585df02e13b44d434', 'name': 'Bor-Yiing Su', 'hidden': False}, {'_id': '681885e585df02e13b44d435', 'name': 'Guyue Huang', 'hidden': False}, {'_id': '681885e585df02e13b44d436', 'name': 'Terry Kong', 'hidden': False}, {'_id': '681885e585df02e13b44d437', 'name': 'Parth Chadha', 'hidden': False}, {'_id': '681885e585df02e13b44d438', 'name': 'Sahil Jain', 'hidden': False}, {'_id': '681885e585df02e13b44d439', 'name': 'Christine Harvey', 'hidden': False}, {'_id': '681885e585df02e13b44d43a', 'name': 'Elad Segal', 'hidden': False}, {'_id': '681885e585df02e13b44d43b', 'name': 'Jining Huang', 'hidden': False}, {'_id': '681885e585df02e13b44d43c', 'name': 'Sergey Kashirsky', 'hidden': False}, {'_id': '681885e585df02e13b44d43d', 'name': 'Robert McQueen', 'hidden': False}, {'_id': '681885e585df02e13b44d43e', 'name': 'Izzy Putterman', 'hidden': False}, {'_id': '681885e585df02e13b44d43f', 'name': 'George Lam', 'hidden': False}, {'_id': '681885e585df02e13b44d440', 'name': 'Arun Venkatesan', 'hidden': False}, {'_id': '681885e585df02e13b44d441', 'name': 'Sherry Wu', 'hidden': False}, {'_id': '681885e585df02e13b44d442', 'name': 'Vinh Nguyen', 'hidden': False}, {'_id': '681885e585df02e13b44d443', 'name': 'Manoj Kilaru', 'hidden': False}, {'_id': '681885e585df02e13b44d444', 'name': 'Andrew Wang', 'hidden': False}, {'_id': '681885e585df02e13b44d445', 'name': 'Anna Warno', 'hidden': False}, {'_id': '681885e585df02e13b44d446', 'name': 'Abhilash Somasamudramath', 'hidden': False}, {'_id': '681885e585df02e13b44d447', 'name': 'Sandip Bhaskar', 'hidden': False}, {'_id': '681885e585df02e13b44d448', 'name': 'Maka Dong', 'hidden': False}, {'_id': '681885e585df02e13b44d449', 'name': 'Nave Assaf', 'hidden': False}, {'_id': '681885e585df02e13b44d44a', 'name': 'Shahar Mor', 'hidden': False}, {'_id': '681885e585df02e13b44d44b', 'name': 'Omer Ullman Argov', 'hidden': False}, {'_id': '681885e585df02e13b44d44c', 'name': 'Scot Junkin', 'hidden': False}, {'_id': '681885e585df02e13b44d44d', 'name': 'Oleksandr Romanenko', 'hidden': False}, {'_id': '681885e585df02e13b44d44e', 'name': 'Pedro Larroy', 'hidden': False}, {'_id': '681885e585df02e13b44d44f', 'name': 'Monika Katariya', 'hidden': False}, {'_id': '681885e585df02e13b44d450', 'name': 'Marco Rovinelli', 'hidden': False}, {'_id': '681885e585df02e13b44d451', 'name': 'Viji Balas', 'hidden': False}, {'_id': '681885e585df02e13b44d452', 'name': 'Nicholas Edelman', 'hidden': False}, {'_id': '681885e585df02e13b44d453', 'name': 'Anahita Bhiwandiwalla', 'hidden': False}, {'_id': '681885e585df02e13b44d454', 'name': 'Muthu Subramaniam', 'hidden': False}, {'_id': '681885e585df02e13b44d455', 'name': 'Smita Ithape', 'hidden': False}, {'_id': '681885e585df02e13b44d456', 'name': 'Karthik Ramamoorthy', 'hidden': False}, {'_id': '681885e585df02e13b44d457', 'name': 'Yuting Wu', 'hidden': False}, {'_id': '681885e585df02e13b44d458', 'name': 'Suguna Varshini Velury', 'hidden': False}, {'_id': '681885e585df02e13b44d459', 'name': 'Omri Almog', 'hidden': False}, {'_id': '681885e585df02e13b44d45a', 'name': 'Joyjit Daw', 'hidden': False}, {'_id': '681885e585df02e13b44d45b', 'name': 'Denys Fridman', 'hidden': False}, {'_id': '681885e585df02e13b44d45c', 'name': 'Erick Galinkin', 'hidden': False}, {'_id': '681885e585df02e13b44d45d', 'name': 'Michael Evans', 'hidden': False}, {'_id': '681885e585df02e13b44d45e', 'name': 'Katherine Luna', 'hidden': False}, {'_id': '681885e585df02e13b44d45f', 'name': 'Leon Derczynski', 'hidden': False}, {'_id': '681885e585df02e13b44d460', 'name': 'Nikki Pope', 'hidden': False}, {'_id': '681885e585df02e13b44d461', 'name': 'Eileen Long', 'hidden': False}, {'_id': '681885e585df02e13b44d462', 'name': 'Seth Schneider', 'hidden': False}, {'_id': '681885e585df02e13b44d463', 'name': 'Guillermo Siman', 'hidden': False}, {'_id': '681885e585df02e13b44d464', 'name': 'Tomasz Grzegorzek', 'hidden': False}, {'_id': '681885e585df02e13b44d465', 'name': 'Pablo Ribalta', 'hidden': False}, {'_id': '681885e585df02e13b44d466', 'name': 'Monika Katariya', 'hidden': False}, {'_id': '681885e585df02e13b44d467', 'name': 'Joey Conway', 'hidden': False}, {'_id': '681885e585df02e13b44d468', 'name': 'Trisha Saar', 'hidden': False}, {'_id': '681885e585df02e13b44d469', 'name': 'Ann Guan', 'hidden': False}, {'_id': '681885e585df02e13b44d46a', 'name': 'Krzysztof Pawelec', 'hidden': False}, {'_id': '681885e585df02e13b44d46b', 'name': 'Shyamala Prayaga', 'hidden': False}, {'_id': '681885e585df02e13b44d46c', 'name': 'Oleksii Kuchaiev', 'hidden': False}, {'_id': '681885e585df02e13b44d46d', 'name': 'Boris Ginsburg', 'hidden': False}, {'_id': '681885e585df02e13b44d46e', 'name': 'Oluwatobi Olabiyi', 'hidden': False}, {'_id': '681885e585df02e13b44d46f', 'name': 'Kari Briski', 'hidden': False}, {'_id': '681885e585df02e13b44d470', 'name': 'Jonathan Cohen', 'hidden': False}, {'_id': '681885e585df02e13b44d471', 'name': 'Bryan Catanzaro', 'hidden': False}, {'_id': '681885e585df02e13b44d472', 'name': 'Jonah Alben', 'hidden': False}, {'_id': '681885e585df02e13b44d473', 'name': 'Yonatan Geifman', 'hidden': False}, {'_id': '681885e585df02e13b44d474', 'name': 'Eric Chung', 'hidden': False}], 'publishedAt': '2025-05-02T01:35:35.000Z', 'submittedOnDailyAt': '2025-05-05T08:04:15.299Z', 'title': 'Llama-Nemotron: Efficient Reasoning Models', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': 'We introduce the Llama-Nemotron series of models, an open family of\\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\\ninference efficiency, and an open license for enterprise use. The family comes\\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\\noffering superior inference throughput and memory efficiency. In this report,\\nwe discuss the training procedure for these models, which entails using neural\\narchitecture search from Llama 3 models for accelerated inference, knowledge\\ndistillation, and continued pretraining, followed by a reasoning-focused\\npost-training stage consisting of two main parts: supervised fine-tuning and\\nlarge scale reinforcement learning. Llama-Nemotron models are the first\\nopen-source models to support a dynamic reasoning toggle, allowing users to\\nswitch between standard chat and reasoning modes during inference. To further\\nsupport open research and facilitate model development, we provide the\\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\\nOpen Model License Agreement. 2. We release the complete post-training dataset:\\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.', 'upvotes': 7, 'discussionId': '681885e685df02e13b44d4b1', 'ai_keywords': ['neural architecture search', 'knowledge distillation', 'supervised fine-tuning', 'reinforcement learning', 'dynamic reasoning toggle', 'NVIDIA Open Model License Agreement']}, 'publishedAt': '2025-05-01T21:35:35.000Z', 'title': 'Llama-Nemotron: Efficient Reasoning Models', 'summary': 'We introduce the Llama-Nemotron series of models, an open family of\\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\\ninference efficiency, and an open license for enterprise use. The family comes\\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\\noffering superior inference throughput and memory efficiency. In this report,\\nwe discuss the training procedure for these models, which entails using neural\\narchitecture search from Llama 3 models for accelerated inference, knowledge\\ndistillation, and continued pretraining, followed by a reasoning-focused\\npost-training stage consisting of two main parts: supervised fine-tuning and\\nlarge scale reinforcement learning. Llama-Nemotron models are the first\\nopen-source models to support a dynamic reasoning toggle, allowing users to\\nswitch between standard chat and reasoning modes during inference. To further\\nsupport open research and facilitate model development, we provide the\\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\\nOpen Model License Agreement. 2. We release the complete post-training dataset:\\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00949.png', 'numComments': 2, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6780}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2505.00023', 'authors': [{'_id': '6818413000ee590453feaf66', 'user': {'_id': '65169ea3fbfe82f36fc6655c', 'avatarUrl': '/avatars/01714ad316a2e06488246e4fe7dcdb52.svg', 'isPro': False, 'fullname': 'Hyun Ji Lee', 'user': 'hyunjilee', 'type': 'user'}, 'name': 'Hyunji Lee', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-05T07:34:41.006Z', 'hidden': False}, {'_id': '6818413000ee590453feaf67', 'user': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'isPro': False, 'fullname': 'Franck Dernoncourt', 'user': 'Franck-Dernoncourt', 'type': 'user'}, 'name': 'Franck Dernoncourt', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-05T07:30:36.382Z', 'hidden': False}, {'_id': '6818413000ee590453feaf68', 'name': 'Trung Bui', 'hidden': False}, {'_id': '6818413000ee590453feaf69', 'user': {'_id': '6690ef3db70d356ed3e05cb0', 'avatarUrl': '/avatars/530f3a0bd7b93e1e7f385c2708335728.svg', 'isPro': False, 'fullname': 'yoon seung-hyun', 'user': 'aifactoryysh', 'type': 'user'}, 'name': 'Seunghyun Yoon', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-05T07:35:01.378Z', 'hidden': False}], 'publishedAt': '2025-04-25T02:40:48.000Z', 'submittedOnDailyAt': '2025-05-05T03:10:26.249Z', 'title': 'CORG: Generating Answers from Complex, Interrelated Contexts', 'submittedOnDailyBy': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'isPro': False, 'fullname': 'Franck Dernoncourt', 'user': 'Franck-Dernoncourt', 'type': 'user'}, 'summary': 'In a real-world corpus, knowledge frequently recurs across documents but\\noften contains inconsistencies due to ambiguous naming, outdated information,\\nor errors, leading to complex interrelationships between contexts. Previous\\nresearch has shown that language models struggle with these complexities,\\ntypically focusing on single factors in isolation. We classify these\\nrelationships into four types: distracting, ambiguous, counterfactual, and\\nduplicated. Our analysis reveals that no single approach effectively addresses\\nall these interrelationships simultaneously. Therefore, we introduce Context\\nOrganizer (CORG), a framework that organizes multiple contexts into\\nindependently processed groups. This design allows the model to efficiently\\nfind all relevant answers while ensuring disambiguation. CORG consists of three\\nkey components: a graph constructor, a reranker, and an aggregator. Our results\\ndemonstrate that CORG balances performance and efficiency effectively,\\noutperforming existing grouping methods and achieving comparable results to\\nmore computationally intensive, single-context approaches.', 'upvotes': 5, 'discussionId': '6818413100ee590453feaf97', 'ai_keywords': ['graph constructor', 'reranker', 'aggregator', 'Context Organizer (CORG)']}, 'publishedAt': '2025-04-24T22:40:48.000Z', 'title': 'CORG: Generating Answers from Complex, Interrelated Contexts', 'summary': 'In a real-world corpus, knowledge frequently recurs across documents but\\noften contains inconsistencies due to ambiguous naming, outdated information,\\nor errors, leading to complex interrelationships between contexts. Previous\\nresearch has shown that language models struggle with these complexities,\\ntypically focusing on single factors in isolation. We classify these\\nrelationships into four types: distracting, ambiguous, counterfactual, and\\nduplicated. Our analysis reveals that no single approach effectively addresses\\nall these interrelationships simultaneously. Therefore, we introduce Context\\nOrganizer (CORG), a framework that organizes multiple contexts into\\nindependently processed groups. This design allows the model to efficiently\\nfind all relevant answers while ensuring disambiguation. CORG consists of three\\nkey components: a graph constructor, a reranker, and an aggregator. Our results\\ndemonstrate that CORG balances performance and efficiency effectively,\\noutperforming existing grouping methods and achieving comparable results to\\nmore computationally intensive, single-context approaches.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00023.png', 'numComments': 1, 'submittedBy': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'fullname': 'Franck Dernoncourt', 'name': 'Franck-Dernoncourt', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2505.00174', 'authors': [{'_id': '681594746a80babbe28775ba', 'user': {'_id': '66225f7100352aeea584d02a', 'avatarUrl': '/avatars/ca13f59bebf73d03a63a935f628aea5c.svg', 'isPro': False, 'fullname': 'Ilan Strauss', 'user': 'strauss-NYC', 'type': 'user'}, 'name': 'Ilan Strauss', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-04T10:04:25.496Z', 'hidden': False}, {'_id': '681594746a80babbe28775bb', 'user': {'_id': '67535114d2a628475a0e7a6e', 'avatarUrl': '/avatars/d7eb574c026817bbc204843c96f1caa6.svg', 'isPro': False, 'fullname': 'Isobel Moure', 'user': 'isobelmoure', 'type': 'user'}, 'name': 'Isobel Moure', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-05T07:35:12.655Z', 'hidden': False}, {'_id': '681594746a80babbe28775bc', 'name': \"Tim O'Reilly\", 'hidden': False}, {'_id': '681594746a80babbe28775bd', 'user': {'_id': '64582e94f8bdc3512d1ee940', 'avatarUrl': '/avatars/e0eb72f06c7da58cb569198540484ab1.svg', 'isPro': False, 'fullname': 'Sruly Rosenblat', 'user': 'sruly', 'type': 'user'}, 'name': 'Sruly Rosenblat', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-05T07:35:24.305Z', 'hidden': False}], 'publishedAt': '2025-04-30T20:44:42.000Z', 'submittedOnDailyAt': '2025-05-05T02:50:33.572Z', 'title': 'Real-World Gaps in AI Governance Research', 'submittedOnDailyBy': {'_id': '66225f7100352aeea584d02a', 'avatarUrl': '/avatars/ca13f59bebf73d03a63a935f628aea5c.svg', 'isPro': False, 'fullname': 'Ilan Strauss', 'user': 'strauss-NYC', 'type': 'user'}, 'summary': 'Drawing on 1,178 safety and reliability papers from 9,439 generative AI\\npapers (January 2020 - March 2025), we compare research outputs of leading AI\\ncompanies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI\\nuniversities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of\\nWashington). We find that corporate AI research increasingly concentrates on\\npre-deployment areas -- model alignment and testing & evaluation -- while\\nattention to deployment-stage issues such as model bias has waned. Significant\\nresearch gaps exist in high-risk deployment domains, including healthcare,\\nfinance, misinformation, persuasive and addictive features, hallucinations, and\\ncopyright. Without improved observability into deployed AI, growing corporate\\nconcentration could deepen knowledge deficits. We recommend expanding external\\nresearcher access to deployment data and systematic observability of in-market\\nAI behaviors.', 'upvotes': 4, 'discussionId': '681594746a80babbe28775e5'}, 'publishedAt': '2025-04-30T16:44:42.000Z', 'title': 'Real-World Gaps in AI Governance Research', 'summary': 'Drawing on 1,178 safety and reliability papers from 9,439 generative AI\\npapers (January 2020 - March 2025), we compare research outputs of leading AI\\ncompanies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI\\nuniversities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of\\nWashington). We find that corporate AI research increasingly concentrates on\\npre-deployment areas -- model alignment and testing & evaluation -- while\\nattention to deployment-stage issues such as model bias has waned. Significant\\nresearch gaps exist in high-risk deployment domains, including healthcare,\\nfinance, misinformation, persuasive and addictive features, hallucinations, and\\ncopyright. Without improved observability into deployed AI, growing corporate\\nconcentration could deepen knowledge deficits. We recommend expanding external\\nresearcher access to deployment data and systematic observability of in-market\\nAI behaviors.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00174.png', 'numComments': 1, 'submittedBy': {'_id': '66225f7100352aeea584d02a', 'avatarUrl': '/avatars/ca13f59bebf73d03a63a935f628aea5c.svg', 'fullname': 'Ilan Strauss', 'name': 'strauss-NYC', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.00562', 'authors': [{'_id': '68184c3fb727bc3cb7301e15', 'user': {'_id': '668e100b97171f3399e07f5d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg', 'isPro': False, 'fullname': 'Yue Meng', 'user': 'yuemithucsd', 'type': 'user'}, 'name': 'Yue Meng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-05T07:36:14.168Z', 'hidden': False}, {'_id': '68184c3fb727bc3cb7301e16', 'name': 'Chuchu Fan', 'hidden': False}], 'publishedAt': '2025-05-01T14:40:07.000Z', 'submittedOnDailyAt': '2025-05-05T03:58:02.420Z', 'title': 'TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching', 'submittedOnDailyBy': {'_id': '668e100b97171f3399e07f5d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg', 'isPro': False, 'fullname': 'Yue Meng', 'user': 'yuemithucsd', 'type': 'user'}, 'summary': \"Learning to solve complex tasks with signal temporal logic (STL)\\nspecifications is crucial to many real-world applications. However, most\\nprevious works only consider fixed or parametrized STL specifications due to\\nthe lack of a diverse STL dataset and encoders to effectively extract temporal\\nlogic information for downstream tasks. In this paper, we propose TeLoGraF,\\nTemporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN)\\nencoder and flow-matching to learn solutions for general STL specifications. We\\nidentify four commonly used STL templates and collect a total of 200K\\nspecifications with paired demonstrations. We conduct extensive experiments in\\nfive simulation environments ranging from simple dynamical models in the 2D\\nspace to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped\\nnavigation. Results show that our method outperforms other baselines in the STL\\nsatisfaction rate. Compared to classical STL planning algorithms, our approach\\nis 10-100X faster in inference and can work on any system dynamics. Besides, we\\nshow our graph-encoding method's capability to solve complex STLs and\\nrobustness to out-distribution STL specifications. Code is available at\\nhttps://github.com/mengyuest/TeLoGraF\", 'upvotes': 2, 'discussionId': '68184c41b727bc3cb7301e6c', 'ai_keywords': ['TeLoGraF', 'Temporal Logic Graph-encoded Flow', 'Graph Neural Networks (GNN)', 'flow-matching', 'STL specifications', 'STL templates', 'STL satisfaction rate', 'dynamical models', 'Franka Panda robot arm', 'Ant quadruped navigation', 'classical STL planning algorithms']}, 'publishedAt': '2025-05-01T10:40:07.000Z', 'title': 'TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching', 'summary': \"Learning to solve complex tasks with signal temporal logic (STL)\\nspecifications is crucial to many real-world applications. However, most\\nprevious works only consider fixed or parametrized STL specifications due to\\nthe lack of a diverse STL dataset and encoders to effectively extract temporal\\nlogic information for downstream tasks. In this paper, we propose TeLoGraF,\\nTemporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN)\\nencoder and flow-matching to learn solutions for general STL specifications. We\\nidentify four commonly used STL templates and collect a total of 200K\\nspecifications with paired demonstrations. We conduct extensive experiments in\\nfive simulation environments ranging from simple dynamical models in the 2D\\nspace to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped\\nnavigation. Results show that our method outperforms other baselines in the STL\\nsatisfaction rate. Compared to classical STL planning algorithms, our approach\\nis 10-100X faster in inference and can work on any system dynamics. Besides, we\\nshow our graph-encoding method's capability to solve complex STLs and\\nrobustness to out-distribution STL specifications. Code is available at\\nhttps://github.com/mengyuest/TeLoGraF\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00562.png', 'numComments': 1, 'submittedBy': {'_id': '668e100b97171f3399e07f5d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg', 'fullname': 'Yue Meng', 'name': 'yuemithucsd', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2504.20859', 'authors': [{'_id': '681713aec075e49c1b22500e', 'user': {'_id': '630d180f3dc31beba6f061c3', 'avatarUrl': '/avatars/7cf70bff453b6e64fcac52f45c6b3730.svg', 'isPro': False, 'fullname': 'guy hadad', 'user': 'guyhadad01', 'type': 'user'}, 'name': 'Guy Hadad', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-05-04T07:14:56.074Z', 'hidden': False}, {'_id': '681713aec075e49c1b22500f', 'name': 'Haggai Roitman', 'hidden': False}, {'_id': '681713aec075e49c1b225010', 'user': {'_id': '638f42e4c4444c6ca8715a06', 'avatarUrl': '/avatars/aae741d00ed1f5ead516c07543e59f3e.svg', 'isPro': False, 'fullname': 'yotam eshel', 'user': 'yeshel', 'type': 'user'}, 'name': 'Yotam Eshel', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-04T07:13:50.951Z', 'hidden': False}, {'_id': '681713aec075e49c1b225011', 'user': {'_id': '63aace84785b8279fe30b5f9', 'avatarUrl': '/avatars/7b4793f6f0a0a8b608d0395c0e92a7eb.svg', 'isPro': False, 'fullname': 'Bracha Shapira', 'user': 'Bshapira', 'type': 'user'}, 'name': 'Bracha Shapira', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-05T07:35:41.141Z', 'hidden': False}, {'_id': '681713aec075e49c1b225012', 'user': {'_id': '64141f0365f4b23aa99507a4', 'avatarUrl': '/avatars/46d599acaa0f492139949dba0f00e030.svg', 'isPro': False, 'fullname': 'Lior Rokach', 'user': 'liorrokach', 'type': 'user'}, 'name': 'Lior Rokach', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-05T07:35:47.274Z', 'hidden': False}], 'publishedAt': '2025-04-29T15:33:20.000Z', 'submittedOnDailyAt': '2025-05-05T03:47:16.624Z', 'title': 'X-Cross: Dynamic Integration of Language Models for Cross-Domain\\n  Sequential Recommendation', 'submittedOnDailyBy': {'_id': '630d180f3dc31beba6f061c3', 'avatarUrl': '/avatars/7cf70bff453b6e64fcac52f45c6b3730.svg', 'isPro': False, 'fullname': 'guy hadad', 'user': 'guyhadad01', 'type': 'user'}, 'summary': \"As new products are emerging daily, recommendation systems are required to\\nquickly adapt to possible new domains without needing extensive retraining.\\nThis work presents ``X-Cross'' -- a novel cross-domain\\nsequential-recommendation model that recommends products in new domains by\\nintegrating several domain-specific language models; each model is fine-tuned\\nwith low-rank adapters (LoRA). Given a recommendation prompt, operating layer\\nby layer, X-Cross dynamically refines the representation of each source\\nlanguage model by integrating knowledge from all other models. These refined\\nrepresentations are propagated from one layer to the next, leveraging the\\nactivations from each domain adapter to ensure domain-specific nuances are\\npreserved while enabling adaptability across domains. Using Amazon datasets for\\nsequential recommendation, X-Cross achieves performance comparable to a model\\nthat is fine-tuned with LoRA, while using only 25% of the additional\\nparameters. In cross-domain tasks, such as adapting from Toys domain to Tools,\\nElectronics or Sports, X-Cross demonstrates robust performance, while requiring\\nabout 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.\\nFurthermore, X-Cross achieves significant improvement in accuracy over\\nalternative cross-domain baselines. Overall, X-Cross enables scalable and\\nadaptive cross-domain recommendations, reducing computational overhead and\\nproviding an efficient solution for data-constrained environments.\", 'upvotes': 2, 'discussionId': '681713aec075e49c1b22503e', 'ai_keywords': ['cross-domain sequential-recommendation', 'domain-specific language models', 'low-rank adapters (LoRA)', 'recommendation prompt', 'activations', 'domain-specific nuances', 'cross-domain tasks', 'computational overhead', 'data-constrained environments']}, 'publishedAt': '2025-04-29T11:33:20.000Z', 'title': 'X-Cross: Dynamic Integration of Language Models for Cross-Domain\\n  Sequential Recommendation', 'summary': \"As new products are emerging daily, recommendation systems are required to\\nquickly adapt to possible new domains without needing extensive retraining.\\nThis work presents ``X-Cross'' -- a novel cross-domain\\nsequential-recommendation model that recommends products in new domains by\\nintegrating several domain-specific language models; each model is fine-tuned\\nwith low-rank adapters (LoRA). Given a recommendation prompt, operating layer\\nby layer, X-Cross dynamically refines the representation of each source\\nlanguage model by integrating knowledge from all other models. These refined\\nrepresentations are propagated from one layer to the next, leveraging the\\nactivations from each domain adapter to ensure domain-specific nuances are\\npreserved while enabling adaptability across domains. Using Amazon datasets for\\nsequential recommendation, X-Cross achieves performance comparable to a model\\nthat is fine-tuned with LoRA, while using only 25% of the additional\\nparameters. In cross-domain tasks, such as adapting from Toys domain to Tools,\\nElectronics or Sports, X-Cross demonstrates robust performance, while requiring\\nabout 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.\\nFurthermore, X-Cross achieves significant improvement in accuracy over\\nalternative cross-domain baselines. Overall, X-Cross enables scalable and\\nadaptive cross-domain recommendations, reducing computational overhead and\\nproviding an efficient solution for data-constrained environments.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20859.png', 'numComments': 1, 'submittedBy': {'_id': '630d180f3dc31beba6f061c3', 'avatarUrl': '/avatars/7cf70bff453b6e64fcac52f45c6b3730.svg', 'fullname': 'guy hadad', 'name': 'guyhadad01', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}"
]