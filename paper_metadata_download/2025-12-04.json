[
    "{'paper': {'id': '2511.21631', 'authors': [{'_id': '692ffb1a26742347f61daf38', 'user': {'_id': '63451cf0a05b51f7ded25505', 'avatarUrl': '/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg', 'isPro': False, 'fullname': 'shuai bai', 'user': 'ShuaiBai623', 'type': 'user'}, 'name': 'Shuai Bai', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:34:29.118Z', 'hidden': False}, {'_id': '692ffb1a26742347f61daf39', 'name': 'Yuxuan Cai', 'hidden': False}, {'_id': '692ffb1a26742347f61daf3a', 'name': 'Ruizhe Chen', 'hidden': False}, {'_id': '692ffb1a26742347f61daf3b', 'name': 'Keqin Chen', 'hidden': False}, {'_id': '692ffb1a26742347f61daf3c', 'user': {'_id': '63f30b870a16587ea970edfe', 'avatarUrl': '/avatars/b58ab2d8a85a6d83462c297de2714ce4.svg', 'isPro': False, 'fullname': 'Xiong-Hui Chen', 'user': 'xionghuichen', 'type': 'user'}, 'name': 'Xionghui Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:35:42.689Z', 'hidden': False}, {'_id': '692ffb1a26742347f61daf3d', 'user': {'_id': '65b2529285b6c21448a10d65', 'avatarUrl': '/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg', 'isPro': False, 'fullname': 'Zesen Cheng', 'user': 'ClownRat', 'type': 'user'}, 'name': 'Zesen Cheng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:35:51.365Z', 'hidden': False}, {'_id': '692ffb1a26742347f61daf3e', 'name': 'Lianghao Deng', 'hidden': False}, {'_id': '692ffb1a26742347f61daf3f', 'name': 'Wei Ding', 'hidden': False}, {'_id': '692ffb1a26742347f61daf40', 'name': 'Chang Gao', 'hidden': False}, {'_id': '692ffb1a26742347f61daf41', 'name': 'Chunjiang Ge', 'hidden': False}, {'_id': '692ffb1a26742347f61daf42', 'name': 'Wenbin Ge', 'hidden': False}, {'_id': '692ffb1a26742347f61daf43', 'name': 'Zhifang Guo', 'hidden': False}, {'_id': '692ffb1a26742347f61daf44', 'user': {'_id': '656f1b21b075b63c90ba02ee', 'avatarUrl': '/avatars/d6856815ef06261394178161e4d511b4.svg', 'isPro': False, 'fullname': 'Huang Qidong', 'user': 'shikiw', 'type': 'user'}, 'name': 'Qidong Huang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T08:48:49.065Z', 'hidden': False}, {'_id': '692ffb1a26742347f61daf45', 'name': 'Jie Huang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf46', 'name': 'Fei Huang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf47', 'name': 'Binyuan Hui', 'hidden': False}, {'_id': '692ffb1a26742347f61daf48', 'name': 'Shutong Jiang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf49', 'name': 'Zhaohai Li', 'hidden': False}, {'_id': '692ffb1a26742347f61daf4a', 'name': 'Mingsheng Li', 'hidden': False}, {'_id': '692ffb1a26742347f61daf4b', 'name': 'Mei Li', 'hidden': False}, {'_id': '692ffb1a26742347f61daf4c', 'user': {'_id': '6346be8f7fb9f11870c63998', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6346be8f7fb9f11870c63998/tFWawSkXL6bv1zgvzFWQd.png', 'isPro': False, 'fullname': 'Kaixin Li', 'user': 'likaixin', 'type': 'user'}, 'name': 'Kaixin Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T08:48:53.648Z', 'hidden': False}, {'_id': '692ffb1a26742347f61daf4d', 'user': {'_id': '67a31313cf9d856beb7f9afb', 'avatarUrl': '/avatars/69395b134716f750545eab35a164e51f.svg', 'isPro': False, 'fullname': 'Zicheng Lin', 'user': 'etonlin', 'type': 'user'}, 'name': 'Zicheng Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:36:50.803Z', 'hidden': False}, {'_id': '692ffb1a26742347f61daf4e', 'name': 'Junyang Lin', 'hidden': False}, {'_id': '692ffb1a26742347f61daf4f', 'name': 'Xuejing Liu', 'hidden': False}, {'_id': '692ffb1a26742347f61daf50', 'name': 'Jiawei Liu', 'hidden': False}, {'_id': '692ffb1a26742347f61daf51', 'name': 'Chenglong Liu', 'hidden': False}, {'_id': '692ffb1a26742347f61daf52', 'name': 'Yang Liu', 'hidden': False}, {'_id': '692ffb1a26742347f61daf53', 'name': 'Dayiheng Liu', 'hidden': False}, {'_id': '692ffb1a26742347f61daf54', 'user': {'_id': '64e72776e9fc9d0475ef5188', 'avatarUrl': '/avatars/d32b9d4e1da5486c3d5f9b04fa29d167.svg', 'isPro': False, 'fullname': 'Shixuan Liu', 'user': 'liusx', 'type': 'user'}, 'name': 'Shixuan Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T08:48:58.470Z', 'hidden': False}, {'_id': '692ffb1a26742347f61daf55', 'name': 'Dunjie Lu', 'hidden': False}, {'_id': '692ffb1a26742347f61daf56', 'name': 'Ruilin Luo', 'hidden': False}, {'_id': '692ffb1a26742347f61daf57', 'name': 'Chenxu Lv', 'hidden': False}, {'_id': '692ffb1a26742347f61daf58', 'name': 'Rui Men', 'hidden': False}, {'_id': '692ffb1a26742347f61daf59', 'name': 'Lingchen Meng', 'hidden': False}, {'_id': '692ffb1a26742347f61daf5a', 'name': 'Xuancheng Ren', 'hidden': False}, {'_id': '692ffb1a26742347f61daf5b', 'name': 'Xingzhang Ren', 'hidden': False}, {'_id': '692ffb1a26742347f61daf5c', 'name': 'Sibo Song', 'hidden': False}, {'_id': '692ffb1a26742347f61daf5d', 'name': 'Yuchong Sun', 'hidden': False}, {'_id': '692ffb1a26742347f61daf5e', 'name': 'Jun Tang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf5f', 'name': 'Jianhong Tu', 'hidden': False}, {'_id': '692ffb1a26742347f61daf60', 'name': 'Jianqiang Wan', 'hidden': False}, {'_id': '692ffb1a26742347f61daf61', 'name': 'Peng Wang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf62', 'name': 'Pengfei Wang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf63', 'name': 'Qiuyue Wang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf64', 'name': 'Yuxuan Wang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf65', 'name': 'Tianbao Xie', 'hidden': False}, {'_id': '692ffb1a26742347f61daf66', 'name': 'Yiheng Xu', 'hidden': False}, {'_id': '692ffb1a26742347f61daf67', 'user': {'_id': '645b10e80c73ea27d13f7aca', 'avatarUrl': '/avatars/95e565306472a15067440b5b43e07a6f.svg', 'isPro': False, 'fullname': 'xuhaiyang', 'user': 'xhyandwyy', 'type': 'user'}, 'name': 'Haiyang Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T08:48:51.583Z', 'hidden': False}, {'_id': '692ffb1a26742347f61daf68', 'name': 'Jin Xu', 'hidden': False}, {'_id': '692ffb1a26742347f61daf69', 'name': 'Zhibo Yang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf6a', 'name': 'Mingkun Yang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf6b', 'name': 'Jianxin Yang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf6c', 'name': 'An Yang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf6d', 'name': 'Bowen Yu', 'hidden': False}, {'_id': '692ffb1a26742347f61daf6e', 'name': 'Fei Zhang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf6f', 'name': 'Hang Zhang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf70', 'name': 'Xi Zhang', 'hidden': False}, {'_id': '692ffb1a26742347f61daf71', 'name': 'Bo Zheng', 'hidden': False}, {'_id': '692ffb1a26742347f61daf72', 'name': 'Humen Zhong', 'hidden': False}, {'_id': '692ffb1a26742347f61daf73', 'name': 'Jingren Zhou', 'hidden': False}, {'_id': '692ffb1a26742347f61daf74', 'name': 'Fan Zhou', 'hidden': False}, {'_id': '692ffb1a26742347f61daf75', 'name': 'Jing Zhou', 'hidden': False}, {'_id': '692ffb1a26742347f61daf76', 'user': {'_id': '627d2723401f42c57b6b7c0c', 'avatarUrl': '/avatars/6ff754e56aaee63d8572881a6a966171.svg', 'isPro': False, 'fullname': 'Yuanzhi Zhu', 'user': 'Yuanzhi', 'type': 'user'}, 'name': 'Yuanzhi Zhu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:36:59.879Z', 'hidden': False}, {'_id': '692ffb1a26742347f61daf77', 'name': 'Ke Zhu', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg'], 'publishedAt': '2025-11-26T17:59:08.000Z', 'submittedOnDailyAt': '2025-12-04T01:02:46.772Z', 'title': 'Qwen3-VL Technical Report', 'submittedOnDailyBy': {'_id': '63451cf0a05b51f7ded25505', 'avatarUrl': '/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg', 'isPro': False, 'fullname': 'shuai bai', 'user': 'ShuaiBai623', 'type': 'user'}, 'summary': 'We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.', 'upvotes': 76, 'discussionId': '692ffb1b26742347f61daf78', 'ai_summary': 'Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.', 'ai_keywords': ['vision-language model', 'interleaved contexts', 'multimodal benchmarks', 'dense variants', 'mixture-of-experts', 'pure-text understanding', 'long-context comprehension', 'multimodal reasoning', 'MMMU', 'visual-math benchmarks', 'interleaved-MRoPE', 'DeepStack', 'text-based time alignment', 'T-RoPE', 'explicit textual timestamp alignment', 'vision-language alignment', 'image-grounded reasoning', 'agentic decision-making', 'multimodal code intelligence'], 'organization': {'_id': '64c8b5837fe12ecd0a7e92eb', 'name': 'Qwen', 'fullname': 'Qwen', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png'}}, 'publishedAt': '2025-11-26T12:59:08.000Z', 'title': 'Qwen3-VL Technical Report', 'summary': 'We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21631.png', 'numComments': 1, 'submittedBy': {'_id': '63451cf0a05b51f7ded25505', 'avatarUrl': '/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg', 'fullname': 'shuai bai', 'name': 'ShuaiBai623', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 41}, 'organization': {'_id': '64c8b5837fe12ecd0a7e92eb', 'name': 'Qwen', 'fullname': 'Qwen', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.02834', 'authors': [{'_id': '69313ee82d1e5b0a7d84da77', 'name': 'Siyuan Yang', 'hidden': False}, {'_id': '69313ee82d1e5b0a7d84da78', 'user': {'_id': '667be442c8c087a184094892', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/667be442c8c087a184094892/rVn2sKB9TuABE-sIzxbAx.jpeg', 'isPro': False, 'fullname': 'Yang Zhang', 'user': 'breezeyoung', 'type': 'user'}, 'name': 'Yang Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T08:43:32.201Z', 'hidden': False}, {'_id': '69313ee82d1e5b0a7d84da79', 'user': {'_id': '6672937ceac0fb1b9e516595', 'avatarUrl': '/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg', 'isPro': False, 'fullname': 'haoran he', 'user': 'haoranhe', 'type': 'user'}, 'name': 'Haoran He', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:43:20.162Z', 'hidden': False}, {'_id': '69313ee82d1e5b0a7d84da7a', 'name': 'Ling Pan', 'hidden': False}, {'_id': '69313ee82d1e5b0a7d84da7b', 'name': 'Xiu Li', 'hidden': False}, {'_id': '69313ee82d1e5b0a7d84da7c', 'name': 'Chenjia Bai', 'hidden': False}, {'_id': '69313ee82d1e5b0a7d84da7d', 'name': 'Xuelong Li', 'hidden': False}], 'publishedAt': '2025-12-02T14:42:54.000Z', 'submittedOnDailyAt': '2025-12-04T06:26:21.029Z', 'title': 'Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach', 'submittedOnDailyBy': {'_id': '667be442c8c087a184094892', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/667be442c8c087a184094892/rVn2sKB9TuABE-sIzxbAx.jpeg', 'isPro': False, 'fullname': 'Yang Zhang', 'user': 'breezeyoung', 'type': 'user'}, 'summary': 'Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose TACO, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.', 'upvotes': 28, 'discussionId': '69313ee82d1e5b0a7d84da7e', 'projectPage': 'https://vla-anti-exploration.github.io/', 'githubRepo': 'https://github.com/breez3young/TACO', 'ai_summary': 'TACO, a test-time-scaling framework with a pseudo-count estimator, enhances the inference stability and success rates of Vision-Language-Action models in downstream tasks by preventing distribution shifts.', 'ai_keywords': ['flow-matching', 'diffusion objectives', 'Vision-Language-Action models', 'pre-training', 'finetuning', 'distribution shift', 'test-time-scaling', 'pseudo-count estimator', 'action chunks', 'anti-exploration principle', 'offline reinforcement learning', 'gradient-free', 'denoising process', 'RoboTwin2.0', 'Robotwin', 'LIBERO', 'SimplerEnv', 'dual-arm platform'], 'githubStars': 5}, 'publishedAt': '2025-12-02T09:42:54.000Z', 'title': 'Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach', 'summary': 'Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose TACO, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02834.png', 'numComments': 2, 'submittedBy': {'_id': '667be442c8c087a184094892', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/667be442c8c087a184094892/rVn2sKB9TuABE-sIzxbAx.jpeg', 'fullname': 'Yang Zhang', 'name': 'breezeyoung', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.03442', 'authors': [{'_id': '693104f92d1e5b0a7d84d9f2', 'user': {'_id': '651c4e0bc0247c08a46ab2a6', 'avatarUrl': '/avatars/3396a34ffb400f576371afc8a5064783.svg', 'isPro': False, 'fullname': 'xxr', 'user': 'xrxing', 'type': 'user'}, 'name': 'Xingrun Xing', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:39:37.521Z', 'hidden': False}, {'_id': '693104f92d1e5b0a7d84d9f3', 'user': {'_id': '65867b038dd42194878e08dc', 'avatarUrl': '/avatars/8965ac2f8e3ccb88759f6e4d84a30c2f.svg', 'isPro': False, 'fullname': 'Zhiyuan Fan', 'user': 'Zhiyuan-Fan', 'type': 'user'}, 'name': 'Zhiyuan Fan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T13:49:38.606Z', 'hidden': False}, {'_id': '693104f92d1e5b0a7d84d9f4', 'name': 'Jie Lou', 'hidden': False}, {'_id': '693104f92d1e5b0a7d84d9f5', 'name': 'Guoqi Li', 'hidden': False}, {'_id': '693104f92d1e5b0a7d84d9f6', 'name': 'Jiajun Zhang', 'hidden': False}, {'_id': '693104f92d1e5b0a7d84d9f7', 'user': {'_id': '64546be9548f22be59842cfe', 'avatarUrl': '/avatars/5582cf189b755d888b35f68a6e3e9bb8.svg', 'isPro': False, 'fullname': 'zdb', 'user': 'debingzhang', 'type': 'user'}, 'name': 'Debing Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:39:57.928Z', 'hidden': False}], 'publishedAt': '2025-12-03T04:51:32.000Z', 'submittedOnDailyAt': '2025-12-04T01:22:22.945Z', 'title': 'PretrainZero: Reinforcement Active Pretraining', 'submittedOnDailyBy': {'_id': '651c4e0bc0247c08a46ab2a6', 'avatarUrl': '/avatars/3396a34ffb400f576371afc8a5064783.svg', 'isPro': False, 'fullname': 'xxr', 'user': 'xrxing', 'type': 'user'}, 'summary': 'Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.', 'upvotes': 24, 'discussionId': '693104fa2d1e5b0a7d84d9f8', 'ai_summary': 'PretrainZero is a reinforcement active learning framework that enhances general reasoning capabilities by pretraining large models on a corpus without verifiable labels, improving performance on benchmarks compared to domain-specific training.', 'ai_keywords': ['reinforcement learning', 'RL', 'active learning', 'active pretraining', 'self-supervised learning', 'pretrained reward models', 'Qwen3-4B-Base', 'MMLU-Pro', 'SuperGPQA', 'math average benchmarks', 'RLVR tasks']}, 'publishedAt': '2025-12-02T23:51:32.000Z', 'title': 'PretrainZero: Reinforcement Active Pretraining', 'summary': 'Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03442.png', 'numComments': 1, 'submittedBy': {'_id': '651c4e0bc0247c08a46ab2a6', 'avatarUrl': '/avatars/3396a34ffb400f576371afc8a5064783.svg', 'fullname': 'xxr', 'name': 'xrxing', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.03405', 'authors': [{'_id': '6930f7432d1e5b0a7d84d996', 'name': 'Jiangtao Wu', 'hidden': False}, {'_id': '6930f7432d1e5b0a7d84d997', 'user': {'_id': '67f9d060395fb1a0d7e4ae21', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GjpOfOuazN7IxcXBpVqRm.png', 'isPro': False, 'fullname': 'Shihao Li', 'user': 'Leexeo', 'type': 'user'}, 'name': 'Shihao Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T08:43:59.201Z', 'hidden': False}, {'_id': '6930f7432d1e5b0a7d84d998', 'name': 'Zhaozhou Bian', 'hidden': False}, {'_id': '6930f7432d1e5b0a7d84d999', 'user': {'_id': '64241749a05235e2f8d34cb0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64241749a05235e2f8d34cb0/o6CY4xS22W8_DIqesFykM.jpeg', 'isPro': False, 'fullname': 'Yuanxing Zhang', 'user': 'LongoXC', 'type': 'user'}, 'name': 'Yuanxing Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:41:05.482Z', 'hidden': False}, {'_id': '6930f7432d1e5b0a7d84d99a', 'name': 'Jialu Chen', 'hidden': False}, {'_id': '6930f7432d1e5b0a7d84d99b', 'user': {'_id': '69059dc64c9138632afde265', 'avatarUrl': '/avatars/3ab6775359128d43bd28de006b94bd51.svg', 'isPro': False, 'fullname': 'runzhe wen', 'user': 'wrz123', 'type': 'user'}, 'name': 'Runzhe Wen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:41:12.901Z', 'hidden': False}, {'_id': '6930f7432d1e5b0a7d84d99c', 'name': 'An Ping', 'hidden': False}, {'_id': '6930f7432d1e5b0a7d84d99d', 'user': {'_id': '68f84bec0a277a9bf73f1e0d', 'avatarUrl': '/avatars/51cb0b5ba6393cbcf7ed79e66f160a7f.svg', 'isPro': False, 'fullname': 'yiwen he', 'user': 'heyween', 'type': 'user'}, 'name': 'Yiwen He', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:41:19.015Z', 'hidden': False}, {'_id': '6930f7432d1e5b0a7d84d99e', 'user': {'_id': '6771194d87c60cdabfa5bc3f', 'avatarUrl': '/avatars/5c3dc84196029661e3decee3641b4df6.svg', 'isPro': False, 'fullname': 'Jiakai Wang', 'user': 'jiakaiW', 'type': 'user'}, 'name': 'Jiakai Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:41:28.040Z', 'hidden': False}, {'_id': '6930f7432d1e5b0a7d84d99f', 'name': 'Jiaheng Liu', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/gWX87F09tk4zl8cMMFIVi.png'], 'publishedAt': '2025-12-03T03:23:24.000Z', 'submittedOnDailyAt': '2025-12-04T00:22:20.618Z', 'title': 'ViDiC: Video Difference Captioning', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.', 'upvotes': 23, 'discussionId': '6930f7432d1e5b0a7d84d9a0', 'projectPage': 'https://vidic-1k.github.io/', 'ai_summary': \"The ViDiC task and ViDiC-1K dataset evaluate Multimodal Large Language Models' ability to describe differences between video pairs, addressing limitations in capturing motion continuity and event evolution.\", 'ai_keywords': ['Image Difference Captioning', 'Video Difference Captioning', 'ViDiC', 'ViDiC-1K', 'Multimodal Large Language Models', 'checklist items', 'dual-checklist framework', 'LLM-as-a-Judge protocol', 'video understanding', 'edit awareness', 'comparative reasoning']}, 'publishedAt': '2025-12-02T22:23:24.000Z', 'title': 'ViDiC: Video Difference Captioning', 'summary': 'Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/gWX87F09tk4zl8cMMFIVi.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03405.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 177}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.03043', 'authors': [{'_id': '692fd92426742347f61dad8d', 'name': 'Kaituo Feng', 'hidden': False}, {'_id': '692fd92426742347f61dad8e', 'name': 'Manyuan Zhang', 'hidden': False}, {'_id': '692fd92426742347f61dad8f', 'name': 'Hongyu Li', 'hidden': False}, {'_id': '692fd92426742347f61dad90', 'name': 'Kaixuan Fan', 'hidden': False}, {'_id': '692fd92426742347f61dad91', 'name': 'Shuang Chen', 'hidden': False}, {'_id': '692fd92426742347f61dad92', 'name': 'Yilei Jiang', 'hidden': False}, {'_id': '692fd92426742347f61dad93', 'user': {'_id': '67e60ae6ac37824273d74389', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YvPKZ_0gyJnvNwM1zK3JS.png', 'isPro': False, 'fullname': 'Dian Zheng', 'user': 'zhengli1013', 'type': 'user'}, 'name': 'Dian Zheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T08:49:05.327Z', 'hidden': False}, {'_id': '692fd92426742347f61dad94', 'name': 'Peiwen Sun', 'hidden': False}, {'_id': '692fd92426742347f61dad95', 'name': 'Yiyuan Zhang', 'hidden': False}, {'_id': '692fd92426742347f61dad96', 'name': 'Haoze Sun', 'hidden': False}, {'_id': '692fd92426742347f61dad97', 'name': 'Yan Feng', 'hidden': False}, {'_id': '692fd92426742347f61dad98', 'name': 'Peng Pei', 'hidden': False}, {'_id': '692fd92426742347f61dad99', 'name': 'Xunliang Cai', 'hidden': False}, {'_id': '692fd92426742347f61dad9a', 'name': 'Xiangyu Yue', 'hidden': False}], 'publishedAt': '2025-12-02T18:59:52.000Z', 'submittedOnDailyAt': '2025-12-04T00:30:53.335Z', 'title': 'OneThinker: All-in-one Reasoning Model for Image and Video', 'submittedOnDailyBy': {'_id': '67079840a9bcb7459b8d2a46', 'avatarUrl': '/avatars/32466863c5554f20cb2775b138832ac3.svg', 'isPro': False, 'fullname': 'Kaituo Feng', 'user': 'KaituoFeng', 'type': 'user'}, 'summary': 'Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.', 'upvotes': 17, 'discussionId': '692fd92426742347f61dad9b', 'projectPage': 'https://github.com/tulerfeng/OneThinker', 'githubRepo': 'https://github.com/tulerfeng/OneThinker', 'ai_summary': 'OneThinker, an all-in-one multimodal reasoning model, unifies image and video understanding across various tasks using RL and demonstrates strong performance and knowledge transfer.', 'ai_keywords': ['Reinforcement learning', 'Multimodal Large Language Models', 'image and video reasoning', 'question answering', 'captioning', 'spatial and temporal grounding', 'tracking', 'segmentation', 'EMA-GRPO', 'reward heterogeneity', 'zero-shot generalization'], 'githubStars': 29}, 'publishedAt': '2025-12-02T13:59:52.000Z', 'title': 'OneThinker: All-in-one Reasoning Model for Image and Video', 'summary': 'Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03043.png', 'numComments': 1, 'submittedBy': {'_id': '67079840a9bcb7459b8d2a46', 'avatarUrl': '/avatars/32466863c5554f20cb2775b138832ac3.svg', 'fullname': 'Kaituo Feng', 'name': 'KaituoFeng', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.03534', 'authors': [{'_id': '6930fb082d1e5b0a7d84d9cc', 'name': 'Subin Kim', 'hidden': False}, {'_id': '6930fb082d1e5b0a7d84d9cd', 'name': 'Sangwoo Mo', 'hidden': False}, {'_id': '6930fb082d1e5b0a7d84d9ce', 'name': 'Mamshad Nayeem Rizve', 'hidden': False}, {'_id': '6930fb082d1e5b0a7d84d9cf', 'name': 'Yiran Xu', 'hidden': False}, {'_id': '6930fb082d1e5b0a7d84d9d0', 'name': 'Difan Liu', 'hidden': False}, {'_id': '6930fb082d1e5b0a7d84d9d1', 'name': 'Jinwoo Shin', 'hidden': False}, {'_id': '6930fb082d1e5b0a7d84d9d2', 'name': 'Tobias Hinz', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63eb83b74dcaaf08763766a9/c0wMy7PfHxzeXmSXhO3Hx.png'], 'publishedAt': '2025-12-03T07:54:05.000Z', 'submittedOnDailyAt': '2025-12-04T00:42:22.046Z', 'title': 'Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation', 'submittedOnDailyBy': {'_id': '63eb83b74dcaaf08763766a9', 'avatarUrl': '/avatars/11e3e00c49a3b74947cce2f1ef624944.svg', 'isPro': False, 'fullname': 'Subin Kim', 'user': 'subin-kim', 'type': 'user'}, 'summary': 'Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.', 'upvotes': 14, 'discussionId': '6930fb092d1e5b0a7d84d9d3', 'projectPage': 'https://subin-kim-cv.github.io/PRIS', 'ai_summary': 'PRIS adaptively revises prompts during inference to enhance alignment with user intent in text-to-visual generation, improving accuracy and quality.', 'ai_keywords': ['Prompt Redesign for Inference-time Scaling', 'PRIS', 'element-level factual correction', 'text-to-image', 'text-to-video', 'VBench 2.0', 'scaling laws', 'inference-time scaling']}, 'publishedAt': '2025-12-03T02:54:05.000Z', 'title': 'Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation', 'summary': 'Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63eb83b74dcaaf08763766a9/c0wMy7PfHxzeXmSXhO3Hx.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03534.png', 'numComments': 1, 'submittedBy': {'_id': '63eb83b74dcaaf08763766a9', 'avatarUrl': '/avatars/11e3e00c49a3b74947cce2f1ef624944.svg', 'fullname': 'Subin Kim', 'name': 'subin-kim', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.04069', 'authors': [{'_id': '693119ad2d1e5b0a7d84da24', 'user': {'_id': '65f327c5761cd77e9411e303', 'avatarUrl': '/avatars/2c6c66e54bb2b31923c24929be5e5936.svg', 'isPro': True, 'fullname': 'Siyi Chen', 'user': 'siyich', 'type': 'user'}, 'name': 'Siyi Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T08:43:39.109Z', 'hidden': False}, {'_id': '693119ad2d1e5b0a7d84da25', 'name': 'Mikaela Angelina Uy', 'hidden': False}, {'_id': '693119ad2d1e5b0a7d84da26', 'user': {'_id': '63d19365b30415240fd6515b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63d19365b30415240fd6515b/eOEYSsyDTfPTDrR6Cm5Jn.jpeg', 'isPro': False, 'fullname': 'Chan Hee Song', 'user': 'chanhee-luke', 'type': 'user'}, 'name': 'Chan Hee Song', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T08:43:36.805Z', 'hidden': False}, {'_id': '693119ad2d1e5b0a7d84da27', 'user': {'_id': '61ee4f4f53fdc44f4e5e197a', 'avatarUrl': '/avatars/f23f56809805c3e06c580d7420f48494.svg', 'isPro': False, 'fullname': 'Faisal Ladhak', 'user': 'fladhak', 'type': 'user'}, 'name': 'Faisal Ladhak', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:41:52.091Z', 'hidden': False}, {'_id': '693119ad2d1e5b0a7d84da28', 'name': 'Adithyavairavan Murali', 'hidden': False}, {'_id': '693119ad2d1e5b0a7d84da29', 'name': 'Qing Qu', 'hidden': False}, {'_id': '693119ad2d1e5b0a7d84da2a', 'name': 'Stan Birchfield', 'hidden': False}, {'_id': '693119ad2d1e5b0a7d84da2b', 'user': {'_id': '6349d3c29a8210ef8570d060', 'avatarUrl': '/avatars/8a20fadfa0f3ece972ee1c914eea665d.svg', 'isPro': False, 'fullname': 'Valts Blukis', 'user': 'valtsblukis', 'type': 'user'}, 'name': 'Valts Blukis', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T10:42:13.005Z', 'hidden': False}, {'_id': '693119ad2d1e5b0a7d84da2c', 'name': 'Jonathan Tremblay', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65f327c5761cd77e9411e303/59cimJFPz5Qprl0Gtu6a8.png', 'https://cdn-uploads.huggingface.co/production/uploads/65f327c5761cd77e9411e303/kt8DCPDGSBdGs_2gvIuYW.png', 'https://cdn-uploads.huggingface.co/production/uploads/65f327c5761cd77e9411e303/3sLkPLp9wvN--ma-LViti.png', 'https://cdn-uploads.huggingface.co/production/uploads/65f327c5761cd77e9411e303/CYaLbuUtCFk_3kXqnt8vi.png'], 'publishedAt': '2025-12-03T18:50:04.000Z', 'submittedOnDailyAt': '2025-12-04T03:48:17.587Z', 'title': 'SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL', 'submittedOnDailyBy': {'_id': '65f327c5761cd77e9411e303', 'avatarUrl': '/avatars/2c6c66e54bb2b31923c24929be5e5936.svg', 'isPro': True, 'fullname': 'Siyi Chen', 'user': 'siyich', 'type': 'user'}, 'summary': \"Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.\", 'upvotes': 12, 'discussionId': '693119ae2d1e5b0a7d84da2d', 'projectPage': 'https://spacetools.github.io/', 'githubRepo': 'https://github.com/spacetools/SpaceTools', 'ai_summary': 'Double Interactive Reinforcement Learning (DIRL) enables Vision Language Models (VLMs) to coordinate multiple tools for precise spatial reasoning, achieving state-of-the-art performance on benchmarks and real-world tasks.', 'ai_keywords': ['Vision Language Models', 'VLMs', 'agentic paradigm', 'depth estimators', 'segmentation models', 'pose estimators', 'Reinforcement Learning', 'DIRL', 'teaching phase', 'exploration phase', 'SpaceTools', 'RoboSpatial-Home', 'BLINK', 'BOP-ASK', 'real-world manipulation', '7-DOF robot'], 'githubStars': 1, 'organization': {'_id': '60262b67268c201cdc8b7d43', 'name': 'nvidia', 'fullname': 'NVIDIA', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png'}}, 'publishedAt': '2025-12-03T13:50:04.000Z', 'title': 'SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL', 'summary': \"Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65f327c5761cd77e9411e303/59cimJFPz5Qprl0Gtu6a8.png', 'https://cdn-uploads.huggingface.co/production/uploads/65f327c5761cd77e9411e303/kt8DCPDGSBdGs_2gvIuYW.png', 'https://cdn-uploads.huggingface.co/production/uploads/65f327c5761cd77e9411e303/3sLkPLp9wvN--ma-LViti.png', 'https://cdn-uploads.huggingface.co/production/uploads/65f327c5761cd77e9411e303/CYaLbuUtCFk_3kXqnt8vi.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04069.png', 'numComments': 1, 'submittedBy': {'_id': '65f327c5761cd77e9411e303', 'avatarUrl': '/avatars/2c6c66e54bb2b31923c24929be5e5936.svg', 'fullname': 'Siyi Chen', 'name': 'siyich', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'organization': {'_id': '60262b67268c201cdc8b7d43', 'name': 'nvidia', 'fullname': 'NVIDIA', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.03746', 'authors': [{'_id': '6930f4782d1e5b0a7d84d964', 'name': 'Zirun Guo', 'hidden': False}, {'_id': '6930f4782d1e5b0a7d84d965', 'name': 'Minjie Hong', 'hidden': False}, {'_id': '6930f4782d1e5b0a7d84d966', 'name': 'Feng Zhang', 'hidden': False}, {'_id': '6930f4782d1e5b0a7d84d967', 'name': 'Kai Jia', 'hidden': False}, {'_id': '6930f4782d1e5b0a7d84d968', 'name': 'Tao Jin', 'hidden': False}], 'publishedAt': '2025-12-03T12:44:15.000Z', 'submittedOnDailyAt': '2025-12-04T00:29:02.611Z', 'title': 'Thinking with Programming Vision: Towards a Unified View for Thinking with Images', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.', 'upvotes': 10, 'discussionId': '6930f4782d1e5b0a7d84d969', 'ai_summary': \"CodeVision, a flexible code-as-tool framework, enhances multimodal large language models' robustness and tool-based reasoning by generating code to handle image operations, overcoming brittleness and improving performance through supervised fine-tuning and reinforcement learning.\", 'ai_keywords': ['MLLMs', 'CodeVision', 'code-as-tool framework', 'image operations', 'Supervised Fine-Tuning', 'Reinforcement Learning', 'process reward function', 'tool composition', 'error recovery', 'Qwen2.5-VL', 'Qwen3-VL']}, 'publishedAt': '2025-12-03T07:44:15.000Z', 'title': 'Thinking with Programming Vision: Towards a Unified View for Thinking with Images', 'summary': 'Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03746.png', 'numComments': 0, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 177}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.04040', 'authors': [{'_id': '6930fd7b2d1e5b0a7d84d9d5', 'user': {'_id': '654ca66fe1671abcbc50a6e4', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/654ca66fe1671abcbc50a6e4/VqcnC17YcqC9Hk65_LxtE.jpeg', 'isPro': False, 'fullname': 'Yicong Hong', 'user': 'YicongHong', 'type': 'user'}, 'name': 'Yicong Hong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:10:34.480Z', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9d6', 'name': 'Yiqun Mei', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9d7', 'name': 'Chongjian Ge', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9d8', 'name': 'Yiran Xu', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9d9', 'name': 'Yang Zhou', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9da', 'user': {'_id': '66303560597dc7c535168f07', 'avatarUrl': '/avatars/a69d52a4a198c6c8451c1d137031bf07.svg', 'isPro': False, 'fullname': 'Sai Bi', 'user': 'saibi', 'type': 'user'}, 'name': 'Sai Bi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:11:08.327Z', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9db', 'name': 'Yannick Hold-Geoffroy', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9dc', 'name': 'Mike Roberts', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9dd', 'name': 'Matthew Fisher', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9de', 'name': 'Eli Shechtman', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9df', 'user': {'_id': '67f54d1b504263bce1431e00', 'avatarUrl': '/avatars/ce2928e59480dbf33aacf941056dcc73.svg', 'isPro': False, 'fullname': 'Kalyan Sunkavalli', 'user': 'kalyanks', 'type': 'user'}, 'name': 'Kalyan Sunkavalli', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:11:30.379Z', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9e0', 'name': 'Feng Liu', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9e1', 'name': 'Zhengqi Li', 'hidden': False}, {'_id': '6930fd7b2d1e5b0a7d84d9e2', 'name': 'Hao Tan', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/oJ3GPW4x1vqqTr6Zir0Nj.qt'], 'publishedAt': '2025-12-03T18:29:20.000Z', 'submittedOnDailyAt': '2025-12-04T00:49:32.724Z', 'title': 'RELIC: Interactive Video World Model with Long-Horizon Memory', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.', 'upvotes': 9, 'discussionId': '6930fd7b2d1e5b0a7d84d9e3', 'projectPage': 'https://relic-worldmodel.github.io/', 'ai_summary': 'RELIC is a unified framework that enables real-time, memory-aware exploration of scenes by integrating long-horizon memory, spatial consistency, and user control using autoregressive video-diffusion distillation.', 'ai_keywords': ['autoregressive video-diffusion distillation', 'latent tokens', 'KV cache', 'relative actions', 'absolute camera poses', 'bidirectional teacher video model', 'causal student generator', 'memory-efficient self-forcing', 'full-context distillation', 'Unreal Engine-rendered dataset', 'interactive world modeling']}, 'publishedAt': '2025-12-03T13:29:20.000Z', 'title': 'RELIC: Interactive Video World Model with Long-Horizon Memory', 'summary': 'A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/oJ3GPW4x1vqqTr6Zir0Nj.qt'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04040.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 177}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2511.22345', 'authors': [{'_id': '692cfbae4397b1ec214f675a', 'user': {'_id': '657c03a5538666d04cd47461', 'avatarUrl': '/avatars/00a7686e08207915ade05b52a84d8e26.svg', 'isPro': False, 'fullname': 'Chen Yang', 'user': 'Y-Sisyphus', 'type': 'user'}, 'name': 'Yang Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-01T09:11:25.208Z', 'hidden': False}, {'_id': '692cfbae4397b1ec214f675b', 'name': 'Xiaowei Xu', 'hidden': False}, {'_id': '692cfbae4397b1ec214f675c', 'user': {'_id': '66615c855fd9d736e670e0a9', 'avatarUrl': '/avatars/0ff3127b513552432a7c651e21d7f283.svg', 'isPro': False, 'fullname': 'wangshuai', 'user': 'wangsssssss', 'type': 'user'}, 'name': 'Shuai Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T10:34:46.581Z', 'hidden': False}, {'_id': '692cfbae4397b1ec214f675d', 'name': 'Chenhui Zhu', 'hidden': False}, {'_id': '692cfbae4397b1ec214f675e', 'name': 'Ruxue Wen', 'hidden': False}, {'_id': '692cfbae4397b1ec214f675f', 'name': 'Xubin Li', 'hidden': False}, {'_id': '692cfbae4397b1ec214f6760', 'name': 'Tiezheng Ge', 'hidden': False}, {'_id': '692cfbae4397b1ec214f6761', 'name': 'Limin Wang', 'hidden': False}], 'publishedAt': '2025-11-27T11:35:08.000Z', 'submittedOnDailyAt': '2025-12-04T06:45:02.234Z', 'title': 'Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment', 'submittedOnDailyBy': {'_id': '657c03a5538666d04cd47461', 'avatarUrl': '/avatars/00a7686e08207915ade05b52a84d8e26.svg', 'isPro': False, 'fullname': 'Chen Yang', 'user': 'Y-Sisyphus', 'type': 'user'}, 'summary': \"Normalizing Flows (NFs) are a class of generative models distinguished by a mathematically invertible architecture, where the forward pass transforms data into a latent space for density estimation, and the reverse pass generates new samples from this space. This characteristic creates an intrinsic synergy between representation learning and data generation. However, the generative quality of standard NFs is limited by poor semantic representations from log-likelihood optimization. To remedy this, we propose a novel alignment strategy that creatively leverages the invertibility of NFs: instead of regularizing the forward pass, we align the intermediate features of the generative (reverse) pass with representations from a powerful vision foundation model, demonstrating superior effectiveness over naive alignment. We also introduce a novel training-free, test-time optimization algorithm for classification, which provides a more intrinsic evaluation of the NF's embedded semantic knowledge. Comprehensive experiments demonstrate that our approach accelerates the training of NFs by over 3.3times, while simultaneously delivering significant improvements in both generative quality and classification accuracy. New state-of-the-art results for NFs are established on ImageNet 64times64 and 256times256. Our code is available at https://github.com/MCG-NJU/FlowBack.\", 'upvotes': 9, 'discussionId': '692cfbaf4397b1ec214f6762', 'githubRepo': 'https://github.com/MCG-NJU/FlowBack', 'ai_summary': 'A novel alignment strategy and test-time optimization algorithm enhance the generative quality and classification accuracy of Normalizing Flows by leveraging invertibility and vision foundation models.', 'ai_keywords': ['Normalizing Flows', 'generative models', 'invertible architecture', 'latent space', 'density estimation', 'log-likelihood optimization', 'semantic representations', 'alignment strategy', 'vision foundation model', 'classification', 'ImageNet'], 'githubStars': 6, 'organization': {'_id': '6314524a5f47a1896274d586', 'name': 'NJU', 'fullname': 'Nanjing University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1662276136108-6314518e5f47a1896274d080.jpeg'}}, 'publishedAt': '2025-11-27T06:35:08.000Z', 'title': 'Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment', 'summary': \"Normalizing Flows (NFs) are a class of generative models distinguished by a mathematically invertible architecture, where the forward pass transforms data into a latent space for density estimation, and the reverse pass generates new samples from this space. This characteristic creates an intrinsic synergy between representation learning and data generation. However, the generative quality of standard NFs is limited by poor semantic representations from log-likelihood optimization. To remedy this, we propose a novel alignment strategy that creatively leverages the invertibility of NFs: instead of regularizing the forward pass, we align the intermediate features of the generative (reverse) pass with representations from a powerful vision foundation model, demonstrating superior effectiveness over naive alignment. We also introduce a novel training-free, test-time optimization algorithm for classification, which provides a more intrinsic evaluation of the NF's embedded semantic knowledge. Comprehensive experiments demonstrate that our approach accelerates the training of NFs by over 3.3times, while simultaneously delivering significant improvements in both generative quality and classification accuracy. New state-of-the-art results for NFs are established on ImageNet 64times64 and 256times256. Our code is available at https://github.com/MCG-NJU/FlowBack.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22345.png', 'numComments': 1, 'submittedBy': {'_id': '657c03a5538666d04cd47461', 'avatarUrl': '/avatars/00a7686e08207915ade05b52a84d8e26.svg', 'fullname': 'Chen Yang', 'name': 'Y-Sisyphus', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'organization': {'_id': '6314524a5f47a1896274d586', 'name': 'NJU', 'fullname': 'Nanjing University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1662276136108-6314518e5f47a1896274d080.jpeg'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.03540', 'authors': [{'_id': '6930f5462d1e5b0a7d84d97c', 'name': 'Ruoxuan Zhang', 'hidden': False}, {'_id': '6930f5462d1e5b0a7d84d97d', 'name': 'Bin Wen', 'hidden': False}, {'_id': '6930f5462d1e5b0a7d84d97e', 'name': 'Hongxia Xie', 'hidden': False}, {'_id': '6930f5462d1e5b0a7d84d97f', 'name': 'Yi Yao', 'hidden': False}, {'_id': '6930f5462d1e5b0a7d84d980', 'name': 'Songhan Zuo', 'hidden': False}, {'_id': '6930f5462d1e5b0a7d84d981', 'name': 'Jian-Yu Jiang-Lin', 'hidden': False}, {'_id': '6930f5462d1e5b0a7d84d982', 'name': 'Hong-Han Shuai', 'hidden': False}, {'_id': '6930f5462d1e5b0a7d84d983', 'user': {'_id': '672c1e6044096b60a3c3144a', 'avatarUrl': '/avatars/420a97204d144db2c6aa3215cab1b9fb.svg', 'isPro': False, 'fullname': 'Wen-Huang Cheng', 'user': 'twbear2024', 'type': 'user'}, 'name': 'Wen-Huang Cheng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:12:06.125Z', 'hidden': False}], 'publishedAt': '2025-12-03T08:01:48.000Z', 'submittedOnDailyAt': '2025-12-04T00:22:58.782Z', 'title': 'CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.', 'upvotes': 4, 'discussionId': '6930f5462d1e5b0a7d84d984', 'githubRepo': 'https://github.com/zhangdaxia22/CookAnything', 'ai_summary': 'CookAnything is a diffusion-based framework that generates coherent image sequences from cooking instructions by incorporating step-wise regional control, flexible positional encoding, and cross-step consistency.', 'ai_keywords': ['diffusion models', 'Step-wise Regional Control', 'Flexible RoPE', 'Cross-Step Consistency Control', 'recipe illustration', 'denoising process', 'temporal coherence', 'spatial diversity'], 'githubStars': 3}, 'publishedAt': '2025-12-03T03:01:48.000Z', 'title': 'CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation', 'summary': 'Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03540.png', 'numComments': 0, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 177}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.02807', 'authors': [{'_id': '692fa3e726742347f61daaed', 'user': {'_id': '647d834618274bce03013cc2', 'avatarUrl': '/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg', 'isPro': True, 'fullname': 'yixuan', 'user': 'yixuantt', 'type': 'user'}, 'name': 'Yixuan Tang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T08:49:17.118Z', 'hidden': False}, {'_id': '692fa3e726742347f61daaee', 'name': 'Yi Yang', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/647d834618274bce03013cc2/cZZyyxj8hmNALZazd6lsA.png'], 'publishedAt': '2025-12-02T14:21:29.000Z', 'submittedOnDailyAt': '2025-12-04T01:32:56.439Z', 'title': 'SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment', 'submittedOnDailyBy': {'_id': '647d834618274bce03013cc2', 'avatarUrl': '/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg', 'isPro': True, 'fullname': 'yixuan', 'user': 'yixuantt', 'type': 'user'}, 'summary': 'Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.', 'upvotes': 4, 'discussionId': '692fa3e726742347f61daaef', 'ai_summary': 'Stable rank, an intrinsic quality signal derived from model representations, improves LLM alignment with human preferences through reinforcement learning without external supervision.', 'ai_keywords': ['large language models', 'human preferences', 'external supervision', 'human annotations', 'reward models', 'reward hacking', 'self-evaluation', 'stable rank', 'hidden states', 'effective dimensionality', 'variance', 'RewardBench', 'task accuracy', 'Best-of-N sampling', 'Stable Rank Group Relative Policy Optimization', 'SR-GRPO', 'Qwen2.5-1.5B-Instruct', 'STEM', 'mathematical reasoning']}, 'publishedAt': '2025-12-02T09:21:29.000Z', 'title': 'SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment', 'summary': 'Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/647d834618274bce03013cc2/cZZyyxj8hmNALZazd6lsA.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02807.png', 'numComments': 1, 'submittedBy': {'_id': '647d834618274bce03013cc2', 'avatarUrl': '/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg', 'fullname': 'yixuan', 'name': 'yixuantt', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.03073', 'authors': [{'_id': '693179412d1e5b0a7d84db10', 'name': 'Shayne Longpre', 'hidden': False}, {'_id': '693179412d1e5b0a7d84db11', 'user': {'_id': '5e70f6048ce3c604d78fe133', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg', 'isPro': True, 'fullname': 'Christopher Akiki', 'user': 'christopher', 'type': 'user'}, 'name': 'Christopher Akiki', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T13:48:37.891Z', 'hidden': False}, {'_id': '693179412d1e5b0a7d84db12', 'name': 'Campbell Lund', 'hidden': False}, {'_id': '693179412d1e5b0a7d84db13', 'name': 'Atharva Kulkarni', 'hidden': False}, {'_id': '693179412d1e5b0a7d84db14', 'name': 'Emily Chen', 'hidden': False}, {'_id': '693179412d1e5b0a7d84db15', 'name': 'Irene Solaiman', 'hidden': False}, {'_id': '693179412d1e5b0a7d84db16', 'name': 'Avijit Ghosh', 'hidden': False}, {'_id': '693179412d1e5b0a7d84db17', 'name': 'Yacine Jernite', 'hidden': False}, {'_id': '693179412d1e5b0a7d84db18', 'name': 'Lucie-Aime Kaffee', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/5e70f6048ce3c604d78fe133/pPf6m1cV2DrKRj63FzCR7.png'], 'publishedAt': '2025-11-27T12:50:25.000Z', 'submittedOnDailyAt': '2025-12-04T10:18:08.518Z', 'title': 'Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem', 'submittedOnDailyBy': {'_id': '5e70f6048ce3c604d78fe133', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg', 'isPro': True, 'fullname': 'Christopher Akiki', 'user': 'christopher', 'type': 'user'}, 'summary': 'Since 2019, the Hugging Face Model Hub has been the primary global platform for sharing open weight AI models. By releasing a dataset of the complete history of weekly model downloads (June 2020-August 2025) alongside model metadata, we provide the most rigorous examination to-date of concentration dynamics and evolving characteristics in the open model economy. Our analysis spans 851,000 models, over 200 aggregated attributes per model, and 2.2B downloads. We document a fundamental rebalancing of economic power: US open-weight industry dominance by Google, Meta, and OpenAI has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry, with DeepSeek and Qwen models potentially heralding a new consolidation of market power. We identify statistically significant shifts in model properties, a 17X increase in average model size, rapid growth in multimodal generation (3.4X), quantization (5X), and mixture-of-experts architectures (7X), alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025. We expose a new layer of developer intermediaries that has emerged, focused on quantizing and adapting base models for both efficiency and artistic expression. To enable continued research and oversight, we release the complete dataset with an interactive dashboard for real-time monitoring of concentration dynamics and evolving properties in the open model economy.', 'upvotes': 3, 'discussionId': '693179422d1e5b0a7d84db19', 'ai_summary': 'The analysis of Hugging Face Model Hub data reveals shifts in the open model economy, including declining US industry dominance, growing Chinese influence, and significant changes in model properties like size, multimodal generation, quantization, and mixture-of-experts architectures.', 'ai_keywords': ['open weight AI models', 'concentration dynamics', 'economic power', 'model downloads', 'model metadata', 'multimodal generation', 'quantization', 'mixture-of-experts architectures', 'data transparency', 'open weights models', 'open source models', 'developer intermediaries'], 'organization': {'_id': '691f73aa5eba0b29ca01c74f', 'name': 'economies-open-ai', 'fullname': 'Economies'}}, 'publishedAt': '2025-11-27T07:50:25.000Z', 'title': 'Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem', 'summary': 'Since 2019, the Hugging Face Model Hub has been the primary global platform for sharing open weight AI models. By releasing a dataset of the complete history of weekly model downloads (June 2020-August 2025) alongside model metadata, we provide the most rigorous examination to-date of concentration dynamics and evolving characteristics in the open model economy. Our analysis spans 851,000 models, over 200 aggregated attributes per model, and 2.2B downloads. We document a fundamental rebalancing of economic power: US open-weight industry dominance by Google, Meta, and OpenAI has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry, with DeepSeek and Qwen models potentially heralding a new consolidation of market power. We identify statistically significant shifts in model properties, a 17X increase in average model size, rapid growth in multimodal generation (3.4X), quantization (5X), and mixture-of-experts architectures (7X), alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025. We expose a new layer of developer intermediaries that has emerged, focused on quantizing and adapting base models for both efficiency and artistic expression. To enable continued research and oversight, we release the complete dataset with an interactive dashboard for real-time monitoring of concentration dynamics and evolving properties in the open model economy.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/5e70f6048ce3c604d78fe133/pPf6m1cV2DrKRj63FzCR7.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03073.png', 'numComments': 1, 'submittedBy': {'_id': '5e70f6048ce3c604d78fe133', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg', 'fullname': 'Christopher Akiki', 'name': 'christopher', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 118}, 'organization': {'_id': '691f73aa5eba0b29ca01c74f', 'name': 'economies-open-ai', 'fullname': 'Economies'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2511.20515', 'authors': [{'_id': '692ffb1c26742347f61daf81', 'name': 'Kuniaki Saito', 'hidden': False}, {'_id': '692ffb1c26742347f61daf82', 'user': {'_id': '651a7684a1a5e5d617e28f84', 'avatarUrl': '/avatars/95f5582df7c13fe4a392006125e6cc0f.svg', 'isPro': True, 'fullname': 'Risa Shinoda', 'user': 'risashinoda', 'type': 'user'}, 'name': 'Risa Shinoda', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T08:48:47.311Z', 'hidden': False}, {'_id': '692ffb1c26742347f61daf83', 'name': 'Shohei Tanaka', 'hidden': False}, {'_id': '692ffb1c26742347f61daf84', 'name': 'Tosho Hirasawa', 'hidden': False}, {'_id': '692ffb1c26742347f61daf85', 'name': 'Fumio Okura', 'hidden': False}, {'_id': '692ffb1c26742347f61daf86', 'user': {'_id': '6268cab5dc80d589a9f8ea82', 'avatarUrl': '/avatars/2897993439d641478aa4e797551fccfb.svg', 'isPro': False, 'fullname': 'Yoshitaka Ushiku', 'user': 'yushiku', 'type': 'user'}, 'name': 'Yoshitaka Ushiku', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:12:41.107Z', 'hidden': False}], 'publishedAt': '2025-11-25T17:19:47.000Z', 'submittedOnDailyAt': '2025-12-04T00:48:45.054Z', 'title': 'AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs', 'submittedOnDailyBy': {'_id': '651a7684a1a5e5d617e28f84', 'avatarUrl': '/avatars/95f5582df7c13fe4a392006125e6cc0f.svg', 'isPro': True, 'fullname': 'Risa Shinoda', 'user': 'risashinoda', 'type': 'user'}, 'summary': 'Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.', 'upvotes': 3, 'discussionId': '692ffb1c26742347f61daf87', 'projectPage': 'https://dahlian00.github.io/AlignBench/', 'ai_summary': 'AlignBench evaluates image-text models using detailed captions generated by diverse models, revealing insights into their alignment and compositional reasoning capabilities.', 'ai_keywords': ['CLIP', 'image-text alignment', 'AlignBench', 'image-to-text', 'text-to-image', 'decoder-based VLMs', 'compositional reasoning', 'self-preference']}, 'publishedAt': '2025-11-25T12:19:47.000Z', 'title': 'AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs', 'summary': 'Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20515.png', 'numComments': 1, 'submittedBy': {'_id': '651a7684a1a5e5d617e28f84', 'avatarUrl': '/avatars/95f5582df7c13fe4a392006125e6cc0f.svg', 'fullname': 'Risa Shinoda', 'name': 'risashinoda', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.04072', 'authors': [{'_id': '693133f52d1e5b0a7d84da69', 'user': {'_id': '64e78a03e3953cd90bcad620', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64e78a03e3953cd90bcad620/Rj_-xLJUsxdRmNhvRbssq.jpeg', 'isPro': True, 'fullname': 'Zayne Sprague', 'user': 'Zaynes', 'type': 'user'}, 'name': 'Zayne Sprague', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:13:12.681Z', 'hidden': False}, {'_id': '693133f52d1e5b0a7d84da6a', 'name': 'Jack Lu', 'hidden': False}, {'_id': '693133f52d1e5b0a7d84da6b', 'name': 'Manya Wadhwa', 'hidden': False}, {'_id': '693133f52d1e5b0a7d84da6c', 'user': {'_id': '6365c1158b6bd0d9f6605d3d', 'avatarUrl': '/avatars/1e3e6573e0cb2bb1b9d8043045120ee6.svg', 'isPro': False, 'fullname': 'Sedrick Keh', 'user': 'sedrickkeh', 'type': 'user'}, 'name': 'Sedrick Keh', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:12:53.518Z', 'hidden': False}, {'_id': '693133f52d1e5b0a7d84da6d', 'name': 'Mengye Ren', 'hidden': False}, {'_id': '693133f52d1e5b0a7d84da6e', 'user': {'_id': '65be9918b54ab5b37d1b67a7', 'avatarUrl': '/avatars/9953707affb6881724c8efb2abf0c668.svg', 'isPro': False, 'fullname': 'Greg Durrett', 'user': 'gregdurrett', 'type': 'user'}, 'name': 'Greg Durrett', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:13:18.166Z', 'hidden': False}], 'publishedAt': '2025-12-03T18:54:53.000Z', 'submittedOnDailyAt': '2025-12-04T04:41:19.927Z', 'title': 'SkillFactory: Self-Distillation For Learning Cognitive Behaviors', 'submittedOnDailyBy': {'_id': '6365c1158b6bd0d9f6605d3d', 'avatarUrl': '/avatars/1e3e6573e0cb2bb1b9d8043045120ee6.svg', 'isPro': False, 'fullname': 'Sedrick Keh', 'user': 'sedrickkeh', 'type': 'user'}, 'summary': 'Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren\\'t exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.', 'upvotes': 2, 'discussionId': '693133f62d1e5b0a7d84da6f', 'ai_summary': 'SkillFactory is a method for fine-tuning models to learn cognitive skills through supervised fine-tuning before reinforcement learning, enhancing their robustness and generalization post-RL.', 'ai_keywords': ['reasoning models', 'long chains of thought', 'verification', 'backtracking', 'retrying', 'reinforcement learning', 'SkillFactory', 'fine-tuning', 'supervised fine-tuning', 'silver SFT traces', 'cognitive skills', 'robustness', 'generalization', 'regression', 'inductive biases']}, 'publishedAt': '2025-12-03T13:54:53.000Z', 'title': 'SkillFactory: Self-Distillation For Learning Cognitive Behaviors', 'summary': 'Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren\\'t exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04072.png', 'numComments': 1, 'submittedBy': {'_id': '6365c1158b6bd0d9f6605d3d', 'avatarUrl': '/avatars/1e3e6573e0cb2bb1b9d8043045120ee6.svg', 'fullname': 'Sedrick Keh', 'name': 'sedrickkeh', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 10}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.04032', 'authors': [{'_id': '69316ebc2d1e5b0a7d84daf1', 'user': {'_id': '651e7084570ba4662812114b', 'avatarUrl': '/avatars/1438e5caa483f63dc0da5ee7508ef7eb.svg', 'isPro': False, 'fullname': 'Andreas Koukounas', 'user': 'koukandre', 'type': 'user'}, 'name': 'Andreas Koukounas', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T13:48:20.346Z', 'hidden': False}, {'_id': '69316ebc2d1e5b0a7d84daf2', 'user': {'_id': '64f8620e492828088373ddf9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64f8620e492828088373ddf9/g6XQmUzEPNMQYNc34gYpR.jpeg', 'isPro': False, 'fullname': 'Georgios Mastrapas', 'user': 'gmastrapas', 'type': 'user'}, 'name': 'Georgios Mastrapas', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T13:50:03.276Z', 'hidden': False}, {'_id': '69316ebc2d1e5b0a7d84daf3', 'name': 'Florian Hnicke', 'hidden': False}, {'_id': '69316ebc2d1e5b0a7d84daf4', 'name': 'Sedigheh Eslami', 'hidden': False}, {'_id': '69316ebc2d1e5b0a7d84daf5', 'name': 'Guillaume Roncari', 'hidden': False}, {'_id': '69316ebc2d1e5b0a7d84daf6', 'name': 'Scott Martens', 'hidden': False}, {'_id': '69316ebc2d1e5b0a7d84daf7', 'user': {'_id': '603763514de52ff951d89793', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png', 'isPro': False, 'fullname': 'Han Xiao', 'user': 'hanxiao', 'type': 'user'}, 'name': 'Han Xiao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T13:48:28.867Z', 'hidden': False}], 'publishedAt': '2025-12-03T18:13:41.000Z', 'submittedOnDailyAt': '2025-12-04T09:16:06.410Z', 'title': 'Jina-VLM: Small Multilingual Vision Language Model', 'submittedOnDailyBy': {'_id': '603763514de52ff951d89793', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png', 'isPro': False, 'fullname': 'Han Xiao', 'user': 'hanxiao', 'type': 'user'}, 'summary': 'We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.', 'upvotes': 2, 'discussionId': '69316ebc2d1e5b0a7d84daf8', 'ai_summary': 'Jina-VLM, a 2.4B parameter vision-language model, achieves top performance in multilingual visual question answering using a SigLIP2 vision encoder and Qwen3 language backbone with an attention-pooling connector.', 'ai_keywords': ['SigLIP2', 'Qwen3', 'attention-pooling connector', 'token-efficient processing', 'arbitrary-resolution images', 'multilingual visual question answering', 'VQA benchmarks', 'text-only performance'], 'organization': {'_id': '63563e0c2d14fcd7d83743cf', 'name': 'jinaai', 'fullname': 'Jina AI', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/603763514de52ff951d89793/wD54VbAHHyHop3uYlJKl4.png'}}, 'publishedAt': '2025-12-03T13:13:41.000Z', 'title': 'Jina-VLM: Small Multilingual Vision Language Model', 'summary': 'We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04032.png', 'numComments': 1, 'submittedBy': {'_id': '603763514de52ff951d89793', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png', 'fullname': 'Han Xiao', 'name': 'hanxiao', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 14}, 'organization': {'_id': '63563e0c2d14fcd7d83743cf', 'name': 'jinaai', 'fullname': 'Jina AI', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/603763514de52ff951d89793/wD54VbAHHyHop3uYlJKl4.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.03383', 'authors': [{'_id': '69311f792d1e5b0a7d84da37', 'user': {'_id': '65ca7d8d5cf913133d3b493a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65ca7d8d5cf913133d3b493a/i9qx6xdYeGSuwsxgQCb7N.png', 'isPro': False, 'fullname': 'Hung-Yueh Chiang', 'user': 'hychiang', 'type': 'user'}, 'name': 'Hung-Yueh Chiang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T08:43:34.992Z', 'hidden': False}, {'_id': '69311f792d1e5b0a7d84da38', 'user': {'_id': '6589f238d1331d552b1b26f5', 'avatarUrl': '/avatars/4a496d5791a79df9718e4a71845ae8eb.svg', 'isPro': False, 'fullname': 'Chi-Chih Chang', 'user': 'shadowpa0327', 'type': 'user'}, 'name': 'Chi-Chih Chang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:13:39.196Z', 'hidden': False}, {'_id': '69311f792d1e5b0a7d84da39', 'name': 'Yu-Chen Lu', 'hidden': False}, {'_id': '69311f792d1e5b0a7d84da3a', 'name': 'Chien-Yu Lin', 'hidden': False}, {'_id': '69311f792d1e5b0a7d84da3b', 'name': 'Kai-Chiang Wu', 'hidden': False}, {'_id': '69311f792d1e5b0a7d84da3c', 'name': 'Mohamed S. Abdelfattah', 'hidden': False}, {'_id': '69311f792d1e5b0a7d84da3d', 'name': 'Diana Marculescu', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65ca7d8d5cf913133d3b493a/LeGwptd-4EmQ4lKUHosDD.png'], 'publishedAt': '2025-12-03T02:33:39.000Z', 'submittedOnDailyAt': '2025-12-04T03:15:28.799Z', 'title': 'UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs', 'submittedOnDailyBy': {'_id': '65ca7d8d5cf913133d3b493a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65ca7d8d5cf913133d3b493a/i9qx6xdYeGSuwsxgQCb7N.png', 'isPro': False, 'fullname': 'Hung-Yueh Chiang', 'user': 'hychiang', 'type': 'user'}, 'summary': 'Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.', 'upvotes': 2, 'discussionId': '69311f792d1e5b0a7d84da3e', 'projectPage': 'https://hychiang.info/projects/uniql/', 'githubRepo': 'https://github.com/enyac-group/UniQL', 'ai_summary': 'UniQL, a unified post-training quantization and low-rank compression framework, enhances the deployment of large language models on mobile devices by reducing memory usage and improving token throughput while maintaining accuracy.', 'ai_keywords': ['post-training quantization', 'low-rank compression', 'edge LLMs', 'Transformers', 'State Space Models', 'hybrid models', 'structured weight-sorting', 'quantization-aware singular value decomposition', 'state-aware weight sorting', 'fused rotary positional embedding', 'on-device configurable pruning rates'], 'githubStars': 4, 'organization': {'_id': '679c32d81850ceed707b7b55', 'name': 'ut-enyac', 'fullname': 'Energy-Aware Computing Lab @ UT ECE', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/65ca7d8d5cf913133d3b493a/H3Yj4ix1yDD5-97pbGJdL.png'}}, 'publishedAt': '2025-12-02T21:33:39.000Z', 'title': 'UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs', 'summary': 'Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65ca7d8d5cf913133d3b493a/LeGwptd-4EmQ4lKUHosDD.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03383.png', 'numComments': 1, 'submittedBy': {'_id': '65ca7d8d5cf913133d3b493a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65ca7d8d5cf913133d3b493a/i9qx6xdYeGSuwsxgQCb7N.png', 'fullname': 'Hung-Yueh Chiang', 'name': 'hychiang', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'organization': {'_id': '679c32d81850ceed707b7b55', 'name': 'ut-enyac', 'fullname': 'Energy-Aware Computing Lab @ UT ECE', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/65ca7d8d5cf913133d3b493a/H3Yj4ix1yDD5-97pbGJdL.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.03979', 'authors': [{'_id': '6931a6342d1e5b0a7d84db59', 'name': 'Jin-Ting He', 'hidden': False}, {'_id': '6931a6342d1e5b0a7d84db5a', 'name': 'Fu-Jen Tsai', 'hidden': False}, {'_id': '6931a6342d1e5b0a7d84db5b', 'name': 'Yan-Tsung Peng', 'hidden': False}, {'_id': '6931a6342d1e5b0a7d84db5c', 'name': 'Min-Hung Chen', 'hidden': False}, {'_id': '6931a6342d1e5b0a7d84db5d', 'name': 'Chia-Wen Lin', 'hidden': False}, {'_id': '6931a6342d1e5b0a7d84db5e', 'name': 'Yen-Yu Lin', 'hidden': False}], 'publishedAt': '2025-12-03T17:10:44.000Z', 'submittedOnDailyAt': '2025-12-04T12:50:53.199Z', 'title': 'BlurDM: A Blur Diffusion Model for Image Deblurring', 'submittedOnDailyBy': {'_id': '64ae22dd1aee69ece065cdcd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png', 'isPro': False, 'fullname': 'Min-Hung Chen', 'user': 'cmhungsteve', 'type': 'user'}, 'summary': 'Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.', 'upvotes': 1, 'discussionId': '6931a6342d1e5b0a7d84db5f', 'ai_summary': 'Blur Diffusion Model (BlurDM) integrates blur formation into diffusion for image deblurring, enhancing deblurring methods by simultaneously denoising and deblurring images.', 'ai_keywords': ['diffusion models', 'Blur Diffusion Model', 'BlurDM', 'motion blur', 'dual-diffusion', 'dual denoising', 'deblurring', 'latent space', 'prior generation network'], 'organization': {'_id': '60262b67268c201cdc8b7d43', 'name': 'nvidia', 'fullname': 'NVIDIA', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png'}}, 'publishedAt': '2025-12-03T12:10:44.000Z', 'title': 'BlurDM: A Blur Diffusion Model for Image Deblurring', 'summary': 'Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03979.png', 'numComments': 1, 'submittedBy': {'_id': '64ae22dd1aee69ece065cdcd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png', 'fullname': 'Min-Hung Chen', 'name': 'cmhungsteve', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 8}, 'organization': {'_id': '60262b67268c201cdc8b7d43', 'name': 'nvidia', 'fullname': 'NVIDIA', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2512.03794', 'authors': [{'_id': '6930f6d92d1e5b0a7d84d986', 'user': {'_id': '687da36e2eaea8261f1323d6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hFH69bJGIDMikEYyClray.png', 'isPro': False, 'fullname': 'zichuan lin', 'user': 'zichuan-lin', 'type': 'user'}, 'name': 'Zichuan Lin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-12-04T14:56:15.707Z', 'hidden': False}, {'_id': '6930f6d92d1e5b0a7d84d987', 'name': 'Yicheng Liu', 'hidden': False}, {'_id': '6930f6d92d1e5b0a7d84d988', 'name': 'Yang Yang', 'hidden': False}, {'_id': '6930f6d92d1e5b0a7d84d989', 'name': 'Lvfang Tao', 'hidden': False}, {'_id': '6930f6d92d1e5b0a7d84d98a', 'name': 'Deheng Ye', 'hidden': False}], 'publishedAt': '2025-12-03T13:43:30.000Z', 'submittedOnDailyAt': '2025-12-04T13:03:13.693Z', 'title': 'AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition', 'submittedOnDailyBy': {'_id': '687da36e2eaea8261f1323d6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hFH69bJGIDMikEYyClray.png', 'isPro': False, 'fullname': 'zichuan lin', 'user': 'zichuan-lin', 'type': 'user'}, 'summary': 'Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.', 'upvotes': 1, 'discussionId': '6930f6d92d1e5b0a7d84d98b', 'ai_summary': 'AdaptVision, a vision-language model, dynamically adjusts visual token usage through a reinforcement learning framework to balance accuracy and efficiency in visual question answering tasks.', 'ai_keywords': ['Vision-Language Models', 'visual tokens', 'coarse-to-fine approach', 'bounding box tool', 'reinforcement learning', 'Decoupled Turn Policy Optimization', 'DTPO', 'VQA benchmarks'], 'organization': {'_id': '6645f953c39288df638dbdd5', 'name': 'Tencent-Hunyuan', 'fullname': 'Tencent Hunyuan', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png'}}, 'publishedAt': '2025-12-03T08:43:30.000Z', 'title': 'AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition', 'summary': 'Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03794.png', 'numComments': 1, 'submittedBy': {'_id': '687da36e2eaea8261f1323d6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hFH69bJGIDMikEYyClray.png', 'fullname': 'zichuan lin', 'name': 'zichuan-lin', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'organization': {'_id': '6645f953c39288df638dbdd5', 'name': 'Tencent-Hunyuan', 'fullname': 'Tencent Hunyuan', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2512.03771', 'authors': [{'_id': '69312ad72d1e5b0a7d84da54', 'user': {'_id': '62697edaa6a7bba9e46ae4ad', 'avatarUrl': '/avatars/3009bf769a09d946447a0e8c833a04f3.svg', 'isPro': False, 'fullname': 'Itay Yona', 'user': 'tux', 'type': 'user'}, 'name': 'Itay Yona', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:14:30.502Z', 'hidden': False}, {'_id': '69312ad72d1e5b0a7d84da55', 'user': {'_id': '6303edf5eedc089484c69468', 'avatarUrl': '/avatars/698f872020bbe9477eab7a597fc75d4f.svg', 'isPro': False, 'fullname': 'Amir Sarid', 'user': 'aimir', 'type': 'user'}, 'name': 'Amir Sarid', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:14:24.636Z', 'hidden': False}, {'_id': '69312ad72d1e5b0a7d84da56', 'user': {'_id': '684c13ec2bab8cb9a16919b5', 'avatarUrl': '/avatars/feb0f19205d87a2355d6d0c0d50b9c75.svg', 'isPro': False, 'fullname': 'Michael Karasik', 'user': 'MichaelKar', 'type': 'user'}, 'name': 'Michael Karasik', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:14:19.281Z', 'hidden': False}, {'_id': '69312ad72d1e5b0a7d84da57', 'user': {'_id': '62f6942205ca68c0e0ff7b97', 'avatarUrl': '/avatars/b0600de531b4c88f4aa7c30c5ceb06e1.svg', 'isPro': False, 'fullname': 'Yossi Gandelsman', 'user': 'yossig', 'type': 'user'}, 'name': 'Yossi Gandelsman', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:14:13.725Z', 'hidden': False}], 'publishedAt': '2025-12-03T13:19:34.000Z', 'submittedOnDailyAt': '2025-12-04T04:08:36.148Z', 'title': 'In-Context Representation Hijacking', 'submittedOnDailyBy': {'_id': '62697edaa6a7bba9e46ae4ad', 'avatarUrl': '/avatars/3009bf769a09d946447a0e8c833a04f3.svg', 'isPro': False, 'fullname': 'Itay Yona', 'user': 'tux', 'type': 'user'}, 'summary': \"We introduce Doublespeak, a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb) with a benign token (e.g., carrot) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\\\\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.\", 'upvotes': 1, 'discussionId': '69312ad72d1e5b0a7d84da58', 'ai_summary': 'The attack Doublespeak manipulates the internal representation of benign tokens to align with harmful semantics, bypassing safety measures in large language models.', 'ai_keywords': ['in-context representation hijacking', 'large language models', 'LLMs', 'harmful keyword', 'benign token', 'internal representation', 'euphemism', 'semantic overwrite', 'interpretability tools', 'latent space', 'safety alignment']}, 'publishedAt': '2025-12-03T08:19:34.000Z', 'title': 'In-Context Representation Hijacking', 'summary': \"We introduce Doublespeak, a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb) with a benign token (e.g., carrot) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\\\\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03771.png', 'numComments': 1, 'submittedBy': {'_id': '62697edaa6a7bba9e46ae4ad', 'avatarUrl': '/avatars/3009bf769a09d946447a0e8c833a04f3.svg', 'fullname': 'Itay Yona', 'name': 'tux', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2511.20494', 'authors': [{'_id': '692ffb1226742347f61daef9', 'user': {'_id': '6605626876a0652cac85f233', 'avatarUrl': '/avatars/fa008045c9ca4e0d71d40a02de74104d.svg', 'isPro': False, 'fullname': 'j-hoscilowic', 'user': 'j-hoscilowic', 'type': 'user'}, 'name': 'Jakub Hoscilowicz', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-12-04T11:15:07.222Z', 'hidden': False}, {'_id': '692ffb1226742347f61daefa', 'name': 'Artur Janicki', 'hidden': False}], 'publishedAt': '2025-11-25T17:00:31.000Z', 'submittedOnDailyAt': '2025-12-04T07:27:13.832Z', 'title': 'Adversarial Confusion Attack: Disrupting Multimodal Large Language Models', 'submittedOnDailyBy': {'_id': '6605626876a0652cac85f233', 'avatarUrl': '/avatars/fa008045c9ca4e0d71d40a02de74104d.svg', 'isPro': False, 'fullname': 'j-hoscilowic', 'user': 'j-hoscilowic', 'type': 'user'}, 'summary': 'We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Practical applications include embedding such adversarial images into websites to prevent MLLM-powered AI Agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and Adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.', 'upvotes': 0, 'discussionId': '692ffb1226742347f61daeff', 'ai_summary': 'The Adversarial Confusion Attack targets multimodal large language models to induce systematic disruption, leading to incoherent or confidently incorrect outputs, using a small ensemble and basic adversarial techniques.', 'ai_keywords': ['Adversarial Confusion Attack', 'multimodal large language models', 'next-token entropy', 'ensemble', 'PGD', 'Adversarial CAPTCHA', 'incoherent outputs', 'confidently incorrect outputs']}, 'publishedAt': '2025-11-25T12:00:31.000Z', 'title': 'Adversarial Confusion Attack: Disrupting Multimodal Large Language Models', 'summary': 'We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Practical applications include embedding such adversarial images into websites to prevent MLLM-powered AI Agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and Adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20494.png', 'numComments': 1, 'submittedBy': {'_id': '6605626876a0652cac85f233', 'avatarUrl': '/avatars/fa008045c9ca4e0d71d40a02de74104d.svg', 'fullname': 'j-hoscilowic', 'name': 'j-hoscilowic', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}"
]