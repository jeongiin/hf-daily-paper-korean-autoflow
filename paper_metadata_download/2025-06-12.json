[
    "{'paper': {'id': '2506.06395', 'authors': [{'_id': '68492dcf42e4f9106973f437', 'user': {'_id': '6734e315c1aadce903f73aea', 'avatarUrl': '/avatars/95d95c49419372debc201cb63c354b86.svg', 'isPro': False, 'fullname': 'Li Pengyi', 'user': 'LiPengyi29', 'type': 'user'}, 'name': 'Pengyi Li', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-11T07:18:40.287Z', 'hidden': False}, {'_id': '68492dcf42e4f9106973f438', 'user': {'_id': '6626c5d0a329de26e7eb16fa', 'avatarUrl': '/avatars/124f389f768fb666efd8b5a9b54c3b3c.svg', 'isPro': False, 'fullname': 'Matvey Skripkin', 'user': 'barracuda049', 'type': 'user'}, 'name': 'Matvey Skripkin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:42:44.217Z', 'hidden': False}, {'_id': '68492dcf42e4f9106973f439', 'name': 'Alexander Zubrey', 'hidden': False}, {'_id': '68492dcf42e4f9106973f43a', 'user': {'_id': '643984dceb7c5616ef3f5d54', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg', 'isPro': False, 'fullname': 'Andrey Kuznetsov', 'user': 'kuznetsoffandrey', 'type': 'user'}, 'name': 'Andrey Kuznetsov', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:42:42.242Z', 'hidden': False}, {'_id': '68492dcf42e4f9106973f43b', 'user': {'_id': '6169a581d05945bfd8718dfa', 'avatarUrl': '/avatars/1892ab06a7ddb557232777de3cbec470.svg', 'isPro': False, 'fullname': 'Ivan Oseledets', 'user': 'oseledets', 'type': 'user'}, 'name': 'Ivan Oseledets', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:42:40.519Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg'], 'publishedAt': '2025-06-05T19:55:15.000Z', 'submittedOnDailyAt': '2025-06-12T07:02:06.762Z', 'title': 'Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models', 'submittedOnDailyBy': {'_id': '643984dceb7c5616ef3f5d54', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg', 'isPro': False, 'fullname': 'Andrey Kuznetsov', 'user': 'kuznetsoffandrey', 'type': 'user'}, 'summary': \"Large language models (LLMs) excel at reasoning, yet post-training remains\\ncritical for aligning their behavior with task goals. Existing reinforcement\\nlearning (RL) methods often depend on costly human annotations or external\\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\\nwhich uses the model's own confidence as reward signals-eliminating the need\\nfor labels, preference models, or reward engineering. Applied to\\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\\nsimple, scalable post-training method for inference models, requiring only a\\nsmall number of samples and unlabelled supervision.\", 'upvotes': 56, 'discussionId': '68492dd042e4f9106973f43c', 'ai_summary': \"Reinforcement Learning via Self-Confidence (RLSC) improves large language model accuracy using the model's confidence as a reward signal, eliminating the need for human labels or reward engineering.\", 'ai_keywords': ['Reinforcement Learning', 'Large language models', 'self-confidence', 'RLSC']}, 'publishedAt': '2025-06-05T15:55:15.000Z', 'title': 'Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models', 'summary': \"Large language models (LLMs) excel at reasoning, yet post-training remains\\ncritical for aligning their behavior with task goals. Existing reinforcement\\nlearning (RL) methods often depend on costly human annotations or external\\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\\nwhich uses the model's own confidence as reward signals-eliminating the need\\nfor labels, preference models, or reward engineering. Applied to\\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\\nsimple, scalable post-training method for inference models, requiring only a\\nsmall number of samples and unlabelled supervision.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06395.png', 'numComments': 4, 'submittedBy': {'_id': '643984dceb7c5616ef3f5d54', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg', 'fullname': 'Andrey Kuznetsov', 'name': 'kuznetsoffandrey', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 20}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.09113', 'authors': [{'_id': '684a3b0a9b38e1e5a33a683f', 'user': {'_id': '6614f2dca37a503c2320b44e', 'avatarUrl': '/avatars/4bee18c49eff253d6eeb9a1f1509b68b.svg', 'isPro': False, 'fullname': 'gaoyu', 'user': 'gaooyu1520', 'type': 'user'}, 'name': 'Yu Gao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:42:01.191Z', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6840', 'name': 'Haoyuan Guo', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6841', 'name': 'Tuyen Hoang', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6842', 'name': 'Weilin Huang', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6843', 'name': 'Lu Jiang', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6844', 'name': 'Fangyuan Kong', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6845', 'name': 'Huixia Li', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6846', 'name': 'Jiashi Li', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6847', 'name': 'Liang Li', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6848', 'name': 'Xiaojie Li', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6849', 'name': 'Xunsong Li', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a684a', 'name': 'Yifu Li', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a684b', 'name': 'Shanchuan Lin', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a684c', 'name': 'Zhijie Lin', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a684d', 'user': {'_id': '63049b95dae2eb7d083f1bf3', 'avatarUrl': '/avatars/5eaeadc1318724b54ea59873da11275f.svg', 'isPro': False, 'fullname': 'Jiawei Liu', 'user': 'jwliu-cc', 'type': 'user'}, 'name': 'Jiawei Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:41:55.061Z', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a684e', 'name': 'Shu Liu', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a684f', 'name': 'Xiaonan Nie', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6850', 'name': 'Zhiwu Qing', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6851', 'name': 'Yuxi Ren', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6852', 'name': 'Li Sun', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6853', 'name': 'Zhi Tian', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6854', 'name': 'Rui Wang', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6855', 'name': 'Sen Wang', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6856', 'name': 'Guoqiang Wei', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6857', 'name': 'Guohong Wu', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6858', 'user': {'_id': '6381c5d63680a7cf34e08ca9', 'avatarUrl': '/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg', 'isPro': False, 'fullname': 'wujie10558@gmail.com', 'user': 'wujie10', 'type': 'user'}, 'name': 'Jie Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:42:05.891Z', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6859', 'name': 'Ruiqi Xia', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a685a', 'name': 'Fei Xiao', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a685b', 'name': 'Xuefeng Xiao', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a685c', 'name': 'Jiangqiao Yan', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a685d', 'name': 'Ceyuan Yang', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a685e', 'name': 'Jianchao Yang', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a685f', 'name': 'Runkai Yang', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6860', 'name': 'Tao Yang', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6861', 'name': 'Yihang Yang', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6862', 'user': {'_id': '65b62ab582d3845134ef0aab', 'avatarUrl': '/avatars/fb1c22b2937e86f668b06fedb48e57e0.svg', 'isPro': False, 'fullname': 'Zilyu Ye', 'user': 'YeLuoSuiYou', 'type': 'user'}, 'name': 'Zilyu Ye', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:41:58.552Z', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6863', 'name': 'Xuejiao Zeng', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6864', 'name': 'Yan Zeng', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6865', 'name': 'Heng Zhang', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6866', 'name': 'Yang Zhao', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6867', 'name': 'Xiaozheng Zheng', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6868', 'name': 'Peihao Zhu', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a6869', 'name': 'Jiaxin Zou', 'hidden': False}, {'_id': '684a3b0a9b38e1e5a33a686a', 'name': 'Feilong Zuo', 'hidden': False}], 'publishedAt': '2025-06-10T17:56:11.000Z', 'submittedOnDailyAt': '2025-06-12T01:08:56.090Z', 'title': 'Seedance 1.0: Exploring the Boundaries of Video Generation Models', 'submittedOnDailyBy': {'_id': '6381c5d63680a7cf34e08ca9', 'avatarUrl': '/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg', 'isPro': False, 'fullname': 'wujie10558@gmail.com', 'user': 'wujie10', 'type': 'user'}, 'summary': 'Notable breakthroughs in diffusion modeling have propelled rapid improvements\\nin video generation, yet current foundational model still face critical\\nchallenges in simultaneously balancing prompt following, motion plausibility,\\nand visual quality. In this report, we introduce Seedance 1.0, a\\nhigh-performance and inference-efficient video foundation generation model that\\nintegrates several core technical improvements: (i) multi-source data curation\\naugmented with precision and meaningful video captioning, enabling\\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\\ndesign with proposed training paradigm, which allows for natively supporting\\nmulti-shot generation and jointly learning of both text-to-video and\\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\\n(iv) excellent model acceleration achieving ~10x inference speedup through\\nmulti-stage distillation strategies and system-level optimizations. Seedance\\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\\n1.0 stands out with high-quality and fast video generation having superior\\nspatiotemporal fluidity with structural stability, precise instruction\\nadherence in complex multi-subject contexts, native multi-shot narrative\\ncoherence with consistent subject representation.', 'upvotes': 41, 'discussionId': '684a3b0b9b38e1e5a33a686b', 'projectPage': 'https://seed.bytedance.com/seedance', 'ai_summary': 'Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.', 'ai_keywords': ['diffusion modeling', 'multi-source data curation', 'precision and meaningful video captioning', 'efficient architecture', 'training paradigm', 'multi-shot generation', 'text-to-video', 'image-to-video', 'fine-grained supervised fine-tuning', 'video-specific RLHF', 'multi-dimensional reward mechanisms', 'multi-stage distillation strategies', 'model acceleration', 'spatiotemporal fluidity', 'structural stability', 'instruction adherence', 'multi-shot narrative coherence', 'consistent subject representation']}, 'publishedAt': '2025-06-10T13:56:11.000Z', 'title': 'Seedance 1.0: Exploring the Boundaries of Video Generation Models', 'summary': 'Notable breakthroughs in diffusion modeling have propelled rapid improvements\\nin video generation, yet current foundational model still face critical\\nchallenges in simultaneously balancing prompt following, motion plausibility,\\nand visual quality. In this report, we introduce Seedance 1.0, a\\nhigh-performance and inference-efficient video foundation generation model that\\nintegrates several core technical improvements: (i) multi-source data curation\\naugmented with precision and meaningful video captioning, enabling\\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\\ndesign with proposed training paradigm, which allows for natively supporting\\nmulti-shot generation and jointly learning of both text-to-video and\\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\\n(iv) excellent model acceleration achieving ~10x inference speedup through\\nmulti-stage distillation strategies and system-level optimizations. Seedance\\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\\n1.0 stands out with high-quality and fast video generation having superior\\nspatiotemporal fluidity with structural stability, precise instruction\\nadherence in complex multi-subject contexts, native multi-shot narrative\\ncoherence with consistent subject representation.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09113.png', 'numComments': 1, 'submittedBy': {'_id': '6381c5d63680a7cf34e08ca9', 'avatarUrl': '/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg', 'fullname': 'wujie10558@gmail.com', 'name': 'wujie10', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.09350', 'authors': [{'_id': '684a79ca9b38e1e5a33a68bf', 'user': {'_id': '645863f7dc18eb1a9b5d29df', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645863f7dc18eb1a9b5d29df/t49Nnyl4tbkUn7CmQqKZh.jpeg', 'isPro': False, 'fullname': 'Peter Lin', 'user': 'PeterL1n', 'type': 'user'}, 'name': 'Shanchuan Lin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:40:47.762Z', 'hidden': False}, {'_id': '684a79ca9b38e1e5a33a68c0', 'name': 'Ceyuan Yang', 'hidden': False}, {'_id': '684a79ca9b38e1e5a33a68c1', 'name': 'Hao He', 'hidden': False}, {'_id': '684a79ca9b38e1e5a33a68c2', 'name': 'Jianwen Jiang', 'hidden': False}, {'_id': '684a79ca9b38e1e5a33a68c3', 'name': 'Yuxi Ren', 'hidden': False}, {'_id': '684a79ca9b38e1e5a33a68c4', 'name': 'Xin Xia', 'hidden': False}, {'_id': '684a79ca9b38e1e5a33a68c5', 'name': 'Yang Zhao', 'hidden': False}, {'_id': '684a79ca9b38e1e5a33a68c6', 'name': 'Xuefeng Xiao', 'hidden': False}, {'_id': '684a79ca9b38e1e5a33a68c7', 'name': 'Lu Jiang', 'hidden': False}], 'publishedAt': '2025-06-11T03:04:23.000Z', 'submittedOnDailyAt': '2025-06-12T05:25:53.654Z', 'title': 'Autoregressive Adversarial Post-Training for Real-Time Interactive Video\\n  Generation', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': True, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': 'Existing large-scale video generation models are computationally intensive,\\npreventing adoption in real-time and interactive applications. In this work, we\\npropose autoregressive adversarial post-training (AAPT) to transform a\\npre-trained latent video diffusion model into a real-time, interactive video\\ngenerator. Our model autoregressively generates a latent frame at a time using\\na single neural function evaluation (1NFE). The model can stream the result to\\nthe user in real time and receive interactive responses as controls to generate\\nthe next latent frame. Unlike existing approaches, our method explores\\nadversarial training as an effective paradigm for autoregressive generation.\\nThis not only allows us to design an architecture that is more efficient for\\none-step generation while fully utilizing the KV cache, but also enables\\ntraining the model in a student-forcing manner that proves to be effective in\\nreducing error accumulation during long video generation. Our experiments\\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\\na minute long (1440 frames). Visit our research website at\\nhttps://seaweed-apt.com/2', 'upvotes': 27, 'discussionId': '684a79ca9b38e1e5a33a68c8', 'projectPage': 'https://seaweed-apt.com/2', 'ai_summary': 'Autoregressive adversarial post-training transforms a pre-trained latent video diffusion model into a real-time, interactive video generator with efficient one-step generation and reduced error accumulation.', 'ai_keywords': ['autoregressive adversarial post-training', 'latent video diffusion model', 'single neural function evaluation', 'KV cache', 'student-forcing', 'real-time video generation', '24fps', '736x416 resolution', '1280x720 resolution', 'H100 GPUs']}, 'publishedAt': '2025-06-10T23:04:23.000Z', 'title': 'Autoregressive Adversarial Post-Training for Real-Time Interactive Video\\n  Generation', 'summary': 'Existing large-scale video generation models are computationally intensive,\\npreventing adoption in real-time and interactive applications. In this work, we\\npropose autoregressive adversarial post-training (AAPT) to transform a\\npre-trained latent video diffusion model into a real-time, interactive video\\ngenerator. Our model autoregressively generates a latent frame at a time using\\na single neural function evaluation (1NFE). The model can stream the result to\\nthe user in real time and receive interactive responses as controls to generate\\nthe next latent frame. Unlike existing approaches, our method explores\\nadversarial training as an effective paradigm for autoregressive generation.\\nThis not only allows us to design an architecture that is more efficient for\\none-step generation while fully utilizing the KV cache, but also enables\\ntraining the model in a student-forcing manner that proves to be effective in\\nreducing error accumulation during long video generation. Our experiments\\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\\na minute long (1440 frames). Visit our research website at\\nhttps://seaweed-apt.com/2', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09350.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': True, 'isHf': True, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7099}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.09790', 'authors': [{'_id': '684a33989b38e1e5a33a6804', 'user': {'_id': '639c379cdb7c5f35004066cb', 'avatarUrl': '/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg', 'isPro': False, 'fullname': 'Zhenran Xu', 'user': 'imryanxu', 'type': 'user'}, 'name': 'Zhenran Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:42:13.515Z', 'hidden': False}, {'_id': '684a33989b38e1e5a33a6805', 'user': {'_id': '6309995efa440d8b5bd5ddc2', 'avatarUrl': '/avatars/e7350783a8edc5660afd5173818f02f2.svg', 'isPro': False, 'fullname': 'Yiyu Wang', 'user': 'Curya', 'type': 'user'}, 'name': 'Yiyu Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:42:11.706Z', 'hidden': False}, {'_id': '684a33989b38e1e5a33a6806', 'name': 'Xue Yang', 'hidden': False}, {'_id': '684a33989b38e1e5a33a6807', 'name': 'Longyue Wang', 'hidden': False}, {'_id': '684a33989b38e1e5a33a6808', 'name': 'Weihua Luo', 'hidden': False}, {'_id': '684a33989b38e1e5a33a6809', 'name': 'Kaifu Zhang', 'hidden': False}, {'_id': '684a33989b38e1e5a33a680a', 'name': 'Baotian Hu', 'hidden': False}, {'_id': '684a33989b38e1e5a33a680b', 'name': 'Min Zhang', 'hidden': False}], 'publishedAt': '2025-06-11T14:35:15.000Z', 'submittedOnDailyAt': '2025-06-12T00:38:07.422Z', 'title': 'ComfyUI-R1: Exploring Reasoning Models for Workflow Generation', 'submittedOnDailyBy': {'_id': '639c379cdb7c5f35004066cb', 'avatarUrl': '/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg', 'isPro': False, 'fullname': 'Zhenran Xu', 'user': 'imryanxu', 'type': 'user'}, 'summary': 'AI-generated content has evolved from monolithic models to modular workflows,\\nparticularly on platforms like ComfyUI, enabling customization in creative\\npipelines. However, crafting effective workflows requires great expertise to\\norchestrate numerous specialized components, presenting a steep learning curve\\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\\nreasoning model for automated workflow generation. Starting with our curated\\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\\ndata, including node selection, workflow planning, and code-level workflow\\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\\nreinforcement learning for incentivizing reasoning capability, guided by a\\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\\nmodel achieves a 97\\\\% format validity rate, along with high pass rate,\\nnode-level and graph-level F1 scores, significantly surpassing prior\\nstate-of-the-art methods that employ leading closed-source models such as\\nGPT-4o and Claude series. Further analysis highlights the critical role of the\\nreasoning process and the advantage of transforming workflows into code.\\nQualitative comparison reveals our strength in synthesizing intricate workflows\\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\\ncreation.', 'upvotes': 25, 'discussionId': '684a33989b38e1e5a33a680c', 'projectPage': 'https://github.com/AIDC-AI/ComfyUI-Copilot', 'githubRepo': 'https://github.com/AIDC-AI/ComfyUI-Copilot', 'ai_summary': 'ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.', 'ai_keywords': ['modular workflows', 'ComfyUI', 'large reasoning model', 'automated workflow generation', 'chain-of-thought (CoT) reasoning', 'node selection', 'workflow planning', 'code-level workflow representation', 'CoT fine-tuning', 'reinforcement learning', 'fine-grained rule-metric hybrid reward', 'format validity', 'structural integrity', 'node-level fidelity', 'GPT-4o', 'Claude series', 'pass rate', 'node-level F1 scores', 'graph-level F1 scores', 'intricate workflows', 'diverse nodes', 'qualitative comparison', 'AI art creation']}, 'publishedAt': '2025-06-11T10:35:15.000Z', 'title': 'ComfyUI-R1: Exploring Reasoning Models for Workflow Generation', 'summary': 'AI-generated content has evolved from monolithic models to modular workflows,\\nparticularly on platforms like ComfyUI, enabling customization in creative\\npipelines. However, crafting effective workflows requires great expertise to\\norchestrate numerous specialized components, presenting a steep learning curve\\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\\nreasoning model for automated workflow generation. Starting with our curated\\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\\ndata, including node selection, workflow planning, and code-level workflow\\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\\nreinforcement learning for incentivizing reasoning capability, guided by a\\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\\nmodel achieves a 97\\\\% format validity rate, along with high pass rate,\\nnode-level and graph-level F1 scores, significantly surpassing prior\\nstate-of-the-art methods that employ leading closed-source models such as\\nGPT-4o and Claude series. Further analysis highlights the critical role of the\\nreasoning process and the advantage of transforming workflows into code.\\nQualitative comparison reveals our strength in synthesizing intricate workflows\\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\\ncreation.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09790.png', 'numComments': 1, 'submittedBy': {'_id': '639c379cdb7c5f35004066cb', 'avatarUrl': '/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg', 'fullname': 'Zhenran Xu', 'name': 'imryanxu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.09995', 'authors': [{'_id': '684a39639b38e1e5a33a6837', 'name': 'Yuanpeng Tu', 'hidden': False}, {'_id': '684a39639b38e1e5a33a6838', 'name': 'Hao Luo', 'hidden': False}, {'_id': '684a39639b38e1e5a33a6839', 'user': {'_id': '644a1b6401e18bf93a6f45c1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png', 'isPro': False, 'fullname': 'xichen', 'user': 'xichenhku', 'type': 'user'}, 'name': 'Xi Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:42:07.608Z', 'hidden': False}, {'_id': '684a39639b38e1e5a33a683a', 'name': 'Xiang Bai', 'hidden': False}, {'_id': '684a39639b38e1e5a33a683b', 'name': 'Fan Wang', 'hidden': False}, {'_id': '684a39639b38e1e5a33a683c', 'name': 'Hengshuang Zhao', 'hidden': False}], 'publishedAt': '2025-06-11T17:59:53.000Z', 'submittedOnDailyAt': '2025-06-12T00:50:19.796Z', 'title': 'PlayerOne: Egocentric World Simulator', 'submittedOnDailyBy': {'_id': '644a1b6401e18bf93a6f45c1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png', 'isPro': False, 'fullname': 'xichen', 'user': 'xichenhku', 'type': 'user'}, 'summary': 'We introduce PlayerOne, the first egocentric realistic world simulator,\\nfacilitating immersive and unrestricted exploration within vividly dynamic\\nenvironments. Given an egocentric scene image from the user, PlayerOne can\\naccurately construct the corresponding world and generate egocentric videos\\nthat are strictly aligned with the real scene human motion of the user captured\\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\\nfirst performs pretraining on large-scale egocentric text-video pairs for\\ncoarse-level egocentric understanding, followed by finetuning on synchronous\\nmotion-video data extracted from egocentric-exocentric video datasets with our\\nautomatic construction pipeline. Besides, considering the varying importance of\\ndifferent components, we design a part-disentangled motion injection scheme,\\nenabling precise control of part-level movements. In addition, we devise a\\njoint reconstruction framework that progressively models both the 4D scene and\\nvideo frames, ensuring scene consistency in the long-form video generation.\\nExperimental results demonstrate its great generalization ability in precise\\ncontrol of varying human movements and worldconsistent modeling of diverse\\nscenarios. It marks the first endeavor into egocentric real-world simulation\\nand can pave the way for the community to delve into fresh frontiers of world\\nmodeling and its diverse applications.', 'upvotes': 23, 'discussionId': '684a39639b38e1e5a33a683d', 'projectPage': 'https://playerone-hku.github.io/', 'ai_summary': 'PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.', 'ai_keywords': ['egocentric realistic world simulator', 'coarse-to-fine pipeline', 'pretraining', 'finetuning', 'synchronous motion-video data', 'automatic construction pipeline', 'part-disentangled motion injection', 'joint reconstruction framework', '4D scene', 'video frames', 'scene consistency', 'long-form video generation', 'worldconsistent modeling']}, 'publishedAt': '2025-06-11T13:59:53.000Z', 'title': 'PlayerOne: Egocentric World Simulator', 'summary': 'We introduce PlayerOne, the first egocentric realistic world simulator,\\nfacilitating immersive and unrestricted exploration within vividly dynamic\\nenvironments. Given an egocentric scene image from the user, PlayerOne can\\naccurately construct the corresponding world and generate egocentric videos\\nthat are strictly aligned with the real scene human motion of the user captured\\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\\nfirst performs pretraining on large-scale egocentric text-video pairs for\\ncoarse-level egocentric understanding, followed by finetuning on synchronous\\nmotion-video data extracted from egocentric-exocentric video datasets with our\\nautomatic construction pipeline. Besides, considering the varying importance of\\ndifferent components, we design a part-disentangled motion injection scheme,\\nenabling precise control of part-level movements. In addition, we devise a\\njoint reconstruction framework that progressively models both the 4D scene and\\nvideo frames, ensuring scene consistency in the long-form video generation.\\nExperimental results demonstrate its great generalization ability in precise\\ncontrol of varying human movements and worldconsistent modeling of diverse\\nscenarios. It marks the first endeavor into egocentric real-world simulation\\nand can pave the way for the community to delve into fresh frontiers of world\\nmodeling and its diverse applications.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09995.png', 'numComments': 1, 'submittedBy': {'_id': '644a1b6401e18bf93a6f45c1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png', 'fullname': 'xichen', 'name': 'xichenhku', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 43}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.08889', 'authors': [{'_id': '684a39599b38e1e5a33a6822', 'name': 'Yizhao Gao', 'hidden': False}, {'_id': '684a39599b38e1e5a33a6823', 'name': 'Shuming Guo', 'hidden': False}, {'_id': '684a39599b38e1e5a33a6824', 'name': 'Shijie Cao', 'hidden': False}, {'_id': '684a39599b38e1e5a33a6825', 'name': 'Yuqing Xia', 'hidden': False}, {'_id': '684a39599b38e1e5a33a6826', 'name': 'Yu Cheng', 'hidden': False}, {'_id': '684a39599b38e1e5a33a6827', 'name': 'Lei Wang', 'hidden': False}, {'_id': '684a39599b38e1e5a33a6828', 'name': 'Lingxiao Ma', 'hidden': False}, {'_id': '684a39599b38e1e5a33a6829', 'name': 'Yutao Sun', 'hidden': False}, {'_id': '684a39599b38e1e5a33a682a', 'name': 'Tianzhu Ye', 'hidden': False}, {'_id': '684a39599b38e1e5a33a682b', 'name': 'Li Dong', 'hidden': False}, {'_id': '684a39599b38e1e5a33a682c', 'name': 'Hayden Kwok-Hay So', 'hidden': False}, {'_id': '684a39599b38e1e5a33a682d', 'name': 'Yu Hua', 'hidden': False}, {'_id': '684a39599b38e1e5a33a682e', 'name': 'Ting Cao', 'hidden': False}, {'_id': '684a39599b38e1e5a33a682f', 'name': 'Fan Yang', 'hidden': False}, {'_id': '684a39599b38e1e5a33a6830', 'name': 'Mao Yang', 'hidden': False}], 'publishedAt': '2025-06-10T15:17:26.000Z', 'submittedOnDailyAt': '2025-06-12T00:54:43.454Z', 'title': 'SeerAttention-R: Sparse Attention Adaptation for Long Reasoning', 'submittedOnDailyBy': {'_id': '661c96f48921f03a9dae04c3', 'avatarUrl': '/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg', 'isPro': False, 'fullname': 'Yizhao Gao', 'user': 'LongMountain', 'type': 'user'}, 'summary': 'We introduce SeerAttention-R, a sparse attention framework specifically\\ntailored for the long decoding of reasoning models. Extended from\\nSeerAttention, SeerAttention-R retains the design of learning attention\\nsparsity through a self-distilled gating mechanism, while removing query\\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\\ngating, SeerAttention-R is flexible and can be easily integrated into existing\\npretrained model without modifying the original parameters. We demonstrate that\\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\\naccuracy with 4K token budget in AIME benchmark under large sparse attention\\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\\nhttps://github.com/microsoft/SeerAttention.', 'upvotes': 17, 'discussionId': '684a39599b38e1e5a33a6833', 'githubRepo': 'https://github.com/microsoft/SeerAttention', 'ai_summary': 'SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.', 'ai_keywords': ['sparse attention', 'reasoning models', 'self-distilled gating mechanism', 'query pooling', 'lightweight plug-in gating', 'AIME benchmark', 'TileLang', 'sparse decoding kernel', 'FlashAttention-3', 'H100 GPU']}, 'publishedAt': '2025-06-10T11:17:26.000Z', 'title': 'SeerAttention-R: Sparse Attention Adaptation for Long Reasoning', 'summary': 'We introduce SeerAttention-R, a sparse attention framework specifically\\ntailored for the long decoding of reasoning models. Extended from\\nSeerAttention, SeerAttention-R retains the design of learning attention\\nsparsity through a self-distilled gating mechanism, while removing query\\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\\ngating, SeerAttention-R is flexible and can be easily integrated into existing\\npretrained model without modifying the original parameters. We demonstrate that\\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\\naccuracy with 4K token budget in AIME benchmark under large sparse attention\\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\\nhttps://github.com/microsoft/SeerAttention.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08889.png', 'numComments': 1, 'submittedBy': {'_id': '661c96f48921f03a9dae04c3', 'avatarUrl': '/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg', 'fullname': 'Yizhao Gao', 'name': 'LongMountain', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.08570', 'authors': [{'_id': '684982b5d546b83e67deb341', 'user': {'_id': '64de1ffa016e232d2eed1f23', 'avatarUrl': '/avatars/6e6aa0984302254d28a659529f874740.svg', 'isPro': False, 'fullname': 'Or Tal', 'user': 'ortal1602', 'type': 'user'}, 'name': 'Or Tal', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:42:30.762Z', 'hidden': False}, {'_id': '684982b5d546b83e67deb342', 'user': {'_id': '66f526d04970b1adc2651d7e', 'avatarUrl': '/avatars/2dd952cba07867a9e9ab443b37052a95.svg', 'isPro': False, 'fullname': 'Felix Kreuk', 'user': 'felixkreuk1', 'type': 'user'}, 'name': 'Felix Kreuk', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-11T13:20:53.749Z', 'hidden': False}, {'_id': '684982b5d546b83e67deb343', 'name': 'Yossi Adi', 'hidden': False}], 'publishedAt': '2025-06-10T08:37:45.000Z', 'submittedOnDailyAt': '2025-06-12T11:15:01.307Z', 'title': 'Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\\n  Paradigms for Text-to-Music Generation', 'submittedOnDailyBy': {'_id': '6547411a9295970f878aa52e', 'avatarUrl': '/avatars/6e240f0add27bf1a6c04a9618eccdf83.svg', 'isPro': False, 'fullname': 'Michael Hassid', 'user': 'hassid', 'type': 'user'}, 'summary': 'Recent progress in text-to-music generation has enabled models to synthesize\\nhigh-quality musical segments, full compositions, and even respond to\\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\\nsystems differ significantly across many dimensions, such as training datasets,\\nmodeling paradigms, and architectural choices. This diversity complicates\\nefforts to evaluate models fairly and pinpoint which design choices most\\ninfluence performance. While factors like data and architecture are important,\\nin this study we focus exclusively on the modeling paradigm. We conduct a\\nsystematic empirical analysis to isolate its effects, offering insights into\\nassociated trade-offs and emergent behaviors that can guide future\\ntext-to-music generation systems. Specifically, we compare the two arguably\\nmost common modeling paradigms: Auto-Regressive decoding and Conditional\\nFlow-Matching. We conduct a controlled comparison by training all models from\\nscratch using identical datasets, training configurations, and similar backbone\\narchitectures. Performance is evaluated across multiple axes, including\\ngeneration quality, robustness to inference configurations, scalability,\\nadherence to both textual and temporally aligned conditioning, and editing\\ncapabilities in the form of audio inpainting. This comparative study sheds\\nlight on distinct strengths and limitations of each paradigm, providing\\nactionable insights that can inform future architectural and training decisions\\nin the evolving landscape of text-to-music generation. Audio sampled examples\\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM', 'upvotes': 17, 'discussionId': '684982b5d546b83e67deb344', 'ai_summary': 'A systematic comparison of Auto-Regressive decoding and Conditional Flow-Matching in text-to-music generation highlights distinct strengths and limitations of each modeling paradigm.', 'ai_keywords': ['Auto-Regressive decoding', 'Conditional Flow-Matching', 'generation quality', 'robustness', 'scalability', 'textual conditioning', 'temporal alignment', 'audio inpainting']}, 'publishedAt': '2025-06-10T04:37:45.000Z', 'title': 'Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\\n  Paradigms for Text-to-Music Generation', 'summary': 'Recent progress in text-to-music generation has enabled models to synthesize\\nhigh-quality musical segments, full compositions, and even respond to\\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\\nsystems differ significantly across many dimensions, such as training datasets,\\nmodeling paradigms, and architectural choices. This diversity complicates\\nefforts to evaluate models fairly and pinpoint which design choices most\\ninfluence performance. While factors like data and architecture are important,\\nin this study we focus exclusively on the modeling paradigm. We conduct a\\nsystematic empirical analysis to isolate its effects, offering insights into\\nassociated trade-offs and emergent behaviors that can guide future\\ntext-to-music generation systems. Specifically, we compare the two arguably\\nmost common modeling paradigms: Auto-Regressive decoding and Conditional\\nFlow-Matching. We conduct a controlled comparison by training all models from\\nscratch using identical datasets, training configurations, and similar backbone\\narchitectures. Performance is evaluated across multiple axes, including\\ngeneration quality, robustness to inference configurations, scalability,\\nadherence to both textual and temporally aligned conditioning, and editing\\ncapabilities in the form of audio inpainting. This comparative study sheds\\nlight on distinct strengths and limitations of each paradigm, providing\\nactionable insights that can inform future architectural and training decisions\\nin the evolving landscape of text-to-music generation. Audio sampled examples\\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08570.png', 'numComments': 1, 'submittedBy': {'_id': '6547411a9295970f878aa52e', 'avatarUrl': '/avatars/6e240f0add27bf1a6c04a9618eccdf83.svg', 'fullname': 'Michael Hassid', 'name': 'hassid', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.09991', 'authors': [{'_id': '684adf25dbd21a9cc27b0ec8', 'name': 'Xinyu Yang', 'hidden': False}, {'_id': '684adf25dbd21a9cc27b0ec9', 'name': 'Yuwei An', 'hidden': False}, {'_id': '684adf25dbd21a9cc27b0eca', 'name': 'Hongyi Liu', 'hidden': False}, {'_id': '684adf25dbd21a9cc27b0ecb', 'name': 'Tianqi Chen', 'hidden': False}, {'_id': '684adf25dbd21a9cc27b0ecc', 'name': 'Beidi Chen', 'hidden': False}], 'publishedAt': '2025-06-11T17:59:23.000Z', 'submittedOnDailyAt': '2025-06-12T12:38:22.483Z', 'title': 'Multiverse: Your Language Models Secretly Decide How to Parallelize and\\n  Merge Generation', 'submittedOnDailyBy': {'_id': '64f58b970b24e548a85522bc', 'avatarUrl': '/avatars/c8ca1294b5a1edd609694877e335b22f.svg', 'isPro': False, 'fullname': 'Xinyu Yang', 'user': 'Hanyuezhuohua', 'type': 'user'}, 'summary': 'Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\\nparallelism in sequential generation. Inspired by this, we introduce\\nMultiverse, a new generative model that enables natively parallel generation.\\nMultiverse internalizes a MapReduce paradigm, generating automatically through\\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\\nMultiverse 1K by converting them into structured training data using an\\nautomated LLM-assisted pipeline, avoiding costly human annotations.\\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\\nsteps while keeping compatibility with causal attention for efficient training.\\nSystematically, we implement Multiverse Engine to enable parallel inference. It\\nfeatures a dedicated scheduler that dynamically switches between sequential and\\nparallel generation, triggered directly by the model. After a 3-hour\\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\\nMoreover, our budget control experiments show that Multiverse-32B exhibits\\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\\ncontext length. Such scaling further leads to practical efficiency gain,\\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\\nentire Multiverse ecosystem, including data, model weights, engine, supporting\\ntools, as well as complete data curation prompts and detailed training and\\nevaluation recipes.', 'upvotes': 15, 'discussionId': '684adf26dbd21a9cc27b0ecd', 'projectPage': 'https://multiverse4fm.github.io/', 'ai_summary': 'Multiverse, a parallel generative model incorporating a MapReduce paradigm, achieves performance comparable to autoregressive LLMs while offering superior scaling and speed.', 'ai_keywords': ['Autoregressive Large Language Models', 'MapReduce', 'Multiverse', 'adaptive task decomposition', 'parallel subtask execution', 'lossless result synthesis', 'Multiverse Attention', 'causal attention', 'parallel inference', 'dedicated scheduler', 'parallel generation', 'AIME24', 'AIME25', 'superlinear scaling', 'open-sourced ecosystem']}, 'publishedAt': '2025-06-11T13:59:23.000Z', 'title': 'Multiverse: Your Language Models Secretly Decide How to Parallelize and\\n  Merge Generation', 'summary': 'Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\\nparallelism in sequential generation. Inspired by this, we introduce\\nMultiverse, a new generative model that enables natively parallel generation.\\nMultiverse internalizes a MapReduce paradigm, generating automatically through\\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\\nMultiverse 1K by converting them into structured training data using an\\nautomated LLM-assisted pipeline, avoiding costly human annotations.\\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\\nsteps while keeping compatibility with causal attention for efficient training.\\nSystematically, we implement Multiverse Engine to enable parallel inference. It\\nfeatures a dedicated scheduler that dynamically switches between sequential and\\nparallel generation, triggered directly by the model. After a 3-hour\\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\\nMoreover, our budget control experiments show that Multiverse-32B exhibits\\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\\ncontext length. Such scaling further leads to practical efficiency gain,\\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\\nentire Multiverse ecosystem, including data, model weights, engine, supporting\\ntools, as well as complete data curation prompts and detailed training and\\nevaluation recipes.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09991.png', 'numComments': 1, 'submittedBy': {'_id': '64f58b970b24e548a85522bc', 'avatarUrl': '/avatars/c8ca1294b5a1edd609694877e335b22f.svg', 'fullname': 'Xinyu Yang', 'name': 'Hanyuezhuohua', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.09003', 'authors': [{'_id': '6848eed742e4f9106973f2cf', 'user': {'_id': '64c38871f9cd765462fa1a17', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg', 'isPro': False, 'fullname': 'Lei Zhang', 'user': 'Lemoncoke', 'type': 'user'}, 'name': 'Lei Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:42:47.797Z', 'hidden': False}, {'_id': '6848eed742e4f9106973f2d0', 'name': 'Jiaxi Yang', 'hidden': False}, {'_id': '6848eed742e4f9106973f2d1', 'name': 'Min Yang', 'hidden': False}, {'_id': '6848eed742e4f9106973f2d2', 'name': 'Jian Yang', 'hidden': False}, {'_id': '6848eed742e4f9106973f2d3', 'name': 'Mouxiang Chen', 'hidden': False}, {'_id': '6848eed742e4f9106973f2d4', 'name': 'Jiajun Zhang', 'hidden': False}, {'_id': '6848eed742e4f9106973f2d5', 'name': 'Zeyu Cui', 'hidden': False}, {'_id': '6848eed742e4f9106973f2d6', 'name': 'Binyuan Hui', 'hidden': False}, {'_id': '6848eed742e4f9106973f2d7', 'name': 'Junyang Lin', 'hidden': False}], 'publishedAt': '2025-06-10T17:23:33.000Z', 'submittedOnDailyAt': '2025-06-12T00:17:18.390Z', 'title': 'SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner', 'submittedOnDailyBy': {'_id': '64c38871f9cd765462fa1a17', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg', 'isPro': False, 'fullname': 'Lei Zhang', 'user': 'Lemoncoke', 'type': 'user'}, 'summary': 'We introduce **SWE-Flow**, a novel data synthesis framework grounded in\\nTest-Driven Development (TDD). Unlike existing software engineering data that\\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\\ndevelopment steps directly from unit tests, which inherently encapsulate\\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\\nenabling the generation of a structured, step-by-step *development schedule*.\\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\\ntests, and the necessary code modifications, resulting in fully verifiable TDD\\ntasks. With this approach, we generated 16,061 training instances and 2,020\\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\\nbenchmark. Our experiments show that fine-tuning open model on this dataset\\nsignificantly improves performance in TDD-based coding. To facilitate further\\nresearch, we release all code, datasets, models, and Docker images at\\n[Github](https://github.com/Hambaobao/SWE-Flow).', 'upvotes': 13, 'discussionId': '6848eed842e4f9106973f2d8', 'githubRepo': 'https://github.com/Hambaobao/SWE-Flow', 'ai_summary': 'A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.', 'ai_keywords': ['Test-Driven Development (TDD)', 'Runtime Dependency Graph (RDG)', 'SWE-Flow', 'unit tests', 'development schedule', 'SWE-Flow-Eval', 'fine-tuning', 'open model']}, 'publishedAt': '2025-06-10T13:23:33.000Z', 'title': 'SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner', 'summary': 'We introduce **SWE-Flow**, a novel data synthesis framework grounded in\\nTest-Driven Development (TDD). Unlike existing software engineering data that\\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\\ndevelopment steps directly from unit tests, which inherently encapsulate\\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\\nenabling the generation of a structured, step-by-step *development schedule*.\\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\\ntests, and the necessary code modifications, resulting in fully verifiable TDD\\ntasks. With this approach, we generated 16,061 training instances and 2,020\\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\\nbenchmark. Our experiments show that fine-tuning open model on this dataset\\nsignificantly improves performance in TDD-based coding. To facilitate further\\nresearch, we release all code, datasets, models, and Docker images at\\n[Github](https://github.com/Hambaobao/SWE-Flow).', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09003.png', 'numComments': 2, 'submittedBy': {'_id': '64c38871f9cd765462fa1a17', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg', 'fullname': 'Lei Zhang', 'name': 'Lemoncoke', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.09984', 'authors': [{'_id': '684a49fa9b38e1e5a33a6884', 'name': 'Zhenzhi Wang', 'hidden': False}, {'_id': '684a49fa9b38e1e5a33a6885', 'name': 'Jiaqi Yang', 'hidden': False}, {'_id': '684a49fa9b38e1e5a33a6886', 'name': 'Jianwen Jiang', 'hidden': False}, {'_id': '684a49fa9b38e1e5a33a6887', 'name': 'Chao Liang', 'hidden': False}, {'_id': '684a49fa9b38e1e5a33a6888', 'user': {'_id': '64802fcdcc9e514b3b031244', 'avatarUrl': '/avatars/cc5979008bdb21a2be9575865dce909b.svg', 'isPro': False, 'fullname': 'Gaojie Lin', 'user': 'lingaojie', 'type': 'user'}, 'name': 'Gaojie Lin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:41:52.916Z', 'hidden': False}, {'_id': '684a49fa9b38e1e5a33a6889', 'name': 'Zerong Zheng', 'hidden': False}, {'_id': '684a49fa9b38e1e5a33a688a', 'name': 'Ceyuan Yang', 'hidden': False}, {'_id': '684a49fa9b38e1e5a33a688b', 'name': 'Dahua Lin', 'hidden': False}], 'publishedAt': '2025-06-11T17:57:09.000Z', 'submittedOnDailyAt': '2025-06-12T02:02:32.983Z', 'title': 'InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\\n  Conditions', 'submittedOnDailyBy': {'_id': '6519346a186bc3b6997c1aaf', 'avatarUrl': '/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg', 'isPro': False, 'fullname': 'Zhenzhi Wang', 'user': 'zhenzhiwang', 'type': 'user'}, 'summary': \"End-to-end human animation with rich multi-modal conditions, e.g., text,\\nimage and audio has achieved remarkable advancements in recent years. However,\\nmost existing methods could only animate a single subject and inject conditions\\nin a global manner, ignoring scenarios that multiple concepts could appears in\\nthe same video with rich human-human interactions and human-object\\ninteractions. Such global assumption prevents precise and per-identity control\\nof multiple concepts including humans and objects, therefore hinders\\napplications. In this work, we discard the single-entity assumption and\\nintroduce a novel framework that enforces strong, region-specific binding of\\nconditions from modalities to each identity's spatiotemporal footprint. Given\\nreference images of multiple concepts, our method could automatically infer\\nlayout information by leveraging a mask predictor to match appearance cues\\nbetween the denoised video and each reference appearance. Furthermore, we\\ninject local audio condition into its corresponding region to ensure\\nlayout-aligned modality matching in a iterative manner. This design enables the\\nhigh-quality generation of controllable multi-concept human-centric videos.\\nEmpirical results and ablation studies validate the effectiveness of our\\nexplicit layout control for multi-modal conditions compared to implicit\\ncounterparts and other existing methods.\", 'upvotes': 11, 'discussionId': '684a49fa9b38e1e5a33a688c', 'ai_summary': 'A new framework enables precise, per-identity control of multiple concepts in end-to-end human animation by enforcing region-specific binding of multi-modal conditions.', 'ai_keywords': ['human animation', 'multi-modal conditions', 'mask predictor', 'denoised video', 'layout information', 'local audio condition', 'controllable multi-concept videos', 'explicit layout control']}, 'publishedAt': '2025-06-11T13:57:09.000Z', 'title': 'InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\\n  Conditions', 'summary': \"End-to-end human animation with rich multi-modal conditions, e.g., text,\\nimage and audio has achieved remarkable advancements in recent years. However,\\nmost existing methods could only animate a single subject and inject conditions\\nin a global manner, ignoring scenarios that multiple concepts could appears in\\nthe same video with rich human-human interactions and human-object\\ninteractions. Such global assumption prevents precise and per-identity control\\nof multiple concepts including humans and objects, therefore hinders\\napplications. In this work, we discard the single-entity assumption and\\nintroduce a novel framework that enforces strong, region-specific binding of\\nconditions from modalities to each identity's spatiotemporal footprint. Given\\nreference images of multiple concepts, our method could automatically infer\\nlayout information by leveraging a mask predictor to match appearance cues\\nbetween the denoised video and each reference appearance. Furthermore, we\\ninject local audio condition into its corresponding region to ensure\\nlayout-aligned modality matching in a iterative manner. This design enables the\\nhigh-quality generation of controllable multi-concept human-centric videos.\\nEmpirical results and ablation studies validate the effectiveness of our\\nexplicit layout control for multi-modal conditions compared to implicit\\ncounterparts and other existing methods.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09984.png', 'numComments': 1, 'submittedBy': {'_id': '6519346a186bc3b6997c1aaf', 'avatarUrl': '/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg', 'fullname': 'Zhenzhi Wang', 'name': 'zhenzhiwang', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.05309', 'authors': [{'_id': '68483359308cb7e626e80e88', 'user': {'_id': '67fa8526e1bb5094301f17c9', 'avatarUrl': '/avatars/b0ef0f77dc14a498919957f1a6d8688b.svg', 'isPro': False, 'fullname': 'Niv Eckhaus', 'user': 'niveck', 'type': 'user'}, 'name': 'Niv Eckhaus', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-10T15:20:12.219Z', 'hidden': False}, {'_id': '68483359308cb7e626e80e89', 'name': 'Uri Berger', 'hidden': False}, {'_id': '68483359308cb7e626e80e8a', 'name': 'Gabriel Stanovsky', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/DwNbnBFcX3hOkBzFNkygd.gif', 'https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/L5pUeRdEp529o5w9O4ec8.gif', 'https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/2Dj-HZN0Zw7R8iditxIR-.png', 'https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/DVIpPGCmA1yqc7BIuaaZc.png', 'https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/CLZZ513nPDllOTvwAs_Kl.png'], 'publishedAt': '2025-06-05T17:53:44.000Z', 'submittedOnDailyAt': '2025-06-12T11:48:56.909Z', 'title': 'Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\\n  Games', 'submittedOnDailyBy': {'_id': '67fa8526e1bb5094301f17c9', 'avatarUrl': '/avatars/b0ef0f77dc14a498919957f1a6d8688b.svg', 'isPro': False, 'fullname': 'Niv Eckhaus', 'user': 'niveck', 'type': 'user'}, 'summary': \"LLMs are used predominantly in synchronous communication, where a human user\\nand a model communicate in alternating turns. In contrast, many real-world\\nsettings are inherently asynchronous. For example, in group chats, online team\\nmeetings, or social games, there is no inherent notion of turns; therefore, the\\ndecision of when to speak forms a crucial part of the participant's decision\\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\\naddition to determining what to say, also decides when to say it. To evaluate\\nour agent, we collect a unique dataset of online Mafia games, including both\\nhuman participants, as well as our asynchronous agent. Overall, our agent\\nperforms on par with human players, both in game performance, as well as in its\\nability to blend in with the other human players. Our analysis shows that the\\nagent's behavior in deciding when to speak closely mirrors human patterns,\\nalthough differences emerge in message content. We release all our data and\\ncode to support and encourage further research for more realistic asynchronous\\ncommunication between LLM agents. This work paves the way for integration of\\nLLMs into realistic human group settings, from assistance in team discussions\\nto educational and professional environments where complex social dynamics must\\nbe navigated.\", 'upvotes': 7, 'discussionId': '68483359308cb7e626e80e8b', 'projectPage': 'https://niveck.github.io/Time-to-Talk/', 'githubRepo': 'https://github.com/niveck/LLMafia', 'ai_summary': 'An adaptive asynchronous LLM-agent performs similarly to human players in online Mafia games, demonstrating the potential for integrating LLMs into realistic group settings with complex social dynamics.', 'ai_keywords': ['asynchronous communication', 'LLM-agent', 'online Mafia games', 'social dynamics']}, 'publishedAt': '2025-06-05T13:53:44.000Z', 'title': 'Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\\n  Games', 'summary': \"LLMs are used predominantly in synchronous communication, where a human user\\nand a model communicate in alternating turns. In contrast, many real-world\\nsettings are inherently asynchronous. For example, in group chats, online team\\nmeetings, or social games, there is no inherent notion of turns; therefore, the\\ndecision of when to speak forms a crucial part of the participant's decision\\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\\naddition to determining what to say, also decides when to say it. To evaluate\\nour agent, we collect a unique dataset of online Mafia games, including both\\nhuman participants, as well as our asynchronous agent. Overall, our agent\\nperforms on par with human players, both in game performance, as well as in its\\nability to blend in with the other human players. Our analysis shows that the\\nagent's behavior in deciding when to speak closely mirrors human patterns,\\nalthough differences emerge in message content. We release all our data and\\ncode to support and encourage further research for more realistic asynchronous\\ncommunication between LLM agents. This work paves the way for integration of\\nLLMs into realistic human group settings, from assistance in team discussions\\nto educational and professional environments where complex social dynamics must\\nbe navigated.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/DwNbnBFcX3hOkBzFNkygd.gif', 'https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/L5pUeRdEp529o5w9O4ec8.gif', 'https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/2Dj-HZN0Zw7R8iditxIR-.png', 'https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/DVIpPGCmA1yqc7BIuaaZc.png', 'https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/CLZZ513nPDllOTvwAs_Kl.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05309.png', 'numComments': 1, 'submittedBy': {'_id': '67fa8526e1bb5094301f17c9', 'avatarUrl': '/avatars/b0ef0f77dc14a498919957f1a6d8688b.svg', 'fullname': 'Niv Eckhaus', 'name': 'niveck', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.09937', 'authors': [{'_id': '684a3d639b38e1e5a33a686d', 'name': 'Qiao Gu', 'hidden': False}, {'_id': '684a3d639b38e1e5a33a686e', 'name': 'Yuanliang Ju', 'hidden': False}, {'_id': '684a3d639b38e1e5a33a686f', 'name': 'Shengxiang Sun', 'hidden': False}, {'_id': '684a3d639b38e1e5a33a6870', 'name': 'Igor Gilitschenski', 'hidden': False}, {'_id': '684a3d639b38e1e5a33a6871', 'name': 'Haruki Nishimura', 'hidden': False}, {'_id': '684a3d639b38e1e5a33a6872', 'name': 'Masha Itkina', 'hidden': False}, {'_id': '684a3d639b38e1e5a33a6873', 'name': 'Florian Shkurti', 'hidden': False}], 'publishedAt': '2025-06-11T16:59:13.000Z', 'submittedOnDailyAt': '2025-06-12T01:08:05.216Z', 'title': 'SAFE: Multitask Failure Detection for Vision-Language-Action Models', 'submittedOnDailyBy': {'_id': '63d1df92f7f31a66a2d7292c', 'avatarUrl': '/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg', 'isPro': False, 'fullname': 'Qiao Gu', 'user': 'guqiao', 'type': 'user'}, 'summary': 'While vision-language-action models (VLAs) have shown promising robotic\\nbehaviors across a diverse set of manipulation tasks, they achieve limited\\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\\npolicies to safely interact with their environments, we need a failure detector\\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\\nhelp. However, existing failure detectors are trained and tested only on one or\\na few specific tasks, while VLAs require the detector to generalize and detect\\nfailures also in unseen tasks and novel environments. In this paper, we\\nintroduce the multitask failure detection problem and propose SAFE, a failure\\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\\nspace and find that VLAs have sufficient high-level knowledge about task\\nsuccess and failure, which is generic across different tasks. Based on this\\ninsight, we design SAFE to learn from VLA internal features and predict a\\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\\ncompatible with different policy architectures. We test it on OpenVLA, pi_0,\\nand pi_0-FAST in both simulated and real-world environments extensively. We\\ncompare SAFE with diverse baselines and show that SAFE achieves\\nstate-of-the-art failure detection performance and the best trade-off between\\naccuracy and detection time using conformal prediction. More qualitative\\nresults can be found at https://vla-safe.github.io/.', 'upvotes': 4, 'discussionId': '684a3d639b38e1e5a33a6874', 'ai_summary': 'SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.', 'ai_keywords': ['vision-language-action models', 'VLAs', 'multitask failure detection', 'failure detector', 'feature space', 'high-level knowledge', 'scalar prediction', 'rollout', 'conformal prediction']}, 'publishedAt': '2025-06-11T12:59:13.000Z', 'title': 'SAFE: Multitask Failure Detection for Vision-Language-Action Models', 'summary': 'While vision-language-action models (VLAs) have shown promising robotic\\nbehaviors across a diverse set of manipulation tasks, they achieve limited\\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\\npolicies to safely interact with their environments, we need a failure detector\\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\\nhelp. However, existing failure detectors are trained and tested only on one or\\na few specific tasks, while VLAs require the detector to generalize and detect\\nfailures also in unseen tasks and novel environments. In this paper, we\\nintroduce the multitask failure detection problem and propose SAFE, a failure\\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\\nspace and find that VLAs have sufficient high-level knowledge about task\\nsuccess and failure, which is generic across different tasks. Based on this\\ninsight, we design SAFE to learn from VLA internal features and predict a\\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\\ncompatible with different policy architectures. We test it on OpenVLA, pi_0,\\nand pi_0-FAST in both simulated and real-world environments extensively. We\\ncompare SAFE with diverse baselines and show that SAFE achieves\\nstate-of-the-art failure detection performance and the best trade-off between\\naccuracy and detection time using conformal prediction. More qualitative\\nresults can be found at https://vla-safe.github.io/.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09937.png', 'numComments': 1, 'submittedBy': {'_id': '63d1df92f7f31a66a2d7292c', 'avatarUrl': '/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg', 'fullname': 'Qiao Gu', 'name': 'guqiao', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.09736', 'authors': [{'_id': '684ae230dbd21a9cc27b10ba', 'name': 'Yuting Li', 'hidden': False}, {'_id': '684ae230dbd21a9cc27b10bb', 'name': 'Lai Wei', 'hidden': False}, {'_id': '684ae230dbd21a9cc27b10bc', 'name': 'Kaipeng Zheng', 'hidden': False}, {'_id': '684ae230dbd21a9cc27b10bd', 'name': 'Jingyuan Huang', 'hidden': False}, {'_id': '684ae230dbd21a9cc27b10be', 'name': 'Linghe Kong', 'hidden': False}, {'_id': '684ae230dbd21a9cc27b10bf', 'name': 'Lichao Sun', 'hidden': False}, {'_id': '684ae230dbd21a9cc27b10c0', 'name': 'Weiran Huang', 'hidden': False}], 'publishedAt': '2025-06-11T13:39:46.000Z', 'submittedOnDailyAt': '2025-06-12T12:53:09.040Z', 'title': 'Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math\\n  Reasoning', 'submittedOnDailyBy': {'_id': '64a16b1aeacb4b50ba1c889d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg', 'isPro': False, 'fullname': 'Lai Wei', 'user': 'WaltonFuture', 'type': 'user'}, 'summary': 'Despite the rapid progress of multimodal large language models (MLLMs), they\\nhave largely overlooked the importance of visual processing. In a simple yet\\nrevealing experiment, we interestingly find that language-only models, when\\nprovided with image captions, can achieve comparable or even better performance\\nthan MLLMs that consume raw visual inputs. This suggests that current MLLMs may\\ngenerate accurate visual descriptions but fail to effectively integrate them\\nduring reasoning. Motivated by this, we propose a simple visual perturbation\\nframework that enhances perceptual robustness without requiring algorithmic\\nmodifications or additional training data. Our approach introduces three\\ntargeted perturbations: distractor concatenation, dominance-preserving mixup,\\nand random rotation, that can be easily integrated into existing post-training\\npipelines including SFT, DPO, and GRPO. Through extensive experiments across\\nmultiple datasets, we demonstrate consistent improvements in mathematical\\nreasoning performance, with gains comparable to those achieved through\\nalgorithmic changes. Additionally, we achieve competitive performance among\\nopen-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual\\nperturbation. Through comprehensive ablation studies, we analyze the\\neffectiveness of different perturbation strategies, revealing that each\\nperturbation type contributes uniquely to different aspects of visual\\nreasoning. Our findings highlight the critical role of visual perturbation in\\nmultimodal mathematical reasoning: better reasoning begins with better seeing.\\nOur code is available at https://github.com/YutingLi0606/Vision-Matters.', 'upvotes': 3, 'discussionId': '684ae231dbd21a9cc27b10c1', 'ai_summary': \"Visual perturbation framework enhances multimodal models' mathematical reasoning performance without additional training or algorithmic changes.\", 'ai_keywords': ['multimodal large language models', 'visual processing', 'language-only models', 'image captions', 'perceptual robustness', 'visual perturbation', 'distractor concatenation', 'dominance-preserving mixup', 'random rotation', 'post-training pipelines', 'mathematical reasoning', 'open-source models', 'Qwen2.5-VL-7B', 'ablation studies', 'visual reasoning']}, 'publishedAt': '2025-06-11T09:39:46.000Z', 'title': 'Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math\\n  Reasoning', 'summary': 'Despite the rapid progress of multimodal large language models (MLLMs), they\\nhave largely overlooked the importance of visual processing. In a simple yet\\nrevealing experiment, we interestingly find that language-only models, when\\nprovided with image captions, can achieve comparable or even better performance\\nthan MLLMs that consume raw visual inputs. This suggests that current MLLMs may\\ngenerate accurate visual descriptions but fail to effectively integrate them\\nduring reasoning. Motivated by this, we propose a simple visual perturbation\\nframework that enhances perceptual robustness without requiring algorithmic\\nmodifications or additional training data. Our approach introduces three\\ntargeted perturbations: distractor concatenation, dominance-preserving mixup,\\nand random rotation, that can be easily integrated into existing post-training\\npipelines including SFT, DPO, and GRPO. Through extensive experiments across\\nmultiple datasets, we demonstrate consistent improvements in mathematical\\nreasoning performance, with gains comparable to those achieved through\\nalgorithmic changes. Additionally, we achieve competitive performance among\\nopen-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual\\nperturbation. Through comprehensive ablation studies, we analyze the\\neffectiveness of different perturbation strategies, revealing that each\\nperturbation type contributes uniquely to different aspects of visual\\nreasoning. Our findings highlight the critical role of visual perturbation in\\nmultimodal mathematical reasoning: better reasoning begins with better seeing.\\nOur code is available at https://github.com/YutingLi0606/Vision-Matters.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09736.png', 'numComments': 1, 'submittedBy': {'_id': '64a16b1aeacb4b50ba1c889d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg', 'fullname': 'Lai Wei', 'name': 'WaltonFuture', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.08001', 'authors': [{'_id': '684a649f9b38e1e5a33a6895', 'name': 'Zeju Qiu', 'hidden': False}, {'_id': '684a649f9b38e1e5a33a6896', 'name': 'Simon Buchholz', 'hidden': False}, {'_id': '684a649f9b38e1e5a33a6897', 'name': 'Tim Z. Xiao', 'hidden': False}, {'_id': '684a649f9b38e1e5a33a6898', 'name': 'Maximilian Dax', 'hidden': False}, {'_id': '684a649f9b38e1e5a33a6899', 'name': 'Bernhard Schlkopf', 'hidden': False}, {'_id': '684a649f9b38e1e5a33a689a', 'name': 'Weiyang Liu', 'hidden': False}], 'publishedAt': '2025-06-09T17:59:34.000Z', 'submittedOnDailyAt': '2025-06-12T03:55:47.829Z', 'title': 'Reparameterized LLM Training via Orthogonal Equivalence Transformation', 'submittedOnDailyBy': {'_id': '648905d1a15c43c791d4381f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg', 'isPro': False, 'fullname': 'Weiyang Liu', 'user': 'wy1iu', 'type': 'user'}, 'summary': \"While large language models (LLMs) are driving the rapid advancement of\\nartificial intelligence, effectively and reliably training these large models\\nremains one of the field's most significant challenges. To address this\\nchallenge, we propose POET, a novel reParameterized training algorithm that\\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\\nfixed random weight matrix. Because of its provable preservation of spectral\\nproperties of weight matrices, POET can stably optimize the objective function\\nwith improved generalization. We further develop efficient approximations that\\nmake POET flexible and scalable for training large-scale neural networks.\\nExtensive experiments validate the effectiveness and scalability of POET in\\ntraining LLMs.\", 'upvotes': 2, 'discussionId': '684a649f9b38e1e5a33a689b', 'projectPage': 'https://spherelab.ai/poet/', 'githubRepo': 'https://github.com/Sphere-AI-Lab/poet', 'ai_summary': 'POET is a reParameterized training algorithm using Orthogonal Equivalence Transformation to optimize neurons in large language models, ensuring stable training and improved generalization.', 'ai_keywords': ['POET', 'reParameterized training algorithm', 'Orthogonal Equivalence Transformation', 'orthogonal matrices', 'spectral properties', 'weight matrices', 'large language models', 'generalization', 'efficient approximations', 'training', 'scalability']}, 'publishedAt': '2025-06-09T13:59:34.000Z', 'title': 'Reparameterized LLM Training via Orthogonal Equivalence Transformation', 'summary': \"While large language models (LLMs) are driving the rapid advancement of\\nartificial intelligence, effectively and reliably training these large models\\nremains one of the field's most significant challenges. To address this\\nchallenge, we propose POET, a novel reParameterized training algorithm that\\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\\nfixed random weight matrix. Because of its provable preservation of spectral\\nproperties of weight matrices, POET can stably optimize the objective function\\nwith improved generalization. We further develop efficient approximations that\\nmake POET flexible and scalable for training large-scale neural networks.\\nExtensive experiments validate the effectiveness and scalability of POET in\\ntraining LLMs.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08001.png', 'numComments': 1, 'submittedBy': {'_id': '648905d1a15c43c791d4381f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg', 'fullname': 'Weiyang Liu', 'name': 'wy1iu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.09669', 'authors': [{'_id': '684ae246dbd21a9cc27b1126', 'name': 'Lihu Chen', 'hidden': False}, {'_id': '684ae246dbd21a9cc27b1127', 'name': 'Gal Varoquaux', 'hidden': False}], 'publishedAt': '2025-06-11T12:39:48.000Z', 'submittedOnDailyAt': '2025-06-12T13:26:30.505Z', 'title': 'Query-Level Uncertainty in Large Language Models', 'submittedOnDailyBy': {'_id': '63d1954e8eaa4831006600ff', 'avatarUrl': '/avatars/a0380156e0773bf0a6ccc2df6b9ea478.svg', 'isPro': False, 'fullname': 'Lihu Chen', 'user': 'Lihuchen', 'type': 'user'}, 'summary': 'It is important for Large Language Models to be aware of the boundary of\\ntheir knowledge, the mechanism of identifying known and unknown queries. This\\ntype of awareness can help models perform adaptive inference, such as invoking\\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\\nwhich is beneficial to the development of efficient and trustworthy AI. In this\\nwork, we propose a method to detect knowledge boundaries via Query-Level\\nUncertainty, which aims to determine if the model is able to address a given\\nquery without generating any tokens. To this end, we introduce a novel and\\ntraining-free method called Internal Confidence, which leverages\\nself-evaluations across layers and tokens. Empirical results on both factual QA\\nand mathematical reasoning tasks demonstrate that our internal confidence can\\noutperform several baselines. Furthermore, we showcase that our proposed method\\ncan be used for efficient RAG and model cascading, which is able to reduce\\ninference costs while maintaining performance.', 'upvotes': 1, 'discussionId': '684ae246dbd21a9cc27b1128', 'ai_summary': 'A method using Query-Level Uncertainty and Internal Confidence enables Large Language Models to determine knowledge boundaries efficiently, improving adaptability and reducing inference costs.', 'ai_keywords': ['Large Language Models', 'Query-Level Uncertainty', 'Internal Confidence', 'adaptive inference', 'RAG', 'abstention mechanism', 'efficient AI', 'trustworthy AI', 'factual QA', 'mathematical reasoning', 'model cascading']}, 'publishedAt': '2025-06-11T08:39:48.000Z', 'title': 'Query-Level Uncertainty in Large Language Models', 'summary': 'It is important for Large Language Models to be aware of the boundary of\\ntheir knowledge, the mechanism of identifying known and unknown queries. This\\ntype of awareness can help models perform adaptive inference, such as invoking\\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\\nwhich is beneficial to the development of efficient and trustworthy AI. In this\\nwork, we propose a method to detect knowledge boundaries via Query-Level\\nUncertainty, which aims to determine if the model is able to address a given\\nquery without generating any tokens. To this end, we introduce a novel and\\ntraining-free method called Internal Confidence, which leverages\\nself-evaluations across layers and tokens. Empirical results on both factual QA\\nand mathematical reasoning tasks demonstrate that our internal confidence can\\noutperform several baselines. Furthermore, we showcase that our proposed method\\ncan be used for efficient RAG and model cascading, which is able to reduce\\ninference costs while maintaining performance.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09669.png', 'numComments': 1, 'submittedBy': {'_id': '63d1954e8eaa4831006600ff', 'avatarUrl': '/avatars/a0380156e0773bf0a6ccc2df6b9ea478.svg', 'fullname': 'Lihu Chen', 'name': 'Lihuchen', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.09229', 'authors': [{'_id': '684ae250dbd21a9cc27b114c', 'name': 'Sungwon Hwang', 'hidden': False}, {'_id': '684ae250dbd21a9cc27b114d', 'name': 'Hyojin Jang', 'hidden': False}, {'_id': '684ae250dbd21a9cc27b114e', 'name': 'Kinam Kim', 'hidden': False}, {'_id': '684ae250dbd21a9cc27b114f', 'name': 'Minho Park', 'hidden': False}, {'_id': '684ae250dbd21a9cc27b1150', 'name': 'Jaegul choo', 'hidden': False}], 'publishedAt': '2025-06-10T20:34:47.000Z', 'submittedOnDailyAt': '2025-06-12T12:54:25.858Z', 'title': 'Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion\\n  Models', 'submittedOnDailyBy': {'_id': '642fcfc0a043f0ac7deeaae0', 'avatarUrl': '/avatars/6cc46dd480cdc0d86c7a509e22782a13.svg', 'isPro': False, 'fullname': 'Sungwon Hwang', 'user': 'sungwon95', 'type': 'user'}, 'summary': 'Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\\nvideos that reflect specific attributes of training data presents notable\\nchallenges, yet remains underexplored despite its practical importance.\\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\\npromise in improving the convergence and quality of DiT-based image diffusion\\nmodels by aligning, or assimilating, its internal hidden states with external\\npretrained visual features, suggesting its potential for VDM fine-tuning. In\\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\\nempirically show that, while effective for convergence, it is suboptimal in\\npreserving semantic consistency across frames. To address this limitation, we\\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\\ntechnique that aligns hidden states of a frame with external features from\\nneighboring frames. Empirical evaluations on large-scale VDMs, including\\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\\nfidelity and cross-frame semantic coherence when fine-tuned with\\nparameter-efficient methods such as LoRA. We further validate CREPA across\\ndiverse datasets with varying attributes, confirming its broad applicability.\\nProject page: https://crepavideo.github.io', 'upvotes': 1, 'discussionId': '684ae250dbd21a9cc27b1151', 'projectPage': 'https://crepavideo.github.io', 'githubRepo': 'https://github.com/deepshwang/crepa', 'ai_summary': 'Cross-frame Representation Alignment improves video diffusion model fine-tuning by enhancing convergence and semantic coherence across frames.', 'ai_keywords': ['Video Diffusion Models', 'VDMs', 'Representation Alignment', 'REPA', 'DiT-based image diffusion models', 'hidden states', 'external pretrained visual features', 'Cross-frame Representation Alignment', 'CREPA', 'visual fidelity', 'cross-frame semantic coherence', 'parameter-efficient methods', 'LoRA', 'CogVideoX-5B', 'Hunyuan Video']}, 'publishedAt': '2025-06-10T16:34:47.000Z', 'title': 'Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion\\n  Models', 'summary': 'Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\\nvideos that reflect specific attributes of training data presents notable\\nchallenges, yet remains underexplored despite its practical importance.\\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\\npromise in improving the convergence and quality of DiT-based image diffusion\\nmodels by aligning, or assimilating, its internal hidden states with external\\npretrained visual features, suggesting its potential for VDM fine-tuning. In\\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\\nempirically show that, while effective for convergence, it is suboptimal in\\npreserving semantic consistency across frames. To address this limitation, we\\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\\ntechnique that aligns hidden states of a frame with external features from\\nneighboring frames. Empirical evaluations on large-scale VDMs, including\\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\\nfidelity and cross-frame semantic coherence when fine-tuned with\\nparameter-efficient methods such as LoRA. We further validate CREPA across\\ndiverse datasets with varying attributes, confirming its broad applicability.\\nProject page: https://crepavideo.github.io', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09229.png', 'numComments': 1, 'submittedBy': {'_id': '642fcfc0a043f0ac7deeaae0', 'avatarUrl': '/avatars/6cc46dd480cdc0d86c7a509e22782a13.svg', 'fullname': 'Sungwon Hwang', 'name': 'sungwon95', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.09007', 'authors': [{'_id': '6849516c42e4f9106973f4d1', 'name': 'Sophia Tang', 'hidden': False}, {'_id': '6849516c42e4f9106973f4d2', 'name': 'Yinuo Zhang', 'hidden': False}, {'_id': '6849516c42e4f9106973f4d3', 'name': 'Alexander Tong', 'hidden': False}, {'_id': '6849516c42e4f9106973f4d4', 'name': 'Pranam Chatterjee', 'hidden': False}], 'publishedAt': '2025-06-10T17:29:48.000Z', 'submittedOnDailyAt': '2025-06-12T01:32:31.298Z', 'title': 'Branched Schrdinger Bridge Matching', 'submittedOnDailyBy': {'_id': '64cd5b3f0494187a9e8b7c69', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg', 'isPro': False, 'fullname': 'Pranam Chatterjee', 'user': 'pranamanam', 'type': 'user'}, 'summary': 'Predicting the intermediate trajectories between an initial and target\\ndistribution is a central problem in generative modeling. Existing approaches,\\nsuch as flow matching and Schr\\\\\"odinger Bridge Matching, effectively learn\\nmappings between two distributions by modeling a single stochastic path.\\nHowever, these methods are inherently limited to unimodal transitions and\\ncannot capture branched or divergent evolution from a common origin to multiple\\ndistinct outcomes. To address this, we introduce Branched Schr\\\\\"odinger Bridge\\nMatching (BranchSBM), a novel framework that learns branched Schr\\\\\"odinger\\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\\ngrowth processes, enabling the representation of population-level divergence\\ninto multiple terminal distributions. We show that BranchSBM is not only more\\nexpressive but also essential for tasks involving multi-path surface\\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\\nand simulating diverging cellular responses to perturbations.', 'upvotes': 1, 'discussionId': '6849516d42e4f9106973f4d5', 'ai_summary': 'BranchSBM, a novel generative modeling framework, extends Schr\\\\\"odinger Bridge Matching to model branched stochastic paths and multi-path evolution from a single initial distribution to multiple outcomes.', 'ai_keywords': ['flow matching', 'Schr\\\\\"odinger Bridge Matching', 'Branched Schr\\\\\"odinger Bridge Matching', 'BranchSBM', 'time-dependent velocity fields', 'growth processes', 'multi-path surface navigation', 'cell fate bifurcations', 'cellular responses to perturbations']}, 'publishedAt': '2025-06-10T13:29:48.000Z', 'title': 'Branched Schrdinger Bridge Matching', 'summary': 'Predicting the intermediate trajectories between an initial and target\\ndistribution is a central problem in generative modeling. Existing approaches,\\nsuch as flow matching and Schr\\\\\"odinger Bridge Matching, effectively learn\\nmappings between two distributions by modeling a single stochastic path.\\nHowever, these methods are inherently limited to unimodal transitions and\\ncannot capture branched or divergent evolution from a common origin to multiple\\ndistinct outcomes. To address this, we introduce Branched Schr\\\\\"odinger Bridge\\nMatching (BranchSBM), a novel framework that learns branched Schr\\\\\"odinger\\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\\ngrowth processes, enabling the representation of population-level divergence\\ninto multiple terminal distributions. We show that BranchSBM is not only more\\nexpressive but also essential for tasks involving multi-path surface\\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\\nand simulating diverging cellular responses to perturbations.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09007.png', 'numComments': 1, 'submittedBy': {'_id': '64cd5b3f0494187a9e8b7c69', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg', 'fullname': 'Pranam Chatterjee', 'name': 'pranamanam', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.08900', 'authors': [{'_id': '684a96ab9aebf043cf7bdb2e', 'user': {'_id': '655b3383ed8df831286969f0', 'avatarUrl': '/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg', 'isPro': False, 'fullname': 'Jos Morano', 'user': 'j-morano', 'type': 'user'}, 'name': 'Jos Morano', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-12T12:40:45.632Z', 'hidden': False}, {'_id': '684a96ab9aebf043cf7bdb2f', 'name': 'Botond Fazekas', 'hidden': False}, {'_id': '684a96ab9aebf043cf7bdb30', 'name': 'Emese Skei', 'hidden': False}, {'_id': '684a96ab9aebf043cf7bdb31', 'name': 'Ronald Fecso', 'hidden': False}, {'_id': '684a96ab9aebf043cf7bdb32', 'name': 'Taha Emre', 'hidden': False}, {'_id': '684a96ab9aebf043cf7bdb33', 'name': 'Markus Gumpinger', 'hidden': False}, {'_id': '684a96ab9aebf043cf7bdb34', 'name': 'Georg Faustmann', 'hidden': False}, {'_id': '684a96ab9aebf043cf7bdb35', 'name': 'Marzieh Oghbaie', 'hidden': False}, {'_id': '684a96ab9aebf043cf7bdb36', 'name': 'Ursula Schmidt-Erfurth', 'hidden': False}, {'_id': '684a96ab9aebf043cf7bdb37', 'name': 'Hrvoje Bogunovi', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/655b3383ed8df831286969f0/FuwagzdL2fBNhZyfwg22J.png'], 'publishedAt': '2025-06-10T15:25:55.000Z', 'submittedOnDailyAt': '2025-06-12T07:31:36.181Z', 'title': 'MIRAGE: Multimodal foundation model and benchmark for comprehensive\\n  retinal OCT image analysis', 'submittedOnDailyBy': {'_id': '655b3383ed8df831286969f0', 'avatarUrl': '/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg', 'isPro': False, 'fullname': 'Jos Morano', 'user': 'j-morano', 'type': 'user'}, 'summary': 'Artificial intelligence (AI) has become a fundamental tool for assisting\\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\\n(OCT). However, developing AI models often requires extensive annotation, and\\nexisting models tend to underperform on independent, unseen data. Foundation\\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\\npromise in overcoming these challenges. Nonetheless, available FMs for\\nophthalmology lack extensive validation, especially for segmentation tasks, and\\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\\nclassification and segmentation tasks. The comparison with general and\\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\\nboth types of tasks, highlighting its suitability as a basis for the\\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\\nand the evaluation benchmark are publicly available:\\nhttps://github.com/j-morano/MIRAGE.', 'upvotes': 1, 'discussionId': '684a96ab9aebf043cf7bdb38', 'githubRepo': 'https://github.com/j-morano/MIRAGE', 'ai_summary': 'MIRAGE, a multimodal foundation model, excels in OCT and SLO image classification and segmentation, outperforming existing general and specialized models.', 'ai_keywords': ['foundation models', 'multimodal models', 'OCT', 'SLO', 'segmentation tasks', 'evaluation benchmark']}, 'publishedAt': '2025-06-10T11:25:55.000Z', 'title': 'MIRAGE: Multimodal foundation model and benchmark for comprehensive\\n  retinal OCT image analysis', 'summary': 'Artificial intelligence (AI) has become a fundamental tool for assisting\\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\\n(OCT). However, developing AI models often requires extensive annotation, and\\nexisting models tend to underperform on independent, unseen data. Foundation\\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\\npromise in overcoming these challenges. Nonetheless, available FMs for\\nophthalmology lack extensive validation, especially for segmentation tasks, and\\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\\nclassification and segmentation tasks. The comparison with general and\\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\\nboth types of tasks, highlighting its suitability as a basis for the\\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\\nand the evaluation benchmark are publicly available:\\nhttps://github.com/j-morano/MIRAGE.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/655b3383ed8df831286969f0/FuwagzdL2fBNhZyfwg22J.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08900.png', 'numComments': 1, 'submittedBy': {'_id': '655b3383ed8df831286969f0', 'avatarUrl': '/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg', 'fullname': 'Jos Morano', 'name': 'j-morano', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.08008', 'authors': [{'_id': '684ae06cdbd21a9cc27b0ecf', 'name': 'Stephanie Fu', 'hidden': False}, {'_id': '684ae06cdbd21a9cc27b0ed0', 'name': 'Tyler Bonnen', 'hidden': False}, {'_id': '684ae06cdbd21a9cc27b0ed1', 'name': 'Devin Guillory', 'hidden': False}, {'_id': '684ae06cdbd21a9cc27b0ed2', 'name': 'Trevor Darrell', 'hidden': False}], 'publishedAt': '2025-06-09T17:59:54.000Z', 'submittedOnDailyAt': '2025-06-12T12:52:59.641Z', 'title': 'Hidden in plain sight: VLMs overlook their visual representations', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': \"Language provides a natural interface to specify and evaluate performance on\\nvisual tasks. To realize this possibility, vision language models (VLMs) must\\nsuccessfully integrate visual and linguistic information. Our work compares\\nVLMs to a direct readout of their visual encoders to understand their ability\\nto integrate across these modalities. Across a series of vision-centric\\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\\nsubstantially worse than their visual encoders, dropping to near-chance\\nperformance. We investigate these results through a series of analyses across\\nthe entire VLM: namely 1) the degradation of vision representations, 2)\\nbrittleness to task prompt, and 3) the language model's role in solving the\\ntask. We find that the bottleneck in performing these vision-centric tasks lies\\nin this third category; VLMs are not effectively using visual information\\neasily accessible throughout the entire model, and they inherit the language\\npriors present in the LLM. Our work helps diagnose the failure modes of\\nopen-source VLMs, and presents a series of evaluations useful for future\\ninvestigations into visual understanding within VLMs.\", 'upvotes': 1, 'discussionId': '684ae06ddbd21a9cc27b0ed3', 'ai_summary': 'Vision language models perform poorly on vision-centric tasks compared to their visual encoders, primarily due to ineffective utilization of visual information and inherited language priors.', 'ai_keywords': ['vision language models', 'visual encoders', 'depth estimation', 'correspondence', 'vision representations', 'brittleness', 'task prompt', 'language model', 'visual understanding']}, 'publishedAt': '2025-06-09T13:59:54.000Z', 'title': 'Hidden in plain sight: VLMs overlook their visual representations', 'summary': \"Language provides a natural interface to specify and evaluate performance on\\nvisual tasks. To realize this possibility, vision language models (VLMs) must\\nsuccessfully integrate visual and linguistic information. Our work compares\\nVLMs to a direct readout of their visual encoders to understand their ability\\nto integrate across these modalities. Across a series of vision-centric\\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\\nsubstantially worse than their visual encoders, dropping to near-chance\\nperformance. We investigate these results through a series of analyses across\\nthe entire VLM: namely 1) the degradation of vision representations, 2)\\nbrittleness to task prompt, and 3) the language model's role in solving the\\ntask. We find that the bottleneck in performing these vision-centric tasks lies\\nin this third category; VLMs are not effectively using visual information\\neasily accessible throughout the entire model, and they inherit the language\\npriors present in the LLM. Our work helps diagnose the failure modes of\\nopen-source VLMs, and presents a series of evaluations useful for future\\ninvestigations into visual understanding within VLMs.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08008.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 85}, 'isAuthorParticipating': False}"
]