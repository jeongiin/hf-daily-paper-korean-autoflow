[
    "{'paper': {'id': '2502.17129', 'authors': [{'_id': '67bd37cb0d41e01cca99aa8b', 'user': {'_id': '64f033ef82c6eea604c4da8b', 'avatarUrl': '/avatars/51b93fea7fd68b4274ee03701245dcca.svg', 'isPro': False, 'fullname': 'Liu Xiaoran', 'user': 'LiuXR', 'type': 'user'}, 'name': 'Xiaoran Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:40:07.298Z', 'hidden': False}, {'_id': '67bd37cb0d41e01cca99aa8c', 'name': 'Ruixiao Li', 'hidden': False}, {'_id': '67bd37cb0d41e01cca99aa8d', 'name': 'Mianqiu Huang', 'hidden': False}, {'_id': '67bd37cb0d41e01cca99aa8e', 'name': 'Zhigeng Liu', 'hidden': False}, {'_id': '67bd37cb0d41e01cca99aa8f', 'name': 'Yuerong Song', 'hidden': False}, {'_id': '67bd37cb0d41e01cca99aa90', 'name': 'Qipeng Guo', 'hidden': False}, {'_id': '67bd37cb0d41e01cca99aa91', 'name': 'Siyang He', 'hidden': False}, {'_id': '67bd37cb0d41e01cca99aa92', 'name': 'Qiqi Wang', 'hidden': False}, {'_id': '67bd37cb0d41e01cca99aa93', 'name': 'Linlin Li', 'hidden': False}, {'_id': '67bd37cb0d41e01cca99aa94', 'name': 'Qun Liu', 'hidden': False}, {'_id': '67bd37cb0d41e01cca99aa95', 'name': 'Yaqian Zhou', 'hidden': False}, {'_id': '67bd37cb0d41e01cca99aa96', 'name': 'Xuanjing Huang', 'hidden': False}, {'_id': '67bd37cb0d41e01cca99aa97', 'name': 'Xipeng Qiu', 'hidden': False}], 'publishedAt': '2025-02-24T13:19:33.000Z', 'title': 'Thus Spake Long-Context Large Language Model', 'summary': 'Long context is an important topic in Natural Language Processing (NLP),\\nrunning through the development of NLP architectures, and offers immense\\nopportunities for Large Language Models (LLMs) giving LLMs the lifelong\\nlearning potential akin to humans. Unfortunately, the pursuit of a long context\\nis accompanied by numerous obstacles. Nevertheless, long context remains a core\\ncompetitive advantage for LLMs. In the past two years, the context length of\\nLLMs has achieved a breakthrough extension to millions of tokens. Moreover, the\\nresearch on long-context LLMs has expanded from length extrapolation to a\\ncomprehensive focus on architecture, infrastructure, training, and evaluation\\ntechnologies.\\n  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy\\nbetween the journey of extending the context of LLM and the attempts of humans\\nto transcend its mortality. In this survey, We will illustrate how LLM\\nstruggles between the tremendous need for a longer context and its equal need\\nto accept the fact that it is ultimately finite. To achieve this, we give a\\nglobal picture of the lifecycle of long-context LLMs from four perspectives:\\narchitecture, infrastructure, training, and evaluation, showcasing the full\\nspectrum of long-context technologies. At the end of this survey, we will\\npresent 10 unanswered questions currently faced by long-context LLMs. We hope\\nthis survey can serve as a systematic introduction to the research on\\nlong-context LLMs.', 'upvotes': 44, 'discussionId': '67bd37cc0d41e01cca99ab1e'}, 'publishedAt': '2025-02-24T22:27:11.566Z', 'title': 'Thus Spake Long-Context Large Language Model', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17129.png', 'numComments': 1, 'submittedBy': {'_id': '64f033ef82c6eea604c4da8b', 'avatarUrl': '/avatars/51b93fea7fd68b4274ee03701245dcca.svg', 'fullname': 'Liu Xiaoran', 'name': 'LiuXR', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.17258', 'authors': [{'_id': '67bd515c0417e7f92283d3b8', 'name': 'Xiangpeng Yang', 'hidden': False}, {'_id': '67bd515c0417e7f92283d3b9', 'name': 'Linchao Zhu', 'hidden': False}, {'_id': '67bd515c0417e7f92283d3ba', 'name': 'Hehe Fan', 'hidden': False}, {'_id': '67bd515c0417e7f92283d3bb', 'name': 'Yi Yang', 'hidden': False}], 'publishedAt': '2025-02-24T15:39:14.000Z', 'title': 'VideoGrain: Modulating Space-Time Attention for Multi-grained Video\\n  Editing', 'summary': \"Recent advancements in diffusion models have significantly improved video\\ngeneration and editing capabilities. However, multi-grained video editing,\\nwhich encompasses class-level, instance-level, and part-level modifications,\\nremains a formidable challenge. The major difficulties in multi-grained editing\\ninclude semantic misalignment of text-to-region control and feature coupling\\nwithin the diffusion model. To address these difficulties, we present\\nVideoGrain, a zero-shot approach that modulates space-time (cross- and self-)\\nattention mechanisms to achieve fine-grained control over video content. We\\nenhance text-to-region control by amplifying each local prompt's attention to\\nits corresponding spatial-disentangled region while minimizing interactions\\nwith irrelevant areas in cross-attention. Additionally, we improve feature\\nseparation by increasing intra-region awareness and reducing inter-region\\ninterference in self-attention. Extensive experiments demonstrate our method\\nachieves state-of-the-art performance in real-world scenarios. Our code, data,\\nand demos are available at https://knightyxp.github.io/VideoGrain_project_page/\", 'upvotes': 37, 'discussionId': '67bd51620417e7f92283d4e9'}, 'publishedAt': '2025-02-25T00:13:12.214Z', 'title': 'VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17258.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 6209}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.17157', 'authors': [{'_id': '67bd3285ac4a596a43b53205', 'user': {'_id': '646efd223dd912a539e0bd46', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png', 'isPro': True, 'fullname': 'Canyu Zhao', 'user': 'Canyu', 'type': 'user'}, 'name': 'Canyu Zhao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:40:20.829Z', 'hidden': False}, {'_id': '67bd3285ac4a596a43b53206', 'name': 'Mingyu Liu', 'hidden': False}, {'_id': '67bd3285ac4a596a43b53207', 'user': {'_id': '64d60375d7e30889c65e8cf4', 'avatarUrl': '/avatars/640f7c570fc45194557ce7931bdfe87f.svg', 'isPro': False, 'fullname': 'Huanyi Zheng', 'user': 'zhyya', 'type': 'user'}, 'name': 'Huanyi Zheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:40:18.731Z', 'hidden': False}, {'_id': '67bd3285ac4a596a43b53208', 'user': {'_id': '632179745fc60c44fd91fc33', 'avatarUrl': '/avatars/37d4fefbcc19f091dccffefec9706de2.svg', 'isPro': False, 'fullname': 'zhumuzhi', 'user': 'Z-MU-Z', 'type': 'user'}, 'name': 'Muzhi Zhu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:40:11.968Z', 'hidden': False}, {'_id': '67bd3285ac4a596a43b53209', 'name': 'Zhiyue Zhao', 'hidden': False}, {'_id': '67bd3285ac4a596a43b5320a', 'name': 'Hao Chen', 'hidden': False}, {'_id': '67bd3285ac4a596a43b5320b', 'name': 'Tong He', 'hidden': False}, {'_id': '67bd3285ac4a596a43b5320c', 'name': 'Chunhua Shen', 'hidden': False}], 'publishedAt': '2025-02-24T13:51:06.000Z', 'title': 'DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks', 'summary': 'Our primary goal here is to create a good, generalist perception model that\\ncan tackle multiple tasks, within limits on computational resources and\\ntraining data. To achieve this, we resort to text-to-image diffusion models\\npre-trained on billions of images. Our exhaustive evaluation metrics\\ndemonstrate that DICEPTION effectively tackles multiple perception tasks,\\nachieving performance on par with state-of-the-art models. We achieve results\\non par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B\\npixel-level annotated images). Inspired by Wang et al., DICEPTION formulates\\nthe outputs of various perception tasks using color encoding; and we show that\\nthe strategy of assigning random colors to different instances is highly\\neffective in both entity segmentation and semantic segmentation. Unifying\\nvarious perception tasks as conditional image generation enables us to fully\\nleverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently\\ntrained at a cost of orders of magnitude lower, compared to conventional models\\nthat were trained from scratch. When adapting our model to other tasks, it only\\nrequires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION\\nprovides valuable insights and a more promising solution for visual generalist\\nmodels.', 'upvotes': 36, 'discussionId': '67bd328aac4a596a43b532ae'}, 'publishedAt': '2025-02-24T22:39:29.837Z', 'title': 'DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17157.png', 'numComments': 2, 'submittedBy': {'_id': '646efd223dd912a539e0bd46', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png', 'fullname': 'Canyu Zhao', 'name': 'Canyu', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 8}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.15814', 'authors': [{'_id': '67bd3972f077ddf1f98bacda', 'user': {'_id': '66b9bc2dacdbc1d0b39c3b50', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg', 'isPro': False, 'fullname': 'Gallil Maimon', 'user': 'gallilmaimon', 'type': 'user'}, 'name': 'Gallil Maimon', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:39:36.258Z', 'hidden': False}, {'_id': '67bd3972f077ddf1f98bacdb', 'user': {'_id': '644662145004f2cb3af08b27', 'avatarUrl': '/avatars/5f2af24c7410a5db46374d0b84fb479d.svg', 'isPro': False, 'fullname': 'Avishai Elmakies', 'user': 'avishai-elmakies', 'type': 'user'}, 'name': 'Avishai Elmakies', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:39:33.712Z', 'hidden': False}, {'_id': '67bd3972f077ddf1f98bacdc', 'name': 'Yossi Adi', 'hidden': False}], 'publishedAt': '2025-02-19T17:21:15.000Z', 'title': 'Slamming: Training a Speech Language Model on One GPU in a Day', 'summary': 'We introduce Slam, a recipe for training high-quality Speech Language Models\\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\\nanalysis of model initialisation and architecture, synthetic training data,\\npreference optimisation with synthetic data and tweaking all other components.\\nWe empirically demonstrate that this training recipe also scales well with more\\ncompute getting results on par with leading SLMs in a fraction of the compute\\ncost. We hope these insights will make SLM training and research more\\naccessible. In the context of SLM scaling laws, our results far outperform\\npredicted compute optimal performance, giving an optimistic view to SLM\\nfeasibility. See code, data, models, samples at -\\nhttps://pages.cs.huji.ac.il/adiyoss-lab/slamming .', 'upvotes': 31, 'discussionId': '67bd3973f077ddf1f98bacf9'}, 'publishedAt': '2025-02-24T23:14:12.363Z', 'title': 'Slamming: Training a Speech Language Model on One GPU in a Day', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/t93GkoiYRplnXH1Go0MmY.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15814.png', 'numComments': 1, 'submittedBy': {'_id': '66b9bc2dacdbc1d0b39c3b50', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg', 'fullname': 'Gallil Maimon', 'name': 'gallilmaimon', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.16584', 'authors': [{'_id': '67bd42386959e61abd265a9b', 'name': 'Liumeng Xue', 'hidden': False}, {'_id': '67bd42386959e61abd265a9c', 'name': 'Ziya Zhou', 'hidden': False}, {'_id': '67bd42386959e61abd265a9d', 'name': 'Jiahao Pan', 'hidden': False}, {'_id': '67bd42386959e61abd265a9e', 'name': 'Zixuan Li', 'hidden': False}, {'_id': '67bd42386959e61abd265a9f', 'name': 'Shuai Fan', 'hidden': False}, {'_id': '67bd42386959e61abd265aa0', 'name': 'Yinghao Ma', 'hidden': False}, {'_id': '67bd42386959e61abd265aa1', 'name': 'Sitong Cheng', 'hidden': False}, {'_id': '67bd42386959e61abd265aa2', 'name': 'Dongchao Yang', 'hidden': False}, {'_id': '67bd42386959e61abd265aa3', 'name': 'Haohan Guo', 'hidden': False}, {'_id': '67bd42386959e61abd265aa4', 'name': 'Yujia Xiao', 'hidden': False}, {'_id': '67bd42386959e61abd265aa5', 'name': 'Xinsheng Wang', 'hidden': False}, {'_id': '67bd42386959e61abd265aa6', 'name': 'Zixuan Shen', 'hidden': False}, {'_id': '67bd42386959e61abd265aa7', 'name': 'Chuanbo Zhu', 'hidden': False}, {'_id': '67bd42386959e61abd265aa8', 'name': 'Xinshen Zhang', 'hidden': False}, {'_id': '67bd42386959e61abd265aa9', 'name': 'Tianchi Liu', 'hidden': False}, {'_id': '67bd42386959e61abd265aaa', 'name': 'Ruibin Yuan', 'hidden': False}, {'_id': '67bd42386959e61abd265aab', 'name': 'Zeyue Tian', 'hidden': False}, {'_id': '67bd42386959e61abd265aac', 'name': 'Haohe Liu', 'hidden': False}, {'_id': '67bd42386959e61abd265aad', 'name': 'Emmanouil Benetos', 'hidden': False}, {'_id': '67bd42386959e61abd265aae', 'name': 'Ge Zhang', 'hidden': False}, {'_id': '67bd42386959e61abd265aaf', 'name': 'Yike Guo', 'hidden': False}, {'_id': '67bd42386959e61abd265ab0', 'name': 'Wei Xue', 'hidden': False}], 'publishedAt': '2025-02-23T14:24:15.000Z', 'title': 'Audio-FLAN: A Preliminary Release', 'summary': 'Recent advancements in audio tokenization have significantly enhanced the\\nintegration of audio capabilities into large language models (LLMs). However,\\naudio understanding and generation are often treated as distinct tasks,\\nhindering the development of truly unified audio-language models. While\\ninstruction tuning has demonstrated remarkable success in improving\\ngeneralization and zero-shot learning across text and vision, its application\\nto audio remains largely unexplored. A major obstacle is the lack of\\ncomprehensive datasets that unify audio understanding and generation. To\\naddress this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset\\ncovering 80 diverse tasks across speech, music, and sound domains, with over\\n100 million instances. Audio-FLAN lays the foundation for unified\\naudio-language models that can seamlessly handle both understanding (e.g.,\\ntranscription, comprehension) and generation (e.g., speech, music, sound) tasks\\nacross a wide range of audio domains in a zero-shot manner. The Audio-FLAN\\ndataset is available on HuggingFace and GitHub and will be continuously\\nupdated.', 'upvotes': 21, 'discussionId': '67bd423b6959e61abd265b88'}, 'publishedAt': '2025-02-24T23:14:20.487Z', 'title': 'Audio-FLAN: A Preliminary Release', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16584.png', 'numComments': 1, 'submittedBy': {'_id': '5fd6f670053c8345eddc1b68', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/5fd6f670053c8345eddc1b68/cuTsu2krRYHC6zYGD2dpQ.jpeg', 'fullname': 'Ruibin Yuan', 'name': 'a43992899', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 13}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.17435', 'authors': [{'_id': '67bd6b4b8edd1ce8ad5603a0', 'name': 'Chen-Wei Chang', 'hidden': False}, {'_id': '67bd6b4b8edd1ce8ad5603a1', 'name': 'Cheng-De Fan', 'hidden': False}, {'_id': '67bd6b4b8edd1ce8ad5603a2', 'name': 'Chia-Che Chang', 'hidden': False}, {'_id': '67bd6b4b8edd1ce8ad5603a3', 'name': 'Yi-Chen Lo', 'hidden': False}, {'_id': '67bd6b4b8edd1ce8ad5603a4', 'name': 'Yu-Chee Tseng', 'hidden': False}, {'_id': '67bd6b4b8edd1ce8ad5603a5', 'name': 'Jiun-Long Huang', 'hidden': False}, {'_id': '67bd6b4b8edd1ce8ad5603a6', 'name': 'Yu-Lun Liu', 'hidden': False}], 'publishedAt': '2025-02-24T18:59:54.000Z', 'title': 'GCC: Generative Color Constancy via Diffusing a Color Checker', 'summary': \"Color constancy methods often struggle to generalize across different camera\\nsensors due to varying spectral sensitivities. We present GCC, which leverages\\ndiffusion models to inpaint color checkers into images for illumination\\nestimation. Our key innovations include (1) a single-step deterministic\\ninference approach that inpaints color checkers reflecting scene illumination,\\n(2) a Laplacian decomposition technique that preserves checker structure while\\nallowing illumination-dependent color adaptation, and (3) a mask-based data\\naugmentation strategy for handling imprecise color checker annotations. GCC\\ndemonstrates superior robustness in cross-camera scenarios, achieving\\nstate-of-the-art worst-25% error rates of 5.15{\\\\deg} and 4.32{\\\\deg} in\\nbi-directional evaluations. These results highlight our method's stability and\\ngeneralization capability across different camera characteristics without\\nrequiring sensor-specific training, making it a versatile solution for\\nreal-world applications.\", 'upvotes': 18, 'discussionId': '67bd6b4d8edd1ce8ad560401'}, 'publishedAt': '2025-02-25T02:06:00.809Z', 'title': 'GCC: Generative Color Constancy via Diffusing a Color Checker', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/gDAYQUcbNE2Ps2pQFxg_m.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17435.png', 'numComments': 1, 'submittedBy': {'_id': '6459d5da3b6fafd9664807ab', 'avatarUrl': '/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg', 'fullname': 'Yu-Lun Liu', 'name': 'yulunliu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.16614', 'authors': [{'_id': '67bd36334a9a04b9ca9bbb68', 'name': 'Alexander Zhang', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb69', 'name': 'Marcus Dong', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb6a', 'name': 'Jiaheng Liu', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb6b', 'name': 'Wei Zhang', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb6c', 'name': 'Yejie Wang', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb6d', 'name': 'Jian Yang', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb6e', 'name': 'Ge Zhang', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb6f', 'name': 'Tianyu Liu', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb70', 'name': 'Zhongyuan Peng', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb71', 'name': 'Yingshui Tan', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb72', 'name': 'Yuanxing Zhang', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb73', 'name': 'Zhexu Wang', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb74', 'name': 'Weixun Wang', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb75', 'name': 'Yancheng He', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb76', 'name': 'Ken Deng', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb77', 'name': 'Wangchunshu Zhou', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb78', 'name': 'Wenhao Huang', 'hidden': False}, {'_id': '67bd36334a9a04b9ca9bbb79', 'name': 'Zhaoxiang Zhang', 'hidden': False}], 'publishedAt': '2025-02-23T15:36:43.000Z', 'title': 'CodeCriticBench: A Holistic Code Critique Benchmark for Large Language\\n  Models', 'summary': 'The critique capacity of Large Language Models (LLMs) is essential for\\nreasoning abilities, which can provide necessary suggestions (e.g., detailed\\nanalysis and constructive feedback). Therefore, how to evaluate the critique\\ncapacity of LLMs has drawn great attention and several critique benchmarks have\\nbeen proposed. However, existing critique benchmarks usually have the following\\nlimitations: (1). Focusing on diverse reasoning tasks in general domains and\\ninsufficient evaluation on code tasks (e.g., only covering code generation\\ntask), where the difficulty of queries is relatively easy (e.g., the code\\nqueries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive\\nevaluation from different dimensions. To address these limitations, we\\nintroduce a holistic code critique benchmark for LLMs called CodeCriticBench.\\nSpecifically, our CodeCriticBench includes two mainstream code tasks (i.e.,\\ncode generation and code QA) with different difficulties. Besides, the\\nevaluation protocols include basic critique evaluation and advanced critique\\nevaluation for different characteristics, where fine-grained evaluation\\nchecklists are well-designed for advanced settings. Finally, we conduct\\nextensive experimental results of existing LLMs, which show the effectiveness\\nof CodeCriticBench.', 'upvotes': 17, 'discussionId': '67bd36354a9a04b9ca9bbc16'}, 'publishedAt': '2025-02-24T22:17:28.937Z', 'title': 'CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16614.png', 'numComments': 1, 'submittedBy': {'_id': '65377c30e48353201e6fdda0', 'avatarUrl': '/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg', 'fullname': 'Jiaheng Liu', 'name': 'CheeryLJH', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.17407', 'authors': [{'_id': '67bd48d4becb766415a5d19d', 'name': 'Guijin Son', 'hidden': False}, {'_id': '67bd48d4becb766415a5d19e', 'name': 'Jiwoo Hong', 'hidden': False}, {'_id': '67bd48d4becb766415a5d19f', 'user': {'_id': '63e087b6a98d931aa90c1b9c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63e087b6a98d931aa90c1b9c/96c6IT3f1pWGLbRdRDB2U.png', 'isPro': False, 'fullname': 'Hyunwoo Ko', 'user': 'Cartinoe5930', 'type': 'user'}, 'name': 'Hyunwoo Ko', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:39:12.933Z', 'hidden': False}, {'_id': '67bd48d4becb766415a5d1a0', 'name': 'James Thorne', 'hidden': False}], 'publishedAt': '2025-02-24T18:36:15.000Z', 'title': 'Linguistic Generalizability of Test-Time Scaling in Mathematical\\n  Reasoning', 'summary': 'Scaling pre-training compute has proven effective for achieving\\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\\nintroduce MCLM, a multilingual math benchmark featuring competition-level\\nproblems in 55 languages. We test three test-time scaling methods-Outcome\\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\\n\"thinking LLMs\" have recently garnered significant attention, we find that\\ntheir performance is comparable to traditional scaling methods like best-of-N\\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\\naverage gain across other languages-a pattern consistent across the other\\ntest-time scaling methods we studied-higlighting that test-time scaling may not\\ngeneralize as effectively to multilingual tasks. To foster further research, we\\nrelease MCLM, MR1-1.5B, and evaluation results.', 'upvotes': 14, 'discussionId': '67bd48d5becb766415a5d1e9'}, 'publishedAt': '2025-02-24T23:37:53.138Z', 'title': 'Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17407.png', 'numComments': 1, 'submittedBy': {'_id': '60d3e619b8448e1785bbda2a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg', 'fullname': 'GUIJIN SON', 'name': 'amphora', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 45}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.16894', 'authors': [{'_id': '67bd396ea06bae99f3866911', 'user': {'_id': '641aa5e391e3376a057bbd4c', 'avatarUrl': '/avatars/5818797f27444fde078b503774ee081c.svg', 'isPro': False, 'fullname': 'Chenghao Fan', 'user': 'Facico', 'type': 'user'}, 'name': 'Chenghao Fan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:39:38.942Z', 'hidden': False}, {'_id': '67bd396ea06bae99f3866912', 'name': 'Zhenyi Lu', 'hidden': False}, {'_id': '67bd396ea06bae99f3866913', 'name': 'Sichen Liu', 'hidden': False}, {'_id': '67bd396ea06bae99f3866914', 'name': 'Xiaoye Qu', 'hidden': False}, {'_id': '67bd396ea06bae99f3866915', 'name': 'Wei Wei', 'hidden': False}, {'_id': '67bd396ea06bae99f3866916', 'name': 'Chengfeng Gu', 'hidden': False}, {'_id': '67bd396ea06bae99f3866917', 'name': 'Yu Cheng', 'hidden': False}], 'publishedAt': '2025-02-24T06:48:13.000Z', 'title': 'Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and\\n  Mixture-of-Experts Optimization Alignment', 'summary': \"While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for\\nLarge Language Models (LLMs), its performance often falls short of Full\\nFine-Tuning (Full FT). Current methods optimize LoRA by initializing with\\nstatic singular value decomposition (SVD) subsets, leading to suboptimal\\nleveraging of pre-trained knowledge. Another path for improving LoRA is\\nincorporating a Mixture-of-Experts (MoE) architecture. However, weight\\nmisalignment and complex gradient dynamics make it challenging to adopt SVD\\nprior to the LoRA MoE architecture. To mitigate these issues, we propose\\nGreat LoRA Mixture-of-Expert\\n(GOAT), a framework that (1) adaptively integrates relevant priors using an\\nSVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by\\nderiving a theoretical scaling factor. We demonstrate that proper scaling,\\nwithout modifying the architecture or training algorithms, boosts LoRA MoE's\\nefficiency and performance. Experiments across 25 datasets, including natural\\nlanguage understanding, commonsense reasoning, image classification, and\\nnatural language generation, demonstrate GOAT's state-of-the-art performance,\\nclosing the gap with Full FT.\", 'upvotes': 13, 'discussionId': '67bd396fa06bae99f3866964'}, 'publishedAt': '2025-02-24T22:35:41.042Z', 'title': 'Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16894.png', 'numComments': 1, 'submittedBy': {'_id': '641aa5e391e3376a057bbd4c', 'avatarUrl': '/avatars/5818797f27444fde078b503774ee081c.svg', 'fullname': 'Chenghao Fan', 'name': 'Facico', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 12}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.16033', 'authors': [{'_id': '67bd31d0d055a27740b16a30', 'name': 'Qianqi Yan', 'hidden': False}, {'_id': '67bd31d0d055a27740b16a31', 'name': 'Yue Fan', 'hidden': False}, {'_id': '67bd31d0d055a27740b16a32', 'name': 'Hongquan Li', 'hidden': False}, {'_id': '67bd31d0d055a27740b16a33', 'name': 'Shan Jiang', 'hidden': False}, {'_id': '67bd31d0d055a27740b16a34', 'name': 'Yang Zhao', 'hidden': False}, {'_id': '67bd31d0d055a27740b16a35', 'name': 'Xinze Guan', 'hidden': False}, {'_id': '67bd31d0d055a27740b16a36', 'name': 'Ching-Chen Kuo', 'hidden': False}, {'_id': '67bd31d0d055a27740b16a37', 'name': 'Xin Eric Wang', 'hidden': False}], 'publishedAt': '2025-02-22T01:52:37.000Z', 'title': 'Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for\\n  Multimodal Reasoning Models', 'summary': \"Existing Multimodal Large Language Models (MLLMs) are predominantly trained\\nand tested on consistent visual-textual inputs, leaving open the question of\\nwhether they can handle inconsistencies in real-world, layout-rich content. To\\nbridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR)\\nbenchmark to assess MLLMs' ability to detect and reason about semantic\\nmismatches in artifacts such as webpages, presentation slides, and posters.\\nMMIR comprises 534 challenging samples, each containing synthetically injected\\nerrors across five reasoning-heavy categories: Factual Contradiction, Identity\\nMisattribution, Contextual Mismatch, Quantitative Discrepancy, and\\nTemporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing\\nthat models with dedicated multimodal reasoning capabilities, such as o1,\\nsubstantially outperform their counterparts while open-source models remain\\nparticularly vulnerable to inconsistency errors. Detailed error analyses\\nfurther show that models excel in detecting inconsistencies confined to a\\nsingle modality, particularly in text, but struggle with cross-modal conflicts\\nand complex layouts. Probing experiments reveal that single-modality prompting,\\nincluding Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal\\ngains, revealing a key bottleneck in cross-modal reasoning. Our findings\\nhighlight the need for advanced multimodal reasoning and point to future\\nresearch on multimodal inconsistency.\", 'upvotes': 12, 'discussionId': '67bd31d2d055a27740b16ad9'}, 'publishedAt': '2025-02-24T21:59:50.456Z', 'title': 'Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16033.png', 'numComments': 1, 'submittedBy': {'_id': '64679a226192d39142245e5e', 'avatarUrl': '/avatars/05abee0b6317f100923936ca2099e9eb.svg', 'fullname': 'Xin Eric Wang', 'name': 'xw-eric', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.15894', 'authors': [{'_id': '67bd3bd26faf9f04b2170f61', 'name': 'Min Zhao', 'hidden': False}, {'_id': '67bd3bd26faf9f04b2170f62', 'name': 'Guande He', 'hidden': False}, {'_id': '67bd3bd26faf9f04b2170f63', 'name': 'Yixiao Chen', 'hidden': False}, {'_id': '67bd3bd26faf9f04b2170f64', 'user': {'_id': '64c269a52d73768f07ac266c', 'avatarUrl': '/avatars/d497a960f8aef6a974907b68ed750c1c.svg', 'isPro': False, 'fullname': 'Zhu Hongzhou', 'user': 'zhuhz22', 'type': 'user'}, 'name': 'Hongzhou Zhu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:39:23.502Z', 'hidden': False}, {'_id': '67bd3bd26faf9f04b2170f65', 'name': 'Chongxuan Li', 'hidden': False}, {'_id': '67bd3bd26faf9f04b2170f66', 'name': 'Jun Zhu', 'hidden': False}], 'publishedAt': '2025-02-21T19:28:05.000Z', 'title': 'RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion\\n  Transformers', 'summary': 'Recent advancements in video generation have enabled models to synthesize\\nhigh-quality, minute-long videos. However, generating even longer videos with\\ntemporal coherence remains a major challenge, and existing length extrapolation\\nmethods lead to temporal repetition or motion deceleration. In this work, we\\nsystematically analyze the role of frequency components in positional\\nembeddings and identify an intrinsic frequency that primarily governs\\nextrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet\\neffective approach that reduces the intrinsic frequency to suppress repetition\\nwhile preserving motion consistency, without requiring any additional\\nmodifications. RIFLEx offers a true free lunch--achieving high-quality\\n2times extrapolation on state-of-the-art video diffusion transformers in a\\ncompletely training-free manner. Moreover, it enhances quality and enables\\n3times extrapolation by minimal fine-tuning without long videos. Project\\npage and codes:\\nhttps://riflex-video.github.io/{https://riflex-video.github.io/.}', 'upvotes': 9, 'discussionId': '67bd3bd66faf9f04b21710d1'}, 'publishedAt': '2025-02-25T00:09:04.483Z', 'title': 'RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15894.png', 'numComments': 2, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 6209}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.17110', 'authors': [{'_id': '67bd3936daef22cbce6d7ef2', 'name': 'Junyang Wang', 'hidden': False}, {'_id': '67bd3936daef22cbce6d7ef3', 'user': {'_id': '645b10e80c73ea27d13f7aca', 'avatarUrl': '/avatars/95e565306472a15067440b5b43e07a6f.svg', 'isPro': False, 'fullname': 'xuhaiyang', 'user': 'xhyandwyy', 'type': 'user'}, 'name': 'Haiyang Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:39:41.528Z', 'hidden': False}, {'_id': '67bd3936daef22cbce6d7ef4', 'name': 'Xi Zhang', 'hidden': False}, {'_id': '67bd3936daef22cbce6d7ef5', 'name': 'Ming Yan', 'hidden': False}, {'_id': '67bd3936daef22cbce6d7ef6', 'name': 'Ji Zhang', 'hidden': False}, {'_id': '67bd3936daef22cbce6d7ef7', 'name': 'Fei Huang', 'hidden': False}, {'_id': '67bd3936daef22cbce6d7ef8', 'name': 'Jitao Sang', 'hidden': False}], 'publishedAt': '2025-02-24T12:51:23.000Z', 'title': 'Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided\\n  Multi-Agent Collaboration', 'summary': 'The rapid increase in mobile device usage necessitates improved automation\\nfor seamless task management. However, many AI-driven frameworks struggle due\\nto insufficient operational knowledge. Manually written knowledge helps but is\\nlabor-intensive and inefficient. To address these challenges, we introduce\\nMobile-Agent-V, a framework that leverages video guidance to provide rich and\\ncost-effective operational knowledge for mobile automation. Mobile-Agent-V\\nenhances task execution capabilities by leveraging video inputs without\\nrequiring specialized sampling or preprocessing. Mobile-Agent-V integrates a\\nsliding window strategy and incorporates a video agent and deep-reflection\\nagent to ensure that actions align with user instructions. Through this\\ninnovative approach, users can record task processes with guidance, enabling\\nthe system to autonomously learn and execute tasks efficiently. Experimental\\nresults show that Mobile-Agent-V achieves a 30% performance improvement\\ncompared to existing frameworks.', 'upvotes': 8, 'discussionId': '67bd3938daef22cbce6d7f9d'}, 'publishedAt': '2025-02-24T22:31:17.771Z', 'title': 'Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/mshxtP77rrnN07f6ux6_0.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17110.png', 'numComments': 1, 'submittedBy': {'_id': '645b10e80c73ea27d13f7aca', 'avatarUrl': '/avatars/95e565306472a15067440b5b43e07a6f.svg', 'fullname': 'xuhaiyang', 'name': 'xhyandwyy', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.16707', 'authors': [{'_id': '67bd3bcc797e4d53ce0bc70d', 'user': {'_id': '64f8fbd95515d7dcceb906b1', 'avatarUrl': '/avatars/1c7d034de408930b166592465e65fc31.svg', 'isPro': False, 'fullname': 'Yunhai Feng', 'user': 'yunhaif', 'type': 'user'}, 'name': 'Yunhai Feng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:39:31.085Z', 'hidden': False}, {'_id': '67bd3bcc797e4d53ce0bc70e', 'user': {'_id': '62318c0386753f5f41d0e261', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg', 'isPro': False, 'fullname': 'Jiaming Han', 'user': 'csuhan', 'type': 'user'}, 'name': 'Jiaming Han', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:39:28.772Z', 'hidden': False}, {'_id': '67bd3bcc797e4d53ce0bc70f', 'name': 'Zhuoran Yang', 'hidden': False}, {'_id': '67bd3bcc797e4d53ce0bc710', 'name': 'Xiangyu Yue', 'hidden': False}, {'_id': '67bd3bcc797e4d53ce0bc711', 'name': 'Sergey Levine', 'hidden': False}, {'_id': '67bd3bcc797e4d53ce0bc712', 'name': 'Jianlan Luo', 'hidden': False}], 'publishedAt': '2025-02-23T20:42:15.000Z', 'title': 'Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon\\n  Robotic Manipulation', 'summary': 'Solving complex long-horizon robotic manipulation problems requires\\nsophisticated high-level planning capabilities, the ability to reason about the\\nphysical world, and reactively choose appropriate motor skills. Vision-language\\nmodels (VLMs) pretrained on Internet data could in principle offer a framework\\nfor tackling such problems. However, in their current form, VLMs lack both the\\nnuanced understanding of intricate physics required for robotic manipulation\\nand the ability to reason over long horizons to address error compounding\\nissues. In this paper, we introduce a novel test-time computation framework\\nthat enhances VLMs\\' physical reasoning capabilities for multi-stage\\nmanipulation tasks. At its core, our approach iteratively improves a pretrained\\nVLM with a \"reflection\" mechanism - it uses a generative model to imagine\\nfuture world states, leverages these predictions to guide action selection, and\\ncritically reflects on potential suboptimalities to refine its reasoning.\\nExperimental results demonstrate that our method significantly outperforms\\nseveral state-of-the-art commercial VLMs as well as other post-training\\napproaches such as Monte Carlo Tree Search (MCTS). Videos are available at\\nhttps://reflect-vlm.github.io.', 'upvotes': 7, 'discussionId': '67bd3bcf797e4d53ce0bc7ff'}, 'publishedAt': '2025-02-25T01:02:05.395Z', 'title': 'Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16707.png', 'numComments': 1, 'submittedBy': {'_id': '64f8cb8ed04a890f5380d9a4', 'avatarUrl': '/avatars/d6fdfdbb0c10141aa3b4c832d928121b.svg', 'fullname': 'Jianlan Luo', 'name': 'jianlanluo', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.16922', 'authors': [{'_id': '67bd3d6b60186d7478467208', 'user': {'_id': '6643261b8876db14227eeb19', 'avatarUrl': '/avatars/67428c9e37a2273697c0547e1783ec6b.svg', 'isPro': False, 'fullname': 'Zhenglin Wang', 'user': 'wzl0228', 'type': 'user'}, 'name': 'Zhenglin Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:39:15.633Z', 'hidden': False}, {'_id': '67bd3d6b60186d7478467209', 'user': {'_id': '644a4fbc2166258fccc664bc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg', 'isPro': False, 'fullname': 'Jialong Wu', 'user': 'callanwu', 'type': 'user'}, 'name': 'Jialong Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T15:06:02.856Z', 'hidden': False}, {'_id': '67bd3d6b60186d747846720a', 'name': 'Pengfei LI', 'hidden': False}, {'_id': '67bd3d6b60186d747846720b', 'name': 'Yong Jiang', 'hidden': False}, {'_id': '67bd3d6b60186d747846720c', 'name': 'Deyu Zhou', 'hidden': False}], 'publishedAt': '2025-02-24T07:27:54.000Z', 'title': 'Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties', 'summary': 'Temporal reasoning is fundamental to human cognition and is crucial for\\nvarious real-world applications. While recent advances in Large Language Models\\nhave demonstrated promising capabilities in temporal reasoning, existing\\nbenchmarks primarily rely on rule-based construction, lack contextual depth,\\nand involve a limited range of temporal entities. To address these limitations,\\nwe introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate\\nLLMs on temporal reasoning within the extensive scope of Chinese dynastic\\nchronology. CTM emphasizes cross-entity relationships, pairwise temporal\\nalignment, and contextualized and culturally-grounded reasoning, providing a\\ncomprehensive evaluation. Extensive experimental results reveal the challenges\\nposed by CTM and highlight potential avenues for improvement.', 'upvotes': 7, 'discussionId': '67bd3d6c60186d7478467249'}, 'publishedAt': '2025-02-24T22:48:30.357Z', 'title': 'Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16922.png', 'numComments': 3, 'submittedBy': {'_id': '644a4fbc2166258fccc664bc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg', 'fullname': 'Jialong Wu', 'name': 'callanwu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.17055', 'authors': [{'_id': '67bd9b40478ef7c36240c6e6', 'name': 'Tianjin Huang', 'hidden': False}, {'_id': '67bd9b40478ef7c36240c6e7', 'name': 'Haotian Hu', 'hidden': False}, {'_id': '67bd9b40478ef7c36240c6e8', 'name': 'Zhenyu Zhang', 'hidden': False}, {'_id': '67bd9b40478ef7c36240c6e9', 'name': 'Gaojie Jin', 'hidden': False}, {'_id': '67bd9b40478ef7c36240c6ea', 'name': 'Xiang Li', 'hidden': False}, {'_id': '67bd9b40478ef7c36240c6eb', 'name': 'Li Shen', 'hidden': False}, {'_id': '67bd9b40478ef7c36240c6ec', 'name': 'Tianlong Chen', 'hidden': False}, {'_id': '67bd9b40478ef7c36240c6ed', 'name': 'Lu Liu', 'hidden': False}, {'_id': '67bd9b40478ef7c36240c6ee', 'name': 'Qingsong Wen', 'hidden': False}, {'_id': '67bd9b40478ef7c36240c6ef', 'name': 'Zhangyang Wang', 'hidden': False}, {'_id': '67bd9b40478ef7c36240c6f0', 'name': 'Shiwei Liu', 'hidden': False}], 'publishedAt': '2025-02-24T11:09:15.000Z', 'title': 'Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam', 'summary': 'This paper comprehensively evaluates several recently proposed optimizers for\\n4-bit training, revealing that low-bit precision amplifies sensitivity to\\nlearning rates and often causes unstable gradient norms, leading to divergence\\nat higher learning rates. Among these, SPAM, a recent optimizer featuring\\nmomentum reset and spike-aware gradient clipping, achieves the best performance\\nacross various bit levels, but struggles to stabilize gradient norms, requiring\\ncareful learning rate tuning. To address these limitations, we propose\\nStable-SPAM, which incorporates enhanced gradient normalization and clipping\\ntechniques. In particular, Stable-SPAM (1) adaptively updates the clipping\\nthreshold for spiked gradients by tracking their historical maxima; (2)\\nnormalizes the entire gradient matrix based on its historical l_2-norm\\nstatistics; and (3) inherits momentum reset from SPAM to periodically reset\\nthe first and second moments of Adam, mitigating the accumulation of spiked\\ngradients. Extensive experiments show that Stable-SPAM effectively stabilizes\\ngradient norms in 4-bit LLM training, delivering superior performance compared\\nto Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with Stable-SPAM\\noutperforms the BF16 LLaMA-1B trained with Adam by up to 2 perplexity.\\nFurthermore, when both models are trained in 4-bit, Stable-SPAM achieves the\\nsame loss as Adam while requiring only about half the training steps. Code is\\navailable at https://github.com/TianjinYellow/StableSPAM.git.', 'upvotes': 6, 'discussionId': '67bd9b41478ef7c36240c724'}, 'publishedAt': '2025-02-25T05:40:40.152Z', 'title': 'Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17055.png', 'numComments': 1, 'submittedBy': {'_id': '64cd4743a785f2043b32915e', 'avatarUrl': '/avatars/ba0b497a194dfea8449112d71fc67654.svg', 'fullname': 'Tianjin Huang', 'name': 'TianjinHuang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.14132', 'authors': [{'_id': '67b86819d00e69f10c1f31b9', 'user': {'_id': '6231d3ce86753f5f41d39c6f', 'avatarUrl': '/avatars/9b18f368e5f80cfc935b2e339d42a85f.svg', 'isPro': False, 'fullname': 'Nadav Borenstein', 'user': 'Nadav', 'type': 'user'}, 'name': 'Nadav Borenstein', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T09:40:52.278Z', 'hidden': False}, {'_id': '67b86819d00e69f10c1f31ba', 'user': {'_id': '6698cffdb2ebada9f4a7e7d7', 'avatarUrl': '/avatars/e66d946c14595d3b008185f2be8d2f57.svg', 'isPro': False, 'fullname': 'Greta Warren', 'user': 'gretawarren', 'type': 'user'}, 'name': 'Greta Warren', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-21T14:42:45.791Z', 'hidden': False}, {'_id': '67b86819d00e69f10c1f31bb', 'name': 'Desmond Elliott', 'hidden': False}, {'_id': '67b86819d00e69f10c1f31bc', 'name': 'Isabelle Augenstein', 'hidden': False}], 'publishedAt': '2025-02-19T22:26:39.000Z', 'title': 'Can Community Notes Replace Professional Fact-Checkers?', 'summary': 'Two commonly-employed strategies to combat the rise of misinformation on\\nsocial media are (i) fact-checking by professional organisations and (ii)\\ncommunity moderation by platform users. Policy changes by Twitter/X and, more\\nrecently, Meta, signal a shift away from partnerships with fact-checking\\norganisations and towards an increased reliance on crowdsourced community\\nnotes. However, the extent and nature of dependencies between fact-checking and\\nhelpful community notes remain unclear. To address these questions, we use\\nlanguage models to annotate a large corpus of Twitter/X community notes with\\nattributes such as topic, cited sources, and whether they refute claims tied to\\nbroader misinformation narratives. Our analysis reveals that community notes\\ncite fact-checking sources up to five times more than previously reported.\\nFact-checking is especially crucial for notes on posts linked to broader\\nnarratives, which are twice as likely to reference fact-checking sources\\ncompared to other sources. In conclusion, our results show that successful\\ncommunity moderation heavily relies on professional fact-checking.', 'upvotes': 5, 'discussionId': '67b8681bd00e69f10c1f3267'}, 'publishedAt': '2025-02-25T04:11:18.915Z', 'title': 'Can Community Notes Replace Professional Fact-Checkers?', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6231d3ce86753f5f41d39c6f/CwWaf1c9-jOzJ-gD5lvCH.jpeg', 'https://cdn-uploads.huggingface.co/production/uploads/6231d3ce86753f5f41d39c6f/WrrBClUkuDsXHcfxP_N8B.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14132.png', 'numComments': 1, 'submittedBy': {'_id': '6231d3ce86753f5f41d39c6f', 'avatarUrl': '/avatars/9b18f368e5f80cfc935b2e339d42a85f.svg', 'fullname': 'Nadav Borenstein', 'name': 'Nadav', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.15987', 'authors': [{'_id': '67bd46ea3e090b402d70f1f4', 'user': {'_id': '64dfbcb18e2084e1d7b51b46', 'avatarUrl': '/avatars/fafe30beea2d7e8eec3f3ba985c582f7.svg', 'isPro': False, 'fullname': 'Kushal Raj Bhandari', 'user': 'KBhandari11', 'type': 'user'}, 'name': 'Kushal Raj Bhandari', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-02-25T04:30:32.676Z', 'hidden': False}, {'_id': '67bd46ea3e090b402d70f1f5', 'name': 'Pin-Yu Chen', 'hidden': False}, {'_id': '67bd46ea3e090b402d70f1f6', 'name': 'Jianxi Gao', 'hidden': False}], 'publishedAt': '2025-02-21T22:52:19.000Z', 'title': 'Forecasting Open-Weight AI Model Growth on Hugging Face', 'summary': \"As the open-weight AI landscape continues to proliferate-with model\\ndevelopment, significant investment, and user interest-it becomes increasingly\\nimportant to predict which models will ultimately drive innovation and shape AI\\necosystems. Building on parallels with citation dynamics in scientific\\nliterature, we propose a framework to quantify how an open-weight model's\\ninfluence evolves. Specifically, we adapt the model introduced by Wang et al.\\nfor scientific citations, using three key parameters-immediacy, longevity, and\\nrelative fitness-to track the cumulative number of fine-tuned models of an\\nopen-weight model. Our findings reveal that this citation-style approach can\\neffectively capture the diverse trajectories of open-weight model adoption,\\nwith most models fitting well and outliers indicating unique patterns or abrupt\\njumps in usage.\", 'upvotes': 5, 'discussionId': '67bd46ee3e090b402d70f317'}, 'publishedAt': '2025-02-24T23:30:36.556Z', 'title': 'Forecasting Open-Weight AI Model Growth on Hugging Face', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/5e67bdd61009063689407479/kQHArNjaT0CM1KCujtDc1.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15987.png', 'numComments': 2, 'submittedBy': {'_id': '5e67bdd61009063689407479', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1583857146757-5e67bdd61009063689407479.jpeg', 'fullname': 'Clem ðŸ¤—', 'name': 'clem', 'type': 'user', 'isPro': True, 'isHf': True, 'isMod': False, 'followerCount': 2053}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.16701', 'authors': [{'_id': '67bd31d6bf6d46017e515a58', 'user': {'_id': '62543749b777cd32720675c2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1658760912583-62543749b777cd32720675c2.jpeg', 'isPro': False, 'fullname': 'Irene Solaiman', 'user': 'irenesolaiman', 'type': 'user'}, 'name': 'Irene Solaiman', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-02-25T03:43:21.348Z', 'hidden': False}, {'_id': '67bd31d6bf6d46017e515a59', 'name': 'Rishi Bommasani', 'hidden': False}, {'_id': '67bd31d6bf6d46017e515a5a', 'name': 'Dan Hendrycks', 'hidden': False}, {'_id': '67bd31d6bf6d46017e515a5b', 'name': 'Ariel Herbert-Voss', 'hidden': False}, {'_id': '67bd31d6bf6d46017e515a5c', 'name': 'Yacine Jernite', 'hidden': False}, {'_id': '67bd31d6bf6d46017e515a5d', 'name': 'Aviya Skowron', 'hidden': False}, {'_id': '67bd31d6bf6d46017e515a5e', 'name': 'Andrew Trask', 'hidden': False}], 'publishedAt': '2025-02-23T20:06:12.000Z', 'title': 'Beyond Release: Access Considerations for Generative AI Systems', 'summary': 'Generative AI release decisions determine whether system components are made\\navailable, but release does not address many other elements that change how\\nusers and stakeholders are able to engage with a system. Beyond release, access\\nto system components informs potential risks and benefits. Access refers to\\npractical needs, infrastructurally, technically, and societally, in order to\\nuse available components in some way. We deconstruct access along three axes:\\nresourcing, technical usability, and utility. Within each category, a set of\\nvariables per system component clarify tradeoffs. For example, resourcing\\nrequires access to computing infrastructure to serve model weights. We also\\ncompare the accessibility of four high performance language models, two\\nopen-weight and two closed-weight, showing similar considerations for all based\\ninstead on access variables. Access variables set the foundation for being able\\nto scale or increase access to users; we examine the scale of access and how\\nscale affects ability to manage and intervene on risks. This framework better\\nencompasses the landscape and risk-benefit tradeoffs of system releases to\\ninform system release decisions, research, and policy.', 'upvotes': 5, 'discussionId': '67bd31d7bf6d46017e515a7e'}, 'publishedAt': '2025-02-24T21:59:15.571Z', 'title': 'Beyond Release: Access Considerations for Generative AI Systems', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/LwZmJUoXiJriC_c1DZ7qM.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16701.png', 'numComments': 1, 'submittedBy': {'_id': '62543749b777cd32720675c2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1658760912583-62543749b777cd32720675c2.jpeg', 'fullname': 'Irene Solaiman', 'name': 'irenesolaiman', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 79}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.15425', 'authors': [{'_id': '67bda01d87919b52fc418533', 'user': {'_id': '65e98cd8e19214e9d151f29e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65e98cd8e19214e9d151f29e/XjQzoVgKVzv8AZBWFQnHz.jpeg', 'isPro': False, 'fullname': 'Giuseppe Paolo', 'user': 'GPaolo', 'type': 'user'}, 'name': 'Giuseppe Paolo', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T14:09:33.376Z', 'hidden': False}, {'_id': '67bda01d87919b52fc418534', 'name': 'Abdelhakim Benechehab', 'hidden': False}, {'_id': '67bda01d87919b52fc418535', 'name': 'Hamza Cherkaoui', 'hidden': False}, {'_id': '67bda01d87919b52fc418536', 'name': 'Albert Thomas', 'hidden': False}, {'_id': '67bda01d87919b52fc418537', 'name': 'BalÃ¡zs KÃ©gl', 'hidden': False}], 'publishedAt': '2025-02-21T12:52:16.000Z', 'title': 'TAG: A Decentralized Framework for Multi-Agent Hierarchical\\n  Reinforcement Learning', 'summary': 'Hierarchical organization is fundamental to biological systems and human\\nsocieties, yet artificial intelligence systems often rely on monolithic\\narchitectures that limit adaptability and scalability. Current hierarchical\\nreinforcement learning (HRL) approaches typically restrict hierarchies to two\\nlevels or require centralized training, which limits their practical\\napplicability. We introduce TAME Agent Framework (TAG), a framework for\\nconstructing fully decentralized hierarchical multi-agent systems.TAG enables\\nhierarchies of arbitrary depth through a novel LevelEnv concept, which\\nabstracts each hierarchy level as the environment for the agents above it. This\\napproach standardizes information flow between levels while preserving loose\\ncoupling, allowing for seamless integration of diverse agent types. We\\ndemonstrate the effectiveness of TAG by implementing hierarchical architectures\\nthat combine different RL agents across multiple levels, achieving improved\\nperformance over classical multi-agent RL baselines on standard benchmarks. Our\\nresults show that decentralized hierarchical organization enhances both\\nlearning speed and final performance, positioning TAG as a promising direction\\nfor scalable multi-agent systems.', 'upvotes': 4, 'discussionId': '67bda01f87919b52fc4185d8'}, 'publishedAt': '2025-02-25T05:51:02.881Z', 'title': 'TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15425.png', 'numComments': 1, 'submittedBy': {'_id': '65e98cd8e19214e9d151f29e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65e98cd8e19214e9d151f29e/XjQzoVgKVzv8AZBWFQnHz.jpeg', 'fullname': 'Giuseppe Paolo', 'name': 'GPaolo', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.17414', 'authors': [{'_id': '67bd526001d5bfa0abfcc5ba', 'name': 'Zeyuan Chen', 'hidden': False}, {'_id': '67bd526001d5bfa0abfcc5bb', 'name': 'Hongyi Xu', 'hidden': False}, {'_id': '67bd526001d5bfa0abfcc5bc', 'name': 'Guoxian Song', 'hidden': False}, {'_id': '67bd526001d5bfa0abfcc5bd', 'name': 'You Xie', 'hidden': False}, {'_id': '67bd526001d5bfa0abfcc5be', 'name': 'Chenxu Zhang', 'hidden': False}, {'_id': '67bd526001d5bfa0abfcc5bf', 'name': 'Xin Chen', 'hidden': False}, {'_id': '67bd526001d5bfa0abfcc5c0', 'name': 'Chao Wang', 'hidden': False}, {'_id': '67bd526001d5bfa0abfcc5c1', 'name': 'Di Chang', 'hidden': False}, {'_id': '67bd526001d5bfa0abfcc5c2', 'name': 'Linjie Luo', 'hidden': False}], 'publishedAt': '2025-02-24T18:47:54.000Z', 'title': 'X-Dancer: Expressive Music to Human Dance Video Generation', 'summary': 'We present X-Dancer, a novel zero-shot music-driven image animation pipeline\\nthat creates diverse and long-range lifelike human dance videos from a single\\nstatic image. As its core, we introduce a unified transformer-diffusion\\nframework, featuring an autoregressive transformer model that synthesize\\nextended and music-synchronized token sequences for 2D body, head and hands\\nposes, which then guide a diffusion model to produce coherent and realistic\\ndance video frames. Unlike traditional methods that primarily generate human\\nmotion in 3D, X-Dancer addresses data limitations and enhances scalability by\\nmodeling a wide spectrum of 2D dance motions, capturing their nuanced alignment\\nwith musical beats through readily available monocular videos. To achieve this,\\nwe first build a spatially compositional token representation from 2D human\\npose labels associated with keypoint confidences, encoding both large\\narticulated body movements (e.g., upper and lower body) and fine-grained\\nmotions (e.g., head and hands). We then design a music-to-motion transformer\\nmodel that autoregressively generates music-aligned dance pose token sequences,\\nincorporating global attention to both musical style and prior motion context.\\nFinally we leverage a diffusion backbone to animate the reference image with\\nthese synthesized pose tokens through AdaIN, forming a fully differentiable\\nend-to-end framework. Experimental results demonstrate that X-Dancer is able to\\nproduce both diverse and characterized dance videos, substantially\\noutperforming state-of-the-art methods in term of diversity, expressiveness and\\nrealism. Code and model will be available for research purposes.', 'upvotes': 4, 'discussionId': '67bd526101d5bfa0abfcc62c'}, 'publishedAt': '2025-02-25T00:17:51.431Z', 'title': 'X-Dancer: Expressive Music to Human Dance Video Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17414.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 6209}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.15122', 'authors': [{'_id': '67bbd6d5ba0bb31293e11210', 'user': {'_id': '675f68e3074ff89c5c078bf3', 'avatarUrl': '/avatars/e3b78d90f032659d411761f47c3cf43e.svg', 'isPro': False, 'fullname': 'Angus', 'user': 'angus924', 'type': 'user'}, 'name': 'Angus Dempster', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-02-24T02:18:57.914Z', 'hidden': False}, {'_id': '67bbd6d5ba0bb31293e11211', 'name': 'Navid Mohammadi Foumani', 'hidden': False}, {'_id': '67bbd6d5ba0bb31293e11212', 'name': 'Chang Wei Tan', 'hidden': False}, {'_id': '67bbd6d5ba0bb31293e11213', 'name': 'Lynn Miller', 'hidden': False}, {'_id': '67bbd6d5ba0bb31293e11214', 'name': 'Amish Mishra', 'hidden': False}, {'_id': '67bbd6d5ba0bb31293e11215', 'name': 'Mahsa Salehi', 'hidden': False}, {'_id': '67bbd6d5ba0bb31293e11216', 'name': 'Charlotte Pelletier', 'hidden': False}, {'_id': '67bbd6d5ba0bb31293e11217', 'name': 'Daniel F. Schmidt', 'hidden': False}, {'_id': '67bbd6d5ba0bb31293e11218', 'name': 'Geoffrey I. Webb', 'hidden': False}], 'publishedAt': '2025-02-21T00:54:40.000Z', 'title': 'MONSTER: Monash Scalable Time Series Evaluation Repository', 'summary': 'We introduce MONSTER-the MONash Scalable Time Series Evaluation Repository-a\\ncollection of large datasets for time series classification. The field of time\\nseries classification has benefitted from common benchmarks set by the UCR and\\nUEA time series classification repositories. However, the datasets in these\\nbenchmarks are small, with median sizes of 217 and 255 examples, respectively.\\nIn consequence they favour a narrow subspace of models that are optimised to\\nachieve low classification error on a wide variety of smaller datasets, that\\nis, models that minimise variance, and give little weight to computational\\nissues such as scalability. Our hope is to diversify the field by introducing\\nbenchmarks using larger datasets. We believe that there is enormous potential\\nfor new progress in the field by engaging with the theoretical and practical\\nchallenges of learning effectively from larger quantities of data.', 'upvotes': 2, 'discussionId': '67bbd6d6ba0bb31293e11258'}, 'publishedAt': '2025-02-25T00:37:53.138Z', 'title': 'MONSTER: Monash Scalable Time Series Evaluation Repository', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15122.png', 'numComments': 1, 'submittedBy': {'_id': '675f68e3074ff89c5c078bf3', 'avatarUrl': '/avatars/e3b78d90f032659d411761f47c3cf43e.svg', 'fullname': 'Angus', 'name': 'angus924', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.14429', 'authors': [{'_id': '67b835f98512a3eca052c0ee', 'user': {'_id': '6304ece07424d937fa35fb98', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6304ece07424d937fa35fb98/6qZoqm-Ti8CiDcCHEt1sE.jpeg', 'isPro': False, 'fullname': 'VilÃ©m Zouhar', 'user': 'zouharvi', 'type': 'user'}, 'name': 'VilÃ©m Zouhar', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-25T14:09:46.337Z', 'hidden': False}, {'_id': '67b835f98512a3eca052c0ef', 'name': 'Maike ZÃ¼fle', 'hidden': False}, {'_id': '67b835f98512a3eca052c0f0', 'name': 'Beni Egressy', 'hidden': False}, {'_id': '67b835f98512a3eca052c0f1', 'name': 'Julius Cheng', 'hidden': False}, {'_id': '67b835f98512a3eca052c0f2', 'name': 'Jan Niehues', 'hidden': False}], 'publishedAt': '2025-02-20T10:27:13.000Z', 'title': 'Early-Exit and Instant Confidence Translation Quality Estimation', 'summary': 'Quality estimation is omnipresent in machine translation, for both evaluation\\nand generation. Unfortunately, quality estimation models are often opaque and\\ncomputationally expensive, making them impractical to be part of large-scale\\npipelines. In this work, we tackle two connected challenges: (1) reducing the\\ncost of quality estimation at scale, and (2) developing an inexpensive\\nuncertainty estimation method for quality estimation. To address the latter, we\\nintroduce Instant Confidence COMET, an uncertainty-aware quality estimation\\nmodel that matches the performance of previous approaches at a fraction of\\ntheir costs. We extend this to Early-Exit COMET, a quality estimation model\\nthat can compute quality scores and associated confidences already at early\\nmodel layers, allowing us to early-exit computations and reduce evaluation\\ncosts. We also apply our model to machine translation reranking. We combine\\nEarly-Exit COMET with an upper confidence bound bandit algorithm to find the\\nbest candidate from a large pool without having to run the full evaluation\\nmodel on all candidates. In both cases (evaluation and reranking) our methods\\nreduce the required compute by 50% with very little degradation in performance.', 'upvotes': 0, 'discussionId': '67b835fa8512a3eca052c11e'}, 'publishedAt': '2025-02-25T09:17:04.777Z', 'title': 'Early-Exit and Instant Confidence Translation Quality Estimation', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6304ece07424d937fa35fb98/FbzjxTx-i9oevQ-i0c_CH.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14429.png', 'numComments': 1, 'submittedBy': {'_id': '6304ece07424d937fa35fb98', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6304ece07424d937fa35fb98/6qZoqm-Ti8CiDcCHEt1sE.jpeg', 'fullname': 'VilÃ©m Zouhar', 'name': 'zouharvi', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 12}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.17237', 'authors': [{'_id': '67bdcb947186ab0e92d9ebf6', 'user': {'_id': '67a9e16f0710558f7bd8947a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/2SacRuI2bOsBgctaxWNGl.png', 'isPro': False, 'fullname': 'Gabriele Berton', 'user': 'gberton', 'type': 'user'}, 'name': 'Gabriele Berton', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-02-25T13:54:29.302Z', 'hidden': False}, {'_id': '67bdcb947186ab0e92d9ebf7', 'name': 'Carlo Masone', 'hidden': False}], 'publishedAt': '2025-02-24T15:14:55.000Z', 'title': 'MegaLoc: One Retrieval to Place Them All', 'summary': 'Retrieving images from the same location as a given query is an important\\ncomponent of multiple computer vision tasks, like Visual Place Recognition,\\nLandmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However,\\nexisting solutions are built to specifically work for one of these tasks, and\\nare known to fail when the requirements slightly change or when they meet\\nout-of-distribution data. In this paper we combine a variety of existing\\nmethods, training techniques, and datasets to train a retrieval model, called\\nMegaLoc, that is performant on multiple tasks. We find that MegaLoc (1)\\nachieves state of the art on a large number of Visual Place Recognition\\ndatasets, (2) impressive results on common Landmark Retrieval datasets, and (3)\\nsets a new state of the art for Visual Localization on the LaMAR datasets,\\nwhere we only changed the retrieval method to the existing localization\\npipeline. The code for MegaLoc is available at\\nhttps://github.com/gmberton/MegaLoc', 'upvotes': 0, 'discussionId': '67bdcb957186ab0e92d9ec34'}, 'publishedAt': '2025-02-25T09:00:19.900Z', 'title': 'MegaLoc: One Retrieval to Place Them All', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17237.png', 'numComments': 1, 'submittedBy': {'_id': '67a9e16f0710558f7bd8947a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/2SacRuI2bOsBgctaxWNGl.png', 'fullname': 'Gabriele Berton', 'name': 'gberton', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.13074', 'authors': [{'_id': '67bd8759fdecc637bd621e6b', 'name': 'Omer Angel', 'hidden': False}, {'_id': '67bd8759fdecc637bd621e6c', 'name': 'Emmanuel Jacob', 'hidden': False}, {'_id': '67bd8759fdecc637bd621e6d', 'name': 'Brett Kolesnik', 'hidden': False}, {'_id': '67bd8759fdecc637bd621e6e', 'name': 'GrÃ©gory Miermont', 'hidden': False}], 'publishedAt': '2025-02-18T17:21:44.000Z', 'title': 'The snake in the Brownian sphere', 'summary': \"The Brownian sphere is a random metric space, homeomorphic to the\\ntwo-dimensional sphere, which arises as the universal scaling limit of many\\ntypes of random planar maps. The direct construction of the Brownian sphere is\\nvia a continuous analogue of the Cori--Vauquelin--Schaeffer (CVS) bijection.\\nThe CVS bijection maps labeled trees to planar maps, and the continuous version\\nmaps Aldous' continuum random tree with Brownian labels (the Brownian snake) to\\nthe Brownian sphere. In this work, we describe the inverse of the continuous\\nCVS bijection, by constructing the Brownian snake as a measurable function of\\nthe Brownian sphere. Special care is needed to work with the orientation of the\\nBrownian sphere.\", 'upvotes': 0, 'discussionId': '67bd875afdecc637bd621e95'}, 'publishedAt': '2025-02-25T04:03:39.758Z', 'title': 'The snake in the Brownian sphere', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13074.png', 'numComments': 1, 'submittedBy': {'_id': '636d12455aaed143cd665607', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1679399015950-636d12455aaed143cd665607.png', 'fullname': 'ZLW', 'name': 'ZarkLngeW', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.15167', 'authors': [{'_id': '67bc7ea06f88ef9a2b8283d3', 'name': 'Chuan Cui', 'hidden': False}, {'_id': '67bc7ea06f88ef9a2b8283d4', 'name': 'Kejiang Chen', 'hidden': False}, {'_id': '67bc7ea06f88ef9a2b8283d5', 'name': 'Zhihua Wei', 'hidden': False}, {'_id': '67bc7ea06f88ef9a2b8283d6', 'name': 'Wen Shen', 'hidden': False}, {'_id': '67bc7ea06f88ef9a2b8283d7', 'name': 'Weiming Zhang', 'hidden': False}, {'_id': '67bc7ea06f88ef9a2b8283d8', 'name': 'Nenghai Yu', 'hidden': False}], 'publishedAt': '2025-02-21T03:05:45.000Z', 'title': 'M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image\\n  Quality Assessment', 'summary': 'The rapid advancement of AI-generated image (AGI) models has introduced\\nsignificant challenges in evaluating their quality, which requires considering\\nmultiple dimensions such as perceptual quality, prompt correspondence, and\\nauthenticity. To address these challenges, we propose M3-AGIQA, a comprehensive\\nframework for AGI quality assessment that is Multimodal, Multi-Round, and\\nMulti-Aspect. Our approach leverages the capabilities of Multimodal Large\\nLanguage Models (MLLMs) as joint text and image encoders and distills advanced\\ncaptioning capabilities from online MLLMs into a local model via Low-Rank\\nAdaptation (LoRA) fine-tuning. The framework includes a structured multi-round\\nevaluation mechanism, where intermediate image descriptions are generated to\\nprovide deeper insights into the quality, correspondence, and authenticity\\naspects. To align predictions with human perceptual judgments, a predictor\\nconstructed by an xLSTM and a regression head is incorporated to process\\nsequential logits and predict Mean Opinion Scores (MOSs). Extensive experiments\\nconducted on multiple benchmark datasets demonstrate that M3-AGIQA achieves\\nstate-of-the-art performance, effectively capturing nuanced aspects of AGI\\nquality. Furthermore, cross-dataset validation confirms its strong\\ngeneralizability. The code is available at\\nhttps://github.com/strawhatboy/M3-AGIQA.', 'upvotes': 0, 'discussionId': '67bc7ea26f88ef9a2b828473'}, 'publishedAt': '2025-02-25T03:36:50.480Z', 'title': 'M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15167.png', 'numComments': 1, 'submittedBy': {'_id': '5f1158120c833276f61f1a84', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg', 'fullname': 'Niels Rogge', 'name': 'nielsr', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 772}, 'isAuthorParticipating': False}"
]