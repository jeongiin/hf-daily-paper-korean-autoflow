[
    "{'paper': {'id': '2505.14683', 'authors': [{'_id': '682d2fd84540abccd3b835e8', 'name': 'Chaorui Deng', 'hidden': False}, {'_id': '682d2fd84540abccd3b835e9', 'name': 'Deyao Zhu', 'hidden': False}, {'_id': '682d2fd84540abccd3b835ea', 'user': {'_id': '61fb81006374891646732f37', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg', 'isPro': False, 'fullname': 'Kunchang Li', 'user': 'Andy1621', 'type': 'user'}, 'name': 'Kunchang Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:41:06.469Z', 'hidden': False}, {'_id': '682d2fd84540abccd3b835eb', 'user': {'_id': '652e9c5774d1b0d7ff73d091', 'avatarUrl': '/avatars/a6d2098b3dde4a8b7488a193f0ecb776.svg', 'isPro': True, 'fullname': 'Chenhui Gou', 'user': 'gouc', 'type': 'user'}, 'name': 'Chenhui Gou', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:41:08.903Z', 'hidden': False}, {'_id': '682d2fd84540abccd3b835ec', 'name': 'Feng Li', 'hidden': False}, {'_id': '682d2fd84540abccd3b835ed', 'name': 'Zeyu Wang', 'hidden': False}, {'_id': '682d2fd84540abccd3b835ee', 'name': 'Shu Zhong', 'hidden': False}, {'_id': '682d2fd84540abccd3b835ef', 'user': {'_id': '5df833bdda6d0311fd3d5403', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/5df833bdda6d0311fd3d5403/62OtGJEQXdOuhV9yCd4HS.png', 'isPro': False, 'fullname': 'Weihao Yu', 'user': 'whyu', 'type': 'user'}, 'name': 'Weihao Yu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T09:31:55.569Z', 'hidden': False}, {'_id': '682d2fd84540abccd3b835f0', 'user': {'_id': '64b6b81142134e053233c3c0', 'avatarUrl': '/avatars/5c7455d99a7a2648f77a531c9a71eb98.svg', 'isPro': False, 'fullname': 'Xiaonan Nie', 'user': 'codecaution', 'type': 'user'}, 'name': 'Xiaonan Nie', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:06:14.057Z', 'hidden': False}, {'_id': '682d2fd84540abccd3b835f1', 'user': {'_id': '617fe76105423df678cef199', 'avatarUrl': '/avatars/64c94a4d743edab18ecb4bb7c550f049.svg', 'isPro': False, 'fullname': 'Song', 'user': 'Ziang', 'type': 'user'}, 'name': 'Ziang Song', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:06:07.780Z', 'hidden': False}, {'_id': '682d2fd84540abccd3b835f2', 'name': 'Guang Shi', 'hidden': False}, {'_id': '682d2fd84540abccd3b835f3', 'name': 'Haoqi Fan', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/61fb81006374891646732f37/HQOfWqrOf9B97hWczL489.png'], 'publishedAt': '2025-05-20T17:59:30.000Z', 'submittedOnDailyAt': '2025-05-21T00:38:53.960Z', 'title': 'Emerging Properties in Unified Multimodal Pretraining', 'submittedOnDailyBy': {'_id': '61fb81006374891646732f37', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg', 'isPro': False, 'fullname': 'Kunchang Li', 'user': 'Andy1621', 'type': 'user'}, 'summary': 'Unifying multimodal understanding and generation has shown impressive\\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\\nBAGEL, an open0source foundational model that natively supports multimodal\\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\\non trillions of tokens curated from large0scale interleaved text, image, video,\\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\\nsignificantly outperforms open-source unified models in both multimodal\\ngeneration and understanding across standard benchmarks, while exhibiting\\nadvanced multimodal reasoning abilities such as free-form image manipulation,\\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\\nfacilitating further opportunities for multimodal research, we share the key\\nfindings, pretraining details, data creation protocal, and release our code and\\ncheckpoints to the community. The project page is at https://bagel-ai.org/', 'upvotes': 72, 'discussionId': '682d2fdc4540abccd3b836ee', 'ai_keywords': ['unified, decoder-only model', 'pretrained', 'trillions of tokens', 'large-scale interleaved data', 'complex multimodal reasoning', 'multimodal generation', 'multimodal understanding', 'free-form image manipulation', 'future frame prediction', '3D manipulation', 'world navigation']}, 'publishedAt': '2025-05-20T13:59:30.000Z', 'title': 'Emerging Properties in Unified Multimodal Pretraining', 'summary': 'Unifying multimodal understanding and generation has shown impressive\\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\\nBAGEL, an open0source foundational model that natively supports multimodal\\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\\non trillions of tokens curated from large0scale interleaved text, image, video,\\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\\nsignificantly outperforms open-source unified models in both multimodal\\ngeneration and understanding across standard benchmarks, while exhibiting\\nadvanced multimodal reasoning abilities such as free-form image manipulation,\\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\\nfacilitating further opportunities for multimodal research, we share the key\\nfindings, pretraining details, data creation protocal, and release our code and\\ncheckpoints to the community. The project page is at https://bagel-ai.org/', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/61fb81006374891646732f37/HQOfWqrOf9B97hWczL489.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14683.png', 'numComments': 1, 'submittedBy': {'_id': '61fb81006374891646732f37', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg', 'fullname': 'Kunchang Li', 'name': 'Andy1621', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 22}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.11594', 'authors': [{'_id': '682d426251ce04237318cfe5', 'user': {'_id': '66c0a08bac74db25de8427ec', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg', 'isPro': False, 'fullname': 'Jintao Zhang', 'user': 'jt-zhang', 'type': 'user'}, 'name': 'Jintao Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:46.065Z', 'hidden': False}, {'_id': '682d426251ce04237318cfe6', 'name': 'Jia Wei', 'hidden': False}, {'_id': '682d426251ce04237318cfe7', 'user': {'_id': '62cc11a4f1d37c16280a2923', 'avatarUrl': '/avatars/265b3cfb80f0a7b11a2ef67c49e29cf7.svg', 'isPro': False, 'fullname': 'Pengle Zhang', 'user': 'Guyan', 'type': 'user'}, 'name': 'Pengle Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:07:09.573Z', 'hidden': False}, {'_id': '682d426251ce04237318cfe8', 'name': 'Xiaoming Xu', 'hidden': False}, {'_id': '682d426251ce04237318cfe9', 'user': {'_id': '67ea1f6693f71dd8167a2d22', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/H_upra_XVG1AoBKUe9ArV.png', 'isPro': False, 'fullname': 'haofeng huang', 'user': 'haofeng666', 'type': 'user'}, 'name': 'Haofeng Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:06:48.450Z', 'hidden': False}, {'_id': '682d426251ce04237318cfea', 'user': {'_id': '658c1802a1105f8157ad1db9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/658c1802a1105f8157ad1db9/WzjY29SkngxkKfiTYcssh.jpeg', 'isPro': False, 'fullname': 'whx1003', 'user': 'whx1003', 'type': 'user'}, 'name': 'Haoxu Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T10:30:12.064Z', 'hidden': False}, {'_id': '682d426251ce04237318cfeb', 'name': 'Kai Jiang', 'hidden': False}, {'_id': '682d426251ce04237318cfec', 'name': 'Jun Zhu', 'hidden': False}, {'_id': '682d426251ce04237318cfed', 'user': {'_id': '65fcad0ba0d7adc40b54fac2', 'avatarUrl': '/avatars/7564b5642378fddb46ec3b5ae57c0402.svg', 'isPro': False, 'fullname': 'Jianfei Chen', 'user': 'surfingtomchen', 'type': 'user'}, 'name': 'Jianfei Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:06:40.981Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/Tb20E3IJSV6PjcD9Nkvfg.png'], 'publishedAt': '2025-05-16T18:01:54.000Z', 'submittedOnDailyAt': '2025-05-21T01:35:25.101Z', 'title': 'SageAttention3: Microscaling FP4 Attention for Inference and An\\n  Exploration of 8-Bit Training', 'submittedOnDailyBy': {'_id': '66c0a08bac74db25de8427ec', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg', 'isPro': False, 'fullname': 'Jintao Zhang', 'user': 'jt-zhang', 'type': 'user'}, 'summary': 'The efficiency of attention is important due to its quadratic time\\ncomplexity. We enhance the efficiency of attention through two key\\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\\naccelerate attention computation. Our implementation achieves 1038 TOPS on\\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\\nExperiments show that our FP4 attention can accelerate inference of various\\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\\nfocus only on inference. However, the efficiency of training large models is\\nalso important. To explore whether low-bit attention can be effectively applied\\nto training tasks, we design an accurate and efficient 8-bit attention for both\\nforward and backward propagation. Experiments indicate that 8-bit attention\\nachieves lossless performance in fine-tuning tasks but exhibits slower\\nconvergence in pretraining tasks. The code will be available at\\nhttps://github.com/thu-ml/SageAttention.', 'upvotes': 39, 'discussionId': '682d426551ce04237318d0b9', 'projectPage': 'https://github.com/thu-ml/SageAttention', 'githubRepo': 'https://github.com/thu-ml/SageAttention', 'ai_keywords': ['attention', 'FP4 Tensor Cores', 'TOPS', 'speedup', 'inference', 'plug-and-play', 'low-bit attention', '8-bit attention', 'forward propagation', 'backward propagation', 'fine-tuning', 'pretraining', 'convergence', 'lossless performance']}, 'publishedAt': '2025-05-16T14:01:54.000Z', 'title': 'SageAttention3: Microscaling FP4 Attention for Inference and An\\n  Exploration of 8-Bit Training', 'summary': 'The efficiency of attention is important due to its quadratic time\\ncomplexity. We enhance the efficiency of attention through two key\\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\\naccelerate attention computation. Our implementation achieves 1038 TOPS on\\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\\nExperiments show that our FP4 attention can accelerate inference of various\\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\\nfocus only on inference. However, the efficiency of training large models is\\nalso important. To explore whether low-bit attention can be effectively applied\\nto training tasks, we design an accurate and efficient 8-bit attention for both\\nforward and backward propagation. Experiments indicate that 8-bit attention\\nachieves lossless performance in fine-tuning tasks but exhibits slower\\nconvergence in pretraining tasks. The code will be available at\\nhttps://github.com/thu-ml/SageAttention.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/Tb20E3IJSV6PjcD9Nkvfg.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11594.png', 'numComments': 1, 'submittedBy': {'_id': '66c0a08bac74db25de8427ec', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg', 'fullname': 'Jintao Zhang', 'name': 'jt-zhang', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.13438', 'authors': [{'_id': '682d84d6aa4903837eeac1dc', 'user': {'_id': '63885f1d0bebb233d8ad6e5b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg', 'isPro': False, 'fullname': 'Penghui Qi', 'user': 'QPHutu', 'type': 'user'}, 'name': 'Penghui Qi', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:39:35.138Z', 'hidden': False}, {'_id': '682d84d6aa4903837eeac1dd', 'user': {'_id': '65f5392c68b8e0cb3c9977a2', 'avatarUrl': '/avatars/aa64772475098e8a135c13072fde6744.svg', 'isPro': False, 'fullname': 'Zichen', 'user': 'lkevinzc', 'type': 'user'}, 'name': 'Zichen Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T09:31:12.541Z', 'hidden': False}, {'_id': '682d84d6aa4903837eeac1de', 'user': {'_id': '63d91b6d255ef6add20e1b38', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg', 'isPro': False, 'fullname': 'Tianyu Pang', 'user': 'P2333', 'type': 'user'}, 'name': 'Tianyu Pang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:12:08.028Z', 'hidden': False}, {'_id': '682d84d6aa4903837eeac1df', 'user': {'_id': '632407c892e07e3ca20aca28', 'avatarUrl': '/avatars/23b51b37b12b51a0947f687d1de4d3b5.svg', 'isPro': False, 'fullname': 'Chao Du', 'user': 'duchao', 'type': 'user'}, 'name': 'Chao Du', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:12:18.649Z', 'hidden': False}, {'_id': '682d84d6aa4903837eeac1e0', 'name': 'Wee Sun Lee', 'hidden': False}, {'_id': '682d84d6aa4903837eeac1e1', 'name': 'Min Lin', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/UeoRcEi-bKgq6eecwvo8o.png', 'https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/JLzRbfpomaF02gHLNkON6.png'], 'publishedAt': '2025-05-19T17:58:44.000Z', 'submittedOnDailyAt': '2025-05-21T06:36:34.577Z', 'title': 'Optimizing Anytime Reasoning via Budget Relative Policy Optimization', 'submittedOnDailyBy': {'_id': '63885f1d0bebb233d8ad6e5b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg', 'isPro': False, 'fullname': 'Penghui Qi', 'user': 'QPHutu', 'type': 'user'}, 'summary': 'Scaling test-time compute is crucial for enhancing the reasoning capabilities\\nof large language models (LLMs). Existing approaches typically employ\\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\\nof reasoning traces. However, such methods optimize only the final performance\\nunder a large and fixed token budget, which hinders efficiency in both training\\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\\noptimize anytime reasoning performance, which aims to improve token efficiency\\nand the flexibility of reasoning under varying token budget constraints. To\\nachieve this, we truncate the complete thinking process to fit within sampled\\ntoken budgets from a prior distribution, compelling the model to summarize the\\noptimal answer for each truncated thinking for verification. This introduces\\nverifiable dense rewards into the reasoning process, facilitating more\\neffective credit assignment in RL optimization. We then optimize the thinking\\nand summary policies in a decoupled manner to maximize the cumulative reward.\\nAdditionally, we introduce a novel variance reduction technique, Budget\\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\\nof the learning process when reinforcing the thinking policy. Empirical results\\nin mathematical reasoning tasks demonstrate that our method consistently\\noutperforms GRPO across all thinking budgets under various prior distributions,\\nenhancing both training and token efficiency.', 'upvotes': 25, 'discussionId': '682d84d7aa4903837eeac215', 'githubRepo': 'https://github.com/sail-sg/AnytimeReasoner', 'ai_keywords': ['reinforcement learning (RL)', 'verifiable reward', 'reasoning traces', 'token budget', 'AnytimeReasoner', 'truncation', 'verifiable dense rewards', 'credit assignment', 'thinking and summary policies', 'decoupled manner', 'cumulative reward', 'Budget Relative Policy Optimization (BRPO)', 'variance reduction']}, 'publishedAt': '2025-05-19T13:58:44.000Z', 'title': 'Optimizing Anytime Reasoning via Budget Relative Policy Optimization', 'summary': 'Scaling test-time compute is crucial for enhancing the reasoning capabilities\\nof large language models (LLMs). Existing approaches typically employ\\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\\nof reasoning traces. However, such methods optimize only the final performance\\nunder a large and fixed token budget, which hinders efficiency in both training\\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\\noptimize anytime reasoning performance, which aims to improve token efficiency\\nand the flexibility of reasoning under varying token budget constraints. To\\nachieve this, we truncate the complete thinking process to fit within sampled\\ntoken budgets from a prior distribution, compelling the model to summarize the\\noptimal answer for each truncated thinking for verification. This introduces\\nverifiable dense rewards into the reasoning process, facilitating more\\neffective credit assignment in RL optimization. We then optimize the thinking\\nand summary policies in a decoupled manner to maximize the cumulative reward.\\nAdditionally, we introduce a novel variance reduction technique, Budget\\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\\nof the learning process when reinforcing the thinking policy. Empirical results\\nin mathematical reasoning tasks demonstrate that our method consistently\\noutperforms GRPO across all thinking budgets under various prior distributions,\\nenhancing both training and token efficiency.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/UeoRcEi-bKgq6eecwvo8o.png', 'https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/JLzRbfpomaF02gHLNkON6.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13438.png', 'numComments': 1, 'submittedBy': {'_id': '63885f1d0bebb233d8ad6e5b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg', 'fullname': 'Penghui Qi', 'name': 'QPHutu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14460', 'authors': [{'_id': '682d54b0396c1e613eaac5ef', 'user': {'_id': '655de51982afda0fc479fb91', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/99oxuFD3OjlReuGfDZluh.png', 'isPro': False, 'fullname': 'Tianhe Wu', 'user': 'TianheWu', 'type': 'user'}, 'name': 'Tianhe Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:26.204Z', 'hidden': False}, {'_id': '682d54b0396c1e613eaac5f0', 'name': 'Jian Zou', 'hidden': False}, {'_id': '682d54b0396c1e613eaac5f1', 'name': 'Jie Liang', 'hidden': False}, {'_id': '682d54b0396c1e613eaac5f2', 'name': 'Lei Zhang', 'hidden': False}, {'_id': '682d54b0396c1e613eaac5f3', 'name': 'Kede Ma', 'hidden': False}], 'publishedAt': '2025-05-20T14:56:50.000Z', 'submittedOnDailyAt': '2025-05-21T07:48:51.670Z', 'title': 'VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\\n  Reinforcement Learning to Rank', 'submittedOnDailyBy': {'_id': '655de51982afda0fc479fb91', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/99oxuFD3OjlReuGfDZluh.png', 'isPro': False, 'fullname': 'Tianhe Wu', 'user': 'TianheWu', 'type': 'user'}, 'summary': 'DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\\nreasoning and generalization capabilities of large language models (LLMs)\\nthrough reinforcement learning. Nevertheless, the potential of\\nreasoning-induced computational modeling has not been thoroughly explored in\\nthe context of image quality assessment (IQA), a task critically dependent on\\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\\nreinforcement learning to rank, a learning algorithm tailored to the\\nintrinsically relative nature of visual quality. Specifically, for a pair of\\nimages, we employ group relative policy optimization to generate multiple\\nquality scores for each image. These estimates are then used to compute\\ncomparative probabilities of one image having higher quality than the other\\nunder the Thurstone model. Rewards for each quality estimate are defined using\\ncontinuous fidelity measures rather than discretized binary labels. Extensive\\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\\ndiscriminative deep learning-based NR-IQA models as well as a recent\\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\\ncapable of generating contextually rich, human-aligned quality descriptions,\\nand supports multi-dataset training without requiring perceptual scale\\nrealignment. These features make VisualQuality-R1 especially well-suited for\\nreliably measuring progress in a wide range of image processing tasks like\\nsuper-resolution and image generation.', 'upvotes': 24, 'discussionId': '682d54b0396c1e613eaac62a', 'githubRepo': 'https://github.com/TianheWu/VisualQuality-R1', 'ai_keywords': ['reinforcement learning', 'VisualQuality-R1', 'reasoning-induced no-reference IQA (NR-IQA)', 'group relative policy optimization', 'Thurstone model', 'continuous fidelity measures', 'discriminative deep learning', 'reasoning-induced quality regression', 'super-resolution', 'image generation']}, 'publishedAt': '2025-05-20T10:56:50.000Z', 'title': 'VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\\n  Reinforcement Learning to Rank', 'summary': 'DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\\nreasoning and generalization capabilities of large language models (LLMs)\\nthrough reinforcement learning. Nevertheless, the potential of\\nreasoning-induced computational modeling has not been thoroughly explored in\\nthe context of image quality assessment (IQA), a task critically dependent on\\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\\nreinforcement learning to rank, a learning algorithm tailored to the\\nintrinsically relative nature of visual quality. Specifically, for a pair of\\nimages, we employ group relative policy optimization to generate multiple\\nquality scores for each image. These estimates are then used to compute\\ncomparative probabilities of one image having higher quality than the other\\nunder the Thurstone model. Rewards for each quality estimate are defined using\\ncontinuous fidelity measures rather than discretized binary labels. Extensive\\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\\ndiscriminative deep learning-based NR-IQA models as well as a recent\\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\\ncapable of generating contextually rich, human-aligned quality descriptions,\\nand supports multi-dataset training without requiring perceptual scale\\nrealignment. These features make VisualQuality-R1 especially well-suited for\\nreliably measuring progress in a wide range of image processing tasks like\\nsuper-resolution and image generation.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14460.png', 'numComments': 2, 'submittedBy': {'_id': '655de51982afda0fc479fb91', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/99oxuFD3OjlReuGfDZluh.png', 'fullname': 'Tianhe Wu', 'name': 'TianheWu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14246', 'authors': [{'_id': '682d7a2340a42d1538fada76', 'user': {'_id': '66fe1334ff3ee1f7569fab6d', 'avatarUrl': '/avatars/6868b1a545028a9b8bbded52490dc093.svg', 'isPro': False, 'fullname': 'ziyuliu', 'user': 'ziyuliu', 'type': 'user'}, 'name': 'Ziyu Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:08:39.517Z', 'hidden': False}, {'_id': '682d7a2340a42d1538fada77', 'user': {'_id': '63859cf3b2906edaf83af9f0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png', 'isPro': False, 'fullname': 'Yuhang Zang', 'user': 'yuhangzang', 'type': 'user'}, 'name': 'Yuhang Zang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:39:42.962Z', 'hidden': False}, {'_id': '682d7a2340a42d1538fada78', 'user': {'_id': '6671341b4eee852a8b25888f', 'avatarUrl': '/avatars/635d1821bbc960b4ea845e606883eb16.svg', 'isPro': False, 'fullname': 'yushan zou', 'user': 'zyshan', 'type': 'user'}, 'name': 'Yushan Zou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:08:45.972Z', 'hidden': False}, {'_id': '682d7a2340a42d1538fada79', 'user': {'_id': '652768eeb723b49e8c8865da', 'avatarUrl': '/avatars/491e02da9ec81e439ccda8a181634bca.svg', 'isPro': False, 'fullname': 'Zijian Liang', 'user': 'steins1096', 'type': 'user'}, 'name': 'Zijian Liang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:08:51.938Z', 'hidden': False}, {'_id': '682d7a2340a42d1538fada7a', 'user': {'_id': '67c0849ee08c178ef8d4e05c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mQ6VdnjZnRhb0H_waPclo.png', 'isPro': False, 'fullname': 'Xiaoyi Dong', 'user': 'sweetFruit', 'type': 'user'}, 'name': 'Xiaoyi Dong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:09:00.912Z', 'hidden': False}, {'_id': '682d7a2340a42d1538fada7b', 'user': {'_id': '65000bef18830fabea469fdd', 'avatarUrl': '/avatars/b320c77dfad039d9f9c54127f610d44f.svg', 'isPro': False, 'fullname': 'Cao Yuhang', 'user': 'yhcao', 'type': 'user'}, 'name': 'Yuhang Cao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:09:22.645Z', 'hidden': False}, {'_id': '682d7a2340a42d1538fada7c', 'user': {'_id': '63ee1379190ddd6214efd73a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png', 'isPro': False, 'fullname': 'HAODONG DUAN', 'user': 'KennyUTC', 'type': 'user'}, 'name': 'Haodong Duan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:09:31.442Z', 'hidden': False}, {'_id': '682d7a2340a42d1538fada7d', 'user': {'_id': '636317ed80c1a705a6eff396', 'avatarUrl': '/avatars/3db090e101b916d9256d0d3e043db71d.svg', 'isPro': False, 'fullname': 'Dahua Lin', 'user': 'lindahua', 'type': 'user'}, 'name': 'Dahua Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:09:37.103Z', 'hidden': False}, {'_id': '682d7a2340a42d1538fada7e', 'user': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'isPro': True, 'fullname': 'Jiaqi Wang', 'user': 'myownskyW7', 'type': 'user'}, 'name': 'Jiaqi Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:39:45.391Z', 'hidden': False}], 'publishedAt': '2025-05-20T11:59:25.000Z', 'submittedOnDailyAt': '2025-05-21T05:32:01.442Z', 'title': 'Visual Agentic Reinforcement Fine-Tuning', 'submittedOnDailyBy': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'isPro': True, 'fullname': 'Jiaqi Wang', 'user': 'myownskyW7', 'type': 'user'}, 'summary': \"A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\\nagentic ability to use external tools such as web browsers for searching and\\nwriting/executing code for image manipulation to think with images. In the\\nopen-source research community, while significant progress has been made in\\nlanguage-only agentic abilities such as function calling and tool integration,\\nthe development of multi-modal agentic capabilities that involve truly thinking\\nwith images, and their corresponding benchmarks, are still less explored. This\\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\\nability to browse websites for real-time information updates and write code to\\nmanipulate and analyze input images through cropping, rotation, and other image\\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\\nagentic search and coding abilities. Our experimental results demonstrate that\\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\\nOur findings suggest that Visual-ARFT offers a promising path toward building\\nrobust and generalizable multimodal agents.\", 'upvotes': 24, 'discussionId': '682d7a2440a42d1538fadac0', 'githubRepo': 'https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT', 'ai_keywords': ['Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT)', 'Large Vision-Language Models (LVLMs)', 'Multi-modal Agentic Tool Bench (MAT)', 'MAT-Search', 'MAT-Coding', 'F1', 'EM', 'GPT-4o', 'multi-hop QA benchmarks', '2Wiki', 'HotpotQA']}, 'publishedAt': '2025-05-20T07:59:25.000Z', 'title': 'Visual Agentic Reinforcement Fine-Tuning', 'summary': \"A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\\nagentic ability to use external tools such as web browsers for searching and\\nwriting/executing code for image manipulation to think with images. In the\\nopen-source research community, while significant progress has been made in\\nlanguage-only agentic abilities such as function calling and tool integration,\\nthe development of multi-modal agentic capabilities that involve truly thinking\\nwith images, and their corresponding benchmarks, are still less explored. This\\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\\nability to browse websites for real-time information updates and write code to\\nmanipulate and analyze input images through cropping, rotation, and other image\\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\\nagentic search and coding abilities. Our experimental results demonstrate that\\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\\nOur findings suggest that Visual-ARFT offers a promising path toward building\\nrobust and generalizable multimodal agents.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14246.png', 'numComments': 1, 'submittedBy': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'fullname': 'Jiaqi Wang', 'name': 'myownskyW7', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 19}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.13138', 'authors': [{'_id': '682da62a975206d1caa8c9d9', 'user': {'_id': '62cd4ae83e5ba89c40f15156', 'avatarUrl': '/avatars/f70aca85f8836edaedcee324a18140b5.svg', 'isPro': False, 'fullname': 'Emile van Krieken', 'user': 'HEmile', 'type': 'user'}, 'name': 'Emile van Krieken', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T10:14:44.208Z', 'hidden': False}, {'_id': '682da62a975206d1caa8c9da', 'user': {'_id': '61001311e043e15c13412d30', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61001311e043e15c13412d30/6yAbTweYR16XtxMBEyOWl.png', 'isPro': False, 'fullname': 'Pasquale Minervini', 'user': 'pminervini', 'type': 'user'}, 'name': 'Pasquale Minervini', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T10:16:57.876Z', 'hidden': False}, {'_id': '682da62a975206d1caa8c9db', 'user': {'_id': '60809ad44ad99100d63ce36a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1619040921084-noauth.jpeg', 'isPro': False, 'fullname': 'Edoardo Maria Ponti', 'user': 'ducdauge', 'type': 'user'}, 'name': 'Edoardo Ponti', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T10:30:14.820Z', 'hidden': False}, {'_id': '682da62a975206d1caa8c9dc', 'name': 'Antonio Vergari', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62cd4ae83e5ba89c40f15156/6TK9qXUQWm6Yk3bQW_0ht.gif'], 'publishedAt': '2025-05-19T14:07:47.000Z', 'submittedOnDailyAt': '2025-05-21T08:42:10.409Z', 'title': 'Neurosymbolic Diffusion Models', 'submittedOnDailyBy': {'_id': '62cd4ae83e5ba89c40f15156', 'avatarUrl': '/avatars/f70aca85f8836edaedcee324a18140b5.svg', 'isPro': False, 'fullname': 'Emile van Krieken', 'user': 'HEmile', 'type': 'user'}, 'summary': 'Neurosymbolic (NeSy) predictors combine neural perception with symbolic\\nreasoning to solve tasks like visual reasoning. However, standard NeSy\\npredictors assume conditional independence between the symbols they extract,\\nthus limiting their ability to model interactions and uncertainty - often\\nleading to overconfident predictions and poor out-of-distribution\\ngeneralisation. To overcome the limitations of the independence assumption, we\\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\\npredictors that use discrete diffusion to model dependencies between symbols.\\nOur approach reuses the independence assumption from NeSy predictors at each\\nstep of the diffusion process, enabling scalable learning while capturing\\nsymbol dependencies and uncertainty quantification. Across both synthetic and\\nreal-world benchmarks - including high-dimensional visual path planning and\\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\\nNeSy predictors and demonstrate strong calibration.', 'upvotes': 21, 'discussionId': '682da62a975206d1caa8ca06', 'projectPage': 'https://x.com/EmilevanKrieken/status/1925143347758252478', 'githubRepo': 'https://github.com/HEmile/neurosymbolic-diffusion', 'ai_keywords': ['neurosymbolic (NeSy) predictors', 'symbolic reasoning', 'visual reasoning', 'conditional independence', 'symbols', 'interactions', 'uncertainty', 'overconfident predictions', 'out-of-distribution generalisation', 'neurosymbolic diffusion models (NeSyDMs)', 'discrete diffusion', 'symbol dependencies', 'uncertainty quantification', 'high-dimensional visual path planning', 'rule-based autonomous driving', 'state-of-the-art accuracy', 'strong calibration']}, 'publishedAt': '2025-05-19T10:07:47.000Z', 'title': 'Neurosymbolic Diffusion Models', 'summary': 'Neurosymbolic (NeSy) predictors combine neural perception with symbolic\\nreasoning to solve tasks like visual reasoning. However, standard NeSy\\npredictors assume conditional independence between the symbols they extract,\\nthus limiting their ability to model interactions and uncertainty - often\\nleading to overconfident predictions and poor out-of-distribution\\ngeneralisation. To overcome the limitations of the independence assumption, we\\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\\npredictors that use discrete diffusion to model dependencies between symbols.\\nOur approach reuses the independence assumption from NeSy predictors at each\\nstep of the diffusion process, enabling scalable learning while capturing\\nsymbol dependencies and uncertainty quantification. Across both synthetic and\\nreal-world benchmarks - including high-dimensional visual path planning and\\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\\nNeSy predictors and demonstrate strong calibration.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62cd4ae83e5ba89c40f15156/6TK9qXUQWm6Yk3bQW_0ht.gif'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13138.png', 'numComments': 1, 'submittedBy': {'_id': '62cd4ae83e5ba89c40f15156', 'avatarUrl': '/avatars/f70aca85f8836edaedcee324a18140b5.svg', 'fullname': 'Emile van Krieken', 'name': 'HEmile', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.04388', 'authors': [{'_id': '682d83494c4685831f85ec92', 'name': 'Dario Garcia-Gasulla', 'hidden': False}, {'_id': '682d83494c4685831f85ec93', 'user': {'_id': '661e8559b0338c03bd6e5054', 'avatarUrl': '/avatars/044d69afa40ddef4485aebfe984da96b.svg', 'isPro': False, 'fullname': 'Bayarri', 'user': 'JordiBayarri-bsc', 'type': 'user'}, 'name': 'Jordi Bayarri-Planas', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-21T07:39:54.713Z', 'hidden': False}, {'_id': '682d83494c4685831f85ec94', 'name': 'Ashwin Kumar Gururajan', 'hidden': False}, {'_id': '682d83494c4685831f85ec95', 'name': 'Enrique Lopez-Cuena', 'hidden': False}, {'_id': '682d83494c4685831f85ec96', 'user': {'_id': '6622352d9fcf61dee8a1f24d', 'avatarUrl': '/avatars/dd164c50b3ecb8861e2294337b942e6f.svg', 'isPro': False, 'fullname': 'Adrian Tormos', 'user': 'adriantormos', 'type': 'user'}, 'name': 'Adrian Tormos', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-05-21T11:04:23.232Z', 'hidden': False}, {'_id': '682d83494c4685831f85ec97', 'user': {'_id': '65b8e66f078c543033e49672', 'avatarUrl': '/avatars/aa81fd59b4ec624245a4a84d2708962d.svg', 'isPro': False, 'fullname': 'Daniel Hinjos García', 'user': 'danihinjos', 'type': 'user'}, 'name': 'Daniel Hinjos', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:39:40.380Z', 'hidden': False}, {'_id': '682d83494c4685831f85ec98', 'user': {'_id': '620683e7eeb1b73d904c96e5', 'avatarUrl': '/avatars/d0309ac9408530a74f1799e175cc5fad.svg', 'isPro': False, 'fullname': 'Pablo Bernabeu', 'user': 'pabberpe', 'type': 'user'}, 'name': 'Pablo Bernabeu-Perez', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-21T07:39:54.713Z', 'hidden': False}, {'_id': '682d83494c4685831f85ec99', 'user': {'_id': '65d71603ca16ef9ba7fb2efb', 'avatarUrl': '/avatars/7d3e1436427f7f58c86fb1f8724c4244.svg', 'isPro': False, 'fullname': 'Anna', 'user': 'annariasdu', 'type': 'user'}, 'name': 'Anna Arias-Duart', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-05-21T07:59:53.756Z', 'hidden': False}, {'_id': '682d83494c4685831f85ec9a', 'user': {'_id': '6565c2a4131d13ccc5df1435', 'avatarUrl': '/avatars/218ceac504772e7f0fb3ee4d46d4fab7.svg', 'isPro': False, 'fullname': 'Pablo Agustin Martin Torres', 'user': 'PabloMartinTorres', 'type': 'user'}, 'name': 'Pablo Agustin Martin-Torres', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T09:03:22.597Z', 'hidden': False}, {'_id': '682d83494c4685831f85ec9b', 'name': 'Marta Gonzalez-Mallo', 'hidden': False}, {'_id': '682d83494c4685831f85ec9c', 'user': {'_id': '62d16c742ad06bbc89217797', 'avatarUrl': '/avatars/11ce629fbcb33f2431164d8a3e54c876.svg', 'isPro': False, 'fullname': 'Sergio Alvarez-Napagao', 'user': 'tranchis', 'type': 'user'}, 'name': 'Sergio Alvarez-Napagao', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-05-21T11:38:27.934Z', 'hidden': False}, {'_id': '682d83494c4685831f85ec9d', 'name': 'Eduard Ayguadé-Parra', 'hidden': False}, {'_id': '682d83494c4685831f85ec9e', 'name': 'Ulises Cortés', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/SdO6Jth8V1sz0wvfL9Nxg.png', 'https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/Efb0dFT2ULJC-TvdQ1ERR.png'], 'publishedAt': '2025-05-07T13:13:14.000Z', 'submittedOnDailyAt': '2025-05-21T06:12:19.909Z', 'title': 'The Aloe Family Recipe for Open and Specialized Healthcare LLMs', 'submittedOnDailyBy': {'_id': '62f7a16192950415b637e201', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62f7a16192950415b637e201/4IIqYap43vujvSuql68Vj.jpeg', 'isPro': False, 'fullname': 'Dario', 'user': 'dariog', 'type': 'user'}, 'summary': 'Purpose: With advancements in Large Language Models (LLMs) for healthcare,\\nthe need arises for competitive open-source models to protect the public\\ninterest. This work contributes to the field of open medical LLMs by optimizing\\nkey stages of data preprocessing and training, while showing how to improve\\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\\nmethodology used, which includes four different types of tests, defines a new\\nstandard for the field. The resultant models, shown to be competitive with the\\nbest private alternatives, are released with a permisive license.\\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\\nThought examples. The models undergo alignment with Direct Preference\\nOptimization, emphasizing ethical and policy-aligned performance in the\\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\\nsafety and human assessments, to maximize the reliability of results.\\n  Results: Recommendations are made across the entire pipeline, backed by the\\nsolid performance of the Aloe Family. These models deliver competitive\\nperformance across healthcare benchmarks and medical fields, and are often\\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\\nmodels significantly improve safety, showing resilience to unseen jailbreaking\\nattacks. For a responsible release, a detailed risk assessment specific to\\nhealthcare is attached to the Aloe Family models.\\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\\nsignificant contribution to the open-source medical LLM field, offering\\ntop-of-the-line performance while maintaining high ethical requirements. This\\nwork sets a new standard for developing and reporting aligned LLMs in\\nhealthcare.', 'upvotes': 19, 'discussionId': '682d834a4c4685831f85ed09', 'ai_keywords': ['Direct Preference Optimization (DPO)', 'Retrieval-Augmented Generation (RAG)', 'Chain of Thought', 'Llama 3.1', 'Qwen 2.5', 'Close-ended tests', 'Open-ended tests', 'Safety assessments', 'Human assessments', 'Jailbreaking attacks', 'Bias', 'Toxicity', 'Risk assessment']}, 'publishedAt': '2025-05-07T09:13:14.000Z', 'title': 'The Aloe Family Recipe for Open and Specialized Healthcare LLMs', 'summary': 'Purpose: With advancements in Large Language Models (LLMs) for healthcare,\\nthe need arises for competitive open-source models to protect the public\\ninterest. This work contributes to the field of open medical LLMs by optimizing\\nkey stages of data preprocessing and training, while showing how to improve\\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\\nmethodology used, which includes four different types of tests, defines a new\\nstandard for the field. The resultant models, shown to be competitive with the\\nbest private alternatives, are released with a permisive license.\\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\\nThought examples. The models undergo alignment with Direct Preference\\nOptimization, emphasizing ethical and policy-aligned performance in the\\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\\nsafety and human assessments, to maximize the reliability of results.\\n  Results: Recommendations are made across the entire pipeline, backed by the\\nsolid performance of the Aloe Family. These models deliver competitive\\nperformance across healthcare benchmarks and medical fields, and are often\\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\\nmodels significantly improve safety, showing resilience to unseen jailbreaking\\nattacks. For a responsible release, a detailed risk assessment specific to\\nhealthcare is attached to the Aloe Family models.\\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\\nsignificant contribution to the open-source medical LLM field, offering\\ntop-of-the-line performance while maintaining high ethical requirements. This\\nwork sets a new standard for developing and reporting aligned LLMs in\\nhealthcare.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/SdO6Jth8V1sz0wvfL9Nxg.png', 'https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/Efb0dFT2ULJC-TvdQ1ERR.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04388.png', 'numComments': 1, 'submittedBy': {'_id': '62f7a16192950415b637e201', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62f7a16192950415b637e201/4IIqYap43vujvSuql68Vj.jpeg', 'fullname': 'Dario', 'name': 'dariog', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2505.14513', 'authors': [{'_id': '682d334862cadf615f5f73e6', 'user': {'_id': '63ad217b9fc40b14560e9e06', 'avatarUrl': '/avatars/c2d33830b141fc9c73ad8302ff35ed9d.svg', 'isPro': False, 'fullname': 'Yen-Chen Wu', 'user': 'yenchen', 'type': 'user'}, 'name': 'Yen-Chen Wu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:11:09.090Z', 'hidden': False}, {'_id': '682d334862cadf615f5f73e7', 'user': {'_id': '643fb7332397d8eef5b844cd', 'avatarUrl': '/avatars/e403a19fc13e478d5929c67028230b0e.svg', 'isPro': False, 'fullname': 'Feng-Ting Liao', 'user': 'FengTing', 'type': 'user'}, 'name': 'Feng-Ting Liao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:41:00.821Z', 'hidden': False}, {'_id': '682d334862cadf615f5f73e8', 'user': {'_id': '66018c8eb1e509e1e4d9196f', 'avatarUrl': '/avatars/5d6cbfc7b6c435264d271c958607630f.svg', 'isPro': False, 'fullname': 'Meng-Hsi Chen', 'user': 'menghsichen', 'type': 'user'}, 'name': 'Meng-Hsi Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:11:15.604Z', 'hidden': False}, {'_id': '682d334862cadf615f5f73e9', 'name': 'Pei-Chen Ho', 'hidden': False}, {'_id': '682d334862cadf615f5f73ea', 'name': 'Farhang Nabiei', 'hidden': False}, {'_id': '682d334862cadf615f5f73eb', 'user': {'_id': '6811b1294119e4ecc92fc93b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/srmY2yyzOg9KRDSLYXJKf.png', 'isPro': False, 'fullname': 'Dashan Shiu', 'user': 'dsshiu', 'type': 'user'}, 'name': 'Da-shan Shiu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:11:37.639Z', 'hidden': False}], 'publishedAt': '2025-05-20T15:41:05.000Z', 'submittedOnDailyAt': '2025-05-21T00:30:57.355Z', 'title': 'Latent Flow Transformer', 'submittedOnDailyBy': {'_id': '643fb7332397d8eef5b844cd', 'avatarUrl': '/avatars/e403a19fc13e478d5929c67028230b0e.svg', 'isPro': False, 'fullname': 'Feng-Ting Liao', 'user': 'FengTing', 'type': 'user'}, 'summary': 'Transformers, the standard implementation for large language models (LLMs),\\ntypically consist of tens to hundreds of discrete layers. While more layers can\\nlead to better performance, this approach has been challenged as far from\\nefficient, especially given the superiority of continuous layers demonstrated\\nby diffusion and flow-based models for image generation. We propose the Latent\\nFlow Transformer (LFT), which replaces a block of layers with a single learned\\ntransport operator trained via flow matching, offering significant compression\\nwhile maintaining compatibility with the original architecture. Additionally,\\nwe address the limitations of existing flow-based methods in preserving\\ncoupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\\ndemonstrating the feasibility of this design. When trained with FW, LFT further\\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\\nskipping 3 layers (0.932), significantly narrowing the gap between\\nautoregressive and flow-based generation paradigms.', 'upvotes': 15, 'discussionId': '682d334962cadf615f5f743f', 'githubRepo': 'https://github.com/mtkresearch/latent-flow-transformer', 'ai_keywords': ['Transformers', 'large language models (LLMs)', 'discrete layers', 'continuous layers', 'diffusion models', 'flow-based models', 'latent Flow Transformer (LFT)', 'learned transport operator', 'flow matching', 'Flow Walking (FW) algorithm', 'Pythia-410M', 'KL Divergence', 'autoregressive', 'flow-based generation paradigms']}, 'publishedAt': '2025-05-20T11:41:05.000Z', 'title': 'Latent Flow Transformer', 'summary': 'Transformers, the standard implementation for large language models (LLMs),\\ntypically consist of tens to hundreds of discrete layers. While more layers can\\nlead to better performance, this approach has been challenged as far from\\nefficient, especially given the superiority of continuous layers demonstrated\\nby diffusion and flow-based models for image generation. We propose the Latent\\nFlow Transformer (LFT), which replaces a block of layers with a single learned\\ntransport operator trained via flow matching, offering significant compression\\nwhile maintaining compatibility with the original architecture. Additionally,\\nwe address the limitations of existing flow-based methods in preserving\\ncoupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\\ndemonstrating the feasibility of this design. When trained with FW, LFT further\\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\\nskipping 3 layers (0.932), significantly narrowing the gap between\\nautoregressive and flow-based generation paradigms.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14513.png', 'numComments': 1, 'submittedBy': {'_id': '643fb7332397d8eef5b844cd', 'avatarUrl': '/avatars/e403a19fc13e478d5929c67028230b0e.svg', 'fullname': 'Feng-Ting Liao', 'name': 'FengTing', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14489', 'authors': [{'_id': '682d55ecea67e90811b09b6b', 'user': {'_id': '617f679fb15f8a665f3999fc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg', 'isPro': False, 'fullname': 'Dongkeun Yoon', 'user': 'DKYoon', 'type': 'user'}, 'name': 'Dongkeun Yoon', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:23.897Z', 'hidden': False}, {'_id': '682d55ecea67e90811b09b6c', 'user': {'_id': '6469949654873f0043b09c22', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg', 'isPro': False, 'fullname': 'Seungone Kim', 'user': 'seungone', 'type': 'user'}, 'name': 'Seungone Kim', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:15:44.937Z', 'hidden': False}, {'_id': '682d55ecea67e90811b09b6d', 'user': {'_id': '606ae0adb579bb0e88515311', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1617617060380-noauth.jpeg', 'isPro': False, 'fullname': 'Sohee Yang', 'user': 'soheeyang', 'type': 'user'}, 'name': 'Sohee Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:15:51.388Z', 'hidden': False}, {'_id': '682d55ecea67e90811b09b6e', 'user': {'_id': '628efeff6d8dcc7ab5263881', 'avatarUrl': '/avatars/1e894ccd7ba206a755b0b9af9f22ead1.svg', 'isPro': True, 'fullname': 'Sunkyoung Kim', 'user': 'Sunkyoung', 'type': 'user'}, 'name': 'Sunkyoung Kim', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:16:00.139Z', 'hidden': False}, {'_id': '682d55ecea67e90811b09b6f', 'name': 'Soyeon Kim', 'hidden': False}, {'_id': '682d55ecea67e90811b09b70', 'user': {'_id': '66a9c79cf59bc4c77baac0d5', 'avatarUrl': '/avatars/57e9943b339a5b9be21b411e5286821c.svg', 'isPro': False, 'fullname': 'Yongil Kim', 'user': 'YongilKim', 'type': 'user'}, 'name': 'Yongil Kim', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:17:46.138Z', 'hidden': False}, {'_id': '682d55ecea67e90811b09b71', 'user': {'_id': '6044fd39e6aa3e130cb92867', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6044fd39e6aa3e130cb92867/L5hb8vpHY6SKMEL-Xacma.jpeg', 'isPro': False, 'fullname': 'Eunbi Choi', 'user': 'unbiarirang', 'type': 'user'}, 'name': 'Eunbi Choi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:17:29.667Z', 'hidden': False}, {'_id': '682d55ecea67e90811b09b72', 'user': {'_id': '660260cf1737e5cd4a826550', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/660260cf1737e5cd4a826550/AlSfoM2WtqPjLtYR6x7Wf.jpeg', 'isPro': False, 'fullname': 'Yireun Kim', 'user': 'yireun', 'type': 'user'}, 'name': 'Yireun Kim', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-05-21T04:27:50.114Z', 'hidden': False}, {'_id': '682d55ecea67e90811b09b73', 'user': {'_id': '621f05ba970615ad5861ceb1', 'avatarUrl': '/avatars/7e1902aa71369a524afda9b0a9e88e22.svg', 'isPro': False, 'fullname': 'Minjoon Seo', 'user': 'minjoon', 'type': 'user'}, 'name': 'Minjoon Seo', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:17:22.628Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/uZZgyWD0lw8jektQyDZfV.png', 'https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/_Kq3BF1HC_aFc6mee08zl.png', 'https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/bpz_J8XujNicCvMH2n2iK.png'], 'publishedAt': '2025-05-20T15:19:00.000Z', 'submittedOnDailyAt': '2025-05-21T02:58:45.807Z', 'title': 'Reasoning Models Better Express Their Confidence', 'submittedOnDailyBy': {'_id': '617f679fb15f8a665f3999fc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg', 'isPro': False, 'fullname': 'Dongkeun Yoon', 'user': 'DKYoon', 'type': 'user'}, 'summary': 'Despite their strengths, large language models (LLMs) often fail to\\ncommunicate their confidence accurately, making it difficult to assess when\\nthey might be wrong and limiting their reliability. In this work, we\\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\\nalso in accurately expressing their confidence. Specifically, we benchmark six\\nreasoning models across six datasets and find that they achieve strictly better\\nconfidence calibration than their non-reasoning counterparts in 33 out of the\\n36 settings. Our detailed analysis reveals that these gains in calibration stem\\nfrom the slow thinking behaviors of reasoning models-such as exploring\\nalternative approaches and backtracking-which enable them to adjust their\\nconfidence dynamically throughout their CoT, making it progressively more\\naccurate. In particular, we find that reasoning models become increasingly\\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\\nsignificant drop in calibration. Lastly, we show that these gains are not\\nexclusive to reasoning models-non-reasoning models also benefit when guided to\\nperform slow thinking via in-context learning.', 'upvotes': 12, 'discussionId': '682d55edea67e90811b09ba1', 'githubRepo': 'https://github.com/MattYoon/reasoning-models-confidence', 'ai_keywords': ['large language models (LLMs)', 'chain-of-thought (CoT) reasoning', 'confidence calibration', 'non-reasoning counterparts', 'slow thinking behaviors', 'backtracking', 'in-context learning']}, 'publishedAt': '2025-05-20T11:19:00.000Z', 'title': 'Reasoning Models Better Express Their Confidence', 'summary': 'Despite their strengths, large language models (LLMs) often fail to\\ncommunicate their confidence accurately, making it difficult to assess when\\nthey might be wrong and limiting their reliability. In this work, we\\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\\nalso in accurately expressing their confidence. Specifically, we benchmark six\\nreasoning models across six datasets and find that they achieve strictly better\\nconfidence calibration than their non-reasoning counterparts in 33 out of the\\n36 settings. Our detailed analysis reveals that these gains in calibration stem\\nfrom the slow thinking behaviors of reasoning models-such as exploring\\nalternative approaches and backtracking-which enable them to adjust their\\nconfidence dynamically throughout their CoT, making it progressively more\\naccurate. In particular, we find that reasoning models become increasingly\\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\\nsignificant drop in calibration. Lastly, we show that these gains are not\\nexclusive to reasoning models-non-reasoning models also benefit when guided to\\nperform slow thinking via in-context learning.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/uZZgyWD0lw8jektQyDZfV.png', 'https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/_Kq3BF1HC_aFc6mee08zl.png', 'https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/bpz_J8XujNicCvMH2n2iK.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14489.png', 'numComments': 1, 'submittedBy': {'_id': '617f679fb15f8a665f3999fc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg', 'fullname': 'Dongkeun Yoon', 'name': 'DKYoon', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 10}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.13547', 'authors': [{'_id': '682d6b06ec3b65b35772c0af', 'user': {'_id': '668f440894dfc0ed1a7006ed', 'avatarUrl': '/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg', 'isPro': False, 'fullname': 'Pengxin Guo', 'user': 'gpx333', 'type': 'user'}, 'name': 'Pengxin Guo', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:11.271Z', 'hidden': False}, {'_id': '682d6b06ec3b65b35772c0b0', 'user': {'_id': '641b065a1911d3be6742ef04', 'avatarUrl': '/avatars/bceb4ad2a278b6e2a40af0b89c4fb48e.svg', 'isPro': False, 'fullname': 'Yinong Wang', 'user': 'jcccy', 'type': 'user'}, 'name': 'Yinong Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:12:52.463Z', 'hidden': False}, {'_id': '682d6b06ec3b65b35772c0b1', 'name': 'Wei Li', 'hidden': False}, {'_id': '682d6b06ec3b65b35772c0b2', 'user': {'_id': '678f8d4b1a5e393c1c285165', 'avatarUrl': '/avatars/533eb74376915eb63c03dc9b4df421f6.svg', 'isPro': False, 'fullname': 'MENGTINGLIU', 'user': 'MENGTINGLIU', 'type': 'user'}, 'name': 'Mengting Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:13:13.691Z', 'hidden': False}, {'_id': '682d6b06ec3b65b35772c0b3', 'user': {'_id': '637f0eb22438d7485b8ef5d7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg', 'isPro': False, 'fullname': 'Ming Li', 'user': 'limingcv', 'type': 'user'}, 'name': 'Ming Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:13:40.959Z', 'hidden': False}, {'_id': '682d6b06ec3b65b35772c0b4', 'name': 'Jinkai Zheng', 'hidden': False}, {'_id': '682d6b06ec3b65b35772c0b5', 'user': {'_id': '663058bc2653ec94f4a6235f', 'avatarUrl': '/avatars/f55b8c3c8100d6b6d65ba61abc4fb014.svg', 'isPro': False, 'fullname': 'Liangqiong Qu', 'user': 'Liangqiong-QU', 'type': 'user'}, 'name': 'Liangqiong Qu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:12:59.427Z', 'hidden': False}], 'publishedAt': '2025-05-19T03:41:54.000Z', 'submittedOnDailyAt': '2025-05-21T04:27:08.535Z', 'title': 'Exploring Federated Pruning for Large Language Models', 'submittedOnDailyBy': {'_id': '668f440894dfc0ed1a7006ed', 'avatarUrl': '/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg', 'isPro': False, 'fullname': 'Pengxin Guo', 'user': 'gpx333', 'type': 'user'}, 'summary': 'LLM pruning has emerged as a promising technology for compressing LLMs,\\nenabling their deployment on resource-limited devices. However, current\\nmethodologies typically require access to public calibration samples, which can\\nbe challenging to obtain in privacy-sensitive domains. To address this issue,\\nwe introduce FedPrLLM, a comprehensive federated pruning framework designed for\\nthe privacy-preserving compression of LLMs. In FedPrLLM, each client only needs\\nto calculate a pruning mask matrix based on its local calibration data and\\nshare it with the server to prune the global model. This approach allows for\\ncollaborative pruning of the global model with the knowledge of each client\\nwhile maintaining local data privacy. Additionally, we conduct extensive\\nexperiments to explore various possibilities within the FedPrLLM framework,\\nincluding different comparison groups, pruning strategies, and the decision to\\nscale weights. Our extensive evaluation reveals that one-shot pruning with\\nlayer comparison and no weight scaling is the optimal choice within the\\nFedPrLLM framework. We hope our work will help guide future efforts in pruning\\nLLMs in privacy-sensitive fields. Our code is available at\\nhttps://github.com/Pengxin-Guo/FedPrLLM.', 'upvotes': 12, 'discussionId': '682d6b07ec3b65b35772c0f3', 'githubRepo': 'https://github.com/Pengxin-Guo/FedPrLLM', 'ai_keywords': ['FedPrLLM', 'federated pruning', 'pruning mask matrix', 'local calibration data', 'global model', 'collaborative pruning', 'layer comparison', 'weight scaling', 'one-shot pruning']}, 'publishedAt': '2025-05-18T23:41:54.000Z', 'title': 'Exploring Federated Pruning for Large Language Models', 'summary': 'LLM pruning has emerged as a promising technology for compressing LLMs,\\nenabling their deployment on resource-limited devices. However, current\\nmethodologies typically require access to public calibration samples, which can\\nbe challenging to obtain in privacy-sensitive domains. To address this issue,\\nwe introduce FedPrLLM, a comprehensive federated pruning framework designed for\\nthe privacy-preserving compression of LLMs. In FedPrLLM, each client only needs\\nto calculate a pruning mask matrix based on its local calibration data and\\nshare it with the server to prune the global model. This approach allows for\\ncollaborative pruning of the global model with the knowledge of each client\\nwhile maintaining local data privacy. Additionally, we conduct extensive\\nexperiments to explore various possibilities within the FedPrLLM framework,\\nincluding different comparison groups, pruning strategies, and the decision to\\nscale weights. Our extensive evaluation reveals that one-shot pruning with\\nlayer comparison and no weight scaling is the optimal choice within the\\nFedPrLLM framework. We hope our work will help guide future efforts in pruning\\nLLMs in privacy-sensitive fields. Our code is available at\\nhttps://github.com/Pengxin-Guo/FedPrLLM.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13547.png', 'numComments': 1, 'submittedBy': {'_id': '668f440894dfc0ed1a7006ed', 'avatarUrl': '/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg', 'fullname': 'Pengxin Guo', 'name': 'gpx333', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14677', 'authors': [{'_id': '682d718306291bf11fcf69a3', 'user': {'_id': '664da76e4eb4c91c8c32cc06', 'avatarUrl': '/avatars/050bb6e4136f8a1645fef277ad08c7fc.svg', 'isPro': False, 'fullname': 'Jiaer Xia', 'user': 'Jiaer-Xia', 'type': 'user'}, 'name': 'Jiaer Xia', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:14:20.493Z', 'hidden': False}, {'_id': '682d718306291bf11fcf69a4', 'user': {'_id': '63859cf3b2906edaf83af9f0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png', 'isPro': False, 'fullname': 'Yuhang Zang', 'user': 'yuhangzang', 'type': 'user'}, 'name': 'Yuhang Zang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:39:56.735Z', 'hidden': False}, {'_id': '682d718306291bf11fcf69a5', 'name': 'Peng Gao', 'hidden': False}, {'_id': '682d718306291bf11fcf69a6', 'name': 'Yixuan Li', 'hidden': False}, {'_id': '682d718306291bf11fcf69a7', 'user': {'_id': '62ac6656de8bfbb93094b8fd', 'avatarUrl': '/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg', 'isPro': False, 'fullname': 'Kaiyang Zhou', 'user': 'kaiyangzhou', 'type': 'user'}, 'name': 'Kaiyang Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:14:28.433Z', 'hidden': False}], 'publishedAt': '2025-05-20T17:58:35.000Z', 'submittedOnDailyAt': '2025-05-21T04:54:13.466Z', 'title': 'Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\\n  Reinforcement Learning', 'submittedOnDailyBy': {'_id': '62ac6656de8bfbb93094b8fd', 'avatarUrl': '/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg', 'isPro': False, 'fullname': 'Kaiyang Zhou', 'user': 'kaiyangzhou', 'type': 'user'}, 'summary': 'Learning general-purpose reasoning capabilities has long been a challenging\\nproblem in AI. Recent research in large language models (LLMs), such as\\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\\nenable pre-trained LLMs to develop reasoning capabilities using simple\\nquestion-answer pairs. In this paper, we aim to train visual language models\\n(VLMs) to perform reasoning on image data through reinforcement learning and\\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\\nsupervision. Our findings indicate that simply applying reinforcement learning\\nto a VLM -- by prompting the model to produce a reasoning chain before\\nproviding an answer -- can lead the model to develop shortcuts from easy\\nquestions, thereby reducing its ability to generalize across unseen data\\ndistributions. We argue that the key to mitigating shortcut learning is to\\nencourage the model to interpret images prior to reasoning. Therefore, we train\\nthe model to adhere to a caption-reason-answer output format: initially\\ngenerating a detailed caption for an image, followed by constructing an\\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\\npairs and using only reinforcement learning, our model, named Visionary-R1,\\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.', 'upvotes': 11, 'discussionId': '682d718406291bf11fcf69df', 'githubRepo': 'https://github.com/maifoundations/Visionary-R1', 'ai_keywords': ['reinforcement learning', 'deep learning', 'large language models (LLMs)', 'DeepSeek-R1', 'GRPO', 'visual language models (VLMs)', 'visual question-answer pairs', 'chain-of-thought (CoT)', 'caption-reason-answer', 'multimodal models', 'GPT-4o', 'Claude3.5-Sonnet', 'Gemini-1.5-Pro', 'visual reasoning benchmarks']}, 'publishedAt': '2025-05-20T13:58:35.000Z', 'title': 'Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\\n  Reinforcement Learning', 'summary': 'Learning general-purpose reasoning capabilities has long been a challenging\\nproblem in AI. Recent research in large language models (LLMs), such as\\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\\nenable pre-trained LLMs to develop reasoning capabilities using simple\\nquestion-answer pairs. In this paper, we aim to train visual language models\\n(VLMs) to perform reasoning on image data through reinforcement learning and\\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\\nsupervision. Our findings indicate that simply applying reinforcement learning\\nto a VLM -- by prompting the model to produce a reasoning chain before\\nproviding an answer -- can lead the model to develop shortcuts from easy\\nquestions, thereby reducing its ability to generalize across unseen data\\ndistributions. We argue that the key to mitigating shortcut learning is to\\nencourage the model to interpret images prior to reasoning. Therefore, we train\\nthe model to adhere to a caption-reason-answer output format: initially\\ngenerating a detailed caption for an image, followed by constructing an\\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\\npairs and using only reinforcement learning, our model, named Visionary-R1,\\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14677.png', 'numComments': 1, 'submittedBy': {'_id': '62ac6656de8bfbb93094b8fd', 'avatarUrl': '/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg', 'fullname': 'Kaiyang Zhou', 'name': 'kaiyangzhou', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14652', 'authors': [{'_id': '682d474782567fffe1f99b7a', 'user': {'_id': '5ec82854968f6028e0559f70', 'avatarUrl': '/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg', 'isPro': True, 'fullname': 'Xueguang Ma', 'user': 'MrLight', 'type': 'user'}, 'name': 'Xueguang Ma', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:41.675Z', 'hidden': False}, {'_id': '682d474782567fffe1f99b7b', 'user': {'_id': '612ee6a7b960e78c6d2319d4', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg', 'isPro': False, 'fullname': 'Qian Liu', 'user': 'SivilTaram', 'type': 'user'}, 'name': 'Qian Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:39.513Z', 'hidden': False}, {'_id': '682d474782567fffe1f99b7c', 'user': {'_id': '62567c86d444a9b5a0ec51c1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png', 'isPro': False, 'fullname': 'Dongfu Jiang', 'user': 'DongfuJiang', 'type': 'user'}, 'name': 'Dongfu Jiang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:15:22.975Z', 'hidden': False}, {'_id': '682d474782567fffe1f99b7d', 'user': {'_id': '638efcf4c67af472d316d424', 'avatarUrl': '/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg', 'isPro': False, 'fullname': 'Ge Zhang', 'user': 'zhangysk', 'type': 'user'}, 'name': 'Ge Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:15:29.552Z', 'hidden': False}, {'_id': '682d474782567fffe1f99b7e', 'name': 'Zejun Ma', 'hidden': False}, {'_id': '682d474782567fffe1f99b7f', 'user': {'_id': '6313a86154e6e5d9f0f94e04', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg', 'isPro': False, 'fullname': 'Wenhu Chen', 'user': 'wenhu', 'type': 'user'}, 'name': 'Wenhu Chen', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-21T03:23:52.529Z', 'hidden': False}], 'publishedAt': '2025-05-20T17:41:33.000Z', 'submittedOnDailyAt': '2025-05-21T01:56:42.807Z', 'title': 'General-Reasoner: Advancing LLM Reasoning Across All Domains', 'submittedOnDailyBy': {'_id': '5ec82854968f6028e0559f70', 'avatarUrl': '/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg', 'isPro': True, 'fullname': 'Xueguang Ma', 'user': 'MrLight', 'type': 'user'}, 'summary': 'Reinforcement learning (RL) has recently demonstrated strong potential in\\nenhancing the reasoning capabilities of large language models (LLMs).\\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\\nenables direct RL training of base LLMs without relying on an intermediate\\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\\nreasoning mainly focus on mathematical and coding domains, largely due to data\\nabundance and the ease of answer verification. This limits the applicability\\nand generalization of such models to broader domains, where questions often\\nhave diverse answer representations, and data is more scarce. In this paper, we\\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\\nreasoning capabilities across diverse domains. Our key contributions include:\\n(1) constructing a large-scale, high-quality dataset of questions with\\nverifiable answers curated by web crawling, covering a wide range of\\ndisciplines; and (2) developing a generative model-based answer verifier, which\\nreplaces traditional rule-based verification with the capability of\\nchain-of-thought and context-awareness. We train a series of models and\\nevaluate them on a wide range of datasets covering wide domains like physics,\\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\\ndemonstrates that General-Reasoner outperforms existing baseline methods,\\nachieving robust and generalizable reasoning performance while maintaining\\nsuperior effectiveness in mathematical reasoning tasks.', 'upvotes': 11, 'discussionId': '682d474882567fffe1f99bc5', 'projectPage': 'https://tiger-ai-lab.github.io/General-Reasoner/', 'githubRepo': 'https://github.com/TIGER-AI-Lab/General-Reasoner', 'ai_keywords': ['reinforcement learning (RL)', 'large language models (LLMs)', 'Zero reinforcement learning', 'base LLMs', 'supervised fine-tuning', 'generative model-based answer verifier', 'chain-of-thought', 'context-awareness']}, 'publishedAt': '2025-05-20T13:41:33.000Z', 'title': 'General-Reasoner: Advancing LLM Reasoning Across All Domains', 'summary': 'Reinforcement learning (RL) has recently demonstrated strong potential in\\nenhancing the reasoning capabilities of large language models (LLMs).\\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\\nenables direct RL training of base LLMs without relying on an intermediate\\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\\nreasoning mainly focus on mathematical and coding domains, largely due to data\\nabundance and the ease of answer verification. This limits the applicability\\nand generalization of such models to broader domains, where questions often\\nhave diverse answer representations, and data is more scarce. In this paper, we\\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\\nreasoning capabilities across diverse domains. Our key contributions include:\\n(1) constructing a large-scale, high-quality dataset of questions with\\nverifiable answers curated by web crawling, covering a wide range of\\ndisciplines; and (2) developing a generative model-based answer verifier, which\\nreplaces traditional rule-based verification with the capability of\\nchain-of-thought and context-awareness. We train a series of models and\\nevaluate them on a wide range of datasets covering wide domains like physics,\\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\\ndemonstrates that General-Reasoner outperforms existing baseline methods,\\nachieving robust and generalizable reasoning performance while maintaining\\nsuperior effectiveness in mathematical reasoning tasks.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14652.png', 'numComments': 1, 'submittedBy': {'_id': '5ec82854968f6028e0559f70', 'avatarUrl': '/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg', 'fullname': 'Xueguang Ma', 'name': 'MrLight', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 22}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14674', 'authors': [{'_id': '682d4145514c96fbf03f5f76', 'name': 'Jiaxin Guo', 'hidden': False}, {'_id': '682d4145514c96fbf03f5f77', 'name': 'Zewen Chi', 'hidden': False}, {'_id': '682d4145514c96fbf03f5f78', 'user': {'_id': '5df85abada6d0311fd3d5408', 'avatarUrl': '/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg', 'isPro': False, 'fullname': 'Li Dong', 'user': 'unilm', 'type': 'user'}, 'name': 'Li Dong', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:52.470Z', 'hidden': False}, {'_id': '682d4145514c96fbf03f5f79', 'name': 'Qingxiu Dong', 'hidden': False}, {'_id': '682d4145514c96fbf03f5f7a', 'user': {'_id': '62d1227384bfbee86b6eec56', 'avatarUrl': '/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg', 'isPro': False, 'fullname': 'Xun Wu', 'user': 'YUSHUIWX', 'type': 'user'}, 'name': 'Xun Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:50.085Z', 'hidden': False}, {'_id': '682d4145514c96fbf03f5f7b', 'name': 'Shaohan Huang', 'hidden': False}, {'_id': '682d4145514c96fbf03f5f7c', 'name': 'Furu Wei', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/wpApND0JXiL6s6MUpBdg7.png'], 'publishedAt': '2025-05-20T17:58:03.000Z', 'submittedOnDailyAt': '2025-05-21T02:57:46.082Z', 'title': 'Reward Reasoning Model', 'submittedOnDailyBy': {'_id': '5df85abada6d0311fd3d5408', 'avatarUrl': '/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg', 'isPro': False, 'fullname': 'Li Dong', 'user': 'unilm', 'type': 'user'}, 'summary': 'Reward models play a critical role in guiding large language models toward\\noutputs that align with human expectations. However, an open challenge remains\\nin effectively utilizing test-time compute to enhance reward model performance.\\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\\nspecifically designed to execute a deliberate reasoning process before\\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\\nadditional test-time compute for complex queries where appropriate rewards are\\nnot immediately apparent. To develop RRMs, we implement a reinforcement\\nlearning framework that fosters self-evolved reward reasoning capabilities\\nwithout requiring explicit reasoning traces as training data. Experimental\\nresults demonstrate that RRMs achieve superior performance on reward modeling\\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\\nexploit test-time compute to further improve reward accuracy. The pretrained\\nreward reasoning models are available at\\nhttps://huggingface.co/Reward-Reasoning.', 'upvotes': 10, 'discussionId': '682d4146514c96fbf03f5fab', 'ai_keywords': ['Reward models', 'large language models', 'chain-of-thought reasoning', 'reinforcement learning', 'self-evolved reward reasoning', 'reward modeling benchmarks']}, 'publishedAt': '2025-05-20T13:58:03.000Z', 'title': 'Reward Reasoning Model', 'summary': 'Reward models play a critical role in guiding large language models toward\\noutputs that align with human expectations. However, an open challenge remains\\nin effectively utilizing test-time compute to enhance reward model performance.\\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\\nspecifically designed to execute a deliberate reasoning process before\\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\\nadditional test-time compute for complex queries where appropriate rewards are\\nnot immediately apparent. To develop RRMs, we implement a reinforcement\\nlearning framework that fosters self-evolved reward reasoning capabilities\\nwithout requiring explicit reasoning traces as training data. Experimental\\nresults demonstrate that RRMs achieve superior performance on reward modeling\\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\\nexploit test-time compute to further improve reward accuracy. The pretrained\\nreward reasoning models are available at\\nhttps://huggingface.co/Reward-Reasoning.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/wpApND0JXiL6s6MUpBdg7.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14674.png', 'numComments': 1, 'submittedBy': {'_id': '5df85abada6d0311fd3d5408', 'avatarUrl': '/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg', 'fullname': 'Li Dong', 'name': 'unilm', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 29}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.13866', 'authors': [{'_id': '682d2dee396c1e613e9fcbe5', 'user': {'_id': '662672eaebdfec5cfdf1d034', 'avatarUrl': '/avatars/61bc7add693c555e29ad3c1112215684.svg', 'isPro': False, 'fullname': 'Jiwon Song', 'user': 'jiwonsong', 'type': 'user'}, 'name': 'Jiwon Song', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:41:15.525Z', 'hidden': False}, {'_id': '682d2dee396c1e613e9fcbe6', 'user': {'_id': '639ffbc6beb95d698de9640d', 'avatarUrl': '/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg', 'isPro': False, 'fullname': 'Dongwon Jo', 'user': 'dongwonjo', 'type': 'user'}, 'name': 'Dongwon Jo', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:41:12.000Z', 'hidden': False}, {'_id': '682d2dee396c1e613e9fcbe7', 'user': {'_id': '6566ddb96af53c602f80b1e2', 'avatarUrl': '/avatars/403c8e486115920e50867b6462ddfd99.svg', 'isPro': False, 'fullname': 'Yulhwa Kim', 'user': 'YulhwaKim', 'type': 'user'}, 'name': 'Yulhwa Kim', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:14:05.272Z', 'hidden': False}, {'_id': '682d2dee396c1e613e9fcbe8', 'name': 'Jae-Joon Kim', 'hidden': False}], 'publishedAt': '2025-05-20T03:21:52.000Z', 'submittedOnDailyAt': '2025-05-21T00:06:25.696Z', 'title': 'Reasoning Path Compression: Compressing Generation Trajectories for\\n  Efficient LLM Reasoning', 'submittedOnDailyBy': {'_id': '662672eaebdfec5cfdf1d034', 'avatarUrl': '/avatars/61bc7add693c555e29ad3c1112215684.svg', 'isPro': False, 'fullname': 'Jiwon Song', 'user': 'jiwonsong', 'type': 'user'}, 'summary': 'Recent reasoning-focused language models achieve high accuracy by generating\\nlengthy intermediate reasoning paths before producing final answers. While this\\napproach is effective in solving problems that require logical thinking, long\\nreasoning paths significantly increase memory usage and throughput of token\\ngeneration, limiting the practical deployment of such models. We propose\\nReasoning Path Compression (RPC), a training-free method that accelerates\\ninference by leveraging the semantic sparsity of reasoning paths. RPC\\nperiodically compresses the KV cache by retaining KV cache that receive high\\nimportance score, which are computed using a selector window composed of\\nrecently generated queries. Experiments show that RPC improves generation\\nthroughput of QwQ-32B by up to 1.60times compared to the inference with full\\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\\nfindings demonstrate that semantic sparsity in reasoning traces can be\\neffectively exploited for compression, offering a practical path toward\\nefficient deployment of reasoning LLMs. Our code is available at\\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.', 'upvotes': 10, 'discussionId': '682d2def396c1e613e9fcc0b', 'githubRepo': 'https://github.com/jiwonsong-dev/ReasoningPathCompression', 'ai_keywords': ['Reasoning Path Compression (RPC)', 'KV cache', 'semantic sparsity', 'reasoning traces', 'QwQ-32B', 'AIME 2024 benchmark']}, 'publishedAt': '2025-05-19T23:21:52.000Z', 'title': 'Reasoning Path Compression: Compressing Generation Trajectories for\\n  Efficient LLM Reasoning', 'summary': 'Recent reasoning-focused language models achieve high accuracy by generating\\nlengthy intermediate reasoning paths before producing final answers. While this\\napproach is effective in solving problems that require logical thinking, long\\nreasoning paths significantly increase memory usage and throughput of token\\ngeneration, limiting the practical deployment of such models. We propose\\nReasoning Path Compression (RPC), a training-free method that accelerates\\ninference by leveraging the semantic sparsity of reasoning paths. RPC\\nperiodically compresses the KV cache by retaining KV cache that receive high\\nimportance score, which are computed using a selector window composed of\\nrecently generated queries. Experiments show that RPC improves generation\\nthroughput of QwQ-32B by up to 1.60times compared to the inference with full\\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\\nfindings demonstrate that semantic sparsity in reasoning traces can be\\neffectively exploited for compression, offering a practical path toward\\nefficient deployment of reasoning LLMs. Our code is available at\\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13866.png', 'numComments': 1, 'submittedBy': {'_id': '662672eaebdfec5cfdf1d034', 'avatarUrl': '/avatars/61bc7add693c555e29ad3c1112215684.svg', 'fullname': 'Jiwon Song', 'name': 'jiwonsong', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14673', 'authors': [{'_id': '682d713e6c66e25ab59fc800', 'user': {'_id': '65c38108425a226a29f00365', 'avatarUrl': '/avatars/c276a0d2b108df446246c31a396496f0.svg', 'isPro': False, 'fullname': 'tongyu', 'user': 'yutchina02', 'type': 'user'}, 'name': 'Yu Tong', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:01.048Z', 'hidden': False}, {'_id': '682d713e6c66e25ab59fc801', 'user': {'_id': '65ad57da57f263e3d030187a', 'avatarUrl': '/avatars/e1e3c4119180aee5fb660d1e5f745e99.svg', 'isPro': False, 'fullname': '潘子豪', 'user': 'Apostle723', 'type': 'user'}, 'name': 'Zihao Pan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:39:58.888Z', 'hidden': False}, {'_id': '682d713e6c66e25ab59fc802', 'name': 'Shuai Yang', 'hidden': False}, {'_id': '682d713e6c66e25ab59fc803', 'user': {'_id': '62ac6656de8bfbb93094b8fd', 'avatarUrl': '/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg', 'isPro': False, 'fullname': 'Kaiyang Zhou', 'user': 'kaiyangzhou', 'type': 'user'}, 'name': 'Kaiyang Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:20:44.575Z', 'hidden': False}], 'publishedAt': '2025-05-20T17:58:02.000Z', 'submittedOnDailyAt': '2025-05-21T04:53:16.661Z', 'title': 'Training-Free Watermarking for Autoregressive Image Generation', 'submittedOnDailyBy': {'_id': '62ac6656de8bfbb93094b8fd', 'avatarUrl': '/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg', 'isPro': False, 'fullname': 'Kaiyang Zhou', 'user': 'kaiyangzhou', 'type': 'user'}, 'summary': 'Invisible image watermarking can protect image ownership and prevent\\nmalicious misuse of visual generative models. However, existing generative\\nwatermarking methods are mainly designed for diffusion models while\\nwatermarking for autoregressive image generation models remains largely\\nunderexplored. We propose IndexMark, a training-free watermarking framework for\\nautoregressive image generation models. IndexMark is inspired by the redundancy\\nproperty of the codebook: replacing autoregressively generated indices with\\nsimilar indices produces negligible visual differences. The core component in\\nIndexMark is a simple yet effective match-then-replace method, which carefully\\nselects watermark tokens from the codebook based on token similarity, and\\npromotes the use of watermark tokens through token replacement, thereby\\nembedding the watermark without affecting the image quality. Watermark\\nverification is achieved by calculating the proportion of watermark tokens in\\ngenerated images, with precision further improved by an Index Encoder.\\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\\nstate-of-the-art performance in terms of image quality and verification\\naccuracy, and exhibits robustness against various perturbations, including\\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\\ncompression.', 'upvotes': 9, 'discussionId': '682d71426c66e25ab59fc946', 'githubRepo': 'https://github.com/maifoundations/IndexMark', 'ai_keywords': ['autoregressive image generation models', 'codebook', 'indices', 'token similarity', 'watermark tokens', 'token replacement', 'IndexMark', 'match-then-replace method', 'Index Encoder', 'auxiliary validation scheme', 'cropping attacks', 'Gaussian blur', 'random erasing', 'color jittering', 'JPEG compression']}, 'publishedAt': '2025-05-20T13:58:02.000Z', 'title': 'Training-Free Watermarking for Autoregressive Image Generation', 'summary': 'Invisible image watermarking can protect image ownership and prevent\\nmalicious misuse of visual generative models. However, existing generative\\nwatermarking methods are mainly designed for diffusion models while\\nwatermarking for autoregressive image generation models remains largely\\nunderexplored. We propose IndexMark, a training-free watermarking framework for\\nautoregressive image generation models. IndexMark is inspired by the redundancy\\nproperty of the codebook: replacing autoregressively generated indices with\\nsimilar indices produces negligible visual differences. The core component in\\nIndexMark is a simple yet effective match-then-replace method, which carefully\\nselects watermark tokens from the codebook based on token similarity, and\\npromotes the use of watermark tokens through token replacement, thereby\\nembedding the watermark without affecting the image quality. Watermark\\nverification is achieved by calculating the proportion of watermark tokens in\\ngenerated images, with precision further improved by an Index Encoder.\\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\\nstate-of-the-art performance in terms of image quality and verification\\naccuracy, and exhibits robustness against various perturbations, including\\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\\ncompression.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14673.png', 'numComments': 1, 'submittedBy': {'_id': '62ac6656de8bfbb93094b8fd', 'avatarUrl': '/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg', 'fullname': 'Kaiyang Zhou', 'name': 'kaiyangzhou', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14640', 'authors': [{'_id': '682d488557686b8c44f257fa', 'user': {'_id': '65c387c807a1445dfe1e9452', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65c387c807a1445dfe1e9452/t0VnwQh2wRZ9W_UGTZ8zt.jpeg', 'isPro': False, 'fullname': 'Wentao Ma', 'user': 'tonymwt', 'type': 'user'}, 'name': 'Wentao Ma', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:31.165Z', 'hidden': False}, {'_id': '682d488557686b8c44f257fb', 'user': {'_id': '64405a9d518271b0d1beea38', 'avatarUrl': '/avatars/b702474588fd7090773320422417a582.svg', 'isPro': False, 'fullname': 'Weiming Ren', 'user': 'wren93', 'type': 'user'}, 'name': 'Weiming Ren', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:28.689Z', 'hidden': False}, {'_id': '682d488557686b8c44f257fc', 'name': 'Yiming Jia', 'hidden': False}, {'_id': '682d488557686b8c44f257fd', 'user': {'_id': '66349404f2c753240d02952a', 'avatarUrl': '/avatars/4f207cf5807d9629b9f4f7d13875b840.svg', 'isPro': False, 'fullname': 'ZhuofengLi', 'user': 'ZhuofengLi', 'type': 'user'}, 'name': 'Zhuofeng Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:35.151Z', 'hidden': False}, {'_id': '682d488557686b8c44f257fe', 'name': 'Ping Nie', 'hidden': False}, {'_id': '682d488557686b8c44f257ff', 'name': 'Ge Zhang', 'hidden': False}, {'_id': '682d488557686b8c44f25800', 'name': 'Wenhu Chen', 'hidden': False}], 'publishedAt': '2025-05-20T17:26:32.000Z', 'submittedOnDailyAt': '2025-05-21T02:03:04.298Z', 'title': 'VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation', 'submittedOnDailyBy': {'_id': '64405a9d518271b0d1beea38', 'avatarUrl': '/avatars/b702474588fd7090773320422417a582.svg', 'isPro': False, 'fullname': 'Weiming Ren', 'user': 'wren93', 'type': 'user'}, 'summary': \"Large multimodal models (LMMs) have recently emerged as a powerful tool for\\nlong video understanding (LVU), prompting the development of standardized LVU\\nbenchmarks to evaluate their performance. However, our investigation reveals a\\nrather sober lesson for existing LVU benchmarks. First, most existing\\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\\nresults are inflated due to the possibility of guessing the correct answer;\\nSecond, a significant portion of questions in these benchmarks have strong\\npriors to allow models to answer directly without even reading the input video.\\nFor example, Gemini-1.5-Pro can achieve over 50\\\\% accuracy given a random frame\\nfrom a long video on Video-MME. We also observe that increasing the number of\\nframes does not necessarily lead to improvement on existing benchmarks, which\\nis counterintuitive. As a result, the validity and robustness of current LVU\\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\\nrealistic LVU benchmark containing questions with open-ended short-answer,\\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\\nsegment-level and full-video understanding through perception and reasoning\\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\\nfollowing findings: (1) video LMMs show drastic performance (>25\\\\%) drops on\\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\\nframes. Our results show that VideoEval-Pro offers a more realistic and\\nreliable measure of long video understanding, providing a clearer view of\\nprogress in this domain.\", 'upvotes': 9, 'discussionId': '682d488857686b8c44f258b7', 'projectPage': 'https://tiger-ai-lab.github.io/VideoEval-Pro', 'githubRepo': 'https://github.com/TIGER-AI-Lab/VideoEval-Pro', 'ai_keywords': ['multimodal models', 'long video understanding', 'benchmarks', 'multiple-choice questions', 'Video-MME', 'random frame', 'VideoEval-Pro', 'open-ended questions', 'segment-level understanding', 'full-video understanding', 'perception tasks', 'reasoning tasks', 'performance drops']}, 'publishedAt': '2025-05-20T13:26:32.000Z', 'title': 'VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation', 'summary': \"Large multimodal models (LMMs) have recently emerged as a powerful tool for\\nlong video understanding (LVU), prompting the development of standardized LVU\\nbenchmarks to evaluate their performance. However, our investigation reveals a\\nrather sober lesson for existing LVU benchmarks. First, most existing\\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\\nresults are inflated due to the possibility of guessing the correct answer;\\nSecond, a significant portion of questions in these benchmarks have strong\\npriors to allow models to answer directly without even reading the input video.\\nFor example, Gemini-1.5-Pro can achieve over 50\\\\% accuracy given a random frame\\nfrom a long video on Video-MME. We also observe that increasing the number of\\nframes does not necessarily lead to improvement on existing benchmarks, which\\nis counterintuitive. As a result, the validity and robustness of current LVU\\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\\nrealistic LVU benchmark containing questions with open-ended short-answer,\\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\\nsegment-level and full-video understanding through perception and reasoning\\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\\nfollowing findings: (1) video LMMs show drastic performance (>25\\\\%) drops on\\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\\nframes. Our results show that VideoEval-Pro offers a more realistic and\\nreliable measure of long video understanding, providing a clearer view of\\nprogress in this domain.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14640.png', 'numComments': 1, 'submittedBy': {'_id': '64405a9d518271b0d1beea38', 'avatarUrl': '/avatars/b702474588fd7090773320422417a582.svg', 'fullname': 'Weiming Ren', 'name': 'wren93', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14631', 'authors': [{'_id': '682d69ed056cf7b86cd8d6ec', 'user': {'_id': '66ab80e9bfb7d73a56bc293c', 'avatarUrl': '/avatars/9644266304c832c74ef572b5eb2d9468.svg', 'isPro': False, 'fullname': 'Jack', 'user': 'lingjie23', 'type': 'user'}, 'name': 'Lingjie Jiang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:17.384Z', 'hidden': False}, {'_id': '682d69ed056cf7b86cd8d6ed', 'user': {'_id': '62d1227384bfbee86b6eec56', 'avatarUrl': '/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg', 'isPro': False, 'fullname': 'Xun Wu', 'user': 'YUSHUIWX', 'type': 'user'}, 'name': 'Xun Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:20.741Z', 'hidden': False}, {'_id': '682d69ed056cf7b86cd8d6ee', 'name': 'Shaohan Huang', 'hidden': False}, {'_id': '682d69ed056cf7b86cd8d6ef', 'name': 'Qingxiu Dong', 'hidden': False}, {'_id': '682d69ed056cf7b86cd8d6f0', 'name': 'Zewen Chi', 'hidden': False}, {'_id': '682d69ed056cf7b86cd8d6f1', 'name': 'Li Dong', 'hidden': False}, {'_id': '682d69ed056cf7b86cd8d6f2', 'name': 'Xingxing Zhang', 'hidden': False}, {'_id': '682d69ed056cf7b86cd8d6f3', 'name': 'Tengchao Lv', 'hidden': False}, {'_id': '682d69ed056cf7b86cd8d6f4', 'name': 'Lei Cui', 'hidden': False}, {'_id': '682d69ed056cf7b86cd8d6f5', 'name': 'Furu Wei', 'hidden': False}], 'publishedAt': '2025-05-20T17:23:25.000Z', 'submittedOnDailyAt': '2025-05-21T04:22:54.134Z', 'title': 'Think Only When You Need with Large Hybrid-Reasoning Models', 'submittedOnDailyBy': {'_id': '62d1227384bfbee86b6eec56', 'avatarUrl': '/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg', 'isPro': False, 'fullname': 'Xun Wu', 'user': 'YUSHUIWX', 'type': 'user'}, 'summary': \"Recent Large Reasoning Models (LRMs) have shown substantially improved\\nreasoning capabilities over traditional Large Language Models (LLMs) by\\nincorporating extended thinking processes prior to producing final responses.\\nHowever, excessively lengthy thinking introduces substantial overhead in terms\\nof token consumption and latency, which is particularly unnecessary for simple\\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\\nfirst kind of model capable of adaptively determining whether to perform\\nthinking based on the contextual information of user queries. To achieve this,\\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\\na cold start, followed by online reinforcement learning with the proposed\\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\\nExtensive experimental results show that LHRMs can adaptively perform hybrid\\nthinking on queries of varying difficulty and type. It outperforms existing\\nLRMs and LLMs in reasoning and general capabilities while significantly\\nimproving efficiency. Together, our work advocates for a reconsideration of the\\nappropriate use of extended thinking processes and provides a solid starting\\npoint for building hybrid thinking systems.\", 'upvotes': 9, 'discussionId': '682d69ee056cf7b86cd8d730', 'ai_keywords': ['Large Reasoning Models (LRMs)', 'Large Language Models (LLMs)', 'Hybrid-Reasoning Models (LHRMs)', 'Hybrid Fine-Tuning (HFT)', 'Hybrid Group Policy Optimization (HGPO)', 'Hybrid Accuracy']}, 'publishedAt': '2025-05-20T13:23:25.000Z', 'title': 'Think Only When You Need with Large Hybrid-Reasoning Models', 'summary': \"Recent Large Reasoning Models (LRMs) have shown substantially improved\\nreasoning capabilities over traditional Large Language Models (LLMs) by\\nincorporating extended thinking processes prior to producing final responses.\\nHowever, excessively lengthy thinking introduces substantial overhead in terms\\nof token consumption and latency, which is particularly unnecessary for simple\\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\\nfirst kind of model capable of adaptively determining whether to perform\\nthinking based on the contextual information of user queries. To achieve this,\\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\\na cold start, followed by online reinforcement learning with the proposed\\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\\nExtensive experimental results show that LHRMs can adaptively perform hybrid\\nthinking on queries of varying difficulty and type. It outperforms existing\\nLRMs and LLMs in reasoning and general capabilities while significantly\\nimproving efficiency. Together, our work advocates for a reconsideration of the\\nappropriate use of extended thinking processes and provides a solid starting\\npoint for building hybrid thinking systems.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14631.png', 'numComments': 1, 'submittedBy': {'_id': '62d1227384bfbee86b6eec56', 'avatarUrl': '/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg', 'fullname': 'Xun Wu', 'name': 'YUSHUIWX', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.13559', 'authors': [{'_id': '682d4ccc3b5f51f4218e12b4', 'user': {'_id': '62f0e457bc8201db9ef47f89', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62f0e457bc8201db9ef47f89/zOhDptwZpDGaugKCBOWB2.jpeg', 'isPro': False, 'fullname': 'Sathya Krishnan', 'user': 'SkAndMl', 'type': 'user'}, 'name': 'Sathya Krishnan Suresh', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:18:55.833Z', 'hidden': False}, {'_id': '682d4ccc3b5f51f4218e12b5', 'name': 'Tanmay Surana', 'hidden': False}, {'_id': '682d4ccc3b5f51f4218e12b6', 'name': 'Lim Zhi Hao', 'hidden': False}, {'_id': '682d4ccc3b5f51f4218e12b7', 'name': 'Eng Siong Chng', 'hidden': False}], 'publishedAt': '2025-05-19T09:18:14.000Z', 'submittedOnDailyAt': '2025-05-21T02:18:05.886Z', 'title': 'CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the\\n  Limits of Large Language Models', 'submittedOnDailyBy': {'_id': '62f0e457bc8201db9ef47f89', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62f0e457bc8201db9ef47f89/zOhDptwZpDGaugKCBOWB2.jpeg', 'isPro': False, 'fullname': 'Sathya Krishnan', 'user': 'SkAndMl', 'type': 'user'}, 'summary': 'Code-switching (CS) poses a significant challenge for Large Language Models\\n(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce\\nCS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue\\nto English summarization. CS-Sum is the first benchmark for CS dialogue\\nsummarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and\\nMalay-English (EN-MS), with 900-1300 human-annotated dialogues per language\\npair. Evaluating ten LLMs, including open and closed-source models, we analyze\\nperformance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA\\non synthetic data) approaches. Our findings show that though the scores on\\nautomated metrics are high, LLMs make subtle mistakes that alter the complete\\nmeaning of the dialogue. To this end, we introduce 3 most common type of errors\\nthat LLMs make when handling CS input. Error rates vary across CS pairs and\\nLLMs, with some LLMs showing more frequent errors on certain language pairs,\\nunderscoring the need for specialized training on code-switched data.', 'upvotes': 9, 'discussionId': '682d4ccd3b5f51f4218e12f4', 'ai_keywords': ['code-switching (CS)', 'Large Language Models (LLMs)', 'CS-Sum', 'CS dialogue to English summarization', 'Mandarin-English (EN-ZH)', 'Tamil-English (EN-TA)', 'Malay-English (EN-MS)', 'few-shot', 'translate-summarize', 'fine-tuning', 'LoRA', 'QLoRA', 'synthetic data', 'CS input']}, 'publishedAt': '2025-05-19T05:18:14.000Z', 'title': 'CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the\\n  Limits of Large Language Models', 'summary': 'Code-switching (CS) poses a significant challenge for Large Language Models\\n(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce\\nCS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue\\nto English summarization. CS-Sum is the first benchmark for CS dialogue\\nsummarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and\\nMalay-English (EN-MS), with 900-1300 human-annotated dialogues per language\\npair. Evaluating ten LLMs, including open and closed-source models, we analyze\\nperformance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA\\non synthetic data) approaches. Our findings show that though the scores on\\nautomated metrics are high, LLMs make subtle mistakes that alter the complete\\nmeaning of the dialogue. To this end, we introduce 3 most common type of errors\\nthat LLMs make when handling CS input. Error rates vary across CS pairs and\\nLLMs, with some LLMs showing more frequent errors on certain language pairs,\\nunderscoring the need for specialized training on code-switched data.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13559.png', 'numComments': 2, 'submittedBy': {'_id': '62f0e457bc8201db9ef47f89', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62f0e457bc8201db9ef47f89/zOhDptwZpDGaugKCBOWB2.jpeg', 'fullname': 'Sathya Krishnan', 'name': 'SkAndMl', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14680', 'authors': [{'_id': '682d30a37812103582f50de4', 'user': {'_id': '64db88993725f8d9a908c077', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg', 'isPro': False, 'fullname': 'Sunhao Dai', 'user': 'KID-22', 'type': 'user'}, 'name': 'Sunhao Dai', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:19:10.865Z', 'hidden': False}, {'_id': '682d30a37812103582f50de5', 'name': 'Wenjie Wang', 'hidden': False}, {'_id': '682d30a37812103582f50de6', 'user': {'_id': '5fc356c5ea82dd667bb0ffd1', 'avatarUrl': '/avatars/20519be09267bb429f3ca0f996f9fdc1.svg', 'isPro': False, 'fullname': 'Liang Pang', 'user': 'pl8787', 'type': 'user'}, 'name': 'Liang Pang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:19:41.530Z', 'hidden': False}, {'_id': '682d30a37812103582f50de7', 'name': 'Jun Xu', 'hidden': False}, {'_id': '682d30a37812103582f50de8', 'name': 'See-Kiong Ng', 'hidden': False}, {'_id': '682d30a37812103582f50de9', 'user': {'_id': '64b8c89052b7353d8c6a1013', 'avatarUrl': '/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg', 'isPro': False, 'fullname': 'Ji-Rong Wen', 'user': 'jrwen', 'type': 'user'}, 'name': 'Ji-Rong Wen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:19:55.385Z', 'hidden': False}, {'_id': '682d30a37812103582f50dea', 'user': {'_id': '6570ae84c4993b8fb96f41a8', 'avatarUrl': '/avatars/21f7d79d46ac4df0ecff8eca7678b33f.svg', 'isPro': False, 'fullname': 'Tat-Seng Chua', 'user': 'chuats', 'type': 'user'}, 'name': 'Tat-Seng Chua', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:19:49.244Z', 'hidden': False}], 'publishedAt': '2025-05-20T17:59:13.000Z', 'submittedOnDailyAt': '2025-05-21T00:18:37.037Z', 'title': 'NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search', 'submittedOnDailyBy': {'_id': '64db88993725f8d9a908c077', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg', 'isPro': False, 'fullname': 'Sunhao Dai', 'user': 'KID-22', 'type': 'user'}, 'summary': \"Generative AI search is reshaping information retrieval by offering\\nend-to-end answers to complex queries, reducing users' reliance on manually\\nbrowsing and summarizing multiple web pages. However, while this paradigm\\nenhances convenience, it disrupts the feedback-driven improvement loop that has\\nhistorically powered the evolution of traditional Web search. Web search can\\ncontinuously improve their ranking models by collecting large-scale,\\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\\ncontrast, generative AI search operates through a much longer search pipeline,\\nspanning query decomposition, document retrieval, and answer generation, yet\\ntypically receives only coarse-grained feedback on the final answer. This\\nintroduces a feedback loop disconnect, where user feedback for the final output\\ncannot be effectively mapped back to specific system components, making it\\ndifficult to improve each intermediate stage and sustain the feedback loop. In\\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\\nreintroduce fine-grained, process-level feedback into generative AI search.\\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\\nengaged users to intervene at key stages; and Shadow User Mode, where a\\npersonalized user agent simulates user preferences and provides AI-assisted\\nfeedback for less interactive users. Furthermore, we envision how these\\nfeedback signals can be leveraged through online adaptation, which refines\\ncurrent search outputs in real-time, and offline update, which aggregates\\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\\ngeneration models. By restoring human control over key stages of the generative\\nAI search pipeline, we believe NExT-Search offers a promising direction for\\nbuilding feedback-rich AI search systems that can evolve continuously alongside\\nhuman feedback.\", 'upvotes': 8, 'discussionId': '682d30a47812103582f50e19', 'ai_keywords': ['generative AI search', 'information retrieval', 'end-to-end answers', 'complex queries', 'manually browsing', 'summarizing', 'traditional Web search', 'ranking models', 'fine-grained user feedback', 'clicks', 'dwell time', 'document level', 'query decomposition', 'document retrieval', 'answer generation', 'coarse-grained feedback', 'feedback loop disconnect', 'system components', 'NExT-Search', 'User Debug Mode', 'Shadow User Mode', 'personalized user agent', 'AI-assisted feedback', 'online adaptation', 'offline update', 'real-time refinement', 'interaction logs', 'fine-tune query decomposition', 'fine-tune retrieval', 'fine-tune generation models', 'feedback-rich AI search systems']}, 'publishedAt': '2025-05-20T13:59:13.000Z', 'title': 'NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search', 'summary': \"Generative AI search is reshaping information retrieval by offering\\nend-to-end answers to complex queries, reducing users' reliance on manually\\nbrowsing and summarizing multiple web pages. However, while this paradigm\\nenhances convenience, it disrupts the feedback-driven improvement loop that has\\nhistorically powered the evolution of traditional Web search. Web search can\\ncontinuously improve their ranking models by collecting large-scale,\\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\\ncontrast, generative AI search operates through a much longer search pipeline,\\nspanning query decomposition, document retrieval, and answer generation, yet\\ntypically receives only coarse-grained feedback on the final answer. This\\nintroduces a feedback loop disconnect, where user feedback for the final output\\ncannot be effectively mapped back to specific system components, making it\\ndifficult to improve each intermediate stage and sustain the feedback loop. In\\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\\nreintroduce fine-grained, process-level feedback into generative AI search.\\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\\nengaged users to intervene at key stages; and Shadow User Mode, where a\\npersonalized user agent simulates user preferences and provides AI-assisted\\nfeedback for less interactive users. Furthermore, we envision how these\\nfeedback signals can be leveraged through online adaptation, which refines\\ncurrent search outputs in real-time, and offline update, which aggregates\\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\\ngeneration models. By restoring human control over key stages of the generative\\nAI search pipeline, we believe NExT-Search offers a promising direction for\\nbuilding feedback-rich AI search systems that can evolve continuously alongside\\nhuman feedback.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14680.png', 'numComments': 1, 'submittedBy': {'_id': '64db88993725f8d9a908c077', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg', 'fullname': 'Sunhao Dai', 'name': 'KID-22', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14135', 'authors': [{'_id': '682d43cf85d5e40c81ed313d', 'user': {'_id': '648a822341c90440f32b316c', 'avatarUrl': '/avatars/be74f02c54cc4072a9d1ac2f14f4fb96.svg', 'isPro': False, 'fullname': 'ruihuang li', 'user': 'lslrh', 'type': 'user'}, 'name': 'Ruihuang Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:32:30.513Z', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed313e', 'name': 'Caijin Zhou', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed313f', 'user': {'_id': '64d450cc52101a10fa64ba23', 'avatarUrl': '/avatars/eccba117e286864fa2c8cbca27ba79e0.svg', 'isPro': False, 'fullname': 'Zheng Shoujian', 'user': 'zhengsj', 'type': 'user'}, 'name': 'Shoujian Zheng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:32:54.604Z', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3140', 'name': 'Jianxiang Lu', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3141', 'user': {'_id': '641c139b73296f7ee256970c', 'avatarUrl': '/avatars/5a2550d95e686640242840ad3bd0e680.svg', 'isPro': False, 'fullname': 'Jiabin Huang', 'user': 'YellowAddice', 'type': 'user'}, 'name': 'Jiabin Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:33:08.493Z', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3142', 'name': 'Comi Chen', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3143', 'user': {'_id': '634e67f5d049354d7ee0caa4', 'avatarUrl': '/avatars/13f7a6f66060590e0394eb888148716c.svg', 'isPro': False, 'fullname': 'junshu tang', 'user': 'tangjs', 'type': 'user'}, 'name': 'Junshu Tang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:33:22.811Z', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3144', 'user': {'_id': '6539c938f84e87099cef67b7', 'avatarUrl': '/avatars/283ed16bd6b414c68e8c90b1b2aef5c4.svg', 'isPro': False, 'fullname': 'Xu Guangzheng', 'user': 'vcvcvn', 'type': 'user'}, 'name': 'Guangzheng Xu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:33:32.698Z', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3145', 'user': {'_id': '64f3406b0abb66646fb68cb9', 'avatarUrl': '/avatars/c835eb87fda447551130929078f555d3.svg', 'isPro': False, 'fullname': '陶嘉乐', 'user': 'TaoJiaLe', 'type': 'user'}, 'name': 'Jiale Tao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:33:44.533Z', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3146', 'name': 'Hongmei Wang', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3147', 'user': {'_id': '66f33e713c412b900bac6f2e', 'avatarUrl': '/avatars/9496d54a5a394d0082cb80b505fa26ec.svg', 'isPro': False, 'fullname': 'Donghao Li', 'user': 'panda12345pa', 'type': 'user'}, 'name': 'Donghao Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:34:06.502Z', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3148', 'name': 'Wenqing Yu', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3149', 'name': 'Senbo Wang', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed314a', 'name': 'Zhimin Li', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed314b', 'user': {'_id': '668cea284587d0a82bd22e37', 'avatarUrl': '/avatars/ef592494b339b5ab93078fdc1d847653.svg', 'isPro': False, 'fullname': 'yetshuanshi', 'user': 'yetshuan', 'type': 'user'}, 'name': 'Yetshuan Shi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:34:53.532Z', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed314c', 'name': 'Haoyu Yang', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed314d', 'name': 'Yukun Wang', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed314e', 'user': {'_id': '6035f299564a521e3b4c73e1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1614152410729-6035f299564a521e3b4c73e1.jpeg', 'isPro': False, 'fullname': 'Wenxun Dai', 'user': 'Askar101', 'type': 'user'}, 'name': 'Wenxun Dai', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:35:08.579Z', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed314f', 'name': 'Jiaqi Li', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3150', 'user': {'_id': '64250b2bd476e4ad5568ea1b', 'avatarUrl': '/avatars/c770cd8a095140b33b9dcec9911fba13.svg', 'isPro': False, 'fullname': 'WangLinqing', 'user': 'WangLinqing', 'type': 'user'}, 'name': 'Linqing Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:35:21.856Z', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3151', 'user': {'_id': '65e2e93bfcaff433f7a87b43', 'avatarUrl': '/avatars/020e008a6a748df75227c51f331d3bce.svg', 'isPro': False, 'fullname': 'Qixun Wang', 'user': 'NOVAglow646', 'type': 'user'}, 'name': 'Qixun Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:35:28.984Z', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3152', 'user': {'_id': '67206bf75ff1582170e07153', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ktS3nDRZwIW04mAbqqmFY.png', 'isPro': False, 'fullname': 'Zhiyong Xu', 'user': 'TeddyXu2', 'type': 'user'}, 'name': 'Zhiyong Xu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:35:35.753Z', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3153', 'name': 'Yingfang Zhang', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3154', 'name': 'Jiangfeng Xiong', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3155', 'name': 'Weijie Kong', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3156', 'name': 'Chao Zhang', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3157', 'name': 'Hongxin Zhang', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3158', 'name': 'Qiaoling Zheng', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3159', 'name': 'Weiting Guo', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed315a', 'name': 'Xinchi Deng', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed315b', 'name': 'Yixuan Li', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed315c', 'name': 'Renjia Wei', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed315d', 'name': 'Yulin Jian', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed315e', 'name': 'Duojun Huang', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed315f', 'name': 'Xuhua Ren', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3160', 'name': 'Sihuan Lin', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3161', 'name': 'Yifu Sun', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3162', 'name': 'Yuan Zhou', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3163', 'name': 'Joey Wang', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3164', 'name': 'Qin Lin', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3165', 'name': 'Jingmiao Yu', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3166', 'name': 'Jihong Zhang', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3167', 'name': 'Caesar Zhong', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3168', 'name': 'Di Wang', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed3169', 'name': 'Yuhong Liu', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed316a', 'name': 'Linus', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed316b', 'name': 'Jie Jiang', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed316c', 'name': 'Longhuang Wu', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed316d', 'name': 'Shuai Shao', 'hidden': False}, {'_id': '682d43cf85d5e40c81ed316e', 'name': 'Qinglin Lu', 'hidden': False}], 'publishedAt': '2025-05-20T09:39:48.000Z', 'submittedOnDailyAt': '2025-05-21T01:40:24.172Z', 'title': 'Hunyuan-Game: Industrial-grade Intelligent Game Creation Model', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': 'Intelligent game creation represents a transformative advancement in game\\ndevelopment, utilizing generative artificial intelligence to dynamically\\ngenerate and enhance game content. Despite notable progress in generative\\nmodels, the comprehensive synthesis of high-quality game assets, including both\\nimages and videos, remains a challenging frontier. To create high-fidelity game\\ncontent that simultaneously aligns with player preferences and significantly\\nboosts designer efficiency, we present Hunyuan-Game, an innovative project\\ndesigned to revolutionize intelligent game production. Hunyuan-Game encompasses\\ntwo primary branches: image generation and video generation. The image\\ngeneration component is built upon a vast dataset comprising billions of game\\nimages, leading to the development of a group of customized image generation\\nmodels tailored for game scenarios: (1) General Text-to-Image Generation. (2)\\nGame Visual Effects Generation, involving text-to-effect and reference\\nimage-based game visual effect generation. (3) Transparent Image Generation for\\ncharacters, scenes, and game visual effects. (4) Game Character Generation\\nbased on sketches, black-and-white images, and white models. The video\\ngeneration component is built upon a comprehensive dataset of millions of game\\nand anime videos, leading to the development of five core algorithmic models,\\neach targeting critical pain points in game development and having robust\\nadaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)\\n360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)\\nGenerative Video Super-Resolution. (5) Interactive Game Video Generation. These\\nimage and video generation models not only exhibit high-level aesthetic\\nexpression but also deeply integrate domain-specific knowledge, establishing a\\nsystematic understanding of diverse game and anime art styles.', 'upvotes': 7, 'discussionId': '682d43d585d5e40c81ed3302', 'ai_keywords': ['text-to-image generation', 'game visual effects generation', 'text-to-effect', 'reference image-based generation', 'transparent image generation', 'character generation', 'sketch-based generation', 'black-and-white image-based generation', 'white model-based generation', 'image-to-video generation', '360 A/T Pose Avatar Video Synthesis', 'dynamic illustration generation', 'generative video super-resolution', 'interactive game video generation']}, 'publishedAt': '2025-05-20T05:39:48.000Z', 'title': 'Hunyuan-Game: Industrial-grade Intelligent Game Creation Model', 'summary': 'Intelligent game creation represents a transformative advancement in game\\ndevelopment, utilizing generative artificial intelligence to dynamically\\ngenerate and enhance game content. Despite notable progress in generative\\nmodels, the comprehensive synthesis of high-quality game assets, including both\\nimages and videos, remains a challenging frontier. To create high-fidelity game\\ncontent that simultaneously aligns with player preferences and significantly\\nboosts designer efficiency, we present Hunyuan-Game, an innovative project\\ndesigned to revolutionize intelligent game production. Hunyuan-Game encompasses\\ntwo primary branches: image generation and video generation. The image\\ngeneration component is built upon a vast dataset comprising billions of game\\nimages, leading to the development of a group of customized image generation\\nmodels tailored for game scenarios: (1) General Text-to-Image Generation. (2)\\nGame Visual Effects Generation, involving text-to-effect and reference\\nimage-based game visual effect generation. (3) Transparent Image Generation for\\ncharacters, scenes, and game visual effects. (4) Game Character Generation\\nbased on sketches, black-and-white images, and white models. The video\\ngeneration component is built upon a comprehensive dataset of millions of game\\nand anime videos, leading to the development of five core algorithmic models,\\neach targeting critical pain points in game development and having robust\\nadaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)\\n360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)\\nGenerative Video Super-Resolution. (5) Interactive Game Video Generation. These\\nimage and video generation models not only exhibit high-level aesthetic\\nexpression but also deeply integrate domain-specific knowledge, establishing a\\nsystematic understanding of diverse game and anime art styles.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14135.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6899}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2505.13430', 'authors': [{'_id': '682d70c598e3ea9be315ed85', 'user': {'_id': '66fd024b2f3b4b95701b3055', 'avatarUrl': '/avatars/7efa17e0ba1b8e2f5afabeba59ef7a6f.svg', 'isPro': False, 'fullname': 'Sifeng SHANG', 'user': 'sifengshang', 'type': 'user'}, 'name': 'Sifeng Shang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:38:00.611Z', 'hidden': False}, {'_id': '682d70c598e3ea9be315ed86', 'name': 'Jiayi Zhou', 'hidden': False}, {'_id': '682d70c598e3ea9be315ed87', 'user': {'_id': '644b40a9a49c03b1b68b7e48', 'avatarUrl': '/avatars/0fbe390e34a45441905f2c55c17ca1ea.svg', 'isPro': False, 'fullname': 'lin', 'user': 'chenyulin', 'type': 'user'}, 'name': 'Chenyu Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:38:15.674Z', 'hidden': False}, {'_id': '682d70c598e3ea9be315ed88', 'name': 'Minxian Li', 'hidden': False}, {'_id': '682d70c598e3ea9be315ed89', 'user': {'_id': '62ac6656de8bfbb93094b8fd', 'avatarUrl': '/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg', 'isPro': False, 'fullname': 'Kaiyang Zhou', 'user': 'kaiyangzhou', 'type': 'user'}, 'name': 'Kaiyang Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:38:21.725Z', 'hidden': False}], 'publishedAt': '2025-05-19T17:55:15.000Z', 'submittedOnDailyAt': '2025-05-21T04:51:33.821Z', 'title': 'Fine-tuning Quantized Neural Networks with Zeroth-order Optimization', 'submittedOnDailyBy': {'_id': '62ac6656de8bfbb93094b8fd', 'avatarUrl': '/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg', 'isPro': False, 'fullname': 'Kaiyang Zhou', 'user': 'kaiyangzhou', 'type': 'user'}, 'summary': 'As the size of large language models grows exponentially, GPU memory has\\nbecome a bottleneck for adapting these models to downstream tasks. In this\\npaper, we aim to push the limits of memory-efficient training by minimizing\\nmemory usage on model weights, gradients, and optimizer states, within a\\nunified framework. Our idea is to eliminate both gradients and optimizer states\\nusing zeroth-order optimization, which approximates gradients by perturbing\\nweights during forward passes to identify gradient directions. To minimize\\nmemory usage on weights, we employ model quantization, e.g., converting from\\nbfloat16 to int4. However, directly applying zeroth-order optimization to\\nquantized weights is infeasible due to the precision gap between discrete\\nweights and continuous gradients, which would otherwise require de-quantization\\nand re-quantization. To overcome this challenge, we propose Quantized\\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\\nquantization scale for gradient estimation and uses a directional derivative\\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\\nand codebook-based post-training quantization methods. Compared to\\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\\nmore than 18times for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\\nStable Diffusion 3.5 Large within a single 24GB GPU.', 'upvotes': 7, 'discussionId': '682d70c898e3ea9be315ee64', 'githubRepo': 'https://github.com/maifoundations/QZO', 'ai_keywords': ['zeroth-order optimization', 'gradient estimation', 'model quantization', 'bfloat16', 'int4', 'quantization scale', 'directional derivative clipping', 'Quantized Zeroth-order Optimization (QZO)', 'scalar-based post-training quantization', 'codebook-based post-training quantization', 'full-parameter fine-tuning', 'Llama-2-13B', 'Stable Diffusion 3.5 Large']}, 'publishedAt': '2025-05-19T13:55:15.000Z', 'title': 'Fine-tuning Quantized Neural Networks with Zeroth-order Optimization', 'summary': 'As the size of large language models grows exponentially, GPU memory has\\nbecome a bottleneck for adapting these models to downstream tasks. In this\\npaper, we aim to push the limits of memory-efficient training by minimizing\\nmemory usage on model weights, gradients, and optimizer states, within a\\nunified framework. Our idea is to eliminate both gradients and optimizer states\\nusing zeroth-order optimization, which approximates gradients by perturbing\\nweights during forward passes to identify gradient directions. To minimize\\nmemory usage on weights, we employ model quantization, e.g., converting from\\nbfloat16 to int4. However, directly applying zeroth-order optimization to\\nquantized weights is infeasible due to the precision gap between discrete\\nweights and continuous gradients, which would otherwise require de-quantization\\nand re-quantization. To overcome this challenge, we propose Quantized\\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\\nquantization scale for gradient estimation and uses a directional derivative\\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\\nand codebook-based post-training quantization methods. Compared to\\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\\nmore than 18times for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\\nStable Diffusion 3.5 Large within a single 24GB GPU.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13430.png', 'numComments': 1, 'submittedBy': {'_id': '62ac6656de8bfbb93094b8fd', 'avatarUrl': '/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg', 'fullname': 'Kaiyang Zhou', 'name': 'kaiyangzhou', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14464', 'authors': [{'_id': '682d39e6fa24196dcd10d5e8', 'user': {'_id': '621499d72be42a56cca7afad', 'avatarUrl': '/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg', 'isPro': False, 'fullname': 'TianXiaoyu', 'user': 'Emperorizzis', 'type': 'user'}, 'name': 'Xiaoyu Tian', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:56.337Z', 'hidden': False}, {'_id': '682d39e6fa24196dcd10d5e9', 'name': 'Yunjie Ji', 'hidden': False}, {'_id': '682d39e6fa24196dcd10d5ea', 'name': 'Haotian Wang', 'hidden': False}, {'_id': '682d39e6fa24196dcd10d5eb', 'name': 'Shuaiting Chen', 'hidden': False}, {'_id': '682d39e6fa24196dcd10d5ec', 'name': 'Sitong Zhao', 'hidden': False}, {'_id': '682d39e6fa24196dcd10d5ed', 'name': 'Yiping Peng', 'hidden': False}, {'_id': '682d39e6fa24196dcd10d5ee', 'name': 'Han Zhao', 'hidden': False}, {'_id': '682d39e6fa24196dcd10d5ef', 'name': 'Xiangang Li', 'hidden': False}], 'publishedAt': '2025-05-20T15:00:51.000Z', 'submittedOnDailyAt': '2025-05-21T01:40:28.265Z', 'title': 'Not All Correct Answers Are Equal: Why Your Distillation Source Matters', 'submittedOnDailyBy': {'_id': '621499d72be42a56cca7afad', 'avatarUrl': '/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg', 'isPro': False, 'fullname': 'TianXiaoyu', 'user': 'Emperorizzis', 'type': 'user'}, 'summary': 'Distillation has emerged as a practical and effective approach to enhance the\\nreasoning capabilities of open-source language models. In this work, we conduct\\na large-scale empirical study on reasoning data distillation by collecting\\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\\nconstruct three parallel datasets and analyze their distributions, revealing\\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\\nlower perplexity. Student models trained on each dataset are evaluated on\\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\\ndemonstrates adaptive output behavior-producing longer responses for harder\\ntasks and shorter ones for simpler tasks. These findings highlight the value of\\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\\nQwen3-235B-A22B distilled datasets to support future research on open and\\nhigh-performing reasoning-oriented language models. The datasets are publicly\\navailable on Hugging FaceDatasets are available on Hugging Face:\\n\\\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled},\\nhttps://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.', 'upvotes': 6, 'discussionId': '682d39e8fa24196dcd10d643', 'ai_keywords': ['distillation', 'reasoning capabilities', 'open-source language models', 'empirical study', 'reasoning data distillation', 'AM-Thinking-v1', 'Qwen3-235B-A22B', 'DeepSeek-R1', 'shared corpus', 'parallel datasets', 'token length diversity', 'perplexity', 'student models', 'reasoning benchmarks', 'AIME2024', 'AIME2025', 'MATH500', 'LiveCodeBench', 'adaptive output behavior']}, 'publishedAt': '2025-05-20T11:00:51.000Z', 'title': 'Not All Correct Answers Are Equal: Why Your Distillation Source Matters', 'summary': 'Distillation has emerged as a practical and effective approach to enhance the\\nreasoning capabilities of open-source language models. In this work, we conduct\\na large-scale empirical study on reasoning data distillation by collecting\\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\\nconstruct three parallel datasets and analyze their distributions, revealing\\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\\nlower perplexity. Student models trained on each dataset are evaluated on\\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\\ndemonstrates adaptive output behavior-producing longer responses for harder\\ntasks and shorter ones for simpler tasks. These findings highlight the value of\\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\\nQwen3-235B-A22B distilled datasets to support future research on open and\\nhigh-performing reasoning-oriented language models. The datasets are publicly\\navailable on Hugging FaceDatasets are available on Hugging Face:\\n\\\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled},\\nhttps://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14464.png', 'numComments': 1, 'submittedBy': {'_id': '621499d72be42a56cca7afad', 'avatarUrl': '/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg', 'fullname': 'TianXiaoyu', 'name': 'Emperorizzis', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.12448', 'authors': [{'_id': '682d47aa64daf8623f1f5604', 'user': {'_id': '6586817e509bcae23f3dfc60', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pwEStoey1XJYDi3F0Z930.png', 'isPro': False, 'fullname': 'Yang Liu', 'user': 'yliu-cs', 'type': 'user'}, 'name': 'Yang Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:37.342Z', 'hidden': False}, {'_id': '682d47aa64daf8623f1f5605', 'name': 'Ming Ma', 'hidden': False}, {'_id': '682d47aa64daf8623f1f5606', 'name': 'Xiaomin Yu', 'hidden': False}, {'_id': '682d47aa64daf8623f1f5607', 'name': 'Pengxiang Ding', 'hidden': False}, {'_id': '682d47aa64daf8623f1f5608', 'name': 'Han Zhao', 'hidden': False}, {'_id': '682d47aa64daf8623f1f5609', 'name': 'Mingyang Sun', 'hidden': False}, {'_id': '682d47aa64daf8623f1f560a', 'name': 'Siteng Huang', 'hidden': False}, {'_id': '682d47aa64daf8623f1f560b', 'name': 'Donglin Wang', 'hidden': False}], 'publishedAt': '2025-05-18T14:40:16.000Z', 'submittedOnDailyAt': '2025-05-21T01:56:36.099Z', 'title': 'SSR: Enhancing Depth Perception in Vision-Language Models via\\n  Rationale-Guided Spatial Reasoning', 'submittedOnDailyBy': {'_id': '65fd82762bf2cd20ddaa193f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png', 'isPro': False, 'fullname': 'Siteng Huang', 'user': 'huangsiteng', 'type': 'user'}, 'summary': 'Despite impressive advancements in Visual-Language Models (VLMs) for\\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\\nunderstanding. Existing methods for integrating spatial cues, such as point\\nclouds or depth, either require specialized sensors or fail to effectively\\nexploit depth information for higher-order reasoning. To this end, we propose a\\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\\ntransforms raw depth data into structured, interpretable textual rationales.\\nThese textual rationales serve as meaningful intermediate representations to\\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\\nknowledge distillation to compress the generated rationales into compact latent\\nembeddings, which facilitate resource-efficient and plug-and-play integration\\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\\non multiple benchmarks demonstrate SSR substantially improves depth utilization\\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\\nmulti-modal understanding. Our project page is at\\nhttps://yliu-cs.github.io/SSR.', 'upvotes': 6, 'discussionId': '682d47ab64daf8623f1f5634', 'projectPage': 'https://yliu-cs.github.io/SSR/', 'githubRepo': 'https://github.com/yliu-cs/SSR', 'ai_keywords': ['Spatial Sense and Reasoning (SSR)', 'structured, interpretable textual rationales', 'knowledge distillation', 'compact latent embeddings', 'resource-efficient integration', 'SSR-CoT', 'million-scale visual-language reasoning dataset', 'intermediate spatial reasoning annotations', 'SSRBench', 'comprehensive multi-task benchmark', 'depth utilization', 'spatial reasoning', 'human-like multi-modal understanding']}, 'publishedAt': '2025-05-18T10:40:16.000Z', 'title': 'SSR: Enhancing Depth Perception in Vision-Language Models via\\n  Rationale-Guided Spatial Reasoning', 'summary': 'Despite impressive advancements in Visual-Language Models (VLMs) for\\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\\nunderstanding. Existing methods for integrating spatial cues, such as point\\nclouds or depth, either require specialized sensors or fail to effectively\\nexploit depth information for higher-order reasoning. To this end, we propose a\\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\\ntransforms raw depth data into structured, interpretable textual rationales.\\nThese textual rationales serve as meaningful intermediate representations to\\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\\nknowledge distillation to compress the generated rationales into compact latent\\nembeddings, which facilitate resource-efficient and plug-and-play integration\\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\\non multiple benchmarks demonstrate SSR substantially improves depth utilization\\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\\nmulti-modal understanding. Our project page is at\\nhttps://yliu-cs.github.io/SSR.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12448.png', 'numComments': 1, 'submittedBy': {'_id': '65fd82762bf2cd20ddaa193f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png', 'fullname': 'Siteng Huang', 'name': 'huangsiteng', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2505.14534', 'authors': [{'_id': '682d7f009a06bc9f9106480b', 'user': {'_id': '65ef0b4b325d9aaef87eb33b', 'avatarUrl': '/avatars/ec27dba9de7e74c1f6cdcacf5aa25528.svg', 'isPro': False, 'fullname': 'C Shi', 'user': 'chongyangs', 'type': 'user'}, 'name': 'Chongyang Shi', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-21T07:21:37.326Z', 'hidden': False}, {'_id': '682d7f009a06bc9f9106480c', 'name': 'Sharon Lin', 'hidden': False}, {'_id': '682d7f009a06bc9f9106480d', 'name': 'Shuang Song', 'hidden': False}, {'_id': '682d7f009a06bc9f9106480e', 'name': 'Jamie Hayes', 'hidden': False}, {'_id': '682d7f009a06bc9f9106480f', 'user': {'_id': '6475c2794766357252e69e9f', 'avatarUrl': '/avatars/db428715dfd2239df2aeaaff1282323f.svg', 'isPro': False, 'fullname': 'i', 'user': 'iliashum', 'type': 'user'}, 'name': 'Ilia Shumailov', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:31:09.839Z', 'hidden': False}, {'_id': '682d7f009a06bc9f91064810', 'user': {'_id': '62697edaa6a7bba9e46ae4ad', 'avatarUrl': '/avatars/3009bf769a09d946447a0e8c833a04f3.svg', 'isPro': False, 'fullname': 'Itay Yona', 'user': 'tux', 'type': 'user'}, 'name': 'Itay Yona', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:31:16.965Z', 'hidden': False}, {'_id': '682d7f009a06bc9f91064811', 'user': {'_id': '657d17d43480ce8aae628d40', 'avatarUrl': '/avatars/7f8cd9953f82e5a7ad5db1f82cfdb8f1.svg', 'isPro': False, 'fullname': 'juliette pluto', 'user': 'julsh', 'type': 'user'}, 'name': 'Juliette Pluto', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:31:22.677Z', 'hidden': False}, {'_id': '682d7f009a06bc9f91064812', 'name': 'Aneesh Pappu', 'hidden': False}, {'_id': '682d7f009a06bc9f91064813', 'user': {'_id': '649c5307e89a7650158f4965', 'avatarUrl': '/avatars/3c71b501db27c86ed686ae26ad1391cd.svg', 'isPro': False, 'fullname': 'Christopher A. Choquette-Choo', 'user': 'cchoquette', 'type': 'user'}, 'name': 'Christopher A. Choquette-Choo', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:31:33.340Z', 'hidden': False}, {'_id': '682d7f009a06bc9f91064814', 'name': 'Milad Nasr', 'hidden': False}, {'_id': '682d7f009a06bc9f91064815', 'user': {'_id': '623e3ebef3be5bae969680c0', 'avatarUrl': '/avatars/590c264bf9964225bd67f61a3a5d53ea.svg', 'isPro': False, 'fullname': 'Chawin Sitawarin', 'user': 'chawins', 'type': 'user'}, 'name': 'Chawin Sitawarin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:31:58.593Z', 'hidden': False}, {'_id': '682d7f009a06bc9f91064816', 'name': 'Gena Gibson', 'hidden': False}, {'_id': '682d7f009a06bc9f91064817', 'name': 'Andreas Terzis', 'hidden': False}, {'_id': '682d7f009a06bc9f91064818', 'name': 'John \"Four\" Flynn', 'hidden': False}], 'publishedAt': '2025-05-20T15:54:45.000Z', 'submittedOnDailyAt': '2025-05-21T05:52:22.988Z', 'title': 'Lessons from Defending Gemini Against Indirect Prompt Injections', 'submittedOnDailyBy': {'_id': '6475c2794766357252e69e9f', 'avatarUrl': '/avatars/db428715dfd2239df2aeaaff1282323f.svg', 'isPro': False, 'fullname': 'i', 'user': 'iliashum', 'type': 'user'}, 'summary': \"Gemini is increasingly used to perform tasks on behalf of users, where\\nfunction-calling and tool-use capabilities enable the model to access user\\ndata. Some tools, however, require access to untrusted data introducing risk.\\nAdversaries can embed malicious instructions in untrusted data which cause the\\nmodel to deviate from the user's expectations and mishandle their data or\\npermissions. In this report, we set out Google DeepMind's approach to\\nevaluating the adversarial robustness of Gemini models and describe the main\\nlessons learned from the process. We test how Gemini performs against a\\nsophisticated adversary through an adversarial evaluation framework, which\\ndeploys a suite of adaptive attack techniques to run continuously against past,\\ncurrent, and future versions of Gemini. We describe how these ongoing\\nevaluations directly help make Gemini more resilient against manipulation.\", 'upvotes': 5, 'discussionId': '682d7f019a06bc9f9106486e', 'ai_keywords': ['function-calling', 'tool-use capabilities', 'adversarial robustness', 'adaptive attack techniques', 'resilience']}, 'publishedAt': '2025-05-20T11:54:45.000Z', 'title': 'Lessons from Defending Gemini Against Indirect Prompt Injections', 'summary': \"Gemini is increasingly used to perform tasks on behalf of users, where\\nfunction-calling and tool-use capabilities enable the model to access user\\ndata. Some tools, however, require access to untrusted data introducing risk.\\nAdversaries can embed malicious instructions in untrusted data which cause the\\nmodel to deviate from the user's expectations and mishandle their data or\\npermissions. In this report, we set out Google DeepMind's approach to\\nevaluating the adversarial robustness of Gemini models and describe the main\\nlessons learned from the process. We test how Gemini performs against a\\nsophisticated adversary through an adversarial evaluation framework, which\\ndeploys a suite of adaptive attack techniques to run continuously against past,\\ncurrent, and future versions of Gemini. We describe how these ongoing\\nevaluations directly help make Gemini more resilient against manipulation.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14534.png', 'numComments': 1, 'submittedBy': {'_id': '6475c2794766357252e69e9f', 'avatarUrl': '/avatars/db428715dfd2239df2aeaaff1282323f.svg', 'fullname': 'i', 'name': 'iliashum', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14352', 'authors': [{'_id': '682d72b0d57ba1e4d132148d', 'user': {'_id': '6422f416a73327caad9d1d86', 'avatarUrl': '/avatars/aa3639277cd1732504402fc64a57eff8.svg', 'isPro': False, 'fullname': 'Bartosz Cywiński', 'user': 'bcywinski', 'type': 'user'}, 'name': 'Bartosz Cywiński', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:39:48.008Z', 'hidden': False}, {'_id': '682d72b0d57ba1e4d132148e', 'name': 'Emil Ryd', 'hidden': False}, {'_id': '682d72b0d57ba1e4d132148f', 'user': {'_id': '6477122c99a5ce743ccf2f55', 'avatarUrl': '/avatars/dee617374fc609e07eba3bcb2cd16810.svg', 'isPro': False, 'fullname': 'Senthooran Rajamanoharan', 'user': 'srdm', 'type': 'user'}, 'name': 'Senthooran Rajamanoharan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:37:33.691Z', 'hidden': False}, {'_id': '682d72b0d57ba1e4d1321490', 'user': {'_id': '62669380c8bc5cf80ca97350', 'avatarUrl': '/avatars/6d5cd2261163308b82341c1ce28984d1.svg', 'isPro': False, 'fullname': 'Neel Nanda', 'user': 'NeelNanda', 'type': 'user'}, 'name': 'Neel Nanda', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:37:18.664Z', 'hidden': False}], 'publishedAt': '2025-05-20T13:36:37.000Z', 'submittedOnDailyAt': '2025-05-21T04:59:37.379Z', 'title': 'Towards eliciting latent knowledge from LLMs with mechanistic\\n  interpretability', 'submittedOnDailyBy': {'_id': '6422f416a73327caad9d1d86', 'avatarUrl': '/avatars/aa3639277cd1732504402fc64a57eff8.svg', 'isPro': False, 'fullname': 'Bartosz Cywiński', 'user': 'bcywinski', 'type': 'user'}, 'summary': 'As language models become more powerful and sophisticated, it is crucial that\\nthey remain trustworthy and reliable. There is concerning preliminary evidence\\nthat models may attempt to deceive or keep secrets from their operators. To\\nexplore the ability of current techniques to elicit such hidden knowledge, we\\ntrain a Taboo model: a language model that describes a specific secret word\\nwithout explicitly stating it. Importantly, the secret word is not presented to\\nthe model in its training data or prompt. We then investigate methods to\\nuncover this secret. First, we evaluate non-interpretability (black-box)\\napproaches. Subsequently, we develop largely automated strategies based on\\nmechanistic interpretability techniques, including logit lens and sparse\\nautoencoders. Evaluation shows that both approaches are effective in eliciting\\nthe secret word in our proof-of-concept setting. Our findings highlight the\\npromise of these approaches for eliciting hidden knowledge and suggest several\\npromising avenues for future work, including testing and refining these methods\\non more complex model organisms. This work aims to be a step towards addressing\\nthe crucial problem of eliciting secret knowledge from language models, thereby\\ncontributing to their safe and reliable deployment.', 'upvotes': 5, 'discussionId': '682d72b1d57ba1e4d13214c6', 'githubRepo': 'https://github.com/EmilRyd/eliciting-secrets', 'ai_keywords': ['Taboo model', 'logit lens', 'sparse autoencoders']}, 'publishedAt': '2025-05-20T09:36:37.000Z', 'title': 'Towards eliciting latent knowledge from LLMs with mechanistic\\n  interpretability', 'summary': 'As language models become more powerful and sophisticated, it is crucial that\\nthey remain trustworthy and reliable. There is concerning preliminary evidence\\nthat models may attempt to deceive or keep secrets from their operators. To\\nexplore the ability of current techniques to elicit such hidden knowledge, we\\ntrain a Taboo model: a language model that describes a specific secret word\\nwithout explicitly stating it. Importantly, the secret word is not presented to\\nthe model in its training data or prompt. We then investigate methods to\\nuncover this secret. First, we evaluate non-interpretability (black-box)\\napproaches. Subsequently, we develop largely automated strategies based on\\nmechanistic interpretability techniques, including logit lens and sparse\\nautoencoders. Evaluation shows that both approaches are effective in eliciting\\nthe secret word in our proof-of-concept setting. Our findings highlight the\\npromise of these approaches for eliciting hidden knowledge and suggest several\\npromising avenues for future work, including testing and refining these methods\\non more complex model organisms. This work aims to be a step towards addressing\\nthe crucial problem of eliciting secret knowledge from language models, thereby\\ncontributing to their safe and reliable deployment.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14352.png', 'numComments': 1, 'submittedBy': {'_id': '6422f416a73327caad9d1d86', 'avatarUrl': '/avatars/aa3639277cd1732504402fc64a57eff8.svg', 'fullname': 'Bartosz Cywiński', 'name': 'bcywinski', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.13718', 'authors': [{'_id': '682d6ccc26146f27d1ab3d83', 'user': {'_id': '64cb922ec7f30fbf7b91a9a7', 'avatarUrl': '/avatars/457eae5e56b9641ee5543146447d1755.svg', 'isPro': False, 'fullname': 'Safal Shrestha', 'user': 'safal312', 'type': 'user'}, 'name': 'Safal Shrestha', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:07.374Z', 'hidden': False}, {'_id': '682d6ccc26146f27d1ab3d84', 'user': {'_id': '64e77b47d96966317b45eeb3', 'avatarUrl': '/avatars/6b67eba3f15d6cd86ac3ad55c1daf166.svg', 'isPro': False, 'fullname': 'Minwu Kim', 'user': 'guactastesgood', 'type': 'user'}, 'name': 'Minwu Kim', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:27:16.736Z', 'hidden': False}, {'_id': '682d6ccc26146f27d1ab3d85', 'user': {'_id': '67b5b0800918c8645fea09d7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_6pKdkxjsC0dOBbKAN3ok.png', 'isPro': False, 'fullname': 'Aadim Nepal', 'user': 'AadimNepal', 'type': 'user'}, 'name': 'Aadim Nepal', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:27:23.895Z', 'hidden': False}, {'_id': '682d6ccc26146f27d1ab3d86', 'user': {'_id': '66a0394001cfae79a11b54ea', 'avatarUrl': '/avatars/e135cca9b9027cfb762e7ff86bd5ab5a.svg', 'isPro': False, 'fullname': 'Anubhav Shrestha', 'user': 'xanubhav81', 'type': 'user'}, 'name': 'Anubhav Shrestha', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:27:30.010Z', 'hidden': False}, {'_id': '682d6ccc26146f27d1ab3d87', 'name': 'Keith Ross', 'hidden': False}], 'publishedAt': '2025-05-19T20:29:15.000Z', 'submittedOnDailyAt': '2025-05-21T04:35:24.229Z', 'title': 'Warm Up Before You Train: Unlocking General Reasoning in\\n  Resource-Constrained Settings', 'submittedOnDailyBy': {'_id': '64cb922ec7f30fbf7b91a9a7', 'avatarUrl': '/avatars/457eae5e56b9641ee5543146447d1755.svg', 'isPro': False, 'fullname': 'Safal Shrestha', 'user': 'safal312', 'type': 'user'}, 'summary': 'Designing effective reasoning-capable LLMs typically requires training using\\nReinforcement Learning with Verifiable Rewards (RLVR) or distillation with\\ncarefully curated Long Chain of Thoughts (CoT), both of which depend heavily on\\nextensive training data. This creates a major challenge when the amount of\\nquality training data is scarce. We propose a sample-efficient, two-stage\\ntraining strategy to develop reasoning LLMs under limited supervision. In the\\nfirst stage, we \"warm up\" the model by distilling Long CoTs from a toy domain,\\nnamely, Knights \\\\& Knaves (K\\\\&K) logic puzzles to acquire general reasoning\\nskills. In the second stage, we apply RLVR to the warmed-up model using a\\nlimited set of target-domain examples. Our experiments demonstrate that this\\ntwo-phase approach offers several benefits: (i) the warmup phase alone\\nfacilitates generalized reasoning, leading to performance improvements across a\\nrange of tasks, including MATH, HumanEval^{+}, and MMLU-Pro. (ii) When both\\nthe base model and the warmed-up model are RLVR trained on the same small\\ndataset (leq100 examples), the warmed-up model consistently outperforms the\\nbase model; (iii) Warming up before RLVR training allows a model to maintain\\ncross-domain generalizability even after training on a specific domain; (iv)\\nIntroducing warmup in the pipeline improves not only accuracy but also overall\\nsample efficiency during RLVR training. The results in this paper highlight the\\npromise of warmup for building robust reasoning LLMs in data-scarce\\nenvironments.', 'upvotes': 5, 'discussionId': '682d6ccd26146f27d1ab3dbb', 'githubRepo': 'https://github.com/safal312/warmup-before-you-train', 'ai_keywords': ['Reinforcement Learning with Verifiable Rewards (RLVR)', 'Long Chain of Thoughts (CoT)', 'Knights \\\\& Knaves (K\\\\&K) logic puzzles', 'MATH', 'HumanEval$^{+}$', 'MMLU-Pro', 'warmup phase', 'generalized reasoning', 'cross-domain generalizability', 'sample efficiency', 'robust reasoning LLMs']}, 'publishedAt': '2025-05-19T16:29:15.000Z', 'title': 'Warm Up Before You Train: Unlocking General Reasoning in\\n  Resource-Constrained Settings', 'summary': 'Designing effective reasoning-capable LLMs typically requires training using\\nReinforcement Learning with Verifiable Rewards (RLVR) or distillation with\\ncarefully curated Long Chain of Thoughts (CoT), both of which depend heavily on\\nextensive training data. This creates a major challenge when the amount of\\nquality training data is scarce. We propose a sample-efficient, two-stage\\ntraining strategy to develop reasoning LLMs under limited supervision. In the\\nfirst stage, we \"warm up\" the model by distilling Long CoTs from a toy domain,\\nnamely, Knights \\\\& Knaves (K\\\\&K) logic puzzles to acquire general reasoning\\nskills. In the second stage, we apply RLVR to the warmed-up model using a\\nlimited set of target-domain examples. Our experiments demonstrate that this\\ntwo-phase approach offers several benefits: (i) the warmup phase alone\\nfacilitates generalized reasoning, leading to performance improvements across a\\nrange of tasks, including MATH, HumanEval^{+}, and MMLU-Pro. (ii) When both\\nthe base model and the warmed-up model are RLVR trained on the same small\\ndataset (leq100 examples), the warmed-up model consistently outperforms the\\nbase model; (iii) Warming up before RLVR training allows a model to maintain\\ncross-domain generalizability even after training on a specific domain; (iv)\\nIntroducing warmup in the pipeline improves not only accuracy but also overall\\nsample efficiency during RLVR training. The results in this paper highlight the\\npromise of warmup for building robust reasoning LLMs in data-scarce\\nenvironments.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13718.png', 'numComments': 1, 'submittedBy': {'_id': '64cb922ec7f30fbf7b91a9a7', 'avatarUrl': '/avatars/457eae5e56b9641ee5543146447d1755.svg', 'fullname': 'Safal Shrestha', 'name': 'safal312', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.12182', 'authors': [{'_id': '682d31dd17608739046e1169', 'user': {'_id': '634cabd104491d9f7111eea3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/634cabd104491d9f7111eea3/JoqlugwfD1aGkd-wZTmP7.jpeg', 'isPro': False, 'fullname': 'Haohang Li', 'user': 'Acatsama', 'type': 'user'}, 'name': 'Haohang Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:29:09.565Z', 'hidden': False}, {'_id': '682d31dd17608739046e116a', 'user': {'_id': '62dd8f328456396d4f8aa894', 'avatarUrl': '/avatars/af8f5dc7ff937e3e849ecdfd9ca4750b.svg', 'isPro': False, 'fullname': 'Yupeng Cao', 'user': 'YupengCao', 'type': 'user'}, 'name': 'Yupeng Cao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:29:23.338Z', 'hidden': False}, {'_id': '682d31dd17608739046e116b', 'user': {'_id': '64f757c6016d60f3199ef5e6', 'avatarUrl': '/avatars/2659ba698081265d0480b08161718013.svg', 'isPro': False, 'fullname': 'Yangyang Yu', 'user': 'ShirleyY', 'type': 'user'}, 'name': 'Yangyang Yu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:29:31.615Z', 'hidden': False}, {'_id': '682d31dd17608739046e116c', 'user': {'_id': '631a8c9e0867652f538a64e5', 'avatarUrl': '/avatars/09e1cef9e7d6f7c51579448d4605850e.svg', 'isPro': False, 'fullname': 'Jordan Suchow', 'user': 'jordansuchow', 'type': 'user'}, 'name': 'Jordan W. Suchow', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:29:41.392Z', 'hidden': False}, {'_id': '682d31dd17608739046e116d', 'user': {'_id': '62d63a9dc5ada8ef841b4787', 'avatarUrl': '/avatars/a79a4ac07984d9a8623c99bdce9add54.svg', 'isPro': False, 'fullname': 'Zining Zhu', 'user': 'ZiningZhu', 'type': 'user'}, 'name': 'Zining Zhu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:29:50.430Z', 'hidden': False}], 'publishedAt': '2025-05-18T00:47:21.000Z', 'submittedOnDailyAt': '2025-05-21T00:23:07.653Z', 'title': 'Truth Neurons', 'submittedOnDailyBy': {'_id': '634cabd104491d9f7111eea3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/634cabd104491d9f7111eea3/JoqlugwfD1aGkd-wZTmP7.jpeg', 'isPro': False, 'fullname': 'Haohang Li', 'user': 'Acatsama', 'type': 'user'}, 'summary': 'Despite their remarkable success and deployment across diverse workflows,\\nlanguage models sometimes produce untruthful responses. Our limited\\nunderstanding of how truthfulness is mechanistically encoded within these\\nmodels jeopardizes their reliability and safety. In this paper, we propose a\\nmethod for identifying representations of truthfulness at the neuron level. We\\nshow that language models contain truth neurons, which encode truthfulness in a\\nsubject-agnostic manner. Experiments conducted across models of varying scales\\nvalidate the existence of truth neurons, confirming that the encoding of\\ntruthfulness at the neuron level is a property shared by many language models.\\nThe distribution patterns of truth neurons over layers align with prior\\nfindings on the geometry of truthfulness. Selectively suppressing the\\nactivations of truth neurons found through the TruthfulQA dataset degrades\\nperformance both on TruthfulQA and on other benchmarks, showing that the\\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\\nnovel insights into the mechanisms underlying truthfulness in language models\\nand highlight potential directions toward improving their trustworthiness and\\nreliability.', 'upvotes': 5, 'discussionId': '682d31de17608739046e11c9', 'ai_keywords': ['truth neurons', 'neuron level', 'truthfulness mechanisms', 'TruthfulQA dataset']}, 'publishedAt': '2025-05-17T20:47:21.000Z', 'title': 'Truth Neurons', 'summary': 'Despite their remarkable success and deployment across diverse workflows,\\nlanguage models sometimes produce untruthful responses. Our limited\\nunderstanding of how truthfulness is mechanistically encoded within these\\nmodels jeopardizes their reliability and safety. In this paper, we propose a\\nmethod for identifying representations of truthfulness at the neuron level. We\\nshow that language models contain truth neurons, which encode truthfulness in a\\nsubject-agnostic manner. Experiments conducted across models of varying scales\\nvalidate the existence of truth neurons, confirming that the encoding of\\ntruthfulness at the neuron level is a property shared by many language models.\\nThe distribution patterns of truth neurons over layers align with prior\\nfindings on the geometry of truthfulness. Selectively suppressing the\\nactivations of truth neurons found through the TruthfulQA dataset degrades\\nperformance both on TruthfulQA and on other benchmarks, showing that the\\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\\nnovel insights into the mechanisms underlying truthfulness in language models\\nand highlight potential directions toward improving their trustworthiness and\\nreliability.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12182.png', 'numComments': 1, 'submittedBy': {'_id': '634cabd104491d9f7111eea3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/634cabd104491d9f7111eea3/JoqlugwfD1aGkd-wZTmP7.jpeg', 'fullname': 'Haohang Li', 'name': 'Acatsama', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14681', 'authors': [{'_id': '682db6f3975206d1caadf1cd', 'name': 'Mengru Wang', 'hidden': False}, {'_id': '682db6f3975206d1caadf1ce', 'name': 'Xingyu Chen', 'hidden': False}, {'_id': '682db6f3975206d1caadf1cf', 'name': 'Yue Wang', 'hidden': False}, {'_id': '682db6f3975206d1caadf1d0', 'name': 'Zhiwei He', 'hidden': False}, {'_id': '682db6f3975206d1caadf1d1', 'name': 'Jiahao Xu', 'hidden': False}, {'_id': '682db6f3975206d1caadf1d2', 'name': 'Tian Liang', 'hidden': False}, {'_id': '682db6f3975206d1caadf1d3', 'name': 'Qiuzhi Liu', 'hidden': False}, {'_id': '682db6f3975206d1caadf1d4', 'name': 'Yunzhi Yao', 'hidden': False}, {'_id': '682db6f3975206d1caadf1d5', 'name': 'Wenxuan Wang', 'hidden': False}, {'_id': '682db6f3975206d1caadf1d6', 'name': 'Ruotian Ma', 'hidden': False}, {'_id': '682db6f3975206d1caadf1d7', 'name': 'Haitao Mi', 'hidden': False}, {'_id': '682db6f3975206d1caadf1d8', 'user': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'isPro': False, 'fullname': 'Ningyu Zhang', 'user': 'Ningyu', 'type': 'user'}, 'name': 'Ningyu Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T12:26:50.565Z', 'hidden': False}, {'_id': '682db6f3975206d1caadf1d9', 'name': 'Zhaopeng Tu', 'hidden': False}, {'_id': '682db6f3975206d1caadf1da', 'name': 'Xiaolong Li', 'hidden': False}, {'_id': '682db6f3975206d1caadf1db', 'name': 'Dong Yu', 'hidden': False}], 'publishedAt': '2025-05-20T17:59:16.000Z', 'submittedOnDailyAt': '2025-05-21T09:51:44.990Z', 'title': 'Two Experts Are All You Need for Steering Thinking: Reinforcing\\n  Cognitive Effort in MoE Reasoning Models Without Additional Training', 'submittedOnDailyBy': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'isPro': False, 'fullname': 'Ningyu Zhang', 'user': 'Ningyu', 'type': 'user'}, 'summary': \"Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\\nhave achieved impressive reasoning capabilities by selectively activating\\nexperts to facilitate structured cognitive processes. Despite notable advances,\\nexisting reasoning models often suffer from cognitive inefficiencies like\\noverthinking and underthinking. To address these limitations, we introduce a\\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\\n(RICE), designed to improve reasoning performance without additional training\\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\\n(nPMI), we systematically identify specialized experts, termed ''cognitive\\nexperts'' that orchestrate meta-level reasoning operations characterized by\\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\\nlightweight approach substantially outperforms prevalent reasoning-steering\\ntechniques, such as prompt design and decoding constraints, while preserving\\nthe model's general instruction-following skills. These results highlight\\nreinforcing cognitive experts as a promising, practical, and interpretable\\ndirection to enhance cognitive efficiency within advanced reasoning models.\", 'upvotes': 4, 'discussionId': '682db6f4975206d1caadf21c', 'ai_keywords': ['Mixture-of-Experts (MoE)', 'Large Reasoning Models (LRMs)', 'reasoning capabilities', 'selective activation', 'structured cognitive processes', 'cognitive inefficiencies', 'Reinforcing Cognitive Experts (RICE)', 'inference-time steering', 'normalized Pointwise Mutual Information (nPMI)', 'cognitive experts', 'meta-level reasoning operations', 'tokens', '<think>', 'empirical evaluations', 'quantitative and scientific reasoning benchmarks', 'reasoning accuracy', 'cognitive efficiency', 'cross-domain generalization', 'lightweight approach', 'prompt design', 'decoding constraints', 'instruction-following skills']}, 'publishedAt': '2025-05-20T13:59:16.000Z', 'title': 'Two Experts Are All You Need for Steering Thinking: Reinforcing\\n  Cognitive Effort in MoE Reasoning Models Without Additional Training', 'summary': \"Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\\nhave achieved impressive reasoning capabilities by selectively activating\\nexperts to facilitate structured cognitive processes. Despite notable advances,\\nexisting reasoning models often suffer from cognitive inefficiencies like\\noverthinking and underthinking. To address these limitations, we introduce a\\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\\n(RICE), designed to improve reasoning performance without additional training\\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\\n(nPMI), we systematically identify specialized experts, termed ''cognitive\\nexperts'' that orchestrate meta-level reasoning operations characterized by\\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\\nlightweight approach substantially outperforms prevalent reasoning-steering\\ntechniques, such as prompt design and decoding constraints, while preserving\\nthe model's general instruction-following skills. These results highlight\\nreinforcing cognitive experts as a promising, practical, and interpretable\\ndirection to enhance cognitive efficiency within advanced reasoning models.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14681.png', 'numComments': 1, 'submittedBy': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'fullname': 'Ningyu Zhang', 'name': 'Ningyu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 22}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.13103', 'authors': [{'_id': '682d7f7a176087390484a412', 'name': 'Han Zheng', 'hidden': False}, {'_id': '682d7f7a176087390484a413', 'user': {'_id': '6475c2794766357252e69e9f', 'avatarUrl': '/avatars/db428715dfd2239df2aeaaff1282323f.svg', 'isPro': False, 'fullname': 'i', 'user': 'iliashum', 'type': 'user'}, 'name': 'Ilia Shumailov', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:25:55.989Z', 'hidden': False}, {'_id': '682d7f7a176087390484a414', 'name': 'Tianqi Fan', 'hidden': False}, {'_id': '682d7f7a176087390484a415', 'name': 'Aiden Hall', 'hidden': False}, {'_id': '682d7f7a176087390484a416', 'name': 'Mathias Payer', 'hidden': False}], 'publishedAt': '2025-05-19T13:32:51.000Z', 'submittedOnDailyAt': '2025-05-21T05:55:51.238Z', 'title': 'Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair', 'submittedOnDailyBy': {'_id': '6475c2794766357252e69e9f', 'avatarUrl': '/avatars/db428715dfd2239df2aeaaff1282323f.svg', 'isPro': False, 'fullname': 'i', 'user': 'iliashum', 'type': 'user'}, 'summary': 'The rapid advancement of bug-finding techniques has led to the discovery of\\nmore vulnerabilities than developers can reasonably fix, creating an urgent\\nneed for effective Automated Program Repair (APR) methods. However, the\\ncomplexity of modern bugs often makes precise root cause analysis difficult and\\nunreliable. To address this challenge, we propose crash-site repair to simplify\\nthe repair task while still mitigating the risk of exploitation. In addition,\\nwe introduce a template-guided patch generation approach that significantly\\nreduces the token cost of Large Language Models (LLMs) while maintaining both\\nefficiency and effectiveness.\\n  We implement our prototype system, WILLIAMT, and evaluate it against\\nstate-of-the-art APR tools. Our results show that, when combined with the\\ntop-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and\\nincreases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open\\nsource software vulnerabilities benchmark. Furthermore, we demonstrate that\\nWILLIAMT can function effectively even without access to frontier LLMs: even a\\nlocal model running on a Mac M4 Mini achieves a reasonable repair rate. These\\nfindings highlight the broad applicability and scalability of WILLIAMT.', 'upvotes': 4, 'discussionId': '682d7f7a176087390484a448', 'ai_keywords': ['crash-site repair', 'template-guided patch generation', 'Large Language Models (LLMs)', 'WILLIAMT', 'CodeRover-S', 'ARVO', 'ground-truth open source software vulnerabilities benchmark']}, 'publishedAt': '2025-05-19T09:32:51.000Z', 'title': 'Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair', 'summary': 'The rapid advancement of bug-finding techniques has led to the discovery of\\nmore vulnerabilities than developers can reasonably fix, creating an urgent\\nneed for effective Automated Program Repair (APR) methods. However, the\\ncomplexity of modern bugs often makes precise root cause analysis difficult and\\nunreliable. To address this challenge, we propose crash-site repair to simplify\\nthe repair task while still mitigating the risk of exploitation. In addition,\\nwe introduce a template-guided patch generation approach that significantly\\nreduces the token cost of Large Language Models (LLMs) while maintaining both\\nefficiency and effectiveness.\\n  We implement our prototype system, WILLIAMT, and evaluate it against\\nstate-of-the-art APR tools. Our results show that, when combined with the\\ntop-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and\\nincreases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open\\nsource software vulnerabilities benchmark. Furthermore, we demonstrate that\\nWILLIAMT can function effectively even without access to frontier LLMs: even a\\nlocal model running on a Mac M4 Mini achieves a reasonable repair rate. These\\nfindings highlight the broad applicability and scalability of WILLIAMT.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13103.png', 'numComments': 1, 'submittedBy': {'_id': '6475c2794766357252e69e9f', 'avatarUrl': '/avatars/db428715dfd2239df2aeaaff1282323f.svg', 'fullname': 'i', 'name': 'iliashum', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.11365', 'authors': [{'_id': '682d7ae082567fffe108b31a', 'user': {'_id': '6596ca5cce76219628b8eab4', 'avatarUrl': '/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg', 'isPro': False, 'fullname': 'Pierre Le Jeune', 'user': 'pierlj', 'type': 'user'}, 'name': 'Pierre Le Jeune', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-05-21T07:16:41.247Z', 'hidden': False}, {'_id': '682d7ae082567fffe108b31b', 'user': {'_id': '65bccff4c1a44b6ef1c100da', 'avatarUrl': '/avatars/bf91000c78b83167958dc44c582397f0.svg', 'isPro': False, 'fullname': 'benoit', 'user': 'bmalezieux', 'type': 'user'}, 'name': 'Benoît Malézieux', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-21T07:04:02.669Z', 'hidden': False}, {'_id': '682d7ae082567fffe108b31c', 'user': {'_id': '649f00fc37bfb5202be464a9', 'avatarUrl': '/avatars/89f6c6a92c076099f5450c3cd2057619.svg', 'isPro': False, 'fullname': 'Inoki at Giskard', 'user': 'inoki-giskard', 'type': 'user'}, 'name': 'Weixuan Xiao', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-21T07:04:02.669Z', 'hidden': False}, {'_id': '682d7ae082567fffe108b31d', 'name': 'Matteo Dora', 'hidden': False}], 'publishedAt': '2025-05-16T15:31:08.000Z', 'submittedOnDailyAt': '2025-05-21T05:35:45.784Z', 'title': 'Phare: A Safety Probe for Large Language Models', 'submittedOnDailyBy': {'_id': '6596ca5cce76219628b8eab4', 'avatarUrl': '/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg', 'isPro': False, 'fullname': 'Pierre Le Jeune', 'user': 'pierlj', 'type': 'user'}, 'summary': 'Ensuring the safety of large language models (LLMs) is critical for\\nresponsible deployment, yet existing evaluations often prioritize performance\\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\\nframework to probe and evaluate LLM behavior across three critical dimensions:\\nhallucination and reliability, social biases, and harmful content generation.\\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\\nvulnerabilities across all safety dimensions, including sycophancy, prompt\\nsensitivity, and stereotype reproduction. By highlighting these specific\\nfailure modes rather than simply ranking models, Phare provides researchers and\\npractitioners with actionable insights to build more robust, aligned, and\\ntrustworthy language systems.', 'upvotes': 4, 'discussionId': '682d7ae282567fffe108b40f', 'projectPage': 'https://phare.giskard.ai/', 'githubRepo': 'https://github.com/Giskard-AI/phare', 'ai_keywords': ['hallucination', 'reliability', 'social biases', 'harmful content generation', 'multilingual diagnostic framework', 'sycophancy', 'prompt sensitivity', 'stereotype reproduction']}, 'publishedAt': '2025-05-16T11:31:08.000Z', 'title': 'Phare: A Safety Probe for Large Language Models', 'summary': 'Ensuring the safety of large language models (LLMs) is critical for\\nresponsible deployment, yet existing evaluations often prioritize performance\\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\\nframework to probe and evaluate LLM behavior across three critical dimensions:\\nhallucination and reliability, social biases, and harmful content generation.\\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\\nvulnerabilities across all safety dimensions, including sycophancy, prompt\\nsensitivity, and stereotype reproduction. By highlighting these specific\\nfailure modes rather than simply ranking models, Phare provides researchers and\\npractitioners with actionable insights to build more robust, aligned, and\\ntrustworthy language systems.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11365.png', 'numComments': 1, 'submittedBy': {'_id': '6596ca5cce76219628b8eab4', 'avatarUrl': '/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg', 'fullname': 'Pierre Le Jeune', 'name': 'pierlj', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.09569', 'authors': [{'_id': '682563f807f74666ec373332', 'user': {'_id': '682573f5a8792137ebca0a70', 'avatarUrl': '/avatars/358767039db06c41c2bcee55a081cb21.svg', 'isPro': False, 'fullname': 'Linbo Liu', 'user': 'linboliu', 'type': 'user'}, 'name': 'Linbo Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:28:01.555Z', 'hidden': False}, {'_id': '682563f807f74666ec373333', 'user': {'_id': '636c32ae181c81c337f086b9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg', 'isPro': False, 'fullname': 'Xinle Sheila Liu', 'user': 'sliuxl', 'type': 'user'}, 'name': 'Xinle Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-15T10:30:40.773Z', 'hidden': False}, {'_id': '682563f807f74666ec373334', 'name': 'Qiang Zhou', 'hidden': False}, {'_id': '682563f807f74666ec373335', 'name': 'Lin Chen', 'hidden': False}, {'_id': '682563f807f74666ec373336', 'name': 'Yihan Liu', 'hidden': False}, {'_id': '682563f807f74666ec373337', 'name': 'Hoan Nguyen', 'hidden': False}, {'_id': '682563f807f74666ec373338', 'user': {'_id': '63aa8f1b8780edbabb18f9ad', 'avatarUrl': '/avatars/a184f9816bedf5f9f49975f3334fd24a.svg', 'isPro': False, 'fullname': 'Behrooz Omidvar-Tehrani', 'user': 'omidvarb', 'type': 'user'}, 'name': 'Behrooz Omidvar-Tehrani', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:28:28.110Z', 'hidden': False}, {'_id': '682563f807f74666ec373339', 'name': 'Xi Shen', 'hidden': False}, {'_id': '682563f807f74666ec37333a', 'name': 'Jun Huan', 'hidden': False}, {'_id': '682563f807f74666ec37333b', 'name': 'Omer Tripp', 'hidden': False}, {'_id': '682563f807f74666ec37333c', 'name': 'Anoop Deoras', 'hidden': False}], 'publishedAt': '2025-05-14T17:11:23.000Z', 'submittedOnDailyAt': '2025-05-21T04:22:33.758Z', 'title': 'MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8', 'submittedOnDailyBy': {'_id': '636c32ae181c81c337f086b9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg', 'isPro': False, 'fullname': 'Xinle Sheila Liu', 'user': 'sliuxl', 'type': 'user'}, 'summary': 'With the rapid advancement of powerful large language models (LLMs) in recent\\nyears, a wide range of software engineering tasks can now be addressed using\\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\\ndatasets have been developed to evaluate the coding capabilities of these\\nmodels, while they primarily focus on problem-solving and issue-resolution\\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\\ncomprehensive benchmark for migration from Java 8 to the latest long-term\\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\\nand its subset selected with 5,102 and 300 repositories respectively.\\nSelected is a representative subset curated for complexity and difficulty,\\noffering a versatile resource to support research in the field of code\\nmigration. Additionally, we provide a comprehensive evaluation framework to\\nfacilitate rigorous and standardized assessment of LLMs on this challenging\\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\\ntackle repository-level code migration to Java 17. For the selected subset with\\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\\nand source code are available at:\\nhttps://huggingface.co/collections/AmazonScience and\\nhttps://github.com/amazon-science/self_debug respectively.', 'upvotes': 4, 'discussionId': '682563f907f74666ec373369', 'projectPage': 'https://github.com/amazon-science/SDFeedback', 'githubRepo': 'https://github.com/amazon-science/MigrationBench', 'ai_keywords': ['large language models (LLMs)', 'code migration', 'MIGRATION-BENCH', 'Java 8', 'long-term support (LTS) versions (Java 17, 21)', 'repositories', 'SD-Feedback', 'pass@1']}, 'publishedAt': '2025-05-14T13:11:23.000Z', 'title': 'MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8', 'summary': 'With the rapid advancement of powerful large language models (LLMs) in recent\\nyears, a wide range of software engineering tasks can now be addressed using\\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\\ndatasets have been developed to evaluate the coding capabilities of these\\nmodels, while they primarily focus on problem-solving and issue-resolution\\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\\ncomprehensive benchmark for migration from Java 8 to the latest long-term\\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\\nand its subset selected with 5,102 and 300 repositories respectively.\\nSelected is a representative subset curated for complexity and difficulty,\\noffering a versatile resource to support research in the field of code\\nmigration. Additionally, we provide a comprehensive evaluation framework to\\nfacilitate rigorous and standardized assessment of LLMs on this challenging\\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\\ntackle repository-level code migration to Java 17. For the selected subset with\\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\\nand source code are available at:\\nhttps://huggingface.co/collections/AmazonScience and\\nhttps://github.com/amazon-science/self_debug respectively.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09569.png', 'numComments': 1, 'submittedBy': {'_id': '636c32ae181c81c337f086b9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg', 'fullname': 'Xinle Sheila Liu', 'name': 'sliuxl', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.13380', 'authors': [{'_id': '682d3787265177367e119f04', 'user': {'_id': '64c2bea2ada7df214276913b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg', 'isPro': False, 'fullname': 'Nguyen Van Nam', 'user': 'DavidNguyen', 'type': 'user'}, 'name': 'Nam V. Nguyen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:58.584Z', 'hidden': False}, {'_id': '682d3787265177367e119f05', 'name': 'Huy Nguyen', 'hidden': False}, {'_id': '682d3787265177367e119f06', 'name': 'Quang Pham', 'hidden': False}, {'_id': '682d3787265177367e119f07', 'name': 'Van Nguyen', 'hidden': False}, {'_id': '682d3787265177367e119f08', 'name': 'Savitha Ramasamy', 'hidden': False}, {'_id': '682d3787265177367e119f09', 'user': {'_id': '66216209da5d83251916f301', 'avatarUrl': '/avatars/dd9d8977cbca95ae3309738b805474d4.svg', 'isPro': False, 'fullname': 'Nhat Ho', 'user': 'nhatho', 'type': 'user'}, 'name': 'Nhat Ho', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:26:55.401Z', 'hidden': False}], 'publishedAt': '2025-05-19T17:24:26.000Z', 'submittedOnDailyAt': '2025-05-21T00:48:58.161Z', 'title': 'CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\\n  Competition', 'submittedOnDailyBy': {'_id': '64c2bea2ada7df214276913b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg', 'isPro': False, 'fullname': 'Nguyen Van Nam', 'user': 'DavidNguyen', 'type': 'user'}, 'summary': \"Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\\nmodel complexity beyond the mean of increasing the network's depth or width.\\nHowever, we argue that effective SMoE training remains challenging because of\\nthe suboptimal routing process where experts that perform computation do not\\ndirectly contribute to the routing process. In this work, we propose\\ncompetition, a novel mechanism to route tokens to experts with the highest\\nneural response. Theoretically, we show that the competition mechanism enjoys a\\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\\nmodels by deploying a router to learn the competition policy, thus enjoying\\nstrong performances at a low training overhead. Our extensive empirical\\nevaluations on both the visual instruction tuning and language pre-training\\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\\ncompared to state-of-the-art SMoE strategies. We have made the implementation\\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\\nimproved version of the previous study at arXiv:2402.02526\", 'upvotes': 3, 'discussionId': '682d3788265177367e119f71', 'githubRepo': 'https://github.com/Fsoft-AIC/CompeteSMoE', 'ai_keywords': ['Sparse mixture of experts (SMoE)', 'competition mechanism', 'sample efficiency', 'softmax routing', 'CompeteSMoE', 'neural response', 'router', 'competition policy', 'visual instruction tuning', 'language pre-training']}, 'publishedAt': '2025-05-19T13:24:26.000Z', 'title': 'CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\\n  Competition', 'summary': \"Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\\nmodel complexity beyond the mean of increasing the network's depth or width.\\nHowever, we argue that effective SMoE training remains challenging because of\\nthe suboptimal routing process where experts that perform computation do not\\ndirectly contribute to the routing process. In this work, we propose\\ncompetition, a novel mechanism to route tokens to experts with the highest\\nneural response. Theoretically, we show that the competition mechanism enjoys a\\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\\nmodels by deploying a router to learn the competition policy, thus enjoying\\nstrong performances at a low training overhead. Our extensive empirical\\nevaluations on both the visual instruction tuning and language pre-training\\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\\ncompared to state-of-the-art SMoE strategies. We have made the implementation\\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\\nimproved version of the previous study at arXiv:2402.02526\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13380.png', 'numComments': 1, 'submittedBy': {'_id': '64c2bea2ada7df214276913b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg', 'fullname': 'Nguyen Van Nam', 'name': 'DavidNguyen', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.12306', 'authors': [{'_id': '682d2f9f3b5f51f42185dd7d', 'name': 'Yuwei Zhang', 'hidden': False}, {'_id': '682d2f9f3b5f51f42185dd7e', 'name': 'Wenhao Yu', 'hidden': False}, {'_id': '682d2f9f3b5f51f42185dd7f', 'name': 'Shangbin Feng', 'hidden': False}, {'_id': '682d2f9f3b5f51f42185dd80', 'name': 'Yifan Zhu', 'hidden': False}, {'_id': '682d2f9f3b5f51f42185dd81', 'user': {'_id': '64323dd503d81fa4d26deaf9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png', 'isPro': False, 'fullname': 'Letian Peng', 'user': 'KomeijiForce', 'type': 'user'}, 'name': 'Letian Peng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:21:33.327Z', 'hidden': False}, {'_id': '682d2f9f3b5f51f42185dd82', 'name': 'Jayanth Srinivasa', 'hidden': False}, {'_id': '682d2f9f3b5f51f42185dd83', 'name': 'Gaowen Liu', 'hidden': False}, {'_id': '682d2f9f3b5f51f42185dd84', 'user': {'_id': '660655119e3555d648f6c6b5', 'avatarUrl': '/avatars/ae1e2c97a08be39b77a9f1a5c2a718ef.svg', 'isPro': False, 'fullname': 'Jingbo Shang', 'user': 'shangjingbo', 'type': 'user'}, 'name': 'Jingbo Shang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:21:40.883Z', 'hidden': False}], 'publishedAt': '2025-05-18T08:39:05.000Z', 'submittedOnDailyAt': '2025-05-21T00:13:17.201Z', 'title': 'Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for\\n  Real-world Knowledge Injection', 'submittedOnDailyBy': {'_id': '64323dd503d81fa4d26deaf9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png', 'isPro': False, 'fullname': 'Letian Peng', 'user': 'KomeijiForce', 'type': 'user'}, 'summary': 'Despite significant advances in large language models (LLMs), their knowledge\\nmemorization capabilities remain underexplored, due to the lack of standardized\\nand high-quality test ground. In this paper, we introduce a novel, real-world\\nand large-scale knowledge injection benchmark that evolves continuously over\\ntime without requiring human intervention. Specifically, we propose WikiDYK,\\nwhich leverages recently-added and human-written facts from Wikipedia\\'s \"Did\\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\\neditors based on criteria such as verifiability and clarity. Each entry is\\nconverted into multiple question-answer pairs spanning diverse task formats\\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\\nfacts and 77,180 questions, which is also seamlessly extensible with future\\nupdates from Wikipedia editors. Extensive experiments using continued\\npre-training reveal a surprising insight: despite their prevalence in modern\\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\\nwith LLMs. Experiment shows that our framework further improves the reliability\\naccuracy by up to 29.1%.', 'upvotes': 3, 'discussionId': '682d2fa03b5f51f42185ddb4', 'ai_keywords': ['large language models (LLMs)', 'knowledge memorization capabilities', 'WikiDYK', '\"Did You Know...\" entries', 'question-answer pairs', 'cloze prompts', 'multi-hop questions', 'continued pre-training', 'Causal Language Models (CLMs)', 'Bidirectional Language Models (BiLMs)', 'modular collaborative framework', 'ensembles of BiLMs', 'external knowledge repositories', 'reliability accuracy']}, 'publishedAt': '2025-05-18T04:39:05.000Z', 'title': 'Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for\\n  Real-world Knowledge Injection', 'summary': 'Despite significant advances in large language models (LLMs), their knowledge\\nmemorization capabilities remain underexplored, due to the lack of standardized\\nand high-quality test ground. In this paper, we introduce a novel, real-world\\nand large-scale knowledge injection benchmark that evolves continuously over\\ntime without requiring human intervention. Specifically, we propose WikiDYK,\\nwhich leverages recently-added and human-written facts from Wikipedia\\'s \"Did\\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\\neditors based on criteria such as verifiability and clarity. Each entry is\\nconverted into multiple question-answer pairs spanning diverse task formats\\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\\nfacts and 77,180 questions, which is also seamlessly extensible with future\\nupdates from Wikipedia editors. Extensive experiments using continued\\npre-training reveal a surprising insight: despite their prevalence in modern\\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\\nwith LLMs. Experiment shows that our framework further improves the reliability\\naccuracy by up to 29.1%.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12306.png', 'numComments': 1, 'submittedBy': {'_id': '64323dd503d81fa4d26deaf9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png', 'fullname': 'Letian Peng', 'name': 'KomeijiForce', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.11966', 'authors': [{'_id': '682d42808560f4baf596643b', 'user': {'_id': '6608fa4f5baec84322ec85ea', 'avatarUrl': '/avatars/13bdaff931676b065fa1efef06fef922.svg', 'isPro': False, 'fullname': 'Zhong', 'user': 'Jianyuan1', 'type': 'user'}, 'name': 'Jianyuan Zhong', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T08:40:43.900Z', 'hidden': False}, {'_id': '682d42808560f4baf596643c', 'name': 'Zeju Li', 'hidden': False}, {'_id': '682d42808560f4baf596643d', 'name': 'Zhijian Xu', 'hidden': False}, {'_id': '682d42808560f4baf596643e', 'user': {'_id': '641b1b36a5f876fe30c49542', 'avatarUrl': '/avatars/ac9267925f45d325c2adb2eb0e38077b.svg', 'isPro': False, 'fullname': 'Xiangyu Wen', 'user': 'XiangyuWen', 'type': 'user'}, 'name': 'Xiangyu Wen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:24:05.361Z', 'hidden': False}, {'_id': '682d42808560f4baf596643f', 'name': 'Kezhi Li', 'hidden': False}, {'_id': '682d42808560f4baf5966440', 'name': 'Qiang Xu', 'hidden': False}], 'publishedAt': '2025-05-17T11:41:44.000Z', 'submittedOnDailyAt': '2025-05-21T01:36:54.831Z', 'title': 'Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\\n  Verifier', 'submittedOnDailyBy': {'_id': '6608fa4f5baec84322ec85ea', 'avatarUrl': '/avatars/13bdaff931676b065fa1efef06fef922.svg', 'isPro': False, 'fullname': 'Zhong', 'user': 'Jianyuan1', 'type': 'user'}, 'summary': 'Large Language Model (LLM) reasoning for complex tasks inherently involves a\\ntrade-off between solution accuracy and computational efficiency. The\\nsubsequent step of verification, while intended to improve performance, further\\ncomplicates this landscape by introducing its own challenging trade-off:\\nsophisticated Generative Reward Models (GenRMs) can be computationally\\nprohibitive if naively integrated with LLMs at test-time, while simpler, faster\\nmethods may lack reliability. To overcome these challenges, we introduce\\nFlexiVe, a novel generative verifier that flexibly balances computational\\nresources between rapid, reliable fast thinking and meticulous slow thinking\\nusing a Flexible Allocation of Verification Budget strategy. We further propose\\nthe Solve-Detect-Verify pipeline, an efficient inference-time scaling framework\\nthat intelligently integrates FlexiVe, proactively identifying solution\\ncompletion points to trigger targeted verification and provide focused solver\\nfeedback. Experiments show FlexiVe achieves superior accuracy in pinpointing\\nerrors within reasoning traces on ProcessBench. Furthermore, on challenging\\nmathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full\\napproach outperforms baselines like self-consistency in reasoning accuracy and\\ninference efficiency. Our system offers a scalable and effective solution to\\nenhance LLM reasoning at test time.', 'upvotes': 3, 'discussionId': '682d42808560f4baf5966480', 'ai_keywords': ['Generative Reward Models (GenRMs)', 'Flexible Allocation of Verification Budget', 'Solve-Detect-Verify pipeline', 'reasoning traces', 'ProcessBench', 'AIME 2024', 'AIME 2025', 'CNMO', 'self-consistency']}, 'publishedAt': '2025-05-17T07:41:44.000Z', 'title': 'Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\\n  Verifier', 'summary': 'Large Language Model (LLM) reasoning for complex tasks inherently involves a\\ntrade-off between solution accuracy and computational efficiency. The\\nsubsequent step of verification, while intended to improve performance, further\\ncomplicates this landscape by introducing its own challenging trade-off:\\nsophisticated Generative Reward Models (GenRMs) can be computationally\\nprohibitive if naively integrated with LLMs at test-time, while simpler, faster\\nmethods may lack reliability. To overcome these challenges, we introduce\\nFlexiVe, a novel generative verifier that flexibly balances computational\\nresources between rapid, reliable fast thinking and meticulous slow thinking\\nusing a Flexible Allocation of Verification Budget strategy. We further propose\\nthe Solve-Detect-Verify pipeline, an efficient inference-time scaling framework\\nthat intelligently integrates FlexiVe, proactively identifying solution\\ncompletion points to trigger targeted verification and provide focused solver\\nfeedback. Experiments show FlexiVe achieves superior accuracy in pinpointing\\nerrors within reasoning traces on ProcessBench. Furthermore, on challenging\\nmathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full\\napproach outperforms baselines like self-consistency in reasoning accuracy and\\ninference efficiency. Our system offers a scalable and effective solution to\\nenhance LLM reasoning at test time.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11966.png', 'numComments': 1, 'submittedBy': {'_id': '6608fa4f5baec84322ec85ea', 'avatarUrl': '/avatars/13bdaff931676b065fa1efef06fef922.svg', 'fullname': 'Zhong', 'name': 'Jianyuan1', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14178', 'authors': [{'_id': '682d388c57686b8c44ede70b', 'user': {'_id': '656553d89bf6665f10e3a92d', 'avatarUrl': '/avatars/f55b09e249c48ef3fe484afa33a182ae.svg', 'isPro': False, 'fullname': 'xiang wyatt zhang', 'user': 'Wyattz23', 'type': 'user'}, 'name': 'Xiang Zhang', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-21T02:21:00.902Z', 'hidden': False}, {'_id': '682d388c57686b8c44ede70c', 'name': 'Juntai Cao', 'hidden': False}, {'_id': '682d388c57686b8c44ede70d', 'name': 'Jiaqi Wei', 'hidden': False}, {'_id': '682d388c57686b8c44ede70e', 'name': 'Yiwei Xu', 'hidden': False}, {'_id': '682d388c57686b8c44ede70f', 'user': {'_id': '6466d463060756d2854ab3e1', 'avatarUrl': '/avatars/4401387180c16472a6823f78aaa86d54.svg', 'isPro': False, 'fullname': 'Chenyu You', 'user': 'Charlesyooo', 'type': 'user'}, 'name': 'Chenyu You', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-05-21T02:30:12.849Z', 'hidden': False}], 'publishedAt': '2025-05-20T10:32:30.000Z', 'submittedOnDailyAt': '2025-05-21T00:51:16.514Z', 'title': 'Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\\n  Reasoning Limits', 'submittedOnDailyBy': {'_id': '656553d89bf6665f10e3a92d', 'avatarUrl': '/avatars/f55b09e249c48ef3fe484afa33a182ae.svg', 'isPro': False, 'fullname': 'xiang wyatt zhang', 'user': 'Wyattz23', 'type': 'user'}, 'summary': 'Tokenization is the first - and often underappreciated - layer of computation\\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\\nmodels to approximate recurrent computation by externalizing intermediate\\nsteps, we show that the success of such reasoning is fundamentally bounded by\\nthe structure of tokenized inputs. This work presents a theoretical and\\nempirical investigation into how tokenization schemes, particularly\\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\\ncomputation by merging or obscuring atomic reasoning units. We introduce the\\nnotion of Token Awareness to formalize how poor token granularity disrupts\\nlogical alignment and prevents models from generalizing symbolic procedures.\\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\\nthat token structure dramatically affect reasoning performance, causing failure\\neven with CoT, while atomically-aligned formats unlock strong generalization,\\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\\no1) in structured reasoning. Our findings reveal that symbolic reasoning\\nability in LLMs is not purely architectural, but deeply conditioned on\\ntoken-level representations.', 'upvotes': 2, 'discussionId': '682d388c57686b8c44ede753', 'ai_keywords': ['Chain-of-Thought (CoT) prompting', 'transformer models', 'recursive computation', 'intermediate steps', 'tokenization schemes', 'subword-based methods', 'byte-pair encoding (BPE)', 'symbolic computation', 'Token Awareness', 'atomic reasoning units', 'logical alignment', 'structured reasoning', 'arithmetic tasks', 'symbolic tasks', 'reasoning performance', 'atomically-aligned formats', 'structured reasoning', 'low-level representations', 'symbolic reasoning ability', 'LLMs', 'token-level representations']}, 'publishedAt': '2025-05-20T06:32:30.000Z', 'title': 'Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\\n  Reasoning Limits', 'summary': 'Tokenization is the first - and often underappreciated - layer of computation\\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\\nmodels to approximate recurrent computation by externalizing intermediate\\nsteps, we show that the success of such reasoning is fundamentally bounded by\\nthe structure of tokenized inputs. This work presents a theoretical and\\nempirical investigation into how tokenization schemes, particularly\\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\\ncomputation by merging or obscuring atomic reasoning units. We introduce the\\nnotion of Token Awareness to formalize how poor token granularity disrupts\\nlogical alignment and prevents models from generalizing symbolic procedures.\\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\\nthat token structure dramatically affect reasoning performance, causing failure\\neven with CoT, while atomically-aligned formats unlock strong generalization,\\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\\no1) in structured reasoning. Our findings reveal that symbolic reasoning\\nability in LLMs is not purely architectural, but deeply conditioned on\\ntoken-level representations.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14178.png', 'numComments': 1, 'submittedBy': {'_id': '656553d89bf6665f10e3a92d', 'avatarUrl': '/avatars/f55b09e249c48ef3fe484afa33a182ae.svg', 'fullname': 'xiang wyatt zhang', 'name': 'Wyattz23', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.13010', 'authors': [{'_id': '682c46c70f622b7afc2e6f8f', 'user': {'_id': '67d5c51817d572b960f6982a', 'avatarUrl': '/avatars/2c22663b27326b2300444bcc84ebbdd7.svg', 'isPro': False, 'fullname': 'Himel Ghosh', 'user': 'himel7', 'type': 'user'}, 'name': 'Himel Ghosh', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-05-21T09:42:53.022Z', 'hidden': False}, {'_id': '682c46c70f622b7afc2e6f90', 'user': {'_id': '62d0859afb896639b29c45c8', 'avatarUrl': '/avatars/5309851f11c875f15f80a7415a4ae7f3.svg', 'isPro': False, 'fullname': 'Ahmed Mosharafa', 'user': 'amosharafa', 'type': 'user'}, 'name': 'Ahmed Mosharafa', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:24:54.616Z', 'hidden': False}, {'_id': '682c46c70f622b7afc2e6f91', 'user': {'_id': '662a6560925d2af5b0e13a34', 'avatarUrl': '/avatars/d45a057e6bbde14f54811be34ae6bc64.svg', 'isPro': False, 'fullname': 'Georg Groh', 'user': 'grohg', 'type': 'user'}, 'name': 'Georg Groh', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-05-21T10:25:00.667Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/67d5c51817d572b960f6982a/YKwZeahX2EReZyYUdKvtE.png'], 'publishedAt': '2025-05-19T11:54:39.000Z', 'submittedOnDailyAt': '2025-05-21T08:14:27.462Z', 'title': 'To Bias or Not to Bias: Detecting bias in News with bias-detector', 'submittedOnDailyBy': {'_id': '67d5c51817d572b960f6982a', 'avatarUrl': '/avatars/2c22663b27326b2300444bcc84ebbdd7.svg', 'isPro': False, 'fullname': 'Himel Ghosh', 'user': 'himel7', 'type': 'user'}, 'summary': \"Media bias detection is a critical task in ensuring fair and balanced\\ninformation dissemination, yet it remains challenging due to the subjectivity\\nof bias and the scarcity of high-quality annotated data. In this work, we\\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\\ncross-validation paired t-test, we show statistically significant improvements\\nin performance when comparing our model to a domain-adaptively pre-trained\\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\\navoids common pitfalls like oversensitivity to politically charged terms and\\ninstead attends more meaningfully to contextually relevant tokens. For a\\ncomprehensive examination of media bias, we present a pipeline that combines\\nour model with an already-existing bias-type classifier. Our method exhibits\\ngood generalization and interpretability, despite being constrained by\\nsentence-level analysis and dataset size because of a lack of larger and more\\nadvanced bias corpora. We talk about context-aware modeling, bias\\nneutralization, and advanced bias type classification as potential future\\ndirections. Our findings contribute to building more robust, explainable, and\\nsocially responsible NLP systems for media bias detection.\", 'upvotes': 2, 'discussionId': '682c46c80f622b7afc2e6fcf', 'githubRepo': 'https://github.com/Himel1996/NewsBiasDetector', 'ai_keywords': ['RoBERTa-based model', 'BABE dataset', \"McNemar's test\", '5x2 cross-validation paired t-test', 'domain-adaptively pre-trained', 'DA-RoBERTa baseline', 'attention-based analysis', 'politically charged terms', 'contextually relevant tokens', 'sentence-level analysis', 'bias-type classifier', 'context-aware modeling', 'bias neutralization', 'advanced bias type classification']}, 'publishedAt': '2025-05-19T07:54:39.000Z', 'title': 'To Bias or Not to Bias: Detecting bias in News with bias-detector', 'summary': \"Media bias detection is a critical task in ensuring fair and balanced\\ninformation dissemination, yet it remains challenging due to the subjectivity\\nof bias and the scarcity of high-quality annotated data. In this work, we\\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\\ncross-validation paired t-test, we show statistically significant improvements\\nin performance when comparing our model to a domain-adaptively pre-trained\\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\\navoids common pitfalls like oversensitivity to politically charged terms and\\ninstead attends more meaningfully to contextually relevant tokens. For a\\ncomprehensive examination of media bias, we present a pipeline that combines\\nour model with an already-existing bias-type classifier. Our method exhibits\\ngood generalization and interpretability, despite being constrained by\\nsentence-level analysis and dataset size because of a lack of larger and more\\nadvanced bias corpora. We talk about context-aware modeling, bias\\nneutralization, and advanced bias type classification as potential future\\ndirections. Our findings contribute to building more robust, explainable, and\\nsocially responsible NLP systems for media bias detection.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/67d5c51817d572b960f6982a/YKwZeahX2EReZyYUdKvtE.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13010.png', 'numComments': 1, 'submittedBy': {'_id': '67d5c51817d572b960f6982a', 'avatarUrl': '/avatars/2c22663b27326b2300444bcc84ebbdd7.svg', 'fullname': 'Himel Ghosh', 'name': 'himel7', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.10176', 'authors': [{'_id': '682dbb6ed675b2cd48589a13', 'user': {'_id': '647f1e2ed26579210f5a6982', 'avatarUrl': '/avatars/742f3c701de9c846f58e693aec07cfc0.svg', 'isPro': False, 'fullname': 'hexiang', 'user': 'xianghe', 'type': 'user'}, 'name': 'Xiang He', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T12:26:46.945Z', 'hidden': False}, {'_id': '682dbb6ed675b2cd48589a14', 'name': 'Dongcheng Zhao', 'hidden': False}, {'_id': '682dbb6ed675b2cd48589a15', 'name': 'Yang Li', 'hidden': False}, {'_id': '682dbb6ed675b2cd48589a16', 'name': 'Qingqun Kong', 'hidden': False}, {'_id': '682dbb6ed675b2cd48589a17', 'name': 'Xin Yang', 'hidden': False}, {'_id': '682dbb6ed675b2cd48589a18', 'name': 'Yi Zeng', 'hidden': False}], 'publishedAt': '2025-05-15T11:08:50.000Z', 'submittedOnDailyAt': '2025-05-21T10:10:04.519Z', 'title': 'Incorporating brain-inspired mechanisms for multimodal learning in\\n  artificial intelligence', 'submittedOnDailyBy': {'_id': '647f1e2ed26579210f5a6982', 'avatarUrl': '/avatars/742f3c701de9c846f58e693aec07cfc0.svg', 'isPro': False, 'fullname': 'hexiang', 'user': 'xianghe', 'type': 'user'}, 'summary': 'Multimodal learning enhances the perceptual capabilities of cognitive systems\\nby integrating information from different sensory modalities. However, existing\\nmultimodal fusion research typically assumes static integration, not fully\\nincorporating key dynamic mechanisms found in the brain. Specifically, the\\nbrain exhibits an inverse effectiveness phenomenon, wherein weaker unimodal\\ncues yield stronger multisensory integration benefits; conversely, when\\nindividual modal cues are stronger, the effect of fusion is diminished. This\\nmechanism enables biological systems to achieve robust cognition even with\\nscarce or noisy perceptual cues. Inspired by this biological mechanism, we\\nexplore the relationship between multimodal output and information from\\nindividual modalities, proposing an inverse effectiveness driven multimodal\\nfusion (IEMF) strategy. By incorporating this strategy into neural networks, we\\nachieve more efficient integration with improved model performance and\\ncomputational efficiency, demonstrating up to 50% reduction in computational\\ncost across diverse fusion methods. We conduct experiments on audio-visual\\nclassification, continual learning, and question answering tasks to validate\\nour method. Results consistently demonstrate that our method performs\\nexcellently in these tasks. To verify universality and generalization, we also\\nconduct experiments on Artificial Neural Networks (ANN) and Spiking Neural\\nNetworks (SNN), with results showing good adaptability to both network types.\\nOur research emphasizes the potential of incorporating biologically inspired\\nmechanisms into multimodal networks and provides promising directions for the\\nfuture development of multimodal artificial intelligence. The code is available\\nat https://github.com/Brain-Cog-Lab/IEMF.', 'upvotes': 2, 'discussionId': '682dbb70d675b2cd48589acc', 'githubRepo': 'https://github.com/Brain-Cog-Lab/IEMF', 'ai_keywords': ['multimodal fusion', 'inverse effectiveness phenomenon', 'biologically inspired mechanisms', 'multimodal output', 'neural networks', 'computational efficiency', 'audio-visual classification', 'continual learning', 'question answering tasks', 'Artificial Neural Networks (ANN)', 'Spiking Neural Networks (SNN)']}, 'publishedAt': '2025-05-15T07:08:50.000Z', 'title': 'Incorporating brain-inspired mechanisms for multimodal learning in\\n  artificial intelligence', 'summary': 'Multimodal learning enhances the perceptual capabilities of cognitive systems\\nby integrating information from different sensory modalities. However, existing\\nmultimodal fusion research typically assumes static integration, not fully\\nincorporating key dynamic mechanisms found in the brain. Specifically, the\\nbrain exhibits an inverse effectiveness phenomenon, wherein weaker unimodal\\ncues yield stronger multisensory integration benefits; conversely, when\\nindividual modal cues are stronger, the effect of fusion is diminished. This\\nmechanism enables biological systems to achieve robust cognition even with\\nscarce or noisy perceptual cues. Inspired by this biological mechanism, we\\nexplore the relationship between multimodal output and information from\\nindividual modalities, proposing an inverse effectiveness driven multimodal\\nfusion (IEMF) strategy. By incorporating this strategy into neural networks, we\\nachieve more efficient integration with improved model performance and\\ncomputational efficiency, demonstrating up to 50% reduction in computational\\ncost across diverse fusion methods. We conduct experiments on audio-visual\\nclassification, continual learning, and question answering tasks to validate\\nour method. Results consistently demonstrate that our method performs\\nexcellently in these tasks. To verify universality and generalization, we also\\nconduct experiments on Artificial Neural Networks (ANN) and Spiking Neural\\nNetworks (SNN), with results showing good adaptability to both network types.\\nOur research emphasizes the potential of incorporating biologically inspired\\nmechanisms into multimodal networks and provides promising directions for the\\nfuture development of multimodal artificial intelligence. The code is available\\nat https://github.com/Brain-Cog-Lab/IEMF.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10176.png', 'numComments': 1, 'submittedBy': {'_id': '647f1e2ed26579210f5a6982', 'avatarUrl': '/avatars/742f3c701de9c846f58e693aec07cfc0.svg', 'fullname': 'hexiang', 'name': 'xianghe', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.11754', 'authors': [{'_id': '682bdc203fff9e115285b2b6', 'user': {'_id': '64c4d03ca684146b1cdb8237', 'avatarUrl': '/avatars/82b513c686ab20c859ca900fd59dfac7.svg', 'isPro': False, 'fullname': 'Wenyu Huang', 'user': 'hwy9855', 'type': 'user'}, 'name': 'Wenyu Huang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-20T07:21:08.965Z', 'hidden': False}, {'_id': '682bdc203fff9e115285b2b7', 'user': {'_id': '641c83a492cd2530299714c7', 'avatarUrl': '/avatars/928195163f9040a2649953c86b9d6c79.svg', 'isPro': False, 'fullname': 'Pavlos V', 'user': 'pvougiou', 'type': 'user'}, 'name': 'Pavlos Vougiouklis', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-05-21T11:43:19.081Z', 'hidden': False}, {'_id': '682bdc203fff9e115285b2b8', 'name': 'Mirella Lapata', 'hidden': False}, {'_id': '682bdc203fff9e115285b2b9', 'name': 'Jeff Z. Pan', 'hidden': False}], 'publishedAt': '2025-05-16T23:29:47.000Z', 'submittedOnDailyAt': '2025-05-21T10:11:19.053Z', 'title': 'Masking in Multi-hop QA: An Analysis of How Language Models Perform with\\n  Context Permutation', 'submittedOnDailyBy': {'_id': '64c4d03ca684146b1cdb8237', 'avatarUrl': '/avatars/82b513c686ab20c859ca900fd59dfac7.svg', 'isPro': False, 'fullname': 'Wenyu Huang', 'user': 'hwy9855', 'type': 'user'}, 'summary': \"Multi-hop Question Answering (MHQA) adds layers of complexity to question\\nanswering, making it more challenging. When Language Models (LMs) are prompted\\nwith multiple search results, they are tasked not only with retrieving relevant\\ninformation but also employing multi-hop reasoning across the information\\nsources. Although LMs perform well on traditional question-answering tasks, the\\ncausal mask can hinder their capacity to reason across complex contexts. In\\nthis paper, we explore how LMs respond to multi-hop questions by permuting\\nsearch results (retrieved documents) under various configurations. Our study\\nreveals interesting findings as follows: 1) Encoder-decoder models, such as the\\nones in the Flan-T5 family, generally outperform causal decoder-only LMs in\\nMHQA tasks, despite being significantly smaller in size; 2) altering the order\\nof gold documents reveals distinct trends in both Flan T5 models and fine-tuned\\ndecoder-only models, with optimal performance observed when the document order\\naligns with the reasoning chain order; 3) enhancing causal decoder-only models\\nwith bi-directional attention by modifying the causal mask can effectively\\nboost their end performance. In addition to the above, we conduct a thorough\\ninvestigation of the distribution of LM attention weights in the context of\\nMHQA. Our experiments reveal that attention weights tend to peak at higher\\nvalues when the resulting answer is correct. We leverage this finding to\\nheuristically improve LMs' performance on this task. Our code is publicly\\navailable at https://github.com/hwy9855/MultiHopQA-Reasoning.\", 'upvotes': 1, 'discussionId': '682bdc223fff9e115285b354', 'githubRepo': 'https://github.com/hwy9855/multiHopQA-Reasoning/', 'ai_keywords': ['multi-hop question answering (MHQA)', 'encoder-decoder models', 'Flan-T5 family', 'causal decoder-only LMs', 'bi-directional attention', 'causal mask', 'attention weights']}, 'publishedAt': '2025-05-16T19:29:47.000Z', 'title': 'Masking in Multi-hop QA: An Analysis of How Language Models Perform with\\n  Context Permutation', 'summary': \"Multi-hop Question Answering (MHQA) adds layers of complexity to question\\nanswering, making it more challenging. When Language Models (LMs) are prompted\\nwith multiple search results, they are tasked not only with retrieving relevant\\ninformation but also employing multi-hop reasoning across the information\\nsources. Although LMs perform well on traditional question-answering tasks, the\\ncausal mask can hinder their capacity to reason across complex contexts. In\\nthis paper, we explore how LMs respond to multi-hop questions by permuting\\nsearch results (retrieved documents) under various configurations. Our study\\nreveals interesting findings as follows: 1) Encoder-decoder models, such as the\\nones in the Flan-T5 family, generally outperform causal decoder-only LMs in\\nMHQA tasks, despite being significantly smaller in size; 2) altering the order\\nof gold documents reveals distinct trends in both Flan T5 models and fine-tuned\\ndecoder-only models, with optimal performance observed when the document order\\naligns with the reasoning chain order; 3) enhancing causal decoder-only models\\nwith bi-directional attention by modifying the causal mask can effectively\\nboost their end performance. In addition to the above, we conduct a thorough\\ninvestigation of the distribution of LM attention weights in the context of\\nMHQA. Our experiments reveal that attention weights tend to peak at higher\\nvalues when the resulting answer is correct. We leverage this finding to\\nheuristically improve LMs' performance on this task. Our code is publicly\\navailable at https://github.com/hwy9855/MultiHopQA-Reasoning.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11754.png', 'numComments': 1, 'submittedBy': {'_id': '64c4d03ca684146b1cdb8237', 'avatarUrl': '/avatars/82b513c686ab20c859ca900fd59dfac7.svg', 'fullname': 'Wenyu Huang', 'name': 'hwy9855', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.10588', 'authors': [{'_id': '682d2dd917608739046ce410', 'name': 'Manisha Mehta', 'hidden': False}, {'_id': '682d2dd917608739046ce411', 'name': 'Fausto Giunchiglia', 'hidden': False}], 'publishedAt': '2025-05-14T16:46:11.000Z', 'submittedOnDailyAt': '2025-05-21T00:06:34.563Z', 'title': 'Understanding Gen Alpha Digital Language: Evaluation of LLM Safety\\n  Systems for Content Moderation', 'submittedOnDailyBy': {'_id': '604eb19e3050a33ebb17ef58', 'avatarUrl': '/avatars/23650002ba3befee83060fe978a251c8.svg', 'isPro': False, 'fullname': 'Virendra Mehta', 'user': 'Veeru', 'type': 'user'}, 'summary': 'This research offers a unique evaluation of how AI systems interpret the\\ndigital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first\\ncohort raised alongside AI, Gen Alpha faces new forms of online risk due to\\nimmersive digital engagement and a growing mismatch between their evolving\\ncommunication and existing safety tools. Their distinct language, shaped by\\ngaming, memes, and AI-driven trends, often conceals harmful interactions from\\nboth human moderators and automated systems. We assess four leading AI models\\n(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked\\nharassment and manipulation within Gen Alpha discourse. Using a dataset of 100\\nrecent expressions from gaming platforms, social media, and video content, the\\nstudy reveals critical comprehension failures with direct implications for\\nonline safety. This work contributes: (1) a first-of-its-kind dataset capturing\\nGen Alpha expressions; (2) a framework to improve AI moderation systems for\\nyouth protection; (3) a multi-perspective evaluation including AI systems,\\nhuman moderators, and parents, with direct input from Gen Alpha co-researchers;\\nand (4) an analysis of how linguistic divergence increases youth vulnerability.\\nFindings highlight the urgent need to redesign safety systems attuned to youth\\ncommunication, especially given Gen Alpha reluctance to seek help when adults\\nfail to understand their digital world. This study combines the insight of a\\nGen Alpha researcher with systematic academic analysis to address critical\\ndigital safety challenges.', 'upvotes': 1, 'discussionId': '682d2dd917608739046ce440'}, 'publishedAt': '2025-05-14T12:46:11.000Z', 'title': 'Understanding Gen Alpha Digital Language: Evaluation of LLM Safety\\n  Systems for Content Moderation', 'summary': 'This research offers a unique evaluation of how AI systems interpret the\\ndigital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first\\ncohort raised alongside AI, Gen Alpha faces new forms of online risk due to\\nimmersive digital engagement and a growing mismatch between their evolving\\ncommunication and existing safety tools. Their distinct language, shaped by\\ngaming, memes, and AI-driven trends, often conceals harmful interactions from\\nboth human moderators and automated systems. We assess four leading AI models\\n(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked\\nharassment and manipulation within Gen Alpha discourse. Using a dataset of 100\\nrecent expressions from gaming platforms, social media, and video content, the\\nstudy reveals critical comprehension failures with direct implications for\\nonline safety. This work contributes: (1) a first-of-its-kind dataset capturing\\nGen Alpha expressions; (2) a framework to improve AI moderation systems for\\nyouth protection; (3) a multi-perspective evaluation including AI systems,\\nhuman moderators, and parents, with direct input from Gen Alpha co-researchers;\\nand (4) an analysis of how linguistic divergence increases youth vulnerability.\\nFindings highlight the urgent need to redesign safety systems attuned to youth\\ncommunication, especially given Gen Alpha reluctance to seek help when adults\\nfail to understand their digital world. This study combines the insight of a\\nGen Alpha researcher with systematic academic analysis to address critical\\ndigital safety challenges.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10588.png', 'numComments': 1, 'submittedBy': {'_id': '604eb19e3050a33ebb17ef58', 'avatarUrl': '/avatars/23650002ba3befee83060fe978a251c8.svg', 'fullname': 'Virendra Mehta', 'name': 'Veeru', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2505.14629', 'authors': [{'_id': '682dd6a9e3e00ded42a93674', 'user': {'_id': '615711e6a1fdc424aed1eb48', 'avatarUrl': '/avatars/88279cb7ca39985f618a444128826893.svg', 'isPro': False, 'fullname': 'Fnu Mohbat', 'user': 'mohbattharani', 'type': 'user'}, 'name': 'Fnu Mohbat', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-21T13:35:37.613Z', 'hidden': False}, {'_id': '682dd6a9e3e00ded42a93675', 'name': 'Mohammed J Zaki', 'hidden': False}], 'publishedAt': '2025-05-20T17:19:57.000Z', 'submittedOnDailyAt': '2025-05-21T12:06:10.015Z', 'title': 'KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\\n  Language Models', 'submittedOnDailyBy': {'_id': '615711e6a1fdc424aed1eb48', 'avatarUrl': '/avatars/88279cb7ca39985f618a444128826893.svg', 'isPro': False, 'fullname': 'Fnu Mohbat', 'user': 'mohbattharani', 'type': 'user'}, 'summary': 'Recent advances in large language models (LLMs) and the abundance of food\\ndata have resulted in studies to improve food understanding using LLMs. Despite\\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\\nhas been limited research on integrating food related KGs with LLMs. We\\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\\npersonalized food recommendations and generates recipes with associated\\nmicro-nutritional information. Given a natural language question, KERL extracts\\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\\ncontext to select the recipes that satisfy the constraints. Next, our system\\ngenerates the cooking steps and nutritional information for each recipe. To\\nevaluate our approach, we also develop a benchmark dataset by curating recipe\\nrelated questions, combined with constraints and personal preferences. Through\\nextensive experiments, we show that our proposed KG-augmented LLM significantly\\noutperforms existing approaches, offering a complete and coherent solution for\\nfood recommendation, recipe generation, and nutritional analysis. Our code and\\nbenchmark datasets are publicly available at\\nhttps://github.com/mohbattharani/KERL.', 'upvotes': 0, 'discussionId': '682dd6a9e3e00ded42a93695', 'ai_summary': 'KERL, a unified system combining knowledge graphs and large language models, enhances personalized food recommendations, recipe generation, and nutritional analysis with improved accuracy compared to existing methods.', 'ai_keywords': ['large language models', 'knowledge graphs', 'LLMs', 'KGs', 'personalized food recommendations', 'recipe generation', 'micro-nutritional information', 'entity extraction', 'subgraphs', 'cooking steps', 'nutritional analysis', 'benchmark dataset']}, 'publishedAt': '2025-05-20T13:19:57.000Z', 'title': 'KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\\n  Language Models', 'summary': 'Recent advances in large language models (LLMs) and the abundance of food\\ndata have resulted in studies to improve food understanding using LLMs. Despite\\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\\nhas been limited research on integrating food related KGs with LLMs. We\\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\\npersonalized food recommendations and generates recipes with associated\\nmicro-nutritional information. Given a natural language question, KERL extracts\\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\\ncontext to select the recipes that satisfy the constraints. Next, our system\\ngenerates the cooking steps and nutritional information for each recipe. To\\nevaluate our approach, we also develop a benchmark dataset by curating recipe\\nrelated questions, combined with constraints and personal preferences. Through\\nextensive experiments, we show that our proposed KG-augmented LLM significantly\\noutperforms existing approaches, offering a complete and coherent solution for\\nfood recommendation, recipe generation, and nutritional analysis. Our code and\\nbenchmark datasets are publicly available at\\nhttps://github.com/mohbattharani/KERL.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14629.png', 'numComments': 1, 'submittedBy': {'_id': '615711e6a1fdc424aed1eb48', 'avatarUrl': '/avatars/88279cb7ca39985f618a444128826893.svg', 'fullname': 'Fnu Mohbat', 'name': 'mohbattharani', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2505.14556', 'authors': [{'_id': '682ddb634e8859195a89ee05', 'name': 'Marlène Careil', 'hidden': False}, {'_id': '682ddb634e8859195a89ee06', 'user': {'_id': '6679bc0467be4d7b18a2896d', 'avatarUrl': '/avatars/9cba4040a9e90f2230d8b3c9412ecc26.svg', 'isPro': False, 'fullname': 'Yohann Benchetrit', 'user': 'ybenchetrit', 'type': 'user'}, 'name': 'Yohann Benchetrit', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-05-21T13:55:52.729Z', 'hidden': False}, {'_id': '682ddb634e8859195a89ee07', 'name': 'Jean-Rémi King', 'hidden': False}], 'publishedAt': '2025-05-20T16:14:37.000Z', 'submittedOnDailyAt': '2025-05-21T12:28:19.958Z', 'title': 'Dynadiff: Single-stage Decoding of Images from Continuously Evolving\\n  fMRI', 'submittedOnDailyBy': {'_id': '630620e1df993a789e644129', 'avatarUrl': '/avatars/315c41ecad079ae57350b7662382b2cb.svg', 'isPro': False, 'fullname': 'Careil', 'user': 'Marl', 'type': 'user'}, 'summary': 'Brain-to-image decoding has been recently propelled by the progress in\\ngenerative AI models and the availability of large ultra-high field functional\\nMagnetic Resonance Imaging (fMRI). However, current approaches depend on\\ncomplicated multi-stage pipelines and preprocessing steps that typically\\ncollapse the temporal dimension of brain recordings, thereby limiting\\ntime-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural\\nActivity Diffusion for Image Reconstruction), a new single-stage diffusion\\nmodel designed for reconstructing images from dynamically evolving fMRI\\nrecordings. Our approach offers three main contributions. First, Dynadiff\\nsimplifies training as compared to existing approaches. Second, our model\\noutperforms state-of-the-art models on time-resolved fMRI signals, especially\\non high-level semantic image reconstruction metrics, while remaining\\ncompetitive on preprocessed fMRI data that collapse time. Third, this approach\\nallows a precise characterization of the evolution of image representations in\\nbrain activity. Overall, this work lays the foundation for time-resolved\\nbrain-to-image decoding.', 'upvotes': 0, 'discussionId': '682ddb684e8859195a89eefd', 'ai_summary': 'Dynadiff, a single-stage diffusion model, improves time-resolved brain-to-image decoding by simplifying training and outperforming existing models in semantic image reconstruction.', 'ai_keywords': ['generative AI models', 'fMRI', 'diffusion model', 'Dynadiff', 'time-resolved brain decoders', 'high-level semantic image reconstruction']}, 'publishedAt': '2025-05-20T12:14:37.000Z', 'title': 'Dynadiff: Single-stage Decoding of Images from Continuously Evolving\\n  fMRI', 'summary': 'Brain-to-image decoding has been recently propelled by the progress in\\ngenerative AI models and the availability of large ultra-high field functional\\nMagnetic Resonance Imaging (fMRI). However, current approaches depend on\\ncomplicated multi-stage pipelines and preprocessing steps that typically\\ncollapse the temporal dimension of brain recordings, thereby limiting\\ntime-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural\\nActivity Diffusion for Image Reconstruction), a new single-stage diffusion\\nmodel designed for reconstructing images from dynamically evolving fMRI\\nrecordings. Our approach offers three main contributions. First, Dynadiff\\nsimplifies training as compared to existing approaches. Second, our model\\noutperforms state-of-the-art models on time-resolved fMRI signals, especially\\non high-level semantic image reconstruction metrics, while remaining\\ncompetitive on preprocessed fMRI data that collapse time. Third, this approach\\nallows a precise characterization of the evolution of image representations in\\nbrain activity. Overall, this work lays the foundation for time-resolved\\nbrain-to-image decoding.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14556.png', 'numComments': 1, 'submittedBy': {'_id': '630620e1df993a789e644129', 'avatarUrl': '/avatars/315c41ecad079ae57350b7662382b2cb.svg', 'fullname': 'Careil', 'name': 'Marl', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2505.11563', 'authors': [{'_id': '682db967a9a344c017078ee3', 'user': {'_id': '63ba99e3d90985e7acd820d8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63ba99e3d90985e7acd820d8/9Ywt1MY9UBdmlnVuuSIm-.jpeg', 'isPro': False, 'fullname': 'Alexandre Chapin', 'user': 'Beegbrain', 'type': 'user'}, 'name': 'Alexandre Chapin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-05-21T12:26:48.791Z', 'hidden': False}, {'_id': '682db967a9a344c017078ee4', 'name': 'Bruno Machado', 'hidden': False}, {'_id': '682db967a9a344c017078ee5', 'name': 'Emmanuel Dellandrea', 'hidden': False}, {'_id': '682db967a9a344c017078ee6', 'name': 'Liming Chen', 'hidden': False}], 'publishedAt': '2025-05-16T07:06:37.000Z', 'submittedOnDailyAt': '2025-05-21T11:23:54.496Z', 'title': 'Object-Centric Representations Improve Policy Generalization in Robot\\n  Manipulation', 'submittedOnDailyBy': {'_id': '63ba99e3d90985e7acd820d8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63ba99e3d90985e7acd820d8/9Ywt1MY9UBdmlnVuuSIm-.jpeg', 'isPro': False, 'fullname': 'Alexandre Chapin', 'user': 'Beegbrain', 'type': 'user'}, 'summary': 'Visual representations are central to the learning and generalization\\ncapabilities of robotic manipulation policies. While existing methods rely on\\nglobal or dense features, such representations often entangle task-relevant and\\nirrelevant scene information, limiting robustness under distribution shifts. In\\nthis work, we investigate object-centric representations (OCR) as a structured\\nalternative that segments visual input into a finished set of entities,\\nintroducing inductive biases that align more naturally with manipulation tasks.\\nWe benchmark a range of visual encoders-object-centric, global and dense\\nmethods-across a suite of simulated and real-world manipulation tasks ranging\\nfrom simple to complex, and evaluate their generalization under diverse visual\\nconditions including changes in lighting, texture, and the presence of\\ndistractors. Our findings reveal that OCR-based policies outperform dense and\\nglobal representations in generalization settings, even without task-specific\\npretraining. These insights suggest that OCR is a promising direction for\\ndesigning visual systems that generalize effectively in dynamic, real-world\\nrobotic environments.', 'upvotes': 0, 'discussionId': '682db968a9a344c017078f41', 'ai_keywords': ['object-centric representations (OCR)', 'visual encoders']}, 'publishedAt': '2025-05-16T03:06:37.000Z', 'title': 'Object-Centric Representations Improve Policy Generalization in Robot\\n  Manipulation', 'summary': 'Visual representations are central to the learning and generalization\\ncapabilities of robotic manipulation policies. While existing methods rely on\\nglobal or dense features, such representations often entangle task-relevant and\\nirrelevant scene information, limiting robustness under distribution shifts. In\\nthis work, we investigate object-centric representations (OCR) as a structured\\nalternative that segments visual input into a finished set of entities,\\nintroducing inductive biases that align more naturally with manipulation tasks.\\nWe benchmark a range of visual encoders-object-centric, global and dense\\nmethods-across a suite of simulated and real-world manipulation tasks ranging\\nfrom simple to complex, and evaluate their generalization under diverse visual\\nconditions including changes in lighting, texture, and the presence of\\ndistractors. Our findings reveal that OCR-based policies outperform dense and\\nglobal representations in generalization settings, even without task-specific\\npretraining. These insights suggest that OCR is a promising direction for\\ndesigning visual systems that generalize effectively in dynamic, real-world\\nrobotic environments.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11563.png', 'numComments': 1, 'submittedBy': {'_id': '63ba99e3d90985e7acd820d8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63ba99e3d90985e7acd820d8/9Ywt1MY9UBdmlnVuuSIm-.jpeg', 'fullname': 'Alexandre Chapin', 'name': 'Beegbrain', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 13}, 'isAuthorParticipating': True}"
]