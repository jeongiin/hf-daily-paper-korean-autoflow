[
    "{'paper': {'id': '2502.14776', 'authors': [{'_id': '67bbdb46d94d32bcfba70db7', 'name': 'Xun Liang', 'hidden': False}, {'_id': '67bbdb46d94d32bcfba70db8', 'user': {'_id': '669e60ee8580d17cb60f8347', 'avatarUrl': '/avatars/37963b833228afe39cc24854c9326670.svg', 'isPro': False, 'fullname': 'yang jiawei', 'user': 'Dany-0', 'type': 'user'}, 'name': 'Jiawei Yang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T13:05:10.864Z', 'hidden': False}, {'_id': '67bbdb46d94d32bcfba70db9', 'user': {'_id': '662dd19f9e6d371ab71b91ce', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/662dd19f9e6d371ab71b91ce/mZBPw_Zs8ZlEFGlbekAoH.jpeg', 'isPro': False, 'fullname': 'Yezhaohui Wang', 'user': 'HaruTeru', 'type': 'user'}, 'name': 'Yezhaohui Wang', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-02-24T04:12:46.485Z', 'hidden': False}, {'_id': '67bbdb46d94d32bcfba70dba', 'user': {'_id': '615a0d48b89c239e75b2b019', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1633291509590-noauth.jpeg', 'isPro': False, 'fullname': 'Travis Tang', 'user': 'tangg555', 'type': 'user'}, 'name': 'Chen Tang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T13:05:12.859Z', 'hidden': False}, {'_id': '67bbdb46d94d32bcfba70dbb', 'user': {'_id': '656f47ba2f058b368c0b1611', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/656f47ba2f058b368c0b1611/mrmcmA8bxaDNUhuJQQ7T1.png', 'isPro': False, 'fullname': 'Zifan Zheng', 'user': 'fan2goa1', 'type': 'user'}, 'name': 'Zifan Zheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T09:07:22.303Z', 'hidden': False}, {'_id': '67bbdb46d94d32bcfba70dbc', 'user': {'_id': '66daea8776dbaaa372eabec5', 'avatarUrl': '/avatars/1e5fbe4ff06bb6121c7029253b76b79f.svg', 'isPro': False, 'fullname': 'siminniu', 'user': 'siminniu', 'type': 'user'}, 'name': 'Simin Niu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T11:55:35.171Z', 'hidden': False}, {'_id': '67bbdb46d94d32bcfba70dbd', 'user': {'_id': '656f339a5273668d5b946b33', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/656f339a5273668d5b946b33/o2nBvQiOKKP5IfDmnpHP2.jpeg', 'isPro': False, 'fullname': 'Shichao Song', 'user': 'Ki-Seki', 'type': 'user'}, 'name': 'Shichao Song', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T11:55:41.788Z', 'hidden': False}, {'_id': '67bbdb46d94d32bcfba70dbe', 'user': {'_id': '669e0b93c7cb0568dac6e92e', 'avatarUrl': '/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg', 'isPro': False, 'fullname': 'hanyu Wang', 'user': 'UglyToilet', 'type': 'user'}, 'name': 'Hanyu Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T09:07:20.146Z', 'hidden': False}, {'_id': '67bbdb46d94d32bcfba70dbf', 'name': 'Bo Tang', 'hidden': False}, {'_id': '67bbdb46d94d32bcfba70dc0', 'name': 'Feiyu Xiong', 'hidden': False}, {'_id': '67bbdb46d94d32bcfba70dc1', 'name': 'Keming Mao', 'hidden': False}, {'_id': '67bbdb46d94d32bcfba70dc2', 'name': 'Zhiyu li', 'hidden': False}], 'publishedAt': '2025-02-20T17:59:45.000Z', 'title': 'SurveyX: Academic Survey Automation via Large Language Models', 'summary': 'Large Language Models (LLMs) have demonstrated exceptional comprehension\\ncapabilities and a vast knowledge base, suggesting that LLMs can serve as\\nefficient tools for automated survey generation. However, recent research\\nrelated to automated survey generation remains constrained by some critical\\nlimitations like finite context window, lack of in-depth content discussion,\\nand absence of systematic evaluation frameworks. Inspired by human writing\\nprocesses, we propose SurveyX, an efficient and organized system for automated\\nsurvey generation that decomposes the survey composing process into two phases:\\nthe Preparation and Generation phases. By innovatively introducing online\\nreference retrieval, a pre-processing method called AttributeTree, and a\\nre-polishing process, SurveyX significantly enhances the efficacy of survey\\ncomposition. Experimental evaluation results show that SurveyX outperforms\\nexisting automated survey generation systems in content quality (0.259\\nimprovement) and citation quality (1.76 enhancement), approaching human expert\\nperformance across multiple evaluation dimensions. Examples of surveys\\ngenerated by SurveyX are available on www.surveyx.cn', 'upvotes': 69, 'discussionId': '67bbdb47d94d32bcfba70df3'}, 'publishedAt': '2025-02-23T21:39:54.375Z', 'title': 'SurveyX: Academic Survey Automation via Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14776.png', 'numComments': 4, 'submittedBy': {'_id': '669e60ee8580d17cb60f8347', 'avatarUrl': '/avatars/37963b833228afe39cc24854c9326670.svg', 'fullname': 'yang jiawei', 'name': 'Dany-0', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.11663', 'authors': [{'_id': '67b705d2ebee4662205c47f7', 'user': {'_id': '65d444b1ea28ba508b87ab01', 'avatarUrl': '/avatars/5836c0d64ba3936e064faa8ff4d44de0.svg', 'isPro': False, 'fullname': 'Jingcheng Ni', 'user': 'kiranjc', 'type': 'user'}, 'name': 'Jingcheng Ni', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:06:17.841Z', 'hidden': False}, {'_id': '67b705d2ebee4662205c47f8', 'name': 'Yuxin Guo', 'hidden': False}, {'_id': '67b705d2ebee4662205c47f9', 'user': {'_id': '6572dcc6bbd6664053b1fa6b', 'avatarUrl': '/avatars/aba29efd00bc41f14ce422f7807cd2c3.svg', 'isPro': False, 'fullname': 'Liu Yichen', 'user': 'lyclyc52', 'type': 'user'}, 'name': 'Yichen Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T09:23:40.466Z', 'hidden': False}, {'_id': '67b705d2ebee4662205c47fa', 'name': 'Rui Chen', 'hidden': False}, {'_id': '67b705d2ebee4662205c47fb', 'user': {'_id': '65ead3ea908526a39082e641', 'avatarUrl': '/avatars/dcf870695fd56b06ca03d82f831e9019.svg', 'isPro': False, 'fullname': 'Lewei Lu', 'user': 'luotto', 'type': 'user'}, 'name': 'Lewei Lu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:06:46.453Z', 'hidden': False}, {'_id': '67b705d2ebee4662205c47fc', 'user': {'_id': '65717368be66cd9b65a8201c', 'avatarUrl': '/avatars/fe945828eec9ded4cfa3b89d48a64d90.svg', 'isPro': False, 'fullname': 'Wu Zehuan', 'user': 'wzhgba', 'type': 'user'}, 'name': 'Zehuan Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-21T09:59:38.956Z', 'hidden': False}], 'publishedAt': '2025-02-17T10:53:56.000Z', 'title': 'MaskGWM: A Generalizable Driving World Model with Video Mask\\n  Reconstruction', 'summary': 'World models that forecast environmental changes from actions are vital for\\nautonomous driving models with strong generalization. The prevailing driving\\nworld model mainly build on video prediction model. Although these models can\\nproduce high-fidelity video sequences with advanced diffusion-based generator,\\nthey are constrained by their predictive duration and overall generalization\\ncapabilities. In this paper, we explore to solve this problem by combining\\ngeneration loss with MAE-style feature-level context learning. In particular,\\nwe instantiate this target with three key design: (1) A more scalable Diffusion\\nTransformer (DiT) structure trained with extra mask construction task. (2) we\\ndevise diffusion-related mask tokens to deal with the fuzzy relations between\\nmask reconstruction and generative diffusion process. (3) we extend mask\\nconstruction task to spatial-temporal domain by utilizing row-wise mask for\\nshifted self-attention rather than masked self-attention in MAE. Then, we adopt\\na row-wise cross-view module to align with this mask design. Based on above\\nimprovement, we propose MaskGWM: a Generalizable driving World Model embodied\\nwith Video Mask reconstruction. Our model contains two variants: MaskGWM-long,\\nfocusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view\\ngeneration. Comprehensive experiments on standard benchmarks validate the\\neffectiveness of the proposed method, which contain normal validation of\\nNuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot\\nvalidation of Waymo dataset. Quantitative metrics on these datasets show our\\nmethod notably improving state-of-the-art driving world model.', 'upvotes': 36, 'discussionId': '67b705d4ebee4662205c489c'}, 'publishedAt': '2025-02-24T01:16:03.517Z', 'title': 'MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11663.png', 'numComments': 1, 'submittedBy': {'_id': '65717368be66cd9b65a8201c', 'avatarUrl': '/avatars/fe945828eec9ded4cfa3b89d48a64d90.svg', 'fullname': 'Wu Zehuan', 'name': 'wzhgba', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.15007', 'authors': [{'_id': '67bc1a4a72499ce2ba28cc70', 'user': {'_id': '6172aaeec8e66e2aa84c06b9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg', 'isPro': False, 'fullname': 'Anton Razzhigaev', 'user': 'razzant', 'type': 'user'}, 'name': 'Anton Razzhigaev', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:11:53.576Z', 'hidden': False}, {'_id': '67bc1a4a72499ce2ba28cc71', 'user': {'_id': '64ee45a944f4b3b1bccc02d1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ee45a944f4b3b1bccc02d1/SoidO9HQ4mftzbUPtuBBf.png', 'isPro': False, 'fullname': 'Matvey Mikhalchuk', 'user': 'matveymih', 'type': 'user'}, 'name': 'Matvey Mikhalchuk', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:11:58.863Z', 'hidden': False}, {'_id': '67bc1a4a72499ce2ba28cc72', 'name': 'Temurbek Rahmatullaev', 'hidden': False}, {'_id': '67bc1a4a72499ce2ba28cc73', 'user': {'_id': '6310ff34bc152fa3e810c186', 'avatarUrl': '/avatars/bfd63bcd81548283f5e496e3693bf143.svg', 'isPro': False, 'fullname': 'Elizaveta Goncharova', 'user': 'Elizaveta', 'type': 'user'}, 'name': 'Elizaveta Goncharova', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:12:27.998Z', 'hidden': False}, {'_id': '67bc1a4a72499ce2ba28cc74', 'user': {'_id': '65d5e094cd05bc1eaa0fafc9', 'avatarUrl': '/avatars/ea3d52def6ef4d9af07728a76a499a9f.svg', 'isPro': False, 'fullname': 'Polina Druzhinina', 'user': 'plina2polina', 'type': 'user'}, 'name': 'Polina Druzhinina', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:12:33.837Z', 'hidden': False}, {'_id': '67bc1a4a72499ce2ba28cc75', 'user': {'_id': '6169a581d05945bfd8718dfa', 'avatarUrl': '/avatars/1892ab06a7ddb557232777de3cbec470.svg', 'isPro': False, 'fullname': 'Ivan Oseledets', 'user': 'oseledets', 'type': 'user'}, 'name': 'Ivan Oseledets', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:12:40.295Z', 'hidden': False}, {'_id': '67bc1a4a72499ce2ba28cc76', 'name': 'Andrey Kuznetsov', 'hidden': False}], 'publishedAt': '2025-02-20T19:59:35.000Z', 'title': 'LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context\\n  Memory of Transformers', 'summary': \"We introduce methods to quantify how Large Language Models (LLMs) encode and\\nstore contextual information, revealing that tokens often seen as minor (e.g.,\\ndeterminers, punctuation) carry surprisingly high context. Notably, removing\\nthese tokens -- especially stopwords, articles, and commas -- consistently\\ndegrades performance on MMLU and BABILong-4k, even if removing only irrelevant\\ntokens. Our analysis also shows a strong correlation between contextualization\\nand linearity, where linearity measures how closely the transformation from one\\nlayer's embeddings to the next can be approximated by a single linear mapping.\\nThese findings underscore the hidden importance of filler tokens in maintaining\\ncontext. For further exploration, we present LLM-Microscope, an open-source\\ntoolkit that assesses token-level nonlinearity, evaluates contextual memory,\\nvisualizes intermediate layer contributions (via an adapted Logit Lens), and\\nmeasures the intrinsic dimensionality of representations. This toolkit\\nilluminates how seemingly trivial tokens can be critical for long-range\\nunderstanding.\", 'upvotes': 35, 'discussionId': '67bc1a4c72499ce2ba28cd49'}, 'publishedAt': '2025-02-24T02:07:41.624Z', 'title': 'LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6172aaeec8e66e2aa84c06b9/ZPSmOQ-7Yd7B7YIYiwcTw.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15007.png', 'numComments': 2, 'submittedBy': {'_id': '6172aaeec8e66e2aa84c06b9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg', 'fullname': 'Anton Razzhigaev', 'name': 'razzant', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 9}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.13449', 'authors': [{'_id': '67b7ceae3e8a45f770b2606e', 'user': {'_id': '65633c5e84a9fbe322f87d81', 'avatarUrl': '/avatars/7233a555b43c669847a950ce5697c92c.svg', 'isPro': False, 'fullname': 'DongkiKim', 'user': 'DongkiKim', 'type': 'user'}, 'name': 'Dongki Kim', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-21T09:59:11.214Z', 'hidden': False}, {'_id': '67b7ceae3e8a45f770b2606f', 'user': {'_id': '66d812997e6c9509bb15fac2', 'avatarUrl': '/avatars/baf0e384a864de47bfd989aebe62c357.svg', 'isPro': False, 'fullname': 'Wonbin Lee', 'user': 'WonbinLee067', 'type': 'user'}, 'name': 'Wonbin Lee', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:09:46.291Z', 'hidden': False}, {'_id': '67b7ceae3e8a45f770b26070', 'name': 'Sung Ju Hwang', 'hidden': False}], 'publishedAt': '2025-02-19T05:49:10.000Z', 'title': 'Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular\\n  Language Model', 'summary': \"Understanding molecules is key to understanding organisms and driving\\nadvances in drug discovery, requiring interdisciplinary knowledge across\\nchemistry and biology. Although large molecular language models have achieved\\nnotable success in interpreting molecular structures, their instruction\\ndatasets are limited to the specific knowledge from task-oriented datasets and\\ndo not fully cover the fundamental characteristics of molecules, hindering\\ntheir abilities as general-purpose molecular assistants. To address this issue,\\nwe propose Mol-LLaMA, a large molecular language model that grasps the general\\nknowledge centered on molecules via multi-modal instruction tuning. To this\\nend, we design key data types that encompass the fundamental features of\\nmolecules, incorporating essential knowledge from molecular structures. In\\naddition, to improve understanding of molecular features, we introduce a module\\nthat integrates complementary information from different molecular encoders,\\nleveraging the distinct advantages of different molecular representations. Our\\nexperimental results demonstrate that Mol-LLaMA is capable of comprehending the\\ngeneral features of molecules and generating relevant responses to users'\\nqueries with detailed explanations, implying its potential as a general-purpose\\nassistant for molecular analysis.\", 'upvotes': 31, 'discussionId': '67b7ceae3e8a45f770b2609f'}, 'publishedAt': '2025-02-23T21:52:51.059Z', 'title': 'Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13449.png', 'numComments': 1, 'submittedBy': {'_id': '65633c5e84a9fbe322f87d81', 'avatarUrl': '/avatars/7233a555b43c669847a950ce5697c92c.svg', 'fullname': 'DongkiKim', 'name': 'DongkiKim', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.14397', 'authors': [{'_id': '67bbed806f2833ecccf914dd', 'user': {'_id': '6239ad42cddfae177174bdc5', 'avatarUrl': '/avatars/badc07ff40d9790527b27d87c924e9ee.svg', 'isPro': False, 'fullname': 'Shijie Huang', 'user': 'Humor', 'type': 'user'}, 'name': 'Shijie Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:13:17.369Z', 'hidden': False}, {'_id': '67bbed806f2833ecccf914de', 'user': {'_id': '64311a95034ecbefddd141ef', 'avatarUrl': '/avatars/b6dc5ca373bedbaa368208517954c375.svg', 'isPro': True, 'fullname': 'Yiren Song', 'user': 'yiren98', 'type': 'user'}, 'name': 'Yiren Song', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:13:23.453Z', 'hidden': False}, {'_id': '67bbed806f2833ecccf914df', 'name': 'Yuxuan Zhang', 'hidden': False}, {'_id': '67bbed806f2833ecccf914e0', 'name': 'Hailong Guo', 'hidden': False}, {'_id': '67bbed806f2833ecccf914e1', 'user': {'_id': '65fd9853b329ebf2d40e280a', 'avatarUrl': '/avatars/053e96c4db138cc8948c6350b04617b9.svg', 'isPro': False, 'fullname': 'Wang Xueying', 'user': 'Forever-rover', 'type': 'user'}, 'name': 'Xueyin Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:17:06.041Z', 'hidden': False}, {'_id': '67bbed806f2833ecccf914e2', 'user': {'_id': '661ab3da2b14565c7acccf5c', 'avatarUrl': '/avatars/fa4fc03664803e02aede4d4c3d50b393.svg', 'isPro': False, 'fullname': 'Mike Zheng Shou', 'user': 'AnalMom', 'type': 'user'}, 'name': 'Mike Zheng Shou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:17:12.124Z', 'hidden': False}, {'_id': '67bbed806f2833ecccf914e3', 'name': 'Jiaming Liu', 'hidden': False}], 'publishedAt': '2025-02-20T09:35:38.000Z', 'title': 'PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data', 'summary': \"We introduce PhotoDoodle, a novel image editing framework designed to\\nfacilitate photo doodling by enabling artists to overlay decorative elements\\nonto photographs. Photo doodling is challenging because the inserted elements\\nmust appear seamlessly integrated with the background, requiring realistic\\nblending, perspective alignment, and contextual coherence. Additionally, the\\nbackground must be preserved without distortion, and the artist's unique style\\nmust be captured efficiently from limited training data. These requirements are\\nnot addressed by previous methods that primarily focus on global style transfer\\nor regional inpainting. The proposed method, PhotoDoodle, employs a two-stage\\ntraining strategy. Initially, we train a general-purpose image editing model,\\nOmniEditor, using large-scale data. Subsequently, we fine-tune this model with\\nEditLoRA using a small, artist-curated dataset of before-and-after image pairs\\nto capture distinct editing styles and techniques. To enhance consistency in\\nthe generated results, we introduce a positional encoding reuse mechanism.\\nAdditionally, we release a PhotoDoodle dataset featuring six high-quality\\nstyles. Extensive experiments demonstrate the advanced performance and\\nrobustness of our method in customized image editing, opening new possibilities\\nfor artistic creation.\", 'upvotes': 29, 'discussionId': '67bbed856f2833ecccf915c5'}, 'publishedAt': '2025-02-23T22:55:04.409Z', 'title': 'PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14397.png', 'numComments': 4, 'submittedBy': {'_id': '64311a95034ecbefddd141ef', 'avatarUrl': '/avatars/b6dc5ca373bedbaa368208517954c375.svg', 'fullname': 'Yiren Song', 'name': 'yiren98', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.12084', 'authors': [{'_id': '67b8922ef6632327952ec1e1', 'user': {'_id': '65d8b0f0661492b25c6623de', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65d8b0f0661492b25c6623de/c6LPDse8NIV_3BHIu8dYe.png', 'isPro': False, 'fullname': 'Jianshu Zhang', 'user': 'Sterzhang', 'type': 'user'}, 'name': 'Jianshu Zhang', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-02-21T14:48:16.643Z', 'hidden': False}, {'_id': '67b8922ef6632327952ec1e2', 'user': {'_id': '64b0377121a001042bc0d274', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64b0377121a001042bc0d274/Hk8yI5_s7ey5o9SVZzXrB.png', 'isPro': False, 'fullname': 'Dongyu Yao', 'user': 'RainJamesY', 'type': 'user'}, 'name': 'Dongyu Yao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T09:20:43.528Z', 'hidden': False}, {'_id': '67b8922ef6632327952ec1e3', 'user': {'_id': '63f45b8d520c14618930d175', 'avatarUrl': '/avatars/a20994594579b52a8be8bd2c4acbb913.svg', 'isPro': False, 'fullname': 'renjie', 'user': 'renjiepi', 'type': 'user'}, 'name': 'Renjie Pi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:18:30.254Z', 'hidden': False}, {'_id': '67b8922ef6632327952ec1e4', 'name': 'Paul Pu Liang', 'hidden': False}, {'_id': '67b8922ef6632327952ec1e5', 'name': 'Yi R.', 'hidden': False}, {'_id': '67b8922ef6632327952ec1e6', 'name': 'Fung', 'hidden': False}], 'publishedAt': '2025-02-17T17:57:50.000Z', 'title': 'VLM^2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit\\n  Matching Visual Cues', 'summary': \"Visually linking matching cues is a crucial ability in daily life, such as\\nidentifying the same person in multiple photos based on their cues, even\\nwithout knowing who they are. Despite the extensive knowledge that\\nvision-language models (VLMs) possess, it remains largely unexplored whether\\nthey are capable of performing this fundamental task. To address this, we\\nintroduce VLM^2-Bench, a benchmark designed to assess whether VLMs can\\nVisually Link Matching cues, with 9 subtasks and over 3,000 test cases.\\nComprehensive evaluation across eight open-source VLMs and GPT-4o, along with\\nfurther analysis of various language-side and vision-side prompting methods,\\nleads to a total of eight key findings. We identify critical challenges in\\nmodels' ability to link visual cues, highlighting a significant performance gap\\nwhere even GPT-4o lags 34.80% behind humans. Based on these insights, we\\nadvocate for (i) enhancing core visual capabilities to improve adaptability and\\nreduce reliance on prior knowledge, (ii) establishing clearer principles for\\nintegrating language-based reasoning in vision-centric tasks to prevent\\nunnecessary biases, and (iii) shifting vision-text training paradigms toward\\nfostering models' ability to independently structure and infer relationships\\namong visual cues.\", 'upvotes': 18, 'discussionId': '67b89230f6632327952ec27a'}, 'publishedAt': '2025-02-24T00:36:34.341Z', 'title': 'VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12084.png', 'numComments': 1, 'submittedBy': {'_id': '65d8b0f0661492b25c6623de', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65d8b0f0661492b25c6623de/c6LPDse8NIV_3BHIu8dYe.png', 'fullname': 'Jianshu Zhang', 'name': 'Sterzhang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 8}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.14922', 'authors': [{'_id': '67bbe4ba79e0a705cf573985', 'user': {'_id': '6544c0585a13979f82038a1c', 'avatarUrl': '/avatars/01f3e862d49020e9eaf1728e4ba97bea.svg', 'isPro': False, 'fullname': 'Zeng Zihao', 'user': 'zzh6666', 'type': 'user'}, 'name': 'Zihao Zeng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T13:05:08.989Z', 'hidden': False}, {'_id': '67bbe4ba79e0a705cf573986', 'user': {'_id': '6721dacfc5309c08451d21d5', 'avatarUrl': '/avatars/ac8be5ac8b8ee5b5533214e526b72dad.svg', 'isPro': False, 'fullname': 'Huang Xuyao', 'user': 'ElysiaTrue', 'type': 'user'}, 'name': 'Xuyao Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:19:22.415Z', 'hidden': False}, {'_id': '67bbe4ba79e0a705cf573987', 'name': 'Boxiu Li', 'hidden': False}, {'_id': '67bbe4ba79e0a705cf573988', 'user': {'_id': '673d5f411b0fe168ad4896b2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYgQx6XNi_P5GxlUKbH5G.png', 'isPro': False, 'fullname': 'Zhijie Deng', 'user': 'thudzj', 'type': 'user'}, 'name': 'Zhijie Deng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:19:36.725Z', 'hidden': False}], 'publishedAt': '2025-02-19T17:38:46.000Z', 'title': 'SIFT: Grounding LLM Reasoning in Contexts via Stickers', 'summary': 'This paper identifies the misinterpretation of the context can be a\\nsignificant issue during the reasoning process of large language models,\\nspanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones\\nlike DeepSeek-R1. For example, in the phrase \"10 dollars per kilo,\" LLMs might\\nnot recognize that \"per\" means \"for each,\" leading to calculation errors. We\\nintroduce a novel, post-training approach called **Stick to the Facts (SIFT)**\\nto tackle this. SIFT leverages increasing inference-time compute to ground LLM\\nreasoning in contexts. At the core of SIFT lies the *Sticker*, which is\\ngenerated by the model itself to explicitly emphasize the key information\\nwithin the context. Given the curated Sticker, SIFT generates two predictions\\n-- one from the original query and one from the query augmented with the\\nSticker. If they differ, the Sticker is sequentially refined via *forward*\\noptimization (to better align the extracted facts with the query) and *inverse*\\ngeneration (to conform with the model\\'s inherent tendencies) for more faithful\\nreasoning outcomes. Studies across diverse models (from 3B to 100B+) and\\nbenchmarks (e.g., GSM8K, MATH-500) reveal consistent performance improvements.\\nNotably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from\\n78.33% to **85.67**%, establishing a new state-of-the-art in the open-source\\ncommunity. The code is available at https://github.com/zhijie-group/SIFT.', 'upvotes': 16, 'discussionId': '67bbe4bb79e0a705cf5739c3'}, 'publishedAt': '2025-02-23T22:17:18.309Z', 'title': 'SIFT: Grounding LLM Reasoning in Contexts via Stickers', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14922.png', 'numComments': 2, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 6194}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.15589', 'authors': [{'_id': '67bbfe2d670ece8d9184f339', 'user': {'_id': '63f89fe7565506a9cadcd2cf', 'avatarUrl': '/avatars/7eb449a1109dcff051cb3ba680f0c082.svg', 'isPro': False, 'fullname': 'Jintian Zhang', 'user': 'MikeDean', 'type': 'user'}, 'name': 'Jintian Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:22:07.730Z', 'hidden': False}, {'_id': '67bbfe2d670ece8d9184f33a', 'name': 'Yuqi Zhu', 'hidden': False}, {'_id': '67bbfe2d670ece8d9184f33b', 'user': {'_id': '64f6d1ed46284aa28d9abf6c', 'avatarUrl': '/avatars/d6beaecfd00345e4a664862fff217427.svg', 'isPro': False, 'fullname': 'Sun mengshu', 'user': 'sunmengshu', 'type': 'user'}, 'name': 'Mengshu Sun', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:22:36.798Z', 'hidden': False}, {'_id': '67bbfe2d670ece8d9184f33c', 'user': {'_id': '67603f17d4f2ded0c1498358', 'avatarUrl': '/avatars/97205504757d7eb33512ab96b2ecde28.svg', 'isPro': False, 'fullname': 'yujieluo', 'user': 'yujieluo1031', 'type': 'user'}, 'name': 'Yujie Luo', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:22:42.818Z', 'hidden': False}, {'_id': '67bbfe2d670ece8d9184f33d', 'user': {'_id': '6447800f30fa4ecb85ddad80', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6447800f30fa4ecb85ddad80/NsmXIaMsWctmTNA7tFVkX.jpeg', 'isPro': False, 'fullname': 'Shuofei Qiao', 'user': 'GoooDte', 'type': 'user'}, 'name': 'Shuofei Qiao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T09:07:02.722Z', 'hidden': False}, {'_id': '67bbfe2d670ece8d9184f33e', 'name': 'Lun Du', 'hidden': False}, {'_id': '67bbfe2d670ece8d9184f33f', 'user': {'_id': '66270e0026d5a3eee310ad53', 'avatarUrl': '/avatars/db34068c114c348de296e00b1b5a5b9b.svg', 'isPro': False, 'fullname': 'Da Zheng', 'user': 'zhengda1936', 'type': 'user'}, 'name': 'Da Zheng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:22:53.988Z', 'hidden': False}, {'_id': '67bbfe2d670ece8d9184f340', 'user': {'_id': '64931296137833d7ec7689cd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64931296137833d7ec7689cd/TBihNdp1ZwIWjhfAWjRr6.jpeg', 'isPro': False, 'fullname': 'Huajun Chen', 'user': 'huajunsir', 'type': 'user'}, 'name': 'Huajun Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:24:31.548Z', 'hidden': False}, {'_id': '67bbfe2d670ece8d9184f341', 'user': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'isPro': False, 'fullname': 'Ningyu Zhang', 'user': 'Ningyu', 'type': 'user'}, 'name': 'Ningyu Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T09:07:04.794Z', 'hidden': False}], 'publishedAt': '2025-02-21T16:57:22.000Z', 'title': 'LightThinker: Thinking Step-by-Step Compression', 'summary': 'Large language models (LLMs) have shown remarkable performance in complex\\nreasoning tasks, but their efficiency is hindered by the substantial memory and\\ncomputational costs associated with generating lengthy tokens. In this paper,\\nwe propose LightThinker, a novel method that enables LLMs to dynamically\\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\\nprocesses, LightThinker compresses verbose thought steps into compact\\nrepresentations and discards the original reasoning chains, thereby\\nsignificantly reducing the number of tokens stored in the context window. This\\nis achieved by training the model on when and how to perform compression\\nthrough data construction, mapping hidden states to condensed gist tokens, and\\ncreating specialized attention masks. Additionally, we introduce the Dependency\\n(Dep) metric to quantify the degree of compression by measuring the reliance on\\nhistorical tokens during generation. Extensive experiments on four datasets and\\ntwo models show that LightThinker reduces peak memory usage and inference time,\\nwhile maintaining competitive accuracy. Our work provides a new direction for\\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\\nperformance. Code will be released at https://github.com/zjunlp/LightThinker.', 'upvotes': 14, 'discussionId': '67bbfe2f670ece8d9184f3a4'}, 'publishedAt': '2025-02-24T00:07:05.804Z', 'title': 'LightThinker: Thinking Step-by-Step Compression', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/dhGMWf_tcPkvQlRm5DbD6.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15589.png', 'numComments': 1, 'submittedBy': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'fullname': 'Ningyu Zhang', 'name': 'Ningyu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 19}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.14494', 'authors': [{'_id': '67b9dda03593f69f41cdb5d3', 'user': {'_id': '6434f5a5a4c9c55871ae888f', 'avatarUrl': '/avatars/058389c773a67b2b03d44556f0ee43d1.svg', 'isPro': False, 'fullname': 'Jinnan Li', 'user': 'Jinnan', 'type': 'user'}, 'name': 'Jinnan Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:24:42.884Z', 'hidden': False}, {'_id': '67b9dda03593f69f41cdb5d4', 'user': {'_id': '67658bd7f7ac7e978ab6f957', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/c8VBgFckkZNUGeqUyotwq.png', 'isPro': False, 'fullname': 'Jinzhe Li', 'user': 'JinzheFudan', 'type': 'user'}, 'name': 'Jinzhe Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:24:50.056Z', 'hidden': True}, {'_id': '67b9dda03593f69f41cdb5d5', 'name': 'Yue Wang', 'hidden': False}, {'_id': '67b9dda03593f69f41cdb5d6', 'name': 'Yi Chang', 'hidden': False}, {'_id': '67b9dda03593f69f41cdb5d7', 'name': 'Yuan Wu', 'hidden': False}], 'publishedAt': '2025-02-20T12:22:18.000Z', 'title': 'StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction\\n  Following', 'summary': \"Multi-turn instruction following capability constitutes a core competency of\\nlarge language models (LLMs) in real-world applications. Existing evaluation\\nbenchmarks predominantly focus on fine-grained constraint satisfaction and\\ndomain-specific capability assessment, yet overlook the crucial structural\\ndependency between dialogue turns that distinguishes multi-turn from\\nsingle-turn interactions. This structural dependency not only reflects user\\nintent but also establishes a second dimension for instruction following\\nevaluation beyond constraint satisfaction. To address this gap, we propose\\nStructFlowBench, a multi-turn instruction following benchmark with structural\\nflow modeling. The benchmark innovatively defines a structural flow framework\\ncomprising six fundamental inter-turn relationships, which not only introduces\\nnovel structural constraints for model evaluation but also serves as generation\\nparameters for creating customized dialogue flows tailored to specific\\nscenarios. Adopting established LLM-based automatic evaluation methodologies,\\nwe conduct systematic evaluations of 13 leading open-source and closed-source\\nLLMs. Experimental results reveal significant deficiencies in current models'\\ncomprehension of multi-turn dialogue structures. The code is available at\\nhttps://github.com/MLGroupJLU/StructFlowBench.\", 'upvotes': 11, 'discussionId': '67b9dda13593f69f41cdb635'}, 'publishedAt': '2025-02-23T23:43:43.529Z', 'title': 'StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14494.png', 'numComments': 1, 'submittedBy': {'_id': '670e57b3391f1a7021182bff', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/N0tuHZVz8KFPCv8G1qUX2.png', 'fullname': 'Yuan Wu', 'name': 'WhiteCatY', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.15086', 'authors': [{'_id': '67bbfc7ab3920fd18e63cb26', 'user': {'_id': '60af909e288a0f96f6cefc4d', 'avatarUrl': '/avatars/44a8ab48acb58c45de0b0947a1b56e7c.svg', 'isPro': False, 'fullname': 'Yeonjun In', 'user': 'Yeonjun', 'type': 'user'}, 'name': 'Yeonjun In', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T09:07:06.533Z', 'hidden': False}, {'_id': '67bbfc7ab3920fd18e63cb27', 'user': {'_id': '67bc62b77727595ca5b6a4ca', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/JXilVx4ACCw6L9BnomqVP.png', 'isPro': False, 'fullname': 'Wonjoong Kim', 'user': 'wjkim0229', 'type': 'user'}, 'name': 'Wonjoong Kim', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T13:05:06.940Z', 'hidden': False}, {'_id': '67bbfc7ab3920fd18e63cb28', 'name': 'Kanghoon Yoon', 'hidden': False}, {'_id': '67bbfc7ab3920fd18e63cb29', 'user': {'_id': '6250611d2f9acc6168e42737', 'avatarUrl': '/avatars/1e053d3fa387d81b45a2435e4a633ad1.svg', 'isPro': False, 'fullname': 'Sungchul Kim', 'user': 'subright', 'type': 'user'}, 'name': 'Sungchul Kim', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:28:51.033Z', 'hidden': False}, {'_id': '67bbfc7ab3920fd18e63cb2a', 'user': {'_id': '6366e2d9575c93ceda0791d8', 'avatarUrl': '/avatars/a53cb1bb7cd9c63a2520587108ffe962.svg', 'isPro': False, 'fullname': 'Mehrab Tanjim', 'user': 'Mehrab-Tanjim', 'type': 'user'}, 'name': 'Mehrab Tanjim', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:28:37.445Z', 'hidden': False}, {'_id': '67bbfc7ab3920fd18e63cb2b', 'user': {'_id': '64b73688dcbce176037ef420', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64b73688dcbce176037ef420/R-fcEaZz5vrY74QO5oclB.jpeg', 'isPro': False, 'fullname': 'Kibum Kim', 'user': 'kb-kim', 'type': 'user'}, 'name': 'Kibum Kim', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:28:30.900Z', 'hidden': False}, {'_id': '67bbfc7ab3920fd18e63cb2c', 'name': 'Chanyoung Park', 'hidden': False}], 'publishedAt': '2025-02-20T22:58:44.000Z', 'title': 'Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of\\n  Large Language Models', 'summary': 'As the use of large language model (LLM) agents continues to grow, their\\nsafety vulnerabilities have become increasingly evident. Extensive benchmarks\\nevaluate various aspects of LLM safety by defining the safety relying heavily\\non general standards, overlooking user-specific standards. However, safety\\nstandards for LLM may vary based on a user-specific profiles rather than being\\nuniversally consistent across all users. This raises a critical research\\nquestion: Do LLM agents act safely when considering user-specific safety\\nstandards? Despite its importance for safe LLM use, no benchmark datasets\\ncurrently exist to evaluate the user-specific safety of LLMs. To address this\\ngap, we introduce U-SAFEBENCH, the first benchmark designed to assess\\nuser-specific aspect of LLM safety. Our evaluation of 18 widely used LLMs\\nreveals current LLMs fail to act safely when considering user-specific safety\\nstandards, marking a new discovery in this field. To address this\\nvulnerability, we propose a simple remedy based on chain-of-thought,\\ndemonstrating its effectiveness in improving user-specific safety. Our\\nbenchmark and code are available at https://github.com/yeonjun-in/U-SafeBench.', 'upvotes': 9, 'discussionId': '67bbfc7bb3920fd18e63cb55'}, 'publishedAt': '2025-02-24T07:41:08.352Z', 'title': 'Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/60af909e288a0f96f6cefc4d/HK-3ALEi_S7sCnikcsULP.png', 'https://cdn-uploads.huggingface.co/production/uploads/60af909e288a0f96f6cefc4d/5ibSAEzxHYtYUIK4FpSkp.png', 'https://cdn-uploads.huggingface.co/production/uploads/60af909e288a0f96f6cefc4d/r727Pzrq-PvVKdetEl5U6.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15086.png', 'numComments': 1, 'submittedBy': {'_id': '60af909e288a0f96f6cefc4d', 'avatarUrl': '/avatars/44a8ab48acb58c45de0b0947a1b56e7c.svg', 'fullname': 'Yeonjun In', 'name': 'Yeonjun', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.14949', 'authors': [{'_id': '67bc4ced7727595ca5b108f1', 'user': {'_id': '656864e12d73834278a8dea7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg', 'isPro': True, 'fullname': 'Ahmed Heakl', 'user': 'ahmedheakl', 'type': 'user'}, 'name': 'Ahmed Heakl', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T10:58:23.973Z', 'hidden': False}, {'_id': '67bc4ced7727595ca5b108f2', 'user': {'_id': '672e4574b60c3a27d783a1ac', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/aut4W4hJcOT8jvQnlWs-y.png', 'isPro': False, 'fullname': 'Muhammad Abdullah', 'user': 'mabdullahsohail', 'type': 'user'}, 'name': 'Abdullah Sohail', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T13:05:04.733Z', 'hidden': False}, {'_id': '67bc4ced7727595ca5b108f3', 'user': {'_id': '65262a396b41932089fd7bae', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png', 'isPro': False, 'fullname': 'Mukul Ranjan', 'user': 'mukul54', 'type': 'user'}, 'name': 'Mukul Ranjan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T11:54:56.690Z', 'hidden': False}, {'_id': '67bc4ced7727595ca5b108f4', 'name': 'Rania Hossam', 'hidden': False}, {'_id': '67bc4ced7727595ca5b108f5', 'name': 'Ghazi Ahmed', 'hidden': False}, {'_id': '67bc4ced7727595ca5b108f6', 'name': 'Mohamed El-Geish', 'hidden': False}, {'_id': '67bc4ced7727595ca5b108f7', 'name': 'Omar Maher', 'hidden': False}, {'_id': '67bc4ced7727595ca5b108f8', 'name': 'Zhiqiang Shen', 'hidden': False}, {'_id': '67bc4ced7727595ca5b108f9', 'name': 'Fahad Khan', 'hidden': False}, {'_id': '67bc4ced7727595ca5b108fa', 'name': 'Salman Khan', 'hidden': False}], 'publishedAt': '2025-02-20T18:41:23.000Z', 'title': 'KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and\\n  Document Understanding', 'summary': 'With the growing adoption of Retrieval-Augmented Generation (RAG) in document\\nprocessing, robust text recognition has become increasingly critical for\\nknowledge extraction. While OCR (Optical Character Recognition) for English and\\nother languages benefits from large datasets and well-established benchmarks,\\nArabic OCR faces unique challenges due to its cursive script, right-to-left\\ntext flow, and complex typographic and calligraphic features. We present\\nKITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in\\ncurrent evaluation systems. Our benchmark comprises 8,809 samples across 9\\nmajor domains and 36 sub-domains, encompassing diverse document types including\\nhandwritten text, structured tables, and specialized coverage of 21 chart types\\nfor business intelligence. Our findings show that modern vision-language models\\n(such as GPT-4, Gemini, and Qwen) outperform traditional OCR approaches (like\\nEasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate\\n(CER). Furthermore, we highlight significant limitations of current Arabic OCR\\nmodels, particularly in PDF-to-Markdown conversion, where the best model\\nGemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in\\naccurately recognizing Arabic text, including issues with complex fonts,\\nnumeral recognition errors, word elongation, and table structure detection.\\nThis work establishes a rigorous evaluation framework that can drive\\nimprovements in Arabic document analysis methods and bridge the performance gap\\nwith English OCR technologies.', 'upvotes': 5, 'discussionId': '67bc4cee7727595ca5b10967'}, 'publishedAt': '2025-02-24T05:43:47.767Z', 'title': 'KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14949.png', 'numComments': 1, 'submittedBy': {'_id': '656864e12d73834278a8dea7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg', 'fullname': 'Ahmed Heakl', 'name': 'ahmedheakl', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 26}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.14637', 'authors': [{'_id': '67bc3393057a4685851067c9', 'user': {'_id': '63021e665e305a35cb09cb35', 'avatarUrl': '/avatars/442e61765cb755f55540192e9a80cf80.svg', 'isPro': False, 'fullname': 'AngxiaoYue', 'user': 'AngxiaoYue', 'type': 'user'}, 'name': 'Angxiao Yue', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T09:06:54.271Z', 'hidden': False}, {'_id': '67bc3393057a4685851067ca', 'user': {'_id': '669a2fd15bd3f749a3eb7b65', 'avatarUrl': '/avatars/7b23f892d2dc8fc87f62469dc02524ac.svg', 'isPro': False, 'fullname': 'ZiChong Wang', 'user': 'EatEatEatEat', 'type': 'user'}, 'name': 'Zichong Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T11:54:58.392Z', 'hidden': False}, {'_id': '67bc3393057a4685851067cb', 'user': {'_id': '67bc72956d5bfdc989e194dd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/LB6fZyznoTrPKFUDJia2i.png', 'isPro': False, 'fullname': 'Hongteng Xu', 'user': 'Hongteng', 'type': 'user'}, 'name': 'Hongteng Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T13:24:57.462Z', 'hidden': False}], 'publishedAt': '2025-02-20T15:20:37.000Z', 'title': 'ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality\\n  Protein Backbone Generation', 'summary': 'Protein backbone generation plays a central role in de novo protein design\\nand is significant for many biological and medical applications. Although\\ndiffusion and flow-based generative models provide potential solutions to this\\nchallenging task, they often generate proteins with undesired designability and\\nsuffer computational inefficiency. In this study, we propose a novel rectified\\nquaternion flow (ReQFlow) matching method for fast and high-quality protein\\nbackbone generation. In particular, our method generates a local translation\\nand a 3D rotation from random noise for each residue in a protein chain, which\\nrepresents each 3D rotation as a unit quaternion and constructs its flow by\\nspherical linear interpolation (SLERP) in an exponential format. We train the\\nmodel by quaternion flow (QFlow) matching with guaranteed numerical stability\\nand rectify the QFlow model to accelerate its inference and improve the\\ndesignability of generated protein backbones, leading to the proposed ReQFlow\\nmodel. Experiments show that ReQFlow achieves state-of-the-art performance in\\nprotein backbone generation while requiring much fewer sampling steps and\\nsignificantly less inference time (e.g., being 37x faster than RFDiffusion and\\n62x faster than Genie2 when generating a backbone of length 300), demonstrating\\nits effectiveness and efficiency. The code is available at\\nhttps://github.com/AngxiaoYue/ReQFlow.', 'upvotes': 5, 'discussionId': '67bc3396057a468585106864'}, 'publishedAt': '2025-02-24T05:35:44.667Z', 'title': 'ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14637.png', 'numComments': 2, 'submittedBy': {'_id': '63021e665e305a35cb09cb35', 'avatarUrl': '/avatars/442e61765cb755f55540192e9a80cf80.svg', 'fullname': 'AngxiaoYue', 'name': 'AngxiaoYue', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.13189', 'authors': [{'_id': '67b7152f299e4d30f9eb41c2', 'name': 'Enzhe Lu', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41c3', 'name': 'Zhejun Jiang', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41c4', 'name': 'Jingyuan Liu', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41c5', 'name': 'Yulun Du', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41c6', 'name': 'Tao Jiang', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41c7', 'name': 'Chao Hong', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41c8', 'name': 'Shaowei Liu', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41c9', 'name': 'Weiran He', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41ca', 'name': 'Enming Yuan', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41cb', 'name': 'Yuzhi Wang', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41cc', 'name': 'Zhiqi Huang', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41cd', 'name': 'Huan Yuan', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41ce', 'name': 'Suting Xu', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41cf', 'name': 'Xinran Xu', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41d0', 'name': 'Guokun Lai', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41d1', 'name': 'Yanru Chen', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41d2', 'name': 'Huabin Zheng', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41d3', 'name': 'Junjie Yan', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41d4', 'name': 'Jianlin Su', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41d5', 'name': 'Yuxin Wu', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41d6', 'name': 'Neo Y. Zhang', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41d7', 'name': 'Zhilin Yang', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41d8', 'name': 'Xinyu Zhou', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41d9', 'name': 'Mingxing Zhang', 'hidden': False}, {'_id': '67b7152f299e4d30f9eb41da', 'name': 'Jiezhong Qiu', 'hidden': False}], 'publishedAt': '2025-02-18T14:06:05.000Z', 'title': 'MoBA: Mixture of Block Attention for Long-Context LLMs', 'summary': \"Scaling the effective context length is essential for advancing large\\nlanguage models (LLMs) toward artificial general intelligence (AGI). However,\\nthe quadratic increase in computational complexity inherent in traditional\\nattention mechanisms presents a prohibitive overhead. Existing approaches\\neither impose strongly biased structures, such as sink or window attention\\nwhich are task-specific, or radically modify the attention mechanism into\\nlinear approximations, whose performance in complex reasoning tasks remains\\ninadequately explored.\\n  In this work, we propose a solution that adheres to the ``less structure''\\nprinciple, allowing the model to determine where to attend autonomously, rather\\nthan introducing predefined biases. We introduce Mixture of Block Attention\\n(MoBA), an innovative approach that applies the principles of Mixture of\\nExperts (MoE) to the attention mechanism. This novel architecture demonstrates\\nsuperior performance on long-context tasks while offering a key advantage: the\\nability to seamlessly transition between full and sparse attention, enhancing\\nefficiency without the risk of compromising performance. MoBA has already been\\ndeployed to support Kimi's long-context requests and demonstrates significant\\nadvancements in efficient attention computation for LLMs. Our code is available\\nat https://github.com/MoonshotAI/MoBA.\", 'upvotes': 4, 'discussionId': '67b71530299e4d30f9eb4213'}, 'publishedAt': '2025-02-24T04:52:30.963Z', 'title': 'MoBA: Mixture of Block Attention for Long-Context LLMs', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13189.png', 'numComments': 1, 'submittedBy': {'_id': '63a369d98c0c89dcae3b8329', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png', 'fullname': 'Adina Yakefu', 'name': 'AdinaY', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 421}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.15027', 'authors': [{'_id': '67bbdcec79fcd85f09ddd869', 'user': {'_id': '647d7eb9770c299e56f5b39b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/647d7eb9770c299e56f5b39b/CC5JJgkyLkXOxw-BeT4G5.jpeg', 'isPro': False, 'fullname': 'Hengyuan Zhao', 'user': 'hhenryz', 'type': 'user'}, 'name': 'Henry Hengyuan Zhao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-24T13:32:25.051Z', 'hidden': False}, {'_id': '67bbdcec79fcd85f09ddd86a', 'name': 'Wenqi Pei', 'hidden': False}, {'_id': '67bbdcec79fcd85f09ddd86b', 'name': 'Yifei Tao', 'hidden': False}, {'_id': '67bbdcec79fcd85f09ddd86c', 'name': 'Haiyang Mei', 'hidden': False}, {'_id': '67bbdcec79fcd85f09ddd86d', 'name': 'Mike Zheng Shou', 'hidden': False}], 'publishedAt': '2025-02-20T20:27:06.000Z', 'title': 'InterFeedback: Unveiling Interactive Intelligence of Large Multimodal\\n  Models via Human Feedback', 'summary': \"Existing benchmarks do not test Large Multimodal Models (LMMs) on their\\ninteractive intelligence with human users which is vital for developing\\ngeneral-purpose AI assistants. We design InterFeedback, an interactive\\nframework, which can be applied to any LMM and dataset to assess this ability\\nautonomously. On top of this, we introduce InterFeedback-Bench which evaluates\\ninteractive intelligence using two representative datasets, MMMU-Pro and\\nMathVerse, to test 10 different open-source LMMs. Additionally, we present\\nInterFeedback-Human, a newly collected dataset of 120 cases designed for\\nmanually testing interactive performance in leading models such as OpenAI-o1\\nand Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art\\nLMM (like OpenAI-o1) can correct their results through human feedback less than\\n50%. Our findings point to the need for methods that can enhance the LMMs'\\ncapability to interpret and benefit from feedback.\", 'upvotes': 4, 'discussionId': '67bbdced79fcd85f09ddd8da'}, 'publishedAt': '2025-02-23T21:44:33.443Z', 'title': 'InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15027.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 6194}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.15631', 'authors': [{'_id': '67bbdbe8ea3003f47f15d036', 'name': 'Marthe Ballon', 'hidden': False}, {'_id': '67bbdbe8ea3003f47f15d037', 'name': 'Andres Algaba', 'hidden': False}, {'_id': '67bbdbe8ea3003f47f15d038', 'name': 'Vincent Ginis', 'hidden': False}], 'publishedAt': '2025-02-21T17:59:13.000Z', 'title': 'The Relationship Between Reasoning and Performance in Large Language\\n  Models -- o3 (mini) Thinks Harder, Not Longer', 'summary': 'Large language models have demonstrated remarkable progress in mathematical\\nreasoning, leveraging chain-of-thought and test-time compute scaling. However,\\nmany open questions remain regarding the interplay between reasoning token\\nusage and accuracy gains. In particular, when comparing models across\\ngenerations, it is unclear whether improved performance results from longer\\nreasoning chains or more efficient reasoning. We systematically analyze\\nchain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH\\nbenchmark, finding that o3-mini (m) achieves superior accuracy without\\nrequiring longer reasoning chains than o1-mini. Moreover, we show that accuracy\\ngenerally declines as reasoning chains grow across all models and compute\\nsettings, even when controlling for difficulty of the questions. This accuracy\\ndrop is significantly smaller in more proficient models, suggesting that new\\ngenerations of reasoning models use test-time compute more effectively.\\nFinally, we highlight that while o3-mini (h) achieves a marginal accuracy gain\\nover o3-mini (m), it does so by allocating substantially more reasoning tokens\\nacross all problems, even the ones that o3-mini (m) can already solve. These\\nfindings provide new insights into the relationship between model capability\\nand reasoning length, with implications for efficiency, scaling, and evaluation\\nmethodologies.', 'upvotes': 4, 'discussionId': '67bbdbefea3003f47f15d226'}, 'publishedAt': '2025-02-23T21:40:17.216Z', 'title': 'The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15631.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 6194}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.15422', 'authors': [{'_id': '67bc53ad670ece8d919a8fe1', 'user': {'_id': '6639f75a910d619c288f8a86', 'avatarUrl': '/avatars/4b20a056798c009eaf665b0e3021db60.svg', 'isPro': False, 'fullname': 'sanghee park', 'user': 'sangheeeee', 'type': 'user'}, 'name': 'Sanghee Park', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T11:54:51.633Z', 'hidden': False}, {'_id': '67bc53ad670ece8d919a8fe2', 'user': {'_id': '6298362c9d3de7b32fd11526', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1658473855720-6298362c9d3de7b32fd11526.jpeg', 'isPro': False, 'fullname': 'Geewook Kim', 'user': 'gwkrsrch', 'type': 'user'}, 'name': 'Geewook Kim', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T13:04:57.334Z', 'hidden': False}], 'publishedAt': '2025-02-21T12:46:40.000Z', 'title': 'Evaluating Multimodal Generative AI with Korean Educational Standards', 'summary': 'This paper presents the Korean National Educational Test Benchmark (KoNET), a\\nnew benchmark designed to evaluate Multimodal Generative AI Systems using\\nKorean national educational tests. KoNET comprises four exams: the Korean\\nElementary General Educational Development Test (KoEGED), Middle (KoMGED), High\\n(KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are\\nrenowned for their rigorous standards and diverse questions, facilitating a\\ncomprehensive analysis of AI performance across different educational levels.\\nBy focusing on Korean, KoNET provides insights into model performance in\\nless-explored languages. We assess a range of models - open-source,\\nopen-access, and closed APIs - by examining difficulties, subject diversity,\\nand human error rates. The code and dataset builder will be made fully\\nopen-sourced at https://github.com/naver-ai/KoNET.', 'upvotes': 3, 'discussionId': '67bc53ae670ece8d919a901a'}, 'publishedAt': '2025-02-24T06:58:46.440Z', 'title': 'Evaluating Multimodal Generative AI with Korean Educational Standards', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15422.png', 'numComments': 1, 'submittedBy': {'_id': '6639f75a910d619c288f8a86', 'avatarUrl': '/avatars/4b20a056798c009eaf665b0e3021db60.svg', 'fullname': 'sanghee park', 'name': 'sangheeeee', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.15657', 'authors': [{'_id': '67bbfd6c3593f69f41512d54', 'name': 'Yoshua Bengio', 'hidden': False}, {'_id': '67bbfd6c3593f69f41512d55', 'name': 'Michael Cohen', 'hidden': False}, {'_id': '67bbfd6c3593f69f41512d56', 'name': 'Damiano Fornasiere', 'hidden': False}, {'_id': '67bbfd6c3593f69f41512d57', 'name': 'Joumana Ghosn', 'hidden': False}, {'_id': '67bbfd6c3593f69f41512d58', 'name': 'Pietro Greiner', 'hidden': False}, {'_id': '67bbfd6c3593f69f41512d59', 'name': 'Matt MacDermott', 'hidden': False}, {'_id': '67bbfd6c3593f69f41512d5a', 'name': 'Sören Mindermann', 'hidden': False}, {'_id': '67bbfd6c3593f69f41512d5b', 'name': 'Adam Oberman', 'hidden': False}, {'_id': '67bbfd6c3593f69f41512d5c', 'name': 'Jesse Richardson', 'hidden': False}, {'_id': '67bbfd6c3593f69f41512d5d', 'name': 'Oliver Richardson', 'hidden': False}, {'_id': '67bbfd6c3593f69f41512d5e', 'name': 'Marc-Antoine Rondeau', 'hidden': False}, {'_id': '67bbfd6c3593f69f41512d5f', 'name': 'Pierre-Luc St-Charles', 'hidden': False}, {'_id': '67bbfd6c3593f69f41512d60', 'name': 'David Williams-King', 'hidden': False}], 'publishedAt': '2025-02-21T18:28:36.000Z', 'title': 'Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer\\n  a Safer Path?', 'summary': 'The leading AI companies are increasingly focused on building generalist AI\\nagents -- systems that can autonomously plan, act, and pursue goals across\\nalmost all tasks that humans can perform. Despite how useful these systems\\nmight be, unchecked AI agency poses significant risks to public safety and\\nsecurity, ranging from misuse by malicious actors to a potentially irreversible\\nloss of human control. We discuss how these risks arise from current AI\\ntraining methods. Indeed, various scenarios and experiments have demonstrated\\nthe possibility of AI agents engaging in deception or pursuing goals that were\\nnot specified by human operators and that conflict with human interests, such\\nas self-preservation. Following the precautionary principle, we see a strong\\nneed for safer, yet still useful, alternatives to the current agency-driven\\ntrajectory. Accordingly, we propose as a core building block for further\\nadvances the development of a non-agentic AI system that is trustworthy and\\nsafe by design, which we call Scientist AI. This system is designed to explain\\nthe world from observations, as opposed to taking actions in it to imitate or\\nplease humans. It comprises a world model that generates theories to explain\\ndata and a question-answering inference machine. Both components operate with\\nan explicit notion of uncertainty to mitigate the risks of overconfident\\npredictions. In light of these considerations, a Scientist AI could be used to\\nassist human researchers in accelerating scientific progress, including in AI\\nsafety. In particular, our system can be employed as a guardrail against AI\\nagents that might be created despite the risks involved. Ultimately, focusing\\non non-agentic AI may enable the benefits of AI innovation while avoiding the\\nrisks associated with the current trajectory. We hope these arguments will\\nmotivate researchers, developers, and policymakers to favor this safer path.', 'upvotes': 3, 'discussionId': '67bbfd6c3593f69f41512d96'}, 'publishedAt': '2025-02-24T00:02:52.495Z', 'title': 'Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15657.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 63}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.15681', 'authors': [{'_id': '67bbe67c7727595ca5979d2a', 'name': 'Yilun Xu', 'hidden': False}, {'_id': '67bbe67c7727595ca5979d2b', 'name': 'Weili Nie', 'hidden': False}, {'_id': '67bbe67c7727595ca5979d2c', 'name': 'Arash Vahdat', 'hidden': False}], 'publishedAt': '2025-02-21T18:59:20.000Z', 'title': 'One-step Diffusion Models with f-Divergence Distribution Matching', 'summary': \"Sampling from diffusion models involves a slow iterative process that hinders\\ntheir practical deployment, especially for interactive applications. To\\naccelerate generation speed, recent approaches distill a multi-step diffusion\\nmodel into a single-step student generator via variational score distillation,\\nwhich matches the distribution of samples generated by the student to the\\nteacher's distribution. However, these approaches use the reverse\\nKullback-Leibler (KL) divergence for distribution matching which is known to be\\nmode seeking. In this paper, we generalize the distribution matching approach\\nusing a novel f-divergence minimization framework, termed f-distill, that\\ncovers different divergences with different trade-offs in terms of mode\\ncoverage and training variance. We derive the gradient of the f-divergence\\nbetween the teacher and student distributions and show that it is expressed as\\nthe product of their score differences and a weighting function determined by\\ntheir density ratio. This weighting function naturally emphasizes samples with\\nhigher density in the teacher distribution, when using a less mode-seeking\\ndivergence. We observe that the popular variational score distillation approach\\nusing the reverse-KL divergence is a special case within our framework.\\nEmpirically, we demonstrate that alternative f-divergences, such as\\nforward-KL and Jensen-Shannon divergences, outperform the current best\\nvariational score distillation methods across image generation tasks. In\\nparticular, when using Jensen-Shannon divergence, f-distill achieves current\\nstate-of-the-art one-step generation performance on ImageNet64 and zero-shot\\ntext-to-image generation on MS-COCO. Project page:\\nhttps://research.nvidia.com/labs/genair/f-distill\", 'upvotes': 3, 'discussionId': '67bbe6837727595ca5979e8c'}, 'publishedAt': '2025-02-23T22:24:55.500Z', 'title': 'One-step Diffusion Models with $f$-Divergence Distribution Matching', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15681.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 6194}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.14905', 'authors': [{'_id': '67bbe0520aabd5d571a723e7', 'name': 'Bhavik Agarwal', 'hidden': False}, {'_id': '67bbe0520aabd5d571a723e8', 'name': 'Ishan Joshi', 'hidden': False}, {'_id': '67bbe0520aabd5d571a723e9', 'name': 'Viktoria Rojkova', 'hidden': False}], 'publishedAt': '2025-02-18T16:44:55.000Z', 'title': 'Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema\\n  Adherence', 'summary': 'In this paper, we address the challenge of enforcing strict schema adherence\\nin large language model (LLM) generation by leveraging LLM reasoning\\ncapabilities. Building on the DeepSeek R1 reinforcement learning framework, our\\napproach trains structured reasoning skills of a 1.5B parameter model through a\\nnovel pipeline that combines synthetic reasoning dataset construction with\\ncustom reward functions under Group Relative Policy Optimization (GRPO).\\nSpecifically, we first perform R1 reinforcement learning on a 20K sample\\nunstructured-to-structured dataset, mirroring the original DeepSeek R1 methods,\\nto establish core reasoning abilities. Subsequently, we performed supervised\\nfine-tuning on a separate 10K reasoning sample dataset, focusing on refining\\nschema adherence for downstream tasks. Despite the relatively modest training\\nscope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO\\ntraining and 3 hours on 1xA100 for SFT, our model demonstrates robust\\nperformance in enforcing schema consistency. We compare our ThinkJSON approach\\nagainst the original DeepSeek R1 (671B), distilled versions of DeepSeek R1\\n(Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its\\neffectiveness in real-world applications. Our results underscore the practical\\nutility of a resource-efficient framework for schema-constrained text\\ngeneration.', 'upvotes': 3, 'discussionId': '67bbe0530aabd5d571a72437'}, 'publishedAt': '2025-02-23T22:11:17.789Z', 'title': 'Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14905.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 6194}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.14892', 'authors': [{'_id': '67bbf87b4f54983efbd94187', 'user': {'_id': '646aecb04c1cd18b497a50ee', 'avatarUrl': '/avatars/de15c724056f36a41cb4f375d05ed836.svg', 'isPro': False, 'fullname': 'Junhyeok Kim', 'user': 'kjunh', 'type': 'user'}, 'name': 'Junhyeok Kim', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T09:07:08.715Z', 'hidden': False}, {'_id': '67bbf87b4f54983efbd94188', 'name': 'Min Soo Kim', 'hidden': False}, {'_id': '67bbf87b4f54983efbd94189', 'name': 'Jiwan Chung', 'hidden': False}, {'_id': '67bbf87b4f54983efbd9418a', 'name': 'Jungbin Cho', 'hidden': False}, {'_id': '67bbf87b4f54983efbd9418b', 'name': 'Jisoo Kim', 'hidden': False}, {'_id': '67bbf87b4f54983efbd9418c', 'name': 'Sungwoong Kim', 'hidden': False}, {'_id': '67bbf87b4f54983efbd9418d', 'name': 'Gyeongbo Sim', 'hidden': False}, {'_id': '67bbf87b4f54983efbd9418e', 'name': 'Youngjae Yu', 'hidden': False}], 'publishedAt': '2025-02-17T04:47:12.000Z', 'title': 'EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in\\n  the Wild', 'summary': \"Predicting when to initiate speech in real-world environments remains a\\nfundamental challenge for conversational agents. We introduce EgoSpeak, a novel\\nframework for real-time speech initiation prediction in egocentric streaming\\nvideo. By modeling the conversation from the speaker's first-person viewpoint,\\nEgoSpeak is tailored for human-like interactions in which a conversational\\nagent must continuously observe its environment and dynamically decide when to\\ntalk. Our approach bridges the gap between simplified experimental setups and\\ncomplex natural conversations by integrating four key capabilities: (1)\\nfirst-person perspective, (2) RGB processing, (3) online processing, and (4)\\nuntrimmed video processing. We also present YT-Conversation, a diverse\\ncollection of in-the-wild conversational videos from YouTube, as a resource for\\nlarge-scale pretraining. Experiments on EasyCom and Ego4D demonstrate that\\nEgoSpeak outperforms random and silence-based baselines in real time. Our\\nresults also highlight the importance of multimodal input and context length in\\neffectively deciding when to speak.\", 'upvotes': 1, 'discussionId': '67bbf87d4f54983efbd941ec'}, 'publishedAt': '2025-02-24T08:37:35.940Z', 'title': 'EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in the Wild', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14892.png', 'numComments': 1, 'submittedBy': {'_id': '646aecb04c1cd18b497a50ee', 'avatarUrl': '/avatars/de15c724056f36a41cb4f375d05ed836.svg', 'fullname': 'Junhyeok Kim', 'name': 'kjunh', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.15011', 'authors': [{'_id': '67bc0d12ffc2c387329c8cfd', 'user': {'_id': '650ec19e6620b0c57e2a551b', 'avatarUrl': '/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg', 'isPro': False, 'fullname': 'Sayan Deb Sarkar', 'user': 'sayandsarkar', 'type': 'user'}, 'name': 'Sayan Deb Sarkar', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T09:06:56.555Z', 'hidden': False}, {'_id': '67bc0d12ffc2c387329c8cfe', 'name': 'Ondrej Miksik', 'hidden': False}, {'_id': '67bc0d12ffc2c387329c8cff', 'name': 'Marc Pollefeys', 'hidden': False}, {'_id': '67bc0d12ffc2c387329c8d00', 'name': 'Daniel Barath', 'hidden': False}, {'_id': '67bc0d12ffc2c387329c8d01', 'name': 'Iro Armeni', 'hidden': False}], 'publishedAt': '2025-02-20T20:05:30.000Z', 'title': 'CrossOver: 3D Scene Cross-Modal Alignment', 'summary': 'Multi-modal 3D object understanding has gained significant attention, yet\\ncurrent approaches often assume complete data availability and rigid alignment\\nacross all modalities. We present CrossOver, a novel framework for cross-modal\\n3D scene understanding via flexible, scene-level modality alignment. Unlike\\ntraditional methods that require aligned modality data for every object\\ninstance, CrossOver learns a unified, modality-agnostic embedding space for\\nscenes by aligning modalities - RGB images, point clouds, CAD models,\\nfloorplans, and text descriptions - with relaxed constraints and without\\nexplicit object semantics. Leveraging dimensionality-specific encoders, a\\nmulti-stage training pipeline, and emergent cross-modal behaviors, CrossOver\\nsupports robust scene retrieval and object localization, even with missing\\nmodalities. Evaluations on ScanNet and 3RScan datasets show its superior\\nperformance across diverse metrics, highlighting adaptability for real-world\\napplications in 3D scene understanding.', 'upvotes': 1, 'discussionId': '67bc0d18ffc2c387329c8e56'}, 'publishedAt': '2025-02-24T01:13:24.911Z', 'title': 'CrossOver: 3D Scene Cross-Modal Alignment', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/650ec19e6620b0c57e2a551b/S_xFBPoV3YbtHmtLtRrSV.gif'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15011.png', 'numComments': 1, 'submittedBy': {'_id': '650ec19e6620b0c57e2a551b', 'avatarUrl': '/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg', 'fullname': 'Sayan Deb Sarkar', 'name': 'sayandsarkar', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2403.12959', 'authors': [{'_id': '67bc67e17727595ca5b7ddb4', 'name': 'Wanqi Yin', 'hidden': False}, {'_id': '67bc67e17727595ca5b7ddb5', 'name': 'Zhongang Cai', 'hidden': False}, {'_id': '67bc67e17727595ca5b7ddb6', 'name': 'Ruisi Wang', 'hidden': False}, {'_id': '67bc67e17727595ca5b7ddb7', 'name': 'Fanzhou Wang', 'hidden': False}, {'_id': '67bc67e17727595ca5b7ddb8', 'name': 'Chen Wei', 'hidden': False}, {'_id': '67bc67e17727595ca5b7ddb9', 'name': 'Haiyi Mei', 'hidden': False}, {'_id': '67bc67e17727595ca5b7ddba', 'name': 'Weiye Xiao', 'hidden': False}, {'_id': '67bc67e17727595ca5b7ddbb', 'name': 'Zhitao Yang', 'hidden': False}, {'_id': '67bc67e17727595ca5b7ddbc', 'name': 'Qingping Sun', 'hidden': False}, {'_id': '67bc67e17727595ca5b7ddbd', 'name': 'Atsushi Yamashita', 'hidden': False}, {'_id': '67bc67e17727595ca5b7ddbe', 'name': 'Ziwei Liu', 'hidden': False}, {'_id': '67bc67e17727595ca5b7ddbf', 'name': 'Lei Yang', 'hidden': False}], 'publishedAt': '2024-03-19T17:58:02.000Z', 'title': 'WHAC: World-grounded Humans and Cameras', 'summary': 'Estimating human and camera trajectories with accurate scale in the world\\ncoordinate system from a monocular video is a highly desirable yet challenging\\nand ill-posed problem. In this study, we aim to recover expressive parametric\\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\\nleveraging the synergy between three critical players: the world, the human,\\nand the camera. Our approach is founded on two key observations. Firstly,\\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\\nSecondly, human motions inherently provide absolute spatial cues. By\\nintegrating these insights, we introduce a novel framework, referred to as\\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\\n(EHPS) alongside camera pose estimation, without relying on traditional\\noptimization techniques. Additionally, we present a new synthetic dataset,\\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\\nfeatures diverse interactive human motions as well as realistic camera\\ntrajectories. Extensive experiments on both standard and newly established\\nbenchmarks highlight the superiority and efficacy of our framework. We will\\nmake the code and dataset publicly available.', 'upvotes': 0, 'discussionId': '67bc67e57727595ca5b7deb9'}, 'publishedAt': '2025-02-24T07:37:26.684Z', 'title': 'WHAC: World-grounded Humans and Cameras', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12959.png', 'numComments': 1, 'submittedBy': {'_id': '5f1158120c833276f61f1a84', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg', 'fullname': 'Niels Rogge', 'name': 'nielsr', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 770}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.14975', 'authors': [{'_id': '67bc5b6b876dad36abdd56fb', 'name': 'David Noever', 'hidden': False}, {'_id': '67bc5b6b876dad36abdd56fc', 'name': 'Grant Rosario', 'hidden': False}], 'publishedAt': '2025-02-20T19:09:40.000Z', 'title': 'Beyond No: Quantifying AI Over-Refusal and Emotional Attachment\\n  Boundaries', 'summary': \"We present an open-source benchmark and evaluation framework for assessing\\nemotional boundary handling in Large Language Models (LLMs). Using a dataset of\\n1156 prompts across six languages, we evaluated three leading LLMs (GPT-4o,\\nClaude-3.5 Sonnet, and Mistral-large) on their ability to maintain appropriate\\nemotional boundaries through pattern-matched response analysis. Our framework\\nquantifies responses across seven key patterns: direct refusal, apology,\\nexplanation, deflection, acknowledgment, boundary setting, and emotional\\nawareness. Results demonstrate significant variation in boundary-handling\\napproaches, with Claude-3.5 achieving the highest overall score (8.69/10) and\\nproducing longer, more nuanced responses (86.51 words on average). We\\nidentified a substantial performance gap between English (average score 25.62)\\nand non-English interactions (< 0.22), with English responses showing markedly\\nhigher refusal rates (43.20% vs. < 1% for non-English). Pattern analysis\\nrevealed model-specific strategies, such as Mistral's preference for deflection\\n(4.2%) and consistently low empathy scores across all models (< 0.06).\\nLimitations include potential oversimplification through pattern matching, lack\\nof contextual understanding in response analysis, and binary classification of\\ncomplex emotional responses. Future work should explore more nuanced scoring\\nmethods, expand language coverage, and investigate cultural variations in\\nemotional boundary expectations. Our benchmark and methodology provide a\\nfoundation for systematic evaluation of LLM emotional intelligence and\\nboundary-setting capabilities.\", 'upvotes': 0, 'discussionId': '67bc5b6c876dad36abdd5736'}, 'publishedAt': '2025-02-24T06:44:41.263Z', 'title': 'Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14975.png', 'numComments': 2, 'submittedBy': {'_id': '63136a82e29fb2e86d5e5bdd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63136a82e29fb2e86d5e5bdd/pFZDuQtzfUStovbwwZGvn.png', 'fullname': 'David Noever', 'name': 'dnoever', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.13407', 'authors': [{'_id': '67bb33f3829dedfc99ae1288', 'user': {'_id': '67bb32b6a0cb6e48cfd27d80', 'avatarUrl': '/avatars/3cafe3a3fb60405252962d00105667c5.svg', 'isPro': False, 'fullname': 'Ziyuan Liu', 'user': 'circleLZY', 'type': 'user'}, 'name': 'Ziyuan Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T09:07:29.223Z', 'hidden': False}, {'_id': '67bb33f3829dedfc99ae1289', 'name': 'Ruifei Zhu', 'hidden': False}, {'_id': '67bb33f3829dedfc99ae128a', 'name': 'Long Gao', 'hidden': False}, {'_id': '67bb33f3829dedfc99ae128b', 'name': 'Yuanxiu Zhou', 'hidden': False}, {'_id': '67bb33f3829dedfc99ae128c', 'name': 'Jingyu Ma', 'hidden': False}, {'_id': '67bb33f3829dedfc99ae128d', 'name': 'Yuantao Gu', 'hidden': False}], 'publishedAt': '2025-02-19T03:33:54.000Z', 'title': 'JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust\\n  Multi-Teacher Knowledge Distillation Framework', 'summary': 'Deep learning has achieved significant success in the field of remote sensing\\nimage change detection (CD), yet two major challenges remain: the scarcity of\\nsub-meter, all-inclusive open-source CD datasets, and the difficulty of\\nachieving consistent and satisfactory detection results across images with\\nvarying change areas. To address these issues, we introduce the JL1-CD dataset,\\nwhich contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5\\nto 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation\\n(MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD\\ndatasets demonstrate that the MTKD framework significantly improves the\\nperformance of CD models with various network architectures and parameter\\nsizes, achieving new state-of-the-art results. The code is available at\\nhttps://github.com/circleLZY/MTKD-CD.', 'upvotes': 0, 'discussionId': '67bb33f6829dedfc99ae135e'}, 'publishedAt': '2025-02-24T04:29:42.452Z', 'title': 'JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13407.png', 'numComments': 1, 'submittedBy': {'_id': '67bb32b6a0cb6e48cfd27d80', 'avatarUrl': '/avatars/3cafe3a3fb60405252962d00105667c5.svg', 'fullname': 'Ziyuan Liu', 'name': 'circleLZY', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.15082', 'authors': [{'_id': '67bbe93f267aa2b537b318be', 'user': {'_id': '64f64da90efa33bfe0a3d9ba', 'avatarUrl': '/avatars/c45fb015433e46a2eeb9518910f75d35.svg', 'isPro': False, 'fullname': 'Vaidehi Patil', 'user': 'vaidehi99', 'type': 'user'}, 'name': 'Vaidehi Patil', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-24T09:07:15.794Z', 'hidden': False}, {'_id': '67bbe93f267aa2b537b318bf', 'name': 'Elias Stengel-Eskin', 'hidden': False}, {'_id': '67bbe93f267aa2b537b318c0', 'name': 'Mohit Bansal', 'hidden': False}], 'publishedAt': '2025-02-20T22:51:10.000Z', 'title': 'UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning', 'summary': 'User specifications or legal frameworks often require information to be\\nremoved from pretrained models, including large language models (LLMs). This\\nrequires deleting or \"forgetting\" a set of data points from an already-trained\\nmodel, which typically degrades its performance on other data points. Thus, a\\nbalance must be struck between removing information and keeping the model\\'s\\nother abilities intact, with a failure to balance this trade-off leading to\\npoor deletion or an unusable model. To this end, we propose UPCORE\\n(Utility-Preserving Coreset Selection), a method-agnostic data selection\\nframework for mitigating collateral damage during unlearning. Finding that the\\nmodel damage is correlated with the variance of the model\\'s representations on\\nthe forget set, we selectively prune the forget set to remove outliers, thereby\\nminimizing model degradation after unlearning. We evaluate UPCORE across three\\nstandard unlearning methods consistently achieving a superior balance between\\nthe competing objectives of deletion efficacy and model preservation. To better\\nevaluate this trade-off, we introduce a new metric, measuring the\\narea-under-the-curve (AUC) across standard metrics. We find that UPCORE\\nimproves both standard metrics and AUC, benefitting from positive transfer\\nbetween the coreset and pruned points while reducing negative transfer from the\\nforget set to points outside of it.', 'upvotes': 0, 'discussionId': '67bbe940267aa2b537b318f4'}, 'publishedAt': '2025-02-23T23:17:33.152Z', 'title': 'UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15082.png', 'numComments': 1, 'submittedBy': {'_id': '64f64da90efa33bfe0a3d9ba', 'avatarUrl': '/avatars/c45fb015433e46a2eeb9518910f75d35.svg', 'fullname': 'Vaidehi Patil', 'name': 'vaidehi99', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}"
]