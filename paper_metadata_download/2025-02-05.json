[
    "{'paper': {'id': '2502.01362', 'authors': [{'_id': '67a2ad6ac7caec9bf5a45e61', 'user': {'_id': '672503c59f68afdd63cc81a2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/672503c59f68afdd63cc81a2/lw4ApCTwAKgt_uUyfSVRH.jpeg', 'isPro': False, 'fullname': 'Nikita Gushchin', 'user': 'ngushchin', 'type': 'user'}, 'name': 'Nikita Gushchin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-05T10:14:26.177Z', 'hidden': False}, {'_id': '67a2ad6ac7caec9bf5a45e62', 'user': {'_id': '656a2e59b4020389028dc85f', 'avatarUrl': '/avatars/6fda3bddc3cecba2894233bebb3de968.svg', 'isPro': False, 'fullname': 'David Li', 'user': 'kekchpek', 'type': 'user'}, 'name': 'David Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-05T10:14:24.236Z', 'hidden': False}, {'_id': '67a2ad6ac7caec9bf5a45e63', 'user': {'_id': '64a42977250bfdecd9570a9e', 'avatarUrl': '/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg', 'isPro': False, 'fullname': 'Daniil Selikhanovych', 'user': 'apryc1', 'type': 'user'}, 'name': 'Daniil Selikhanovych', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-05T10:17:09.668Z', 'hidden': False}, {'_id': '67a2ad6ac7caec9bf5a45e64', 'name': 'Evgeny Burnaev', 'hidden': False}, {'_id': '67a2ad6ac7caec9bf5a45e65', 'user': {'_id': '62b6cc49752323892323bc04', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62b6cc49752323892323bc04/gGBld1KJIP9AIpd81L3PC.jpeg', 'isPro': True, 'fullname': 'Dmitry Baranchuk', 'user': 'dbaranchuk', 'type': 'user'}, 'name': 'Dmitry Baranchuk', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-05T10:17:26.518Z', 'hidden': False}, {'_id': '67a2ad6ac7caec9bf5a45e66', 'user': {'_id': '67a31c9ae5b870d5157657db', 'avatarUrl': '/avatars/ca5fd356e3656e1beacb5a28ecaad5be.svg', 'isPro': False, 'fullname': 'Alexander Korotin', 'user': 'akorotin', 'type': 'user'}, 'name': 'Alexander Korotin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-05T10:17:33.816Z', 'hidden': False}], 'publishedAt': '2025-02-03T13:56:03.000Z', 'title': 'Inverse Bridge Matching Distillation', 'summary': 'Learning diffusion bridge models is easy; making them fast and practical is\\nan art. Diffusion bridge models (DBMs) are a promising extension of diffusion\\nmodels for applications in image-to-image translation. However, like many\\nmodern diffusion and flow models, DBMs suffer from the problem of slow\\ninference. To address it, we propose a novel distillation technique based on\\nthe inverse bridge matching formulation and derive the tractable objective to\\nsolve it in practice. Unlike previously developed DBM distillation techniques,\\nthe proposed method can distill both conditional and unconditional types of\\nDBMs, distill models in a one-step generator, and use only the corrupted images\\nfor training. We evaluate our approach for both conditional and unconditional\\ntypes of bridge matching on a wide set of setups, including super-resolution,\\nJPEG restoration, sketch-to-image, and other tasks, and show that our\\ndistillation technique allows us to accelerate the inference of DBMs from 4x to\\n100x and even provide better generation quality than used teacher model\\ndepending on particular setup.', 'upvotes': 21, 'discussionId': '67a2ad70c7caec9bf5a45fb0'}, 'publishedAt': '2025-02-05T03:01:40.464Z', 'title': 'Inverse Bridge Matching Distillation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01362.png', 'numComments': 1, 'submittedBy': {'_id': '672503c59f68afdd63cc81a2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/672503c59f68afdd63cc81a2/lw4ApCTwAKgt_uUyfSVRH.jpeg', 'fullname': 'Nikita Gushchin', 'name': 'ngushchin', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.02492', 'authors': [{'_id': '67a2ec904ea0e3138ac966f2', 'user': {'_id': '6181c72cdcc1df2c9de8a4d8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg', 'isPro': False, 'fullname': 'Hila Chefer', 'user': 'Hila', 'type': 'user'}, 'name': 'Hila Chefer', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-02-05T04:44:03.218Z', 'hidden': False}, {'_id': '67a2ec904ea0e3138ac966f3', 'name': 'Uriel Singer', 'hidden': False}, {'_id': '67a2ec904ea0e3138ac966f4', 'name': 'Amit Zohar', 'hidden': False}, {'_id': '67a2ec904ea0e3138ac966f5', 'name': 'Yuval Kirstain', 'hidden': False}, {'_id': '67a2ec904ea0e3138ac966f6', 'name': 'Adam Polyak', 'hidden': False}, {'_id': '67a2ec904ea0e3138ac966f7', 'name': 'Yaniv Taigman', 'hidden': False}, {'_id': '67a2ec904ea0e3138ac966f8', 'name': 'Lior Wolf', 'hidden': False}, {'_id': '67a2ec904ea0e3138ac966f9', 'name': 'Shelly Sheynin', 'hidden': False}], 'publishedAt': '2025-02-04T17:07:10.000Z', 'title': 'VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion\\n  Generation in Video Models', 'summary': \"Despite tremendous recent progress, generative video models still struggle to\\ncapture real-world motion, dynamics, and physics. We show that this limitation\\narises from the conventional pixel reconstruction objective, which biases\\nmodels toward appearance fidelity at the expense of motion coherence. To\\naddress this, we introduce VideoJAM, a novel framework that instills an\\neffective motion prior to video generators, by encouraging the model to learn a\\njoint appearance-motion representation. VideoJAM is composed of two\\ncomplementary units. During training, we extend the objective to predict both\\nthe generated pixels and their corresponding motion from a single learned\\nrepresentation. During inference, we introduce Inner-Guidance, a mechanism that\\nsteers the generation toward coherent motion by leveraging the model's own\\nevolving motion prediction as a dynamic guidance signal. Notably, our framework\\ncan be applied to any video model with minimal adaptations, requiring no\\nmodifications to the training data or scaling of the model. VideoJAM achieves\\nstate-of-the-art performance in motion coherence, surpassing highly competitive\\nproprietary models while also enhancing the perceived visual quality of the\\ngenerations. These findings emphasize that appearance and motion can be\\ncomplementary and, when effectively integrated, enhance both the visual quality\\nand the coherence of video generation. Project website:\\nhttps://hila-chefer.github.io/videojam-paper.github.io/\", 'upvotes': 19, 'discussionId': '67a2ec934ea0e3138ac9678e'}, 'publishedAt': '2025-02-04T23:46:17.626Z', 'title': 'VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02492.png', 'numComments': 1, 'submittedBy': {'_id': '6181c72cdcc1df2c9de8a4d8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg', 'fullname': 'Hila Chefer', 'name': 'Hila', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 8}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.01718', 'authors': [{'_id': '67a2d995c97974764a8c294c', 'name': 'Huaye Zeng', 'hidden': False}, {'_id': '67a2d995c97974764a8c294d', 'user': {'_id': '62567c86d444a9b5a0ec51c1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png', 'isPro': False, 'fullname': 'Dongfu Jiang', 'user': 'DongfuJiang', 'type': 'user'}, 'name': 'Dongfu Jiang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-05T10:14:14.136Z', 'hidden': False}, {'_id': '67a2d995c97974764a8c294e', 'name': 'Haozhe Wang', 'hidden': False}, {'_id': '67a2d995c97974764a8c294f', 'user': {'_id': '65358802a920f38780b3248a', 'avatarUrl': '/avatars/9415510b598079973c2b0436ad12db9c.svg', 'isPro': False, 'fullname': 'Ping Nie', 'user': 'pingnieuk', 'type': 'user'}, 'name': 'Ping Nie', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-05T10:18:51.897Z', 'hidden': False}, {'_id': '67a2d995c97974764a8c2950', 'name': 'Xiaotong Chen', 'hidden': False}, {'_id': '67a2d995c97974764a8c2951', 'name': 'Wenhu Chen', 'hidden': False}], 'publishedAt': '2025-02-03T18:46:04.000Z', 'title': 'ACECODER: Acing Coder RL via Automated Test-Case Synthesis', 'summary': 'Most progress in recent coder models has been driven by supervised\\nfine-tuning (SFT), while the potential of reinforcement learning (RL) remains\\nlargely unexplored, primarily due to the lack of reliable reward data/model in\\nthe code domain. In this paper, we address this challenge by leveraging\\nautomated large-scale test-case synthesis to enhance code model training.\\nSpecifically, we design a pipeline that generates extensive (question,\\ntest-cases) pairs from existing code data. Using these test cases, we construct\\npreference pairs based on pass rates over sampled programs to train reward\\nmodels with Bradley-Terry loss. It shows an average of 10-point improvement for\\nLlama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through\\nbest-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5.\\nFurthermore, we conduct reinforcement learning with both reward models and\\ntest-case pass rewards, leading to consistent improvements across HumanEval,\\nMBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style\\ntraining to start from Qwen2.5-Coder-base directly and show that our RL\\ntraining can improve model on HumanEval-plus by over 25\\\\% and MBPP-plus by 6\\\\%\\nfor merely 80 optimization steps. We believe our results highlight the huge\\npotential of reinforcement learning in coder models.', 'upvotes': 14, 'discussionId': '67a2d996c97974764a8c29a1'}, 'publishedAt': '2025-02-04T22:23:07.858Z', 'title': 'ACECODER: Acing Coder RL via Automated Test-Case Synthesis', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01718.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5948}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.02584', 'authors': [{'_id': '67a2d59fd5ad3369a66ff394', 'name': 'Zongyu Lin', 'hidden': False}, {'_id': '67a2d59fd5ad3369a66ff395', 'name': 'Yao Tang', 'hidden': False}, {'_id': '67a2d59fd5ad3369a66ff396', 'name': 'Xingcheng Yao', 'hidden': False}, {'_id': '67a2d59fd5ad3369a66ff397', 'name': 'Da Yin', 'hidden': False}, {'_id': '67a2d59fd5ad3369a66ff398', 'name': 'Ziniu Hu', 'hidden': False}, {'_id': '67a2d59fd5ad3369a66ff399', 'name': 'Yizhou Sun', 'hidden': False}, {'_id': '67a2d59fd5ad3369a66ff39a', 'name': 'Kai-Wei Chang', 'hidden': False}], 'publishedAt': '2025-02-04T18:58:31.000Z', 'title': 'QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search', 'summary': 'Language agents have become a promising solution to complex interactive\\ntasks. One of the key ingredients to the success of language agents is the\\nreward model on the trajectory of the agentic workflow, which provides valuable\\nguidance during training or inference. However, due to the lack of annotations\\nof intermediate interactions, most existing works use an outcome reward model\\nto optimize policies across entire trajectories. This may lead to sub-optimal\\npolicies and hinder the overall performance. To address this, we propose QLASS\\n(Q-guided Language Agent Stepwise Search), to automatically generate\\nannotations by estimating Q-values in a stepwise manner for open language\\nagents. By introducing a reasoning tree and performing process reward modeling,\\nQLASS provides effective intermediate guidance for each step. With the stepwise\\nguidance, we propose a Q-guided generation strategy to enable language agents\\nto better adapt to long-term value, resulting in significant performance\\nimprovement during model inference on complex interactive agent tasks. Notably,\\neven with almost half the annotated data, QLASS retains strong performance,\\ndemonstrating its efficiency in handling limited supervision. We also\\nempirically demonstrate that QLASS can lead to more effective decision making\\nthrough qualitative analysis. We will release our code and data.', 'upvotes': 9, 'discussionId': '67a2d5a0d5ad3369a66ff3d4'}, 'publishedAt': '2025-02-04T22:08:25.652Z', 'title': 'QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02584.png', 'numComments': 1, 'submittedBy': {'_id': '634e4670a51d5df8c2d92fce', 'avatarUrl': '/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg', 'fullname': 'Da Yin', 'name': 'DaYin', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.01941', 'authors': [{'_id': '67a2e2a02dd2adbc88755a47', 'user': {'_id': '63024676056ec3a2a8714b24', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg', 'isPro': False, 'fullname': 'Xiang Liu', 'user': 'Dominic789654', 'type': 'user'}, 'name': 'Xiang Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-05T10:12:48.427Z', 'hidden': False}, {'_id': '67a2e2a02dd2adbc88755a48', 'name': 'Zhenheng Tang', 'hidden': False}, {'_id': '67a2e2a02dd2adbc88755a49', 'name': 'Hong Chen', 'hidden': False}, {'_id': '67a2e2a02dd2adbc88755a4a', 'name': 'Peijie Dong', 'hidden': False}, {'_id': '67a2e2a02dd2adbc88755a4b', 'name': 'Zeyu Li', 'hidden': False}, {'_id': '67a2e2a02dd2adbc88755a4c', 'name': 'Xiuze Zhou', 'hidden': False}, {'_id': '67a2e2a02dd2adbc88755a4d', 'name': 'Bo Li', 'hidden': False}, {'_id': '67a2e2a02dd2adbc88755a4e', 'name': 'Xuming Hu', 'hidden': False}, {'_id': '67a2e2a02dd2adbc88755a4f', 'name': 'Xiaowen Chu', 'hidden': False}], 'publishedAt': '2025-02-04T02:23:06.000Z', 'title': 'Can LLMs Maintain Fundamental Abilities under KV Cache Compression?', 'summary': \"This paper investigates an under-explored challenge in large language models\\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\\ncapabilities. While existing methods achieve impressive compression ratios on\\nlong-context benchmarks, their effects on core model capabilities remain\\nunderstudied. We present a comprehensive empirical study evaluating prominent\\nKV cache compression methods across diverse tasks, spanning world knowledge,\\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\\nlong-context understanding and generation.Our analysis reveals that KV cache\\ncompression methods exhibit task-specific performance degradation. Arithmetic\\nreasoning tasks prove particularly sensitive to aggressive compression, with\\ndifferent methods showing performance drops of 17.4%-43.3%. Notably, the\\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\\nto instruction-tuned models, showing only 9.67%-25.53% performance\\ndegradation. Based on our analysis of attention patterns and cross-task\\ncompression performance, we propose ShotKV, a novel compression approach that\\ndistinctly handles prefill and decoding phases while maintaining shot-level\\nsemantic coherence. Empirical results show that ShotKV achieves 9%-18%\\nperformance improvements on long-context generation tasks under aggressive\\ncompression ratios.\", 'upvotes': 7, 'discussionId': '67a2e2a22dd2adbc88755ab4'}, 'publishedAt': '2025-02-04T23:04:25.888Z', 'title': 'Can LLMs Maintain Fundamental Abilities under KV Cache Compression?', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/XcgjmhpXd3dH6LnFZGupJ.png', 'https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/hxWz1iVOUcE76E_K5z-B0.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01941.png', 'numComments': 1, 'submittedBy': {'_id': '63024676056ec3a2a8714b24', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg', 'fullname': 'Xiang Liu', 'name': 'Dominic789654', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.02508', 'authors': [{'_id': '67a2d1f9bc9d072d9459e857', 'user': {'_id': '6553c985a7aded0380b5f928', 'avatarUrl': '/avatars/36109d6f536d2b34d98822b88eac9608.svg', 'isPro': False, 'fullname': 'Maohao Shen', 'user': 'maohaos2', 'type': 'user'}, 'name': 'Maohao Shen', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-02-05T03:00:33.470Z', 'hidden': False}, {'_id': '67a2d1f9bc9d072d9459e858', 'name': 'Guangtao Zeng', 'hidden': False}, {'_id': '67a2d1f9bc9d072d9459e859', 'name': 'Zhenting Qi', 'hidden': False}, {'_id': '67a2d1f9bc9d072d9459e85a', 'name': 'Zhang-Wei Hong', 'hidden': False}, {'_id': '67a2d1f9bc9d072d9459e85b', 'name': 'Zhenfang Chen', 'hidden': False}, {'_id': '67a2d1f9bc9d072d9459e85c', 'name': 'Wei Lu', 'hidden': False}, {'_id': '67a2d1f9bc9d072d9459e85d', 'name': 'Gregory Wornell', 'hidden': False}, {'_id': '67a2d1f9bc9d072d9459e85e', 'name': 'Subhro Das', 'hidden': False}, {'_id': '67a2d1f9bc9d072d9459e85f', 'name': 'David Cox', 'hidden': False}, {'_id': '67a2d1f9bc9d072d9459e860', 'name': 'Chuang Gan', 'hidden': False}], 'publishedAt': '2025-02-04T17:26:58.000Z', 'title': 'Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\\n  Reasoning via Autoregressive Search', 'summary': \"Large language models (LLMs) have demonstrated remarkable reasoning\\ncapabilities across diverse domains. Recent studies have shown that increasing\\ntest-time computation enhances LLMs' reasoning capabilities. This typically\\ninvolves extensive sampling at inference time guided by an external LLM\\nverifier, resulting in a two-player system. Despite external guidance, the\\neffectiveness of this system demonstrates the potential of a single LLM to\\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\\nthe searching capabilities to fundamentally enhance the reasoning abilities of\\na single LLM? This work explores an orthogonal direction focusing on\\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\\nprocess with self-reflection and self-exploration of new strategies). To\\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\\nLLM trained on open-source models and data. Extensive empirical evaluations\\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\\nreasoning benchmarks while exhibits strong generalization to out-of-domain\\ntasks. Code, data, and models will be fully open-sourced.\", 'upvotes': 6, 'discussionId': '67a2d1fcbc9d072d9459e91b'}, 'publishedAt': '2025-02-04T21:55:09.693Z', 'title': 'Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02508.png', 'numComments': 1, 'submittedBy': {'_id': '60ad0de755f970745d4ec28d', 'avatarUrl': '/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg', 'fullname': 'GtZeng', 'name': 'chaoscodes', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 11}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2502.00674', 'authors': [{'_id': '67a362c9b9a2bb11fdba4b9f', 'name': 'Wenzhe Li', 'hidden': False}, {'_id': '67a362c9b9a2bb11fdba4ba0', 'name': 'Yong Lin', 'hidden': False}, {'_id': '67a362c9b9a2bb11fdba4ba1', 'name': 'Mengzhou Xia', 'hidden': False}, {'_id': '67a362c9b9a2bb11fdba4ba2', 'name': 'Chi Jin', 'hidden': False}], 'publishedAt': '2025-02-02T05:23:29.000Z', 'title': 'Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\\n  Beneficial?', 'summary': 'Ensembling outputs from diverse sources is a straightforward yet effective\\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\\nensemble method that aggregates outputs from multiple different Large Language\\nModels (LLMs). This paper raises the question in the context of language\\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\\nensemble method that aggregates outputs from only the single top-performing\\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\\nachieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an\\naverage of 3.8% improvement across various benchmarks, including MMLU, CRUX,\\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\\nunderstand the effectiveness of Self-MoA, we systematically investigate the\\ntrade-off between diversity and quality of outputs under various MoA settings.\\nWe confirm that the MoA performance is rather sensitive to the quality, and\\nmixing different LLMs often lowers the average quality of the models. To\\ncomplement the study, we identify the scenarios where mixing different LLMs\\ncould be helpful. This paper further introduces a sequential version of\\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\\nat once.', 'upvotes': 3, 'discussionId': '67a362cab9a2bb11fdba4bdc'}, 'publishedAt': '2025-02-05T08:09:02.787Z', 'title': 'Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00674.png', 'numComments': 2, 'submittedBy': {'_id': '62f32eab52ad88c930bb3f3b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png', 'fullname': 'Asankhaya Sharma', 'name': 'codelion', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 51}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.19066', 'authors': [{'_id': '67a0f59c5685d37e28880943', 'user': {'_id': '64bcc06fb567ae97c3272d3d', 'avatarUrl': '/avatars/bcb61fe9e575154d84913a1501971f1a.svg', 'isPro': False, 'fullname': 'kim', 'user': 'dahyekim', 'type': 'user'}, 'name': 'Dahye Kim', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-02-03T16:59:53.099Z', 'hidden': False}, {'_id': '67a0f59c5685d37e28880944', 'name': 'Deepti Ghadiyaram', 'hidden': False}], 'publishedAt': '2025-01-31T11:52:47.000Z', 'title': 'Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable\\n  Generations', 'summary': 'Despite the remarkable progress in text-to-image generative models, they are\\nprone to adversarial attacks and inadvertently generate unsafe, unethical\\ncontent. Existing approaches often rely on fine-tuning models to remove\\nspecific concepts, which is computationally expensive, lack scalability, and/or\\ncompromise generation quality. In this work, we propose a novel framework\\nleveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable\\nconcept manipulation in diffusion models. Specifically, we first identify\\ninterpretable monosemantic concepts in the latent space of text embeddings and\\nleverage them to precisely steer the generation away or towards a given concept\\n(e.g., nudity) or to introduce a new concept (e.g., photographic style).\\nThrough extensive experiments, we demonstrate that our approach is very simple,\\nrequires no retraining of the base model nor LoRA adapters, does not compromise\\nthe generation quality, and is robust to adversarial prompt manipulations. Our\\nmethod yields an improvement of 20.01% in unsafe concept removal,\\nis effective in style manipulation, and is sim5x faster than\\ncurrent state-of-the-art.', 'upvotes': 2, 'discussionId': '67a0f5a05685d37e28880a1e'}, 'publishedAt': '2025-02-05T07:44:45.130Z', 'title': 'Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.19066.png', 'numComments': 1, 'submittedBy': {'_id': '64bcc06fb567ae97c3272d3d', 'avatarUrl': '/avatars/bcb61fe9e575154d84913a1501971f1a.svg', 'fullname': 'kim', 'name': 'dahyekim', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2502.01720', 'authors': [{'_id': '67a2fddb4044bf1c86f765a3', 'user': {'_id': '62f6a894c3372328414c7021', 'avatarUrl': '/avatars/e8b10912355712f38f10805c31bea962.svg', 'isPro': False, 'fullname': 'Nupur Kumari', 'user': 'nupurkmr9', 'type': 'user'}, 'name': 'Nupur Kumari', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-02-05T13:37:30.825Z', 'hidden': False}, {'_id': '67a2fddb4044bf1c86f765a4', 'name': 'Xi Yin', 'hidden': False}, {'_id': '67a2fddb4044bf1c86f765a5', 'name': 'Jun-Yan Zhu', 'hidden': False}, {'_id': '67a2fddb4044bf1c86f765a6', 'name': 'Ishan Misra', 'hidden': False}, {'_id': '67a2fddb4044bf1c86f765a7', 'name': 'Samaneh Azadi', 'hidden': False}], 'publishedAt': '2025-02-03T18:59:41.000Z', 'title': 'Generating Multi-Image Synthetic Data for Text-to-Image Customization', 'summary': 'Customization of text-to-image models enables users to insert custom concepts\\nand generate the concepts in unseen settings. Existing methods either rely on\\ncostly test-time optimization or train encoders on single-image training\\ndatasets without multi-image supervision, leading to worse image quality. We\\npropose a simple approach that addresses both limitations. We first leverage\\nexisting text-to-image models and 3D datasets to create a high-quality\\nSynthetic Customization Dataset (SynCD) consisting of multiple images of the\\nsame object in different lighting, backgrounds, and poses. We then propose a\\nnew encoder architecture based on shared attention mechanisms that better\\nincorporate fine-grained visual details from input images. Finally, we propose\\na new inference technique that mitigates overexposure issues during inference\\nby normalizing the text and image guidance vectors. Through extensive\\nexperiments, we show that our model, trained on the synthetic dataset with the\\nproposed encoder and inference algorithm, outperforms existing tuning-free\\nmethods on standard customization benchmarks.', 'upvotes': 2, 'discussionId': '67a2fde34044bf1c86f767ba'}, 'publishedAt': '2025-02-05T00:59:11.275Z', 'title': 'Generating Multi-Image Synthetic Data for Text-to-Image Customization', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01720.png', 'numComments': 1, 'submittedBy': {'_id': '62f6a894c3372328414c7021', 'avatarUrl': '/avatars/e8b10912355712f38f10805c31bea962.svg', 'fullname': 'Nupur Kumari', 'name': 'nupurkmr9', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}"
]