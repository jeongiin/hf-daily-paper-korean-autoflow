[
    "{'paper': {'id': '2505.21115', 'authors': [{'_id': '68372d97e4af3c39dcec8e65', 'user': {'_id': '5dfa8e07da6d0311fd3d5430', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png', 'isPro': False, 'fullname': 'Sergey Pletenev', 'user': 'memyprokotow', 'type': 'user'}, 'name': 'Sergey Pletenev', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:12:55.604Z', 'hidden': False}, {'_id': '68372d97e4af3c39dcec8e66', 'user': {'_id': '660ee18e2dcd816ad14b3739', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg', 'isPro': False, 'fullname': 'Maria Marina', 'user': 'zlatamaria', 'type': 'user'}, 'name': 'Maria Marina', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:12:59.278Z', 'hidden': False}, {'_id': '68372d97e4af3c39dcec8e67', 'user': {'_id': '6682607ece294ddc5e72f4fb', 'avatarUrl': '/avatars/2a304bc3eb56ec7d13297d28fbb062ae.svg', 'isPro': False, 'fullname': 'Ivanov', 'user': 'VirVen', 'type': 'user'}, 'name': 'Nikolay Ivanov', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-02T07:48:42.811Z', 'hidden': False}, {'_id': '68372d97e4af3c39dcec8e68', 'name': 'Daria Galimzianova', 'hidden': False}, {'_id': '68372d97e4af3c39dcec8e69', 'user': {'_id': '643010b2ff56d6c2004699a6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/OYsf1Hp-KAievw_M8XBG9.jpeg', 'isPro': False, 'fullname': 'Krayko Nikita', 'user': 'nakrayko', 'type': 'user'}, 'name': 'Nikita Krayko', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:12:53.523Z', 'hidden': False}, {'_id': '68372d97e4af3c39dcec8e6a', 'name': 'Mikhail Salnikov', 'hidden': False}, {'_id': '68372d97e4af3c39dcec8e6b', 'name': 'Vasily Konovalov', 'hidden': False}, {'_id': '68372d97e4af3c39dcec8e6c', 'name': 'Alexander Panchenko', 'hidden': False}, {'_id': '68372d97e4af3c39dcec8e6d', 'user': {'_id': '63bbfd74141c7d395c471768', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg', 'isPro': False, 'fullname': 'Viktor Moskvoretskii', 'user': 'VityaVitalich', 'type': 'user'}, 'name': 'Viktor Moskvoretskii', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:12:57.325Z', 'hidden': False}], 'publishedAt': '2025-05-27T12:35:13.000Z', 'submittedOnDailyAt': '2025-06-09T07:27:12.232Z', 'title': 'Will It Still Be True Tomorrow? Multilingual Evergreen Question\\n  Classification to Improve Trustworthy QA', 'submittedOnDailyBy': {'_id': '660ee18e2dcd816ad14b3739', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg', 'isPro': False, 'fullname': 'Maria Marina', 'user': 'zlatamaria', 'type': 'user'}, 'summary': 'Large Language Models (LLMs) often hallucinate in question answering (QA)\\ntasks. A key yet underexplored factor contributing to this is the temporality\\nof questions -- whether they are evergreen (answers remain stable over time) or\\nmutable (answers change). In this work, we introduce EverGreenQA, the first\\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\\nencode question temporality explicitly (via verbalized judgments) or implicitly\\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\\nthe practical utility of evergreen classification across three applications:\\nimproving self-knowledge estimation, filtering QA datasets, and explaining\\nGPT-4o retrieval behavior.', 'upvotes': 71, 'discussionId': '68372d98e4af3c39dcec8e88', 'githubRepo': 'https://github.com/s-nlp/Evergreen-classification', 'ai_summary': 'EverGreenQA, a multilingual QA dataset with evergreen labels, is introduced to benchmark LLMs on temporality encoding and assess their performance through verbalized judgments and uncertainty signals.', 'ai_keywords': ['Large Language Models', 'QA', 'evergreen', 'mutable', 'temporality', 'Multilingual QA dataset', 'EG-E5', 'lightweight multilingual classifier', 'SoTA performance', 'self-knowledge estimation', 'filtering QA datasets', 'GPT-4o retrieval behavior']}, 'publishedAt': '2025-05-27T08:35:13.000Z', 'title': 'Will It Still Be True Tomorrow? Multilingual Evergreen Question\\n  Classification to Improve Trustworthy QA', 'summary': 'Large Language Models (LLMs) often hallucinate in question answering (QA)\\ntasks. A key yet underexplored factor contributing to this is the temporality\\nof questions -- whether they are evergreen (answers remain stable over time) or\\nmutable (answers change). In this work, we introduce EverGreenQA, the first\\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\\nencode question temporality explicitly (via verbalized judgments) or implicitly\\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\\nthe practical utility of evergreen classification across three applications:\\nimproving self-knowledge estimation, filtering QA datasets, and explaining\\nGPT-4o retrieval behavior.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21115.png', 'numComments': 3, 'submittedBy': {'_id': '660ee18e2dcd816ad14b3739', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg', 'fullname': 'Maria Marina', 'name': 'zlatamaria', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.01111', 'authors': [{'_id': '6845b6a33ec10bdd8ab4da1b', 'name': 'Shunian Chen', 'hidden': False}, {'_id': '6845b6a33ec10bdd8ab4da1c', 'user': {'_id': '66440e86bfe15e84d369cb03', 'avatarUrl': '/avatars/d15b3b3831bc74138206071612169f64.svg', 'isPro': False, 'fullname': 'Xinyuan Xie', 'user': 'SatsukiVie', 'type': 'user'}, 'name': 'Xinyuan Xie', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:11:42.759Z', 'hidden': False}, {'_id': '6845b6a33ec10bdd8ab4da1d', 'name': 'Zheshu Chen', 'hidden': False}, {'_id': '6845b6a33ec10bdd8ab4da1e', 'name': 'Liyan Zhao', 'hidden': False}, {'_id': '6845b6a33ec10bdd8ab4da1f', 'name': 'Owen Lee', 'hidden': False}, {'_id': '6845b6a33ec10bdd8ab4da20', 'name': 'Zhan Su', 'hidden': False}, {'_id': '6845b6a33ec10bdd8ab4da21', 'name': 'Qilin Sun', 'hidden': False}, {'_id': '6845b6a33ec10bdd8ab4da22', 'name': 'Benyou Wang', 'hidden': False}], 'publishedAt': '2025-06-01T18:29:17.000Z', 'submittedOnDailyAt': '2025-06-09T01:59:54.914Z', 'title': 'FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\\n  Contextual Fusion', 'submittedOnDailyBy': {'_id': '623be9e1d1eb227788764959', 'avatarUrl': '/avatars/b6521b795a59754dbb40123fd4f63b8c.svg', 'isPro': False, 'fullname': 'Shunian Chen', 'user': 'Shunian', 'type': 'user'}, 'summary': 'High-quality, large-scale audio captioning is crucial for advancing audio\\nunderstanding, yet current automated methods often generate captions that lack\\nfine-grained detail and contextual accuracy, primarily due to their reliance on\\nlimited unimodal or superficial multimodal information. Drawing inspiration\\nfrom human auditory perception, which adeptly integrates cross-modal cues and\\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\\nautomated pipeline. This pipeline first employs specialized pretrained models\\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\\nvisual information from associated video). A large language model (LLM) then\\nsynthesizes these rich, multimodal inputs to generate detailed and\\ncontext-aware audio captions. Key contributions of this work include: (1) the\\nproposed scalable method for fine-grained audio caption generation; (2)\\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\\nsuperior audio-text alignment and instruction following. This paper paves the\\nway for more nuanced and accurate automated understanding of complex audio\\nenvironments. Code and data can be found in\\nhttps://github.com/satsuki2486441738/FusionAudio.', 'upvotes': 23, 'discussionId': '6845b6a43ec10bdd8ab4da23', 'githubRepo': 'https://github.com/FreedomIntelligence/FusionAudio', 'ai_summary': 'A novel two-stage pipeline using specialized pretrained models and a large language model enhances audio caption quality by integrating diverse multimodal cues and contextual information.', 'ai_keywords': ['audio captioning', 'auditory perception', 'auditory scene analysis', 'pretrained models', 'large language model', 'FusionAudio', 'CLAP-based audio encoder', 'audio-text alignment', 'instruction following']}, 'publishedAt': '2025-06-01T14:29:17.000Z', 'title': 'FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\\n  Contextual Fusion', 'summary': 'High-quality, large-scale audio captioning is crucial for advancing audio\\nunderstanding, yet current automated methods often generate captions that lack\\nfine-grained detail and contextual accuracy, primarily due to their reliance on\\nlimited unimodal or superficial multimodal information. Drawing inspiration\\nfrom human auditory perception, which adeptly integrates cross-modal cues and\\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\\nautomated pipeline. This pipeline first employs specialized pretrained models\\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\\nvisual information from associated video). A large language model (LLM) then\\nsynthesizes these rich, multimodal inputs to generate detailed and\\ncontext-aware audio captions. Key contributions of this work include: (1) the\\nproposed scalable method for fine-grained audio caption generation; (2)\\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\\nsuperior audio-text alignment and instruction following. This paper paves the\\nway for more nuanced and accurate automated understanding of complex audio\\nenvironments. Code and data can be found in\\nhttps://github.com/satsuki2486441738/FusionAudio.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01111.png', 'numComments': 1, 'submittedBy': {'_id': '623be9e1d1eb227788764959', 'avatarUrl': '/avatars/b6521b795a59754dbb40123fd4f63b8c.svg', 'fullname': 'Shunian Chen', 'name': 'Shunian', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.01872', 'authors': [{'_id': '683e77d41417d107337abf6e', 'user': {'_id': '643f9e2288d9d4488fd81c52', 'avatarUrl': '/avatars/e589c9cbd47022883cf33d7555bee89c.svg', 'isPro': False, 'fullname': 'Tinghui Zhu', 'user': 'DarthZhu', 'type': 'user'}, 'name': 'Tinghui Zhu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:12:47.572Z', 'hidden': False}, {'_id': '683e77d41417d107337abf6f', 'name': 'Kai Zhang', 'hidden': False}, {'_id': '683e77d41417d107337abf70', 'name': 'Muhao Chen', 'hidden': False}, {'_id': '683e77d41417d107337abf71', 'name': 'Yu Su', 'hidden': False}], 'publishedAt': '2025-06-02T17:01:40.000Z', 'submittedOnDailyAt': '2025-06-09T07:51:42.399Z', 'title': 'Is Extending Modality The Right Path Towards Omni-Modality?', 'submittedOnDailyBy': {'_id': '643f9e2288d9d4488fd81c52', 'avatarUrl': '/avatars/e589c9cbd47022883cf33d7555bee89c.svg', 'isPro': False, 'fullname': 'Tinghui Zhu', 'user': 'DarthZhu', 'type': 'user'}, 'summary': 'Omni-modal language models (OLMs) aim to integrate and reason over diverse\\ninput modalities--such as text, images, video, and audio--while maintaining\\nstrong language capabilities. Despite recent advancements, existing models,\\nespecially open-source ones, remain far from true omni-modality, struggling to\\ngeneralize beyond the specific modality pairs they are trained on or to achieve\\nstrong performance when processing multi-modal inputs. We study the effect of\\nextending modality, the dominant technique for training multimodal models,\\nwhere an off-the-shelf language model is fine-tuned on target-domain and\\nlanguage data. Specifically, we investigate three key questions: (1) Does\\nmodality extension compromise core language abilities? (2) Can model merging\\neffectively integrate independently fine-tuned modality-specific models to\\nachieve omni-modality? (3) Does omni-modality extension lead to better\\nknowledge sharing and generalization compared to sequential extension? Through\\nextensive experiments, we analyze these trade-offs and provide insights into\\nthe feasibility of achieving true omni-modality using current approaches.', 'upvotes': 14, 'discussionId': '683e77d41417d107337abf8f', 'projectPage': 'https://darthzhu.github.io/lm-extend-page/', 'githubRepo': 'https://github.com/DarthZhu/lm-extend', 'ai_summary': 'Research investigates the impact of extending modality and model merging on maintaining language abilities and generalization in omni-modal language models.', 'ai_keywords': ['omni-modal language models', 'modality extension', 'fine-tuning', 'language abilities', 'model merging', 'generalization', 'true omni-modality']}, 'publishedAt': '2025-06-02T13:01:40.000Z', 'title': 'Is Extending Modality The Right Path Towards Omni-Modality?', 'summary': 'Omni-modal language models (OLMs) aim to integrate and reason over diverse\\ninput modalities--such as text, images, video, and audio--while maintaining\\nstrong language capabilities. Despite recent advancements, existing models,\\nespecially open-source ones, remain far from true omni-modality, struggling to\\ngeneralize beyond the specific modality pairs they are trained on or to achieve\\nstrong performance when processing multi-modal inputs. We study the effect of\\nextending modality, the dominant technique for training multimodal models,\\nwhere an off-the-shelf language model is fine-tuned on target-domain and\\nlanguage data. Specifically, we investigate three key questions: (1) Does\\nmodality extension compromise core language abilities? (2) Can model merging\\neffectively integrate independently fine-tuned modality-specific models to\\nachieve omni-modality? (3) Does omni-modality extension lead to better\\nknowledge sharing and generalization compared to sequential extension? Through\\nextensive experiments, we analyze these trade-offs and provide insights into\\nthe feasibility of achieving true omni-modality using current approaches.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01872.png', 'numComments': 1, 'submittedBy': {'_id': '643f9e2288d9d4488fd81c52', 'avatarUrl': '/avatars/e589c9cbd47022883cf33d7555bee89c.svg', 'fullname': 'Tinghui Zhu', 'name': 'DarthZhu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.05629', 'authors': [{'_id': '68464b273ec10bdd8ab4da86', 'user': {'_id': '64a6518132cf858d6386ac52', 'avatarUrl': '/avatars/4cabf3dab8b1ba06245ad8024f334181.svg', 'isPro': False, 'fullname': 'Ananth Muppidi', 'user': 'ananthmuppidi', 'type': 'user'}, 'name': 'Ananth Muppidi', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-09T02:47:03.971Z', 'hidden': False}, {'_id': '68464b273ec10bdd8ab4da87', 'user': {'_id': '5f89da6c5d083370c711f37c', 'avatarUrl': '/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg', 'isPro': False, 'fullname': 'Abhilash Nandy', 'user': 'abhi1nandy2', 'type': 'user'}, 'name': 'Abhilash Nandy', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-06-09T03:49:29.446Z', 'hidden': False}, {'_id': '68464b273ec10bdd8ab4da88', 'user': {'_id': '65238ea295df08170c93933d', 'avatarUrl': '/avatars/8364301e324274a550d12f2b184ea10e.svg', 'isPro': False, 'fullname': 'Sambaran Bandyopadhyay', 'user': 'sambaran', 'type': 'user'}, 'name': 'Sambaran Bandyopadhyay', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-09T02:47:03.971Z', 'hidden': False}], 'publishedAt': '2025-06-05T23:13:22.000Z', 'submittedOnDailyAt': '2025-06-09T01:27:41.831Z', 'title': 'Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs', 'submittedOnDailyBy': {'_id': '5f89da6c5d083370c711f37c', 'avatarUrl': '/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg', 'isPro': False, 'fullname': 'Abhilash Nandy', 'user': 'abhi1nandy2', 'type': 'user'}, 'summary': 'The performance of large language models in domain-specific tasks\\nnecessitates fine-tuning, which is computationally expensive and technically\\nchallenging. This paper focuses on parameter-efficient fine-tuning using soft\\nprompting, a promising approach that adapts pre-trained models to downstream\\ntasks by learning a small set of parameters. We propose a novel Input Dependent\\nSoft Prompting technique with a self-Attention Mechanism (ID-SPAM) that\\ngenerates soft prompts based on the input tokens and attends different tokens\\nwith varying importance. Our method is simple and efficient, keeping the number\\nof trainable parameters small. We show the merits of the proposed approach\\ncompared to state-of-the-art techniques on various tasks and show the improved\\nzero shot domain transfer capability.', 'upvotes': 13, 'discussionId': '68464b273ec10bdd8ab4da89', 'ai_summary': 'A new method using input-dependent soft prompting with a self-attention mechanism improves parameter-efficient fine-tuning for large language models, enhancing zero-shot domain transfer.', 'ai_keywords': ['soft prompting', 'parameter-efficient fine-tuning', 'pre-trained models', 'downstream tasks', 'Input Dependent Soft Prompting technique', 'self-Attention Mechanism', 'zero shot domain transfer']}, 'publishedAt': '2025-06-05T19:13:22.000Z', 'title': 'Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs', 'summary': 'The performance of large language models in domain-specific tasks\\nnecessitates fine-tuning, which is computationally expensive and technically\\nchallenging. This paper focuses on parameter-efficient fine-tuning using soft\\nprompting, a promising approach that adapts pre-trained models to downstream\\ntasks by learning a small set of parameters. We propose a novel Input Dependent\\nSoft Prompting technique with a self-Attention Mechanism (ID-SPAM) that\\ngenerates soft prompts based on the input tokens and attends different tokens\\nwith varying importance. Our method is simple and efficient, keeping the number\\nof trainable parameters small. We show the merits of the proposed approach\\ncompared to state-of-the-art techniques on various tasks and show the improved\\nzero shot domain transfer capability.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05629.png', 'numComments': 1, 'submittedBy': {'_id': '5f89da6c5d083370c711f37c', 'avatarUrl': '/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg', 'fullname': 'Abhilash Nandy', 'name': 'abhi1nandy2', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.05984', 'authors': [{'_id': '68463ee43ec10bdd8ab4da6f', 'user': {'_id': '622326ae0129f2097d69a3e2', 'avatarUrl': '/avatars/7665223e3fc8b820ce001e6003daf4d2.svg', 'isPro': False, 'fullname': 'Cheng-Han Chiang', 'user': 'dcml0714', 'type': 'user'}, 'name': 'Cheng-Han Chiang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:11:26.041Z', 'hidden': False}, {'_id': '68463ee43ec10bdd8ab4da70', 'user': {'_id': '64dc191bc307ee5369fbcb04', 'avatarUrl': '/avatars/5a8a0db63a187e85d4ae2fff93a838f0.svg', 'isPro': False, 'fullname': 'Xiaofei Wang', 'user': 'xiaofei-wang', 'type': 'user'}, 'name': 'Xiaofei Wang', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-09T01:54:46.319Z', 'hidden': False}, {'_id': '68463ee43ec10bdd8ab4da71', 'name': 'Chung-Ching Lin', 'hidden': False}, {'_id': '68463ee43ec10bdd8ab4da72', 'name': 'Kevin Lin', 'hidden': False}, {'_id': '68463ee43ec10bdd8ab4da73', 'name': 'Linjie Li', 'hidden': False}, {'_id': '68463ee43ec10bdd8ab4da74', 'name': 'Radu Kopetz', 'hidden': False}, {'_id': '68463ee43ec10bdd8ab4da75', 'name': 'Yao Qian', 'hidden': False}, {'_id': '68463ee43ec10bdd8ab4da76', 'name': 'Zhendong Wang', 'hidden': False}, {'_id': '68463ee43ec10bdd8ab4da77', 'name': 'Zhengyuan Yang', 'hidden': False}, {'_id': '68463ee43ec10bdd8ab4da78', 'name': 'Hung-yi Lee', 'hidden': False}, {'_id': '68463ee43ec10bdd8ab4da79', 'name': 'Lijuan Wang', 'hidden': False}], 'publishedAt': '2025-06-06T11:05:48.000Z', 'submittedOnDailyAt': '2025-06-09T00:28:11.753Z', 'title': 'Audio-Aware Large Language Models as Judges for Speaking Styles', 'submittedOnDailyBy': {'_id': '622326ae0129f2097d69a3e2', 'avatarUrl': '/avatars/7665223e3fc8b820ce001e6003daf4d2.svg', 'isPro': False, 'fullname': 'Cheng-Han Chiang', 'user': 'dcml0714', 'type': 'user'}, 'summary': \"Audio-aware large language models (ALLMs) can understand the textual and\\nnon-textual information in the audio input. In this paper, we explore using\\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\\nstyle instruction following and role-playing. The speaking style we consider\\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\\nshow that the agreement between Gemini and human judges is comparable to the\\nagreement between human evaluators. These promising results show that ALLMs can\\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\\neven GPT-4o-audio, still have room for improvement in controlling the speaking\\nstyle and generating natural dialogues.\", 'upvotes': 10, 'discussionId': '68463ee43ec10bdd8ab4da7a', 'ai_summary': 'Audio-aware large language models can assess speaking styles in audio inputs, demonstrating performance comparable to human judges in evaluating synthesized speech along dimensions like emotion, volume, and pitch.', 'ai_keywords': ['audio-aware large language models', 'ALLMs', 'speaking styles', 'SLMs', 'voice style instruction', 'role-playing', 'emotion', 'volume', 'speaking pace', 'word emphasis', 'pitch control', 'non-verbal elements', 'GPT-4o-audio', 'Gemini-2.5-pro', 'human evaluation', 'agreement', 'speaking style control', 'natural dialogues']}, 'publishedAt': '2025-06-06T07:05:48.000Z', 'title': 'Audio-Aware Large Language Models as Judges for Speaking Styles', 'summary': \"Audio-aware large language models (ALLMs) can understand the textual and\\nnon-textual information in the audio input. In this paper, we explore using\\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\\nstyle instruction following and role-playing. The speaking style we consider\\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\\nshow that the agreement between Gemini and human judges is comparable to the\\nagreement between human evaluators. These promising results show that ALLMs can\\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\\neven GPT-4o-audio, still have room for improvement in controlling the speaking\\nstyle and generating natural dialogues.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05984.png', 'numComments': 1, 'submittedBy': {'_id': '622326ae0129f2097d69a3e2', 'avatarUrl': '/avatars/7665223e3fc8b820ce001e6003daf4d2.svg', 'fullname': 'Cheng-Han Chiang', 'name': 'dcml0714', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.05523', 'authors': [{'_id': '6846657d3ec10bdd8ab4daca', 'user': {'_id': '630bc5ae86b8b9904c33e94b', 'avatarUrl': '/avatars/b176d9b1691c05cc941409dd6c2b2228.svg', 'isPro': False, 'fullname': 'Zikui Cai', 'user': 'Zikui', 'type': 'user'}, 'name': 'Zikui Cai', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:11:17.551Z', 'hidden': False}, {'_id': '6846657d3ec10bdd8ab4dacb', 'name': 'Andrew Wang', 'hidden': False}, {'_id': '6846657d3ec10bdd8ab4dacc', 'name': 'Anirudh Satheesh', 'hidden': False}, {'_id': '6846657d3ec10bdd8ab4dacd', 'name': 'Ankit Nakhawa', 'hidden': False}, {'_id': '6846657d3ec10bdd8ab4dace', 'name': 'Hyunwoo Jae', 'hidden': False}, {'_id': '6846657d3ec10bdd8ab4dacf', 'name': 'Keenan Powell', 'hidden': False}, {'_id': '6846657d3ec10bdd8ab4dad0', 'name': 'Minghui Liu', 'hidden': False}, {'_id': '6846657d3ec10bdd8ab4dad1', 'name': 'Neel Jay', 'hidden': False}, {'_id': '6846657d3ec10bdd8ab4dad2', 'name': 'Sungbin Oh', 'hidden': False}, {'_id': '6846657d3ec10bdd8ab4dad3', 'name': 'Xiyao Wang', 'hidden': False}, {'_id': '6846657d3ec10bdd8ab4dad4', 'name': 'Yongyuan Liang', 'hidden': False}, {'_id': '6846657d3ec10bdd8ab4dad5', 'name': 'Tom Goldstein', 'hidden': False}, {'_id': '6846657d3ec10bdd8ab4dad6', 'name': 'Furong Huang', 'hidden': False}], 'publishedAt': '2025-06-05T19:12:45.000Z', 'submittedOnDailyAt': '2025-06-09T03:10:23.755Z', 'title': 'MORSE-500: A Programmatically Controllable Video Benchmark to\\n  Stress-Test Multimodal Reasoning', 'submittedOnDailyBy': {'_id': '655fed9fdef5905d38b84af3', 'avatarUrl': '/avatars/2cda4182dfd11a1e94743639e62328ea.svg', 'isPro': False, 'fullname': 'Xiyao Wang', 'user': 'russwang', 'type': 'user'}, 'summary': 'Despite rapid advances in vision-language models (VLMs), current benchmarks\\nfor multimodal reasoning fall short in three key dimensions. First, they\\noverwhelmingly rely on static images, failing to capture the temporal\\ncomplexity of real-world environments. Second, they narrowly focus on\\nmathematical problem-solving, neglecting the broader spectrum of reasoning\\nskills -- including abstract, physical, planning, spatial, and temporal\\ncapabilities -- required for robust multimodal intelligence. Third, many\\nbenchmarks quickly saturate, offering limited headroom for diagnosing failure\\nmodes or measuring continued progress. We introduce MORSE-500 (Multimodal\\nReasoning Stress-test Environment), a video benchmark composed of 500 fully\\nscripted clips with embedded questions spanning six complementary reasoning\\ncategories. Each instance is programmatically generated using deterministic\\nPython scripts (via Manim, Matplotlib, MoviePy), generative video models, and\\ncurated real footage. This script-driven design allows fine-grained control\\nover visual complexity, distractor density, and temporal dynamics -- enabling\\ndifficulty to be scaled systematically as models improve. Unlike static\\nbenchmarks that become obsolete once saturated, MORSE-500 is built to evolve:\\nits controllable generation pipeline supports the creation of arbitrarily\\nchallenging new instances, making it ideally suited for stress-testing\\nnext-generation models. Initial experiments with state-of-the-art systems --\\nincluding various Gemini 2.5 Pro and OpenAI o3 which represent the strongest\\navailable at the time, alongside strong open-source models -- reveal\\nsubstantial performance gaps across all categories, with particularly large\\ndeficits in abstract and planning tasks. We release the full dataset,\\ngeneration scripts, and evaluation harness to support transparent,\\nreproducible, and forward-looking multimodal reasoning research.', 'upvotes': 10, 'discussionId': '6846657d3ec10bdd8ab4dad7', 'projectPage': 'https://morse-500.github.io/', 'githubRepo': 'https://github.com/morse-benchmark/morse-500', 'ai_summary': 'MORSE-500, a video benchmark with 500 scripted clips, evaluates multimodal reasoning across six categories, highlighting performance gaps in abstract and planning tasks.', 'ai_keywords': ['Vision-language models', 'MORSE-500', 'multimodal reasoning', 'video benchmark', 'scripted clips', 'reasoning categories', 'Manim', 'Matplotlib', 'MoviePy', 'generative video models', 'controllable generation', 'spatial capabilities', 'temporal capabilities', 'abstract reasoning', 'planning tasks']}, 'publishedAt': '2025-06-05T15:12:45.000Z', 'title': 'MORSE-500: A Programmatically Controllable Video Benchmark to\\n  Stress-Test Multimodal Reasoning', 'summary': 'Despite rapid advances in vision-language models (VLMs), current benchmarks\\nfor multimodal reasoning fall short in three key dimensions. First, they\\noverwhelmingly rely on static images, failing to capture the temporal\\ncomplexity of real-world environments. Second, they narrowly focus on\\nmathematical problem-solving, neglecting the broader spectrum of reasoning\\nskills -- including abstract, physical, planning, spatial, and temporal\\ncapabilities -- required for robust multimodal intelligence. Third, many\\nbenchmarks quickly saturate, offering limited headroom for diagnosing failure\\nmodes or measuring continued progress. We introduce MORSE-500 (Multimodal\\nReasoning Stress-test Environment), a video benchmark composed of 500 fully\\nscripted clips with embedded questions spanning six complementary reasoning\\ncategories. Each instance is programmatically generated using deterministic\\nPython scripts (via Manim, Matplotlib, MoviePy), generative video models, and\\ncurated real footage. This script-driven design allows fine-grained control\\nover visual complexity, distractor density, and temporal dynamics -- enabling\\ndifficulty to be scaled systematically as models improve. Unlike static\\nbenchmarks that become obsolete once saturated, MORSE-500 is built to evolve:\\nits controllable generation pipeline supports the creation of arbitrarily\\nchallenging new instances, making it ideally suited for stress-testing\\nnext-generation models. Initial experiments with state-of-the-art systems --\\nincluding various Gemini 2.5 Pro and OpenAI o3 which represent the strongest\\navailable at the time, alongside strong open-source models -- reveal\\nsubstantial performance gaps across all categories, with particularly large\\ndeficits in abstract and planning tasks. We release the full dataset,\\ngeneration scripts, and evaluation harness to support transparent,\\nreproducible, and forward-looking multimodal reasoning research.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05523.png', 'numComments': 1, 'submittedBy': {'_id': '655fed9fdef5905d38b84af3', 'avatarUrl': '/avatars/2cda4182dfd11a1e94743639e62328ea.svg', 'fullname': 'Xiyao Wang', 'name': 'russwang', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.06276', 'authors': [{'_id': '68466dfb3ec10bdd8ab4dae2', 'name': 'Jiatao Gu', 'hidden': False}, {'_id': '68466dfb3ec10bdd8ab4dae3', 'name': 'Tianrong Chen', 'hidden': False}, {'_id': '68466dfb3ec10bdd8ab4dae4', 'name': 'David Berthelot', 'hidden': False}, {'_id': '68466dfb3ec10bdd8ab4dae5', 'name': 'Huangjie Zheng', 'hidden': False}, {'_id': '68466dfb3ec10bdd8ab4dae6', 'name': 'Yuyang Wang', 'hidden': False}, {'_id': '68466dfb3ec10bdd8ab4dae7', 'name': 'Ruixiang Zhang', 'hidden': False}, {'_id': '68466dfb3ec10bdd8ab4dae8', 'name': 'Laurent Dinh', 'hidden': False}, {'_id': '68466dfb3ec10bdd8ab4dae9', 'name': 'Miguel Angel Bautista', 'hidden': False}, {'_id': '68466dfb3ec10bdd8ab4daea', 'name': 'Josh Susskind', 'hidden': False}, {'_id': '68466dfb3ec10bdd8ab4daeb', 'name': 'Shuangfei Zhai', 'hidden': False}], 'publishedAt': '2025-06-06T17:58:39.000Z', 'submittedOnDailyAt': '2025-06-09T03:58:52.022Z', 'title': 'STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\\n  Synthesis', 'submittedOnDailyBy': {'_id': '6164e72d73996c363c52e66d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png', 'isPro': False, 'fullname': 'Jiatao Gu', 'user': 'thomagram', 'type': 'user'}, 'summary': 'We present STARFlow, a scalable generative model based on normalizing flows\\nthat achieves strong performance in high-resolution image synthesis. The core\\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\\nexpressive power of normalizing flows with the structured modeling capabilities\\nof Autoregressive Transformers. We first establish the theoretical universality\\nof TARFlow for modeling continuous distributions. Building on this foundation,\\nwe introduce several key architectural and algorithmic innovations to\\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\\nTransformer block captures most of the model representational capacity,\\ncomplemented by a few shallow Transformer blocks that are computationally\\nefficient yet substantially beneficial; (2) modeling in the latent space of\\npretrained autoencoders, which proves more effective than direct pixel-level\\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\\nexact maximum likelihood training in continuous spaces without discretization.\\nSTARFlow achieves competitive performance in both class-conditional and\\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\\nmodels in sample quality. To our knowledge, this work is the first successful\\ndemonstration of normalizing flows operating effectively at this scale and\\nresolution.', 'upvotes': 9, 'discussionId': '68466dfb3ec10bdd8ab4daec', 'ai_summary': 'STARFlow, a generative model combining normalizing flows with autoregressive Transformers, achieves competitive image synthesis performance with innovations in architecture and latent space modeling.', 'ai_keywords': ['normalizing flows', 'Transformer Autoregressive Flow', 'TARFlow', 'theoretical universality', 'deep-shallow design', 'pretrained autoencoders', 'latent space', 'guidance algorithm', 'end-to-end normalizing flow', 'exact maximum likelihood training', 'class-conditional', 'text-conditional image generation', 'state-of-the-art diffusion models']}, 'publishedAt': '2025-06-06T13:58:39.000Z', 'title': 'STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\\n  Synthesis', 'summary': 'We present STARFlow, a scalable generative model based on normalizing flows\\nthat achieves strong performance in high-resolution image synthesis. The core\\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\\nexpressive power of normalizing flows with the structured modeling capabilities\\nof Autoregressive Transformers. We first establish the theoretical universality\\nof TARFlow for modeling continuous distributions. Building on this foundation,\\nwe introduce several key architectural and algorithmic innovations to\\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\\nTransformer block captures most of the model representational capacity,\\ncomplemented by a few shallow Transformer blocks that are computationally\\nefficient yet substantially beneficial; (2) modeling in the latent space of\\npretrained autoencoders, which proves more effective than direct pixel-level\\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\\nexact maximum likelihood training in continuous spaces without discretization.\\nSTARFlow achieves competitive performance in both class-conditional and\\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\\nmodels in sample quality. To our knowledge, this work is the first successful\\ndemonstration of normalizing flows operating effectively at this scale and\\nresolution.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06276.png', 'numComments': 1, 'submittedBy': {'_id': '6164e72d73996c363c52e66d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png', 'fullname': 'Jiatao Gu', 'name': 'thomagram', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.05573', 'authors': [{'_id': '6846902a3ec10bdd8ab4db61', 'name': 'Yuchen Lin', 'hidden': False}, {'_id': '6846902a3ec10bdd8ab4db62', 'user': {'_id': '62e18206926f4892a4c782bd', 'avatarUrl': '/avatars/0f89091a5eb72165d2e860d15b339539.svg', 'isPro': False, 'fullname': 'Chenguo Lin', 'user': 'chenguolin', 'type': 'user'}, 'name': 'Chenguo Lin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:11:09.640Z', 'hidden': False}, {'_id': '6846902a3ec10bdd8ab4db63', 'name': 'Panwang Pan', 'hidden': False}, {'_id': '6846902a3ec10bdd8ab4db64', 'name': 'Honglei Yan', 'hidden': False}, {'_id': '6846902a3ec10bdd8ab4db65', 'name': 'Yiqiang Feng', 'hidden': False}, {'_id': '6846902a3ec10bdd8ab4db66', 'name': 'Yadong Mu', 'hidden': False}, {'_id': '6846902a3ec10bdd8ab4db67', 'name': 'Katerina Fragkiadaki', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4'], 'publishedAt': '2025-06-05T20:30:28.000Z', 'submittedOnDailyAt': '2025-06-09T06:14:01.450Z', 'title': 'PartCrafter: Structured 3D Mesh Generation via Compositional Latent\\n  Diffusion Transformers', 'submittedOnDailyBy': {'_id': '62e18206926f4892a4c782bd', 'avatarUrl': '/avatars/0f89091a5eb72165d2e860d15b339539.svg', 'isPro': False, 'fullname': 'Chenguo Lin', 'user': 'chenguolin', 'type': 'user'}, 'summary': 'We introduce PartCrafter, the first structured 3D generative model that\\njointly synthesizes multiple semantically meaningful and geometrically distinct\\n3D meshes from a single RGB image. Unlike existing methods that either produce\\nmonolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an\\nimage and then reconstructing each segment, PartCrafter adopts a unified,\\ncompositional generation architecture that does not rely on pre-segmented\\ninputs. Conditioned on a single image, it simultaneously denoises multiple 3D\\nparts, enabling end-to-end part-aware generation of both individual objects and\\ncomplex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh\\ndiffusion transformer (DiT) trained on whole objects, inheriting the pretrained\\nweights, encoder, and decoder, and introduces two key innovations: (1) A\\ncompositional latent space, where each 3D part is represented by a set of\\ndisentangled latent tokens; (2) A hierarchical attention mechanism that enables\\nstructured information flow both within individual parts and across all parts,\\nensuring global coherence while preserving part-level detail during generation.\\nTo support part-level supervision, we curate a new dataset by mining part-level\\nannotations from large-scale 3D object datasets. Experiments show that\\nPartCrafter outperforms existing approaches in generating decomposable 3D\\nmeshes, including parts that are not directly visible in input images,\\ndemonstrating the strength of part-aware generative priors for 3D understanding\\nand synthesis. Code and training data will be released.', 'upvotes': 9, 'discussionId': '6846902a3ec10bdd8ab4db68', 'projectPage': 'https://wgsxm.github.io/projects/partcrafter', 'githubRepo': 'https://github.com/wgsxm/PartCrafter', 'ai_summary': 'PartCrafter is a unified 3D generative model that synthesizes multiple semantically meaningful 3D meshes from a single image using a compositional latent space and hierarchical attention mechanism.', 'ai_keywords': ['3D generative model', 'multiple 3D meshes', 'RGB image', 'unified compositional generation architecture', 'denoising', '3D diffusion transformer (DiT)', 'compositional latent space', 'disentangled latent tokens', 'hierarchical attention mechanism', 'part-level supervision', 'part-aware generative priors']}, 'publishedAt': '2025-06-05T16:30:28.000Z', 'title': 'PartCrafter: Structured 3D Mesh Generation via Compositional Latent\\n  Diffusion Transformers', 'summary': 'We introduce PartCrafter, the first structured 3D generative model that\\njointly synthesizes multiple semantically meaningful and geometrically distinct\\n3D meshes from a single RGB image. Unlike existing methods that either produce\\nmonolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an\\nimage and then reconstructing each segment, PartCrafter adopts a unified,\\ncompositional generation architecture that does not rely on pre-segmented\\ninputs. Conditioned on a single image, it simultaneously denoises multiple 3D\\nparts, enabling end-to-end part-aware generation of both individual objects and\\ncomplex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh\\ndiffusion transformer (DiT) trained on whole objects, inheriting the pretrained\\nweights, encoder, and decoder, and introduces two key innovations: (1) A\\ncompositional latent space, where each 3D part is represented by a set of\\ndisentangled latent tokens; (2) A hierarchical attention mechanism that enables\\nstructured information flow both within individual parts and across all parts,\\nensuring global coherence while preserving part-level detail during generation.\\nTo support part-level supervision, we curate a new dataset by mining part-level\\nannotations from large-scale 3D object datasets. Experiments show that\\nPartCrafter outperforms existing approaches in generating decomposable 3D\\nmeshes, including parts that are not directly visible in input images,\\ndemonstrating the strength of part-aware generative priors for 3D understanding\\nand synthesis. Code and training data will be released.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05573.png', 'numComments': 2, 'submittedBy': {'_id': '62e18206926f4892a4c782bd', 'avatarUrl': '/avatars/0f89091a5eb72165d2e860d15b339539.svg', 'fullname': 'Chenguo Lin', 'name': 'chenguolin', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.06253', 'authors': [{'_id': '68469b773ec10bdd8ab4db88', 'name': 'Yuping He', 'hidden': False}, {'_id': '68469b773ec10bdd8ab4db89', 'name': 'Yifei Huang', 'hidden': False}, {'_id': '68469b773ec10bdd8ab4db8a', 'user': {'_id': '6392c73390b8e99a6779a7b0', 'avatarUrl': '/avatars/9ff824ab02848120aec5e8de6780bcf1.svg', 'isPro': False, 'fullname': 'Guo Chen', 'user': 'cg1177', 'type': 'user'}, 'name': 'Guo Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:11:02.757Z', 'hidden': False}, {'_id': '68469b773ec10bdd8ab4db8b', 'name': 'Lidong Lu', 'hidden': False}, {'_id': '68469b773ec10bdd8ab4db8c', 'name': 'Baoqi Pei', 'hidden': False}, {'_id': '68469b773ec10bdd8ab4db8d', 'name': 'Jilan Xu', 'hidden': False}, {'_id': '68469b773ec10bdd8ab4db8e', 'name': 'Tong Lu', 'hidden': False}, {'_id': '68469b773ec10bdd8ab4db8f', 'name': 'Yoichi Sato', 'hidden': False}], 'publishedAt': '2025-06-06T17:25:48.000Z', 'submittedOnDailyAt': '2025-06-09T07:00:04.801Z', 'title': 'Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\\n  with Egocentric-Exocentric Vision', 'submittedOnDailyBy': {'_id': '6392c73390b8e99a6779a7b0', 'avatarUrl': '/avatars/9ff824ab02848120aec5e8de6780bcf1.svg', 'isPro': False, 'fullname': 'Guo Chen', 'user': 'cg1177', 'type': 'user'}, 'summary': 'Perceiving the world from both egocentric (first-person) and exocentric\\n(third-person) perspectives is fundamental to human cognition, enabling rich\\nand complementary understanding of dynamic environments. In recent years,\\nallowing the machines to leverage the synergistic potential of these dual\\nperspectives has emerged as a compelling research direction in video\\nunderstanding. In this survey, we provide a comprehensive review of video\\nunderstanding from both exocentric and egocentric viewpoints. We begin by\\nhighlighting the practical applications of integrating egocentric and\\nexocentric techniques, envisioning their potential collaboration across\\ndomains. We then identify key research tasks to realize these applications.\\nNext, we systematically organize and review recent advancements into three main\\nresearch directions: (1) leveraging egocentric data to enhance exocentric\\nunderstanding, (2) utilizing exocentric data to improve egocentric analysis,\\nand (3) joint learning frameworks that unify both perspectives. For each\\ndirection, we analyze a diverse set of tasks and relevant works. Additionally,\\nwe discuss benchmark datasets that support research in both perspectives,\\nevaluating their scope, diversity, and applicability. Finally, we discuss\\nlimitations in current works and propose promising future research directions.\\nBy synthesizing insights from both perspectives, our goal is to inspire\\nadvancements in video understanding and artificial intelligence, bringing\\nmachines closer to perceiving the world in a human-like manner. A GitHub repo\\nof related works can be found at\\nhttps://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.', 'upvotes': 5, 'discussionId': '68469b773ec10bdd8ab4db90', 'projectPage': 'https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision', 'githubRepo': 'https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision', 'ai_summary': 'A survey on leveraging both egocentric and exocentric video understanding for enhancing complementary tasks with a focus on three research directions and benchmark datasets.', 'ai_keywords': ['egocentric', 'exocentric', 'video understanding', 'research tasks', 'benchmark datasets']}, 'publishedAt': '2025-06-06T13:25:48.000Z', 'title': 'Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\\n  with Egocentric-Exocentric Vision', 'summary': 'Perceiving the world from both egocentric (first-person) and exocentric\\n(third-person) perspectives is fundamental to human cognition, enabling rich\\nand complementary understanding of dynamic environments. In recent years,\\nallowing the machines to leverage the synergistic potential of these dual\\nperspectives has emerged as a compelling research direction in video\\nunderstanding. In this survey, we provide a comprehensive review of video\\nunderstanding from both exocentric and egocentric viewpoints. We begin by\\nhighlighting the practical applications of integrating egocentric and\\nexocentric techniques, envisioning their potential collaboration across\\ndomains. We then identify key research tasks to realize these applications.\\nNext, we systematically organize and review recent advancements into three main\\nresearch directions: (1) leveraging egocentric data to enhance exocentric\\nunderstanding, (2) utilizing exocentric data to improve egocentric analysis,\\nand (3) joint learning frameworks that unify both perspectives. For each\\ndirection, we analyze a diverse set of tasks and relevant works. Additionally,\\nwe discuss benchmark datasets that support research in both perspectives,\\nevaluating their scope, diversity, and applicability. Finally, we discuss\\nlimitations in current works and propose promising future research directions.\\nBy synthesizing insights from both perspectives, our goal is to inspire\\nadvancements in video understanding and artificial intelligence, bringing\\nmachines closer to perceiving the world in a human-like manner. A GitHub repo\\nof related works can be found at\\nhttps://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06253.png', 'numComments': 1, 'submittedBy': {'_id': '6392c73390b8e99a6779a7b0', 'avatarUrl': '/avatars/9ff824ab02848120aec5e8de6780bcf1.svg', 'fullname': 'Guo Chen', 'name': 'cg1177', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 19}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.05433', 'authors': [{'_id': '68469df13ec10bdd8ab4db92', 'name': 'Zikang Liu', 'hidden': False}, {'_id': '68469df13ec10bdd8ab4db93', 'name': 'Tongtian Yue', 'hidden': False}, {'_id': '68469df13ec10bdd8ab4db94', 'name': 'Yepeng Tang', 'hidden': False}, {'_id': '68469df13ec10bdd8ab4db95', 'name': 'Longteng Guo', 'hidden': False}, {'_id': '68469df13ec10bdd8ab4db96', 'name': 'Junxian Cai', 'hidden': False}, {'_id': '68469df13ec10bdd8ab4db97', 'name': 'Qingbin Liu', 'hidden': False}, {'_id': '68469df13ec10bdd8ab4db98', 'name': 'Xi Chen', 'hidden': False}, {'_id': '68469df13ec10bdd8ab4db99', 'name': 'Jing Liu', 'hidden': False}], 'publishedAt': '2025-06-05T09:13:37.000Z', 'submittedOnDailyAt': '2025-06-09T07:17:09.252Z', 'title': 'Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward', 'submittedOnDailyBy': {'_id': '6448dcf1b6ac93fe6512e342', 'avatarUrl': '/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg', 'isPro': False, 'fullname': 'Zikang Liu', 'user': 'JohnCage', 'type': 'user'}, 'summary': 'Group Relative Policy Optimization (GRPO) enhances policy learning by\\ncomputing gradients from relative comparisons among candidate outputs that\\nshare a common input prefix. Despite its effectiveness, GRPO introduces\\nsubstantial computational overhead when processing long shared prefixes, which\\nmust be redundantly encoded for each group member. This inefficiency becomes a\\nmajor scalability bottleneck in long-context learning scenarios. We propose\\nPrefix Grouper, an efficient GRPO training algorithm that eliminates redundant\\nprefix computation via a Shared-Prefix Forward strategy. In particular, by\\nrestructuring self-attention into two parts, our method enables the shared\\nprefix to be encoded only once, while preserving full differentiability and\\ncompatibility with end-to-end training. We provide both theoretical and\\nempirical evidence that Prefix Grouper is training-equivalent to standard GRPO:\\nit yields identical forward outputs and backward gradients, ensuring that the\\noptimization dynamics and final policy performance remain unchanged.\\nEmpirically, our experiments confirm that Prefix Grouper achieves consistent\\nresults while significantly reducing the computational cost of training,\\nparticularly in long-prefix scenarios. The proposed method is fully\\nplug-and-play: it is compatible with existing GRPO-based architectures and can\\nbe seamlessly integrated into current training pipelines as a drop-in\\nreplacement, requiring no structural modifications and only minimal changes to\\ninput construction and attention computation. Prefix Grouper enables the use of\\nlarger group sizes under the same computational budget, thereby improving the\\nscalability of GRPO to more complex tasks and larger models. Code is now\\navailable at https://github.com/johncaged/PrefixGrouper', 'upvotes': 4, 'discussionId': '68469df13ec10bdd8ab4db9a', 'githubRepo': 'https://github.com/johncaged/PrefixGrouper', 'ai_summary': 'Prefix Grouper reduces computational overhead in GRPO by encoding shared prefixes only once, improving scalability in long-context scenarios without altering training dynamics or policy performance.', 'ai_keywords': ['Group Relative Policy Optimization (GRPO)', 'self-attention', 'Shared-Prefix Forward strategy', 'computational overhead', 'long-context learning scenarios', 'differentiability', 'end-to-end training', 'training-equivalent']}, 'publishedAt': '2025-06-05T05:13:37.000Z', 'title': 'Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward', 'summary': 'Group Relative Policy Optimization (GRPO) enhances policy learning by\\ncomputing gradients from relative comparisons among candidate outputs that\\nshare a common input prefix. Despite its effectiveness, GRPO introduces\\nsubstantial computational overhead when processing long shared prefixes, which\\nmust be redundantly encoded for each group member. This inefficiency becomes a\\nmajor scalability bottleneck in long-context learning scenarios. We propose\\nPrefix Grouper, an efficient GRPO training algorithm that eliminates redundant\\nprefix computation via a Shared-Prefix Forward strategy. In particular, by\\nrestructuring self-attention into two parts, our method enables the shared\\nprefix to be encoded only once, while preserving full differentiability and\\ncompatibility with end-to-end training. We provide both theoretical and\\nempirical evidence that Prefix Grouper is training-equivalent to standard GRPO:\\nit yields identical forward outputs and backward gradients, ensuring that the\\noptimization dynamics and final policy performance remain unchanged.\\nEmpirically, our experiments confirm that Prefix Grouper achieves consistent\\nresults while significantly reducing the computational cost of training,\\nparticularly in long-prefix scenarios. The proposed method is fully\\nplug-and-play: it is compatible with existing GRPO-based architectures and can\\nbe seamlessly integrated into current training pipelines as a drop-in\\nreplacement, requiring no structural modifications and only minimal changes to\\ninput construction and attention computation. Prefix Grouper enables the use of\\nlarger group sizes under the same computational budget, thereby improving the\\nscalability of GRPO to more complex tasks and larger models. Code is now\\navailable at https://github.com/johncaged/PrefixGrouper', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05433.png', 'numComments': 1, 'submittedBy': {'_id': '6448dcf1b6ac93fe6512e342', 'avatarUrl': '/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg', 'fullname': 'Zikang Liu', 'name': 'JohnCage', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.06199', 'authors': [{'_id': '684637733ec10bdd8ab4da66', 'user': {'_id': '674b2406591d7232820252cd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png', 'isPro': False, 'fullname': 'Hongyan Zhi', 'user': 'Hoyard', 'type': 'user'}, 'name': 'Hongyan Zhi', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:11:27.821Z', 'hidden': False}, {'_id': '684637733ec10bdd8ab4da67', 'name': 'Peihao Chen', 'hidden': False}, {'_id': '684637733ec10bdd8ab4da68', 'name': 'Siyuan Zhou', 'hidden': False}, {'_id': '684637733ec10bdd8ab4da69', 'name': 'Yubo Dong', 'hidden': False}, {'_id': '684637733ec10bdd8ab4da6a', 'name': 'Quanxi Wu', 'hidden': False}, {'_id': '684637733ec10bdd8ab4da6b', 'name': 'Lei Han', 'hidden': False}, {'_id': '684637733ec10bdd8ab4da6c', 'name': 'Mingkui Tan', 'hidden': False}], 'publishedAt': '2025-06-06T16:00:31.000Z', 'submittedOnDailyAt': '2025-06-09T01:42:52.611Z', 'title': '3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\\n  Model', 'submittedOnDailyBy': {'_id': '674b2406591d7232820252cd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png', 'isPro': False, 'fullname': 'Hongyan Zhi', 'user': 'Hoyard', 'type': 'user'}, 'summary': 'Manipulation has long been a challenging task for robots, while humans can\\neffortlessly perform complex interactions with objects, such as hanging a cup\\non the mug rack. A key reason is the lack of a large and uniform dataset for\\nteaching robots manipulation skills. Current robot datasets often record robot\\naction in different action spaces within a simple scene. This hinders the robot\\nto learn a unified and robust action representation for different robots within\\ndiverse scenes. Observing how humans understand a manipulation task, we find\\nthat understanding how the objects should move in the 3D space is a critical\\nclue for guiding actions. This clue is embodiment-agnostic and suitable for\\nboth humans and different robots. Motivated by this, we aim to learn a 3D flow\\nworld model from both human and robot manipulation data. This model predicts\\nthe future movement of the interacting objects in 3D space, guiding action\\nplanning for manipulation. Specifically, we synthesize a large-scale 3D optical\\nflow dataset, named ManiFlow-110k, through a moving object auto-detect\\npipeline. A video diffusion-based world model then learns manipulation physics\\nfrom these data, generating 3D optical flow trajectories conditioned on\\nlanguage instructions. With the generated 3D object optical flow, we propose a\\nflow-guided rendering mechanism, which renders the predicted final state and\\nleverages GPT-4o to assess whether the predicted flow aligns with the task\\ndescription. This equips the robot with a closed-loop planning ability.\\nFinally, we consider the predicted 3D optical flow as constraints for an\\noptimization policy to determine a chunk of robot actions for manipulation.\\nExtensive experiments demonstrate strong generalization across diverse robotic\\nmanipulation tasks and reliable cross-embodiment adaptation without\\nhardware-specific training.', 'upvotes': 3, 'discussionId': '684637733ec10bdd8ab4da6d', 'githubRepo': 'https://github.com/Hoyyyaard/3DFlowAction/', 'ai_summary': 'A 3D flow world model learned from human and robot manipulation data, using video diffusion and GPT-4o, enables robots to perform diverse manipulation tasks with strong generalization and cross-embodiment adaptation.', 'ai_keywords': ['3D flow world model', 'moving object auto-detect pipeline', 'video diffusion-based world model', '3D optical flow dataset', 'ManiFlow-110k', '3D optical flow trajectories', 'flow-guided rendering mechanism', 'GPT-4o', 'closed-loop planning', 'optimization policy', 'cross-embodiment adaptation']}, 'publishedAt': '2025-06-06T12:00:31.000Z', 'title': '3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\\n  Model', 'summary': 'Manipulation has long been a challenging task for robots, while humans can\\neffortlessly perform complex interactions with objects, such as hanging a cup\\non the mug rack. A key reason is the lack of a large and uniform dataset for\\nteaching robots manipulation skills. Current robot datasets often record robot\\naction in different action spaces within a simple scene. This hinders the robot\\nto learn a unified and robust action representation for different robots within\\ndiverse scenes. Observing how humans understand a manipulation task, we find\\nthat understanding how the objects should move in the 3D space is a critical\\nclue for guiding actions. This clue is embodiment-agnostic and suitable for\\nboth humans and different robots. Motivated by this, we aim to learn a 3D flow\\nworld model from both human and robot manipulation data. This model predicts\\nthe future movement of the interacting objects in 3D space, guiding action\\nplanning for manipulation. Specifically, we synthesize a large-scale 3D optical\\nflow dataset, named ManiFlow-110k, through a moving object auto-detect\\npipeline. A video diffusion-based world model then learns manipulation physics\\nfrom these data, generating 3D optical flow trajectories conditioned on\\nlanguage instructions. With the generated 3D object optical flow, we propose a\\nflow-guided rendering mechanism, which renders the predicted final state and\\nleverages GPT-4o to assess whether the predicted flow aligns with the task\\ndescription. This equips the robot with a closed-loop planning ability.\\nFinally, we consider the predicted 3D optical flow as constraints for an\\noptimization policy to determine a chunk of robot actions for manipulation.\\nExtensive experiments demonstrate strong generalization across diverse robotic\\nmanipulation tasks and reliable cross-embodiment adaptation without\\nhardware-specific training.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06199.png', 'numComments': 1, 'submittedBy': {'_id': '674b2406591d7232820252cd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png', 'fullname': 'Hongyan Zhi', 'name': 'Hoyard', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.05817', 'authors': [{'_id': '68467e323ec10bdd8ab4db44', 'user': {'_id': '68345ea8beb0d467e37cd421', 'avatarUrl': '/avatars/70ddbf00db1c517d61af3a3d283edf42.svg', 'isPro': False, 'fullname': 'Zihan Wang', 'user': 'zhwang01', 'type': 'user'}, 'name': 'Zihan Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-09T10:11:13.717Z', 'hidden': False}, {'_id': '68467e323ec10bdd8ab4db45', 'name': 'Siyao Liu', 'hidden': False}, {'_id': '68467e323ec10bdd8ab4db46', 'name': 'Yang Sun', 'hidden': False}, {'_id': '68467e323ec10bdd8ab4db47', 'name': 'Hongyan Li', 'hidden': False}, {'_id': '68467e323ec10bdd8ab4db48', 'name': 'Kai Shen', 'hidden': False}], 'publishedAt': '2025-06-06T07:29:01.000Z', 'submittedOnDailyAt': '2025-06-09T10:26:25.604Z', 'title': 'CodeContests+: High-Quality Test Case Generation for Competitive\\n  Programming', 'submittedOnDailyBy': {'_id': '68345ea8beb0d467e37cd421', 'avatarUrl': '/avatars/70ddbf00db1c517d61af3a3d283edf42.svg', 'isPro': False, 'fullname': 'Zihan Wang', 'user': 'zhwang01', 'type': 'user'}, 'summary': 'Competitive programming, due to its high reasoning difficulty and precise\\ncorrectness feedback, has become a key task for both training and evaluating\\nthe reasoning capabilities of large language models (LLMs). However, while a\\nlarge amount of public problem data, such as problem statements and solutions,\\nis available, the test cases of these problems are often difficult to obtain.\\nTherefore, test case generation is a necessary task for building large-scale\\ndatasets, and the quality of the test cases directly determines the accuracy of\\nthe evaluation. In this paper, we introduce an LLM-based agent system that\\ncreates high-quality test cases for competitive programming problems. We apply\\nthis system to the CodeContests dataset and propose a new version with improved\\ntest cases, named CodeContests+. We evaluated the quality of test cases in\\nCodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels\\nto examine the accuracy of these test cases in evaluation. The results\\nindicated that CodeContests+ achieves significantly higher accuracy than\\nCodeContests, particularly with a notably higher True Positive Rate (TPR).\\nSubsequently, our experiments in LLM Reinforcement Learning (RL) further\\nconfirmed that improvements in test case quality yield considerable advantages\\nfor RL.', 'upvotes': 2, 'discussionId': '68467e323ec10bdd8ab4db49', 'ai_summary': 'An LLM-based system generates high-quality test cases for competitive programming problems, enhancing the accuracy of model evaluation and RL performance.', 'ai_keywords': ['LLM-based agent system', 'test case generation', 'CodeContests', 'CodeContests+', 'True Positive Rate (TPR)', 'LLM Reinforcement Learning (RL)']}, 'publishedAt': '2025-06-06T03:29:01.000Z', 'title': 'CodeContests+: High-Quality Test Case Generation for Competitive\\n  Programming', 'summary': 'Competitive programming, due to its high reasoning difficulty and precise\\ncorrectness feedback, has become a key task for both training and evaluating\\nthe reasoning capabilities of large language models (LLMs). However, while a\\nlarge amount of public problem data, such as problem statements and solutions,\\nis available, the test cases of these problems are often difficult to obtain.\\nTherefore, test case generation is a necessary task for building large-scale\\ndatasets, and the quality of the test cases directly determines the accuracy of\\nthe evaluation. In this paper, we introduce an LLM-based agent system that\\ncreates high-quality test cases for competitive programming problems. We apply\\nthis system to the CodeContests dataset and propose a new version with improved\\ntest cases, named CodeContests+. We evaluated the quality of test cases in\\nCodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels\\nto examine the accuracy of these test cases in evaluation. The results\\nindicated that CodeContests+ achieves significantly higher accuracy than\\nCodeContests, particularly with a notably higher True Positive Rate (TPR).\\nSubsequently, our experiments in LLM Reinforcement Learning (RL) further\\nconfirmed that improvements in test case quality yield considerable advantages\\nfor RL.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05817.png', 'numComments': 1, 'submittedBy': {'_id': '68345ea8beb0d467e37cd421', 'avatarUrl': '/avatars/70ddbf00db1c517d61af3a3d283edf42.svg', 'fullname': 'Zihan Wang', 'name': 'zhwang01', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.04120', 'authors': [{'_id': '6846b4453ec10bdd8ab4dbc9', 'name': 'Ben Moran', 'hidden': False}, {'_id': '6846b4453ec10bdd8ab4dbca', 'name': 'Mauro Comi', 'hidden': False}, {'_id': '6846b4453ec10bdd8ab4dbcb', 'name': 'Steven Bohez', 'hidden': False}, {'_id': '6846b4453ec10bdd8ab4dbcc', 'name': 'Tom Erez', 'hidden': False}, {'_id': '6846b4453ec10bdd8ab4dbcd', 'name': 'Zhibin Li', 'hidden': False}, {'_id': '6846b4453ec10bdd8ab4dbce', 'name': 'Leonard Hasenclever', 'hidden': False}], 'publishedAt': '2025-06-04T16:14:31.000Z', 'submittedOnDailyAt': '2025-06-09T09:17:22.520Z', 'title': 'Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot\\n  Data', 'submittedOnDailyBy': {'_id': '62f6dd9b2e53c2efd33f8207', 'avatarUrl': '/avatars/887d795e0e650a8cc67e66f552187a73.svg', 'isPro': False, 'fullname': 'Mauro Comi', 'user': 'MauroC', 'type': 'user'}, 'summary': 'Creating accurate, physical simulations directly from real-world robot motion\\nholds great value for safe, scalable, and affordable robot learning, yet\\nremains exceptionally challenging. Real robot data suffers from occlusions,\\nnoisy camera poses, dynamic scene elements, which hinder the creation of\\ngeometrically accurate and photorealistic digital twins of unseen objects. We\\nintroduce a novel real-to-sim framework tackling all these challenges at once.\\nOur key insight is a hybrid scene representation merging the photorealistic\\nrendering of 3D Gaussian Splatting with explicit object meshes suitable for\\nphysics simulation within a single representation. We propose an end-to-end\\noptimization pipeline that leverages differentiable rendering and\\ndifferentiable physics within MuJoCo to jointly refine all scene components -\\nfrom object geometry and appearance to robot poses and physical parameters -\\ndirectly from raw and imprecise robot trajectories. This unified optimization\\nallows us to simultaneously achieve high-fidelity object mesh reconstruction,\\ngenerate photorealistic novel views, and perform annotation-free robot pose\\ncalibration. We demonstrate the effectiveness of our approach both in\\nsimulation and on challenging real-world sequences using an ALOHA 2 bi-manual\\nmanipulator, enabling more practical and robust real-to-simulation pipelines.', 'upvotes': 2, 'discussionId': '6846b4453ec10bdd8ab4dbcf', 'ai_summary': 'A novel real-to-sim framework merges 3D Gaussian Splatting and object meshes for accurate physics simulation, refining geometry, appearance, and robot poses from raw trajectories.', 'ai_keywords': ['3D Gaussian Splatting', 'object meshes', 'physics simulation', 'differentiable rendering', 'differentiable physics', 'MuJoCo', 'scene reconstruction', 'photorealistic rendering', 'robot pose calibration', 'ALOHA 2']}, 'publishedAt': '2025-06-04T12:14:31.000Z', 'title': 'Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot\\n  Data', 'summary': 'Creating accurate, physical simulations directly from real-world robot motion\\nholds great value for safe, scalable, and affordable robot learning, yet\\nremains exceptionally challenging. Real robot data suffers from occlusions,\\nnoisy camera poses, dynamic scene elements, which hinder the creation of\\ngeometrically accurate and photorealistic digital twins of unseen objects. We\\nintroduce a novel real-to-sim framework tackling all these challenges at once.\\nOur key insight is a hybrid scene representation merging the photorealistic\\nrendering of 3D Gaussian Splatting with explicit object meshes suitable for\\nphysics simulation within a single representation. We propose an end-to-end\\noptimization pipeline that leverages differentiable rendering and\\ndifferentiable physics within MuJoCo to jointly refine all scene components -\\nfrom object geometry and appearance to robot poses and physical parameters -\\ndirectly from raw and imprecise robot trajectories. This unified optimization\\nallows us to simultaneously achieve high-fidelity object mesh reconstruction,\\ngenerate photorealistic novel views, and perform annotation-free robot pose\\ncalibration. We demonstrate the effectiveness of our approach both in\\nsimulation and on challenging real-world sequences using an ALOHA 2 bi-manual\\nmanipulator, enabling more practical and robust real-to-simulation pipelines.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04120.png', 'numComments': 1, 'submittedBy': {'_id': '62f6dd9b2e53c2efd33f8207', 'avatarUrl': '/avatars/887d795e0e650a8cc67e66f552187a73.svg', 'fullname': 'Mauro Comi', 'name': 'MauroC', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.04255', 'authors': [{'_id': '684312988f9ec8394c514883', 'user': {'_id': '65c43d6d2b723dbc4ddc29d2', 'avatarUrl': '/avatars/ffd685be7f309866c38a164245a917aa.svg', 'isPro': False, 'fullname': 'Kunal Pai', 'user': 'guineapig', 'type': 'user'}, 'name': 'Kunal Pai', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-07T05:45:12.032Z', 'hidden': False}, {'_id': '684312988f9ec8394c514884', 'user': {'_id': '62a0dbe7bff710e3fb05f9ae', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62a0dbe7bff710e3fb05f9ae/uZK0Zkv7YG7jWbweh5tQb.png', 'isPro': False, 'fullname': 'Parth Shah', 'user': 'helloparthshah', 'type': 'user'}, 'name': 'Parth Shah', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-06-06T16:08:57.423Z', 'hidden': False}, {'_id': '684312988f9ec8394c514885', 'name': 'Harshil Patel', 'hidden': False}], 'publishedAt': '2025-06-01T17:33:16.000Z', 'submittedOnDailyAt': '2025-06-09T03:49:09.306Z', 'title': 'HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\\n  Utilization', 'submittedOnDailyBy': {'_id': '65c43d6d2b723dbc4ddc29d2', 'avatarUrl': '/avatars/ffd685be7f309866c38a164245a917aa.svg', 'isPro': False, 'fullname': 'Kunal Pai', 'user': 'guineapig', 'type': 'user'}, 'summary': 'Rapid Large Language Model (LLM) advancements are fueling autonomous\\nMulti-Agent System (MAS) development. However, current frameworks often lack\\nflexibility, resource awareness, model diversity, and autonomous tool creation.\\nThis paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent\\nResource Utilization), a novel MAS framework enhancing flexibility, resource\\nefficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically\\nmanaging specialized \"employee\" agents, instantiated based on task needs and\\nresource constraints (cost, memory). Its hybrid intelligence prioritizes\\nsmaller, local LLMs (via Ollama) while flexibly using external APIs and larger\\nmodels when necessary. An economic model with hiring/firing costs promotes team\\nstability and efficient resource allocation. The system also includes\\nautonomous API tool creation and a memory function. Evaluations on tasks like\\nacademic paper review (58% success), safety assessments (100% on a\\nJailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash\\non GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate\\nHASHIRU\\'s capabilities. Case studies illustrate its self-improvement via\\nautonomous cost model generation, tool integration, and budget management.\\nHASHIRU offers a promising approach for more robust, efficient, and adaptable\\nMAS through dynamic hierarchical control, resource-aware hybrid intelligence,\\nand autonomous functional extension. Source code and benchmarks are available\\nat https://github.com/HASHIRU-AI/HASHIRU and\\nhttps://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is\\navailable at https://hashiruagentx-hashiruai.hf.space upon request.', 'upvotes': 2, 'discussionId': '684312998f9ec8394c514886', 'githubRepo': 'https://github.com/HASHIRU-AI/HASHIRU', 'ai_summary': 'HASHIRU, a novel MAS framework, enhances flexibility, resource efficiency, and adaptability by dynamically managing specialized agents and using a hybrid intelligence approach with smaller, local LLMs and external APIs.', 'ai_keywords': ['Hierarchical Agent System', 'Hybrid Intelligent Resource Utilization', 'HASHIRU', 'CEO agent', 'employee agents', 'Ollama', 'external APIs', 'economic model', 'hiring/firing costs', 'autonomous API tool creation', 'academic paper review', 'safety assessments', 'GSM8K', 'JEEBench', 'SVAMP', 'Gemini 2.0 Flash', 'self-improvement', 'autonomous cost model generation', 'tool integration', 'budget management']}, 'publishedAt': '2025-06-01T13:33:16.000Z', 'title': 'HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\\n  Utilization', 'summary': 'Rapid Large Language Model (LLM) advancements are fueling autonomous\\nMulti-Agent System (MAS) development. However, current frameworks often lack\\nflexibility, resource awareness, model diversity, and autonomous tool creation.\\nThis paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent\\nResource Utilization), a novel MAS framework enhancing flexibility, resource\\nefficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically\\nmanaging specialized \"employee\" agents, instantiated based on task needs and\\nresource constraints (cost, memory). Its hybrid intelligence prioritizes\\nsmaller, local LLMs (via Ollama) while flexibly using external APIs and larger\\nmodels when necessary. An economic model with hiring/firing costs promotes team\\nstability and efficient resource allocation. The system also includes\\nautonomous API tool creation and a memory function. Evaluations on tasks like\\nacademic paper review (58% success), safety assessments (100% on a\\nJailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash\\non GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate\\nHASHIRU\\'s capabilities. Case studies illustrate its self-improvement via\\nautonomous cost model generation, tool integration, and budget management.\\nHASHIRU offers a promising approach for more robust, efficient, and adaptable\\nMAS through dynamic hierarchical control, resource-aware hybrid intelligence,\\nand autonomous functional extension. Source code and benchmarks are available\\nat https://github.com/HASHIRU-AI/HASHIRU and\\nhttps://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is\\navailable at https://hashiruagentx-hashiruai.hf.space upon request.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04255.png', 'numComments': 1, 'submittedBy': {'_id': '65c43d6d2b723dbc4ddc29d2', 'avatarUrl': '/avatars/ffd685be7f309866c38a164245a917aa.svg', 'fullname': 'Kunal Pai', 'name': 'guineapig', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2506.04755', 'authors': [{'_id': '6846ba9d3ec10bdd8ab4dbe3', 'name': 'Shenshen Li', 'hidden': False}, {'_id': '6846ba9d3ec10bdd8ab4dbe4', 'name': 'Kaiyuan Deng', 'hidden': False}, {'_id': '6846ba9d3ec10bdd8ab4dbe5', 'name': 'Lei Wang', 'hidden': False}, {'_id': '6846ba9d3ec10bdd8ab4dbe6', 'name': 'Hao Yang', 'hidden': False}, {'_id': '6846ba9d3ec10bdd8ab4dbe7', 'name': 'Chong Peng', 'hidden': False}, {'_id': '6846ba9d3ec10bdd8ab4dbe8', 'name': 'Peng Yan', 'hidden': False}, {'_id': '6846ba9d3ec10bdd8ab4dbe9', 'name': 'Fumin Shen', 'hidden': False}, {'_id': '6846ba9d3ec10bdd8ab4dbea', 'name': 'Heng Tao Shen', 'hidden': False}, {'_id': '6846ba9d3ec10bdd8ab4dbeb', 'name': 'Xing Xu', 'hidden': False}], 'publishedAt': '2025-06-05T08:40:24.000Z', 'submittedOnDailyAt': '2025-06-09T09:15:34.666Z', 'title': 'Truth in the Few: High-Value Data Selection for Efficient Multi-Modal\\n  Reasoning', 'submittedOnDailyBy': {'_id': '684526dd103a93da4dc7d850', 'avatarUrl': '/avatars/5e756e1f5da6b68afb6c9ed165d3bc28.svg', 'isPro': False, 'fullname': 'Shenshen Li', 'user': 'lss727', 'type': 'user'}, 'summary': \"While multi-modal large language models (MLLMs) have made significant\\nprogress in complex reasoning tasks via reinforcement learning, it is commonly\\nbelieved that extensive training data is necessary for improving multi-modal\\nreasoning ability, inevitably leading to data redundancy and substantial\\ncomputational costs. However, can smaller high-value datasets match or\\noutperform full corpora for multi-modal reasoning in MLLMs? In this work, we\\nchallenge this assumption through a key observation: meaningful multi-modal\\nreasoning is triggered by only a sparse subset of training samples, termed\\ncognitive samples, whereas the majority contribute marginally. Building on this\\ninsight, we propose a novel data selection paradigm termed Reasoning Activation\\nPotential (RAP), which identifies cognitive samples by estimating each sample's\\npotential to stimulate genuine multi-modal reasoning by two complementary\\nestimators: 1) Causal Discrepancy Estimator (CDE) based on the potential\\noutcome model principle, eliminates samples that overly rely on language priors\\nby comparing outputs between multi-modal and text-only inputs; 2) Attention\\nConfidence Estimator (ACE), which exploits token-level self-attention to\\ndiscard samples dominated by irrelevant but over-emphasized tokens in\\nintermediate reasoning stages. Moreover, we introduce a Difficulty-aware\\nReplacement Module (DRM) to substitute trivial instances with cognitively\\nchallenging ones, thereby ensuring complexity for robust multi-modal reasoning.\\nExperiments on six datasets show that our RAP method consistently achieves\\nsuperior performance using only 9.3% of the training data, while reducing\\ncomputational costs by over 43%. Our code is available at\\nhttps://github.com/Leo-ssl/RAP.\", 'upvotes': 1, 'discussionId': '6846ba9d3ec10bdd8ab4dbec', 'ai_summary': 'A new data selection paradigm, Reasoning Activation Potential (RAP), enhances multi-modal reasoning in large language models using minimal high-value datasets, improving performance and reducing computational costs.', 'ai_keywords': ['multi-modal large language models (MLLMs)', 'reinforcement learning', 'cognitive samples', 'Reasoning Activation Potential (RAP)', 'Causal Discrepancy Estimator (CDE)', 'Attention Confidence Estimator (ACE)', 'token-level self-attention', 'Difficulty-aware Replacement Module (DRM)']}, 'publishedAt': '2025-06-05T04:40:24.000Z', 'title': 'Truth in the Few: High-Value Data Selection for Efficient Multi-Modal\\n  Reasoning', 'summary': \"While multi-modal large language models (MLLMs) have made significant\\nprogress in complex reasoning tasks via reinforcement learning, it is commonly\\nbelieved that extensive training data is necessary for improving multi-modal\\nreasoning ability, inevitably leading to data redundancy and substantial\\ncomputational costs. However, can smaller high-value datasets match or\\noutperform full corpora for multi-modal reasoning in MLLMs? In this work, we\\nchallenge this assumption through a key observation: meaningful multi-modal\\nreasoning is triggered by only a sparse subset of training samples, termed\\ncognitive samples, whereas the majority contribute marginally. Building on this\\ninsight, we propose a novel data selection paradigm termed Reasoning Activation\\nPotential (RAP), which identifies cognitive samples by estimating each sample's\\npotential to stimulate genuine multi-modal reasoning by two complementary\\nestimators: 1) Causal Discrepancy Estimator (CDE) based on the potential\\noutcome model principle, eliminates samples that overly rely on language priors\\nby comparing outputs between multi-modal and text-only inputs; 2) Attention\\nConfidence Estimator (ACE), which exploits token-level self-attention to\\ndiscard samples dominated by irrelevant but over-emphasized tokens in\\nintermediate reasoning stages. Moreover, we introduce a Difficulty-aware\\nReplacement Module (DRM) to substitute trivial instances with cognitively\\nchallenging ones, thereby ensuring complexity for robust multi-modal reasoning.\\nExperiments on six datasets show that our RAP method consistently achieves\\nsuperior performance using only 9.3% of the training data, while reducing\\ncomputational costs by over 43%. Our code is available at\\nhttps://github.com/Leo-ssl/RAP.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04755.png', 'numComments': 1, 'submittedBy': {'_id': '684526dd103a93da4dc7d850', 'avatarUrl': '/avatars/5e756e1f5da6b68afb6c9ed165d3bc28.svg', 'fullname': 'Shenshen Li', 'name': 'lss727', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2506.00649', 'authors': [{'_id': '6842edd6a4e3571765bb4916', 'user': {'_id': '64fee0a7a10455384ebba184', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64fee0a7a10455384ebba184/KIzdANoxlbWLLqP2pEaka.jpeg', 'isPro': False, 'fullname': 'Neil de la fuente', 'user': 'neildlf', 'type': 'user'}, 'name': 'Neil De La Fuente', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-07T05:45:23.265Z', 'hidden': False}, {'_id': '6842edd6a4e3571765bb4917', 'user': {'_id': '6253d0cc27a8414d3bbea683', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6253d0cc27a8414d3bbea683/fRck2wXEg-PS11ph9u8ZI.jpeg', 'isPro': False, 'fullname': 'Oscar Sainz', 'user': 'OSainz', 'type': 'user'}, 'name': 'Oscar Sainz', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-06-07T05:45:25.760Z', 'hidden': False}, {'_id': '6842edd6a4e3571765bb4918', 'name': 'Iker Garca-Ferrero', 'hidden': False}, {'_id': '6842edd6a4e3571765bb4919', 'name': 'Eneko Agirre', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/ncvZw46LU97dolkK36sui.jpeg', 'https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/jQdtGSQJ4ixjSVHuzUwh2.jpeg', 'https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/j8nPbiL7B_r33slPVfi9a.png', 'https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/WYVjvaRSoDKI0Oy0wf0De.png', 'https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/zRQJseogEtf-qR6IwWgxz.png', 'https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/D7jgqjQ5dlCSFblJQ04LV.jpeg'], 'publishedAt': '2025-05-31T17:36:18.000Z', 'submittedOnDailyAt': '2025-06-09T09:01:33.414Z', 'title': 'GuideX: Guided Synthetic Data Generation for Zero-Shot Information\\n  Extraction', 'submittedOnDailyBy': {'_id': '64fee0a7a10455384ebba184', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64fee0a7a10455384ebba184/KIzdANoxlbWLLqP2pEaka.jpeg', 'isPro': False, 'fullname': 'Neil de la fuente', 'user': 'neildlf', 'type': 'user'}, 'summary': 'Information Extraction (IE) systems are traditionally domain-specific,\\nrequiring costly adaptation that involves expert schema design, data\\nannotation, and model training. While Large Language Models have shown promise\\nin zero-shot IE, performance degrades significantly in unseen domains where\\nlabel definitions differ. This paper introduces GUIDEX, a novel method that\\nautomatically defines domain-specific schemas, infers guidelines, and generates\\nsynthetically labeled instances, allowing for better out-of-domain\\ngeneralization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art\\nacross seven zeroshot Named Entity Recognition benchmarks. Models trained with\\nGUIDEX gain up to 7 F1 points over previous methods without humanlabeled data,\\nand nearly 2 F1 points higher when combined with it. Models trained on GUIDEX\\ndemonstrate enhanced comprehension of complex, domain-specific annotation\\nschemas. Code, models, and synthetic datasets are available at\\nneilus03.github.io/guidex.com', 'upvotes': 1, 'discussionId': '6842edd7a4e3571765bb491a', 'projectPage': 'https://neilus03.github.io/guidex.com/', 'githubRepo': 'https://github.com/Neilus03/GUIDEX', 'ai_summary': 'GUIDEX enhances zero-shot Named Entity Recognition by automatically defining schemas and inferring guidelines, setting new benchmarks without extensive human-labeled data.', 'ai_keywords': ['Large Language Models', 'zero-shot IE', 'domain-specific schemas', 'Named Entity Recognition', 'fine-tuning', 'Llama 3.1', 'synthetic labeled instances', 'F1 points']}, 'publishedAt': '2025-05-31T13:36:18.000Z', 'title': 'GuideX: Guided Synthetic Data Generation for Zero-Shot Information\\n  Extraction', 'summary': 'Information Extraction (IE) systems are traditionally domain-specific,\\nrequiring costly adaptation that involves expert schema design, data\\nannotation, and model training. While Large Language Models have shown promise\\nin zero-shot IE, performance degrades significantly in unseen domains where\\nlabel definitions differ. This paper introduces GUIDEX, a novel method that\\nautomatically defines domain-specific schemas, infers guidelines, and generates\\nsynthetically labeled instances, allowing for better out-of-domain\\ngeneralization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art\\nacross seven zeroshot Named Entity Recognition benchmarks. Models trained with\\nGUIDEX gain up to 7 F1 points over previous methods without humanlabeled data,\\nand nearly 2 F1 points higher when combined with it. Models trained on GUIDEX\\ndemonstrate enhanced comprehension of complex, domain-specific annotation\\nschemas. Code, models, and synthetic datasets are available at\\nneilus03.github.io/guidex.com', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/ncvZw46LU97dolkK36sui.jpeg', 'https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/jQdtGSQJ4ixjSVHuzUwh2.jpeg', 'https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/j8nPbiL7B_r33slPVfi9a.png', 'https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/WYVjvaRSoDKI0Oy0wf0De.png', 'https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/zRQJseogEtf-qR6IwWgxz.png', 'https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/D7jgqjQ5dlCSFblJQ04LV.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00649.png', 'numComments': 1, 'submittedBy': {'_id': '64fee0a7a10455384ebba184', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64fee0a7a10455384ebba184/KIzdANoxlbWLLqP2pEaka.jpeg', 'fullname': 'Neil de la fuente', 'name': 'neildlf', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}"
]