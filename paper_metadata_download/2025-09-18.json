[
    "{'paper': {'id': '2509.14008', 'authors': [{'_id': '68cbb58d5a7803ff3be42ecc', 'name': 'Hasan Abed Al Kader Hammoud', 'hidden': False}, {'_id': '68cbb58d5a7803ff3be42ecd', 'user': {'_id': '665ee96b9f29909d03abfa37', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/665ee96b9f29909d03abfa37/ipctrsDzM-3_s8GTzWsd2.png', 'isPro': False, 'fullname': 'Mohammad Zbeeb', 'user': 'zbeeb', 'type': 'user'}, 'name': 'Mohammad Zbeeb', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:25:47.410Z', 'hidden': False}, {'_id': '68cbb58d5a7803ff3be42ece', 'name': 'Bernard Ghanem', 'hidden': False}], 'publishedAt': '2025-09-17T14:19:28.000Z', 'submittedOnDailyAt': '2025-09-18T06:07:38.010Z', 'title': 'Hala Technical Report: Building Arabic-Centric Instruction & Translation\\n  Models at Scale', 'submittedOnDailyBy': {'_id': '642b51385bf2355d02a23d15', 'avatarUrl': '/avatars/87985347643b2647555f2453fa4d94fb.svg', 'isPro': True, 'fullname': 'Hasan Abed Al Kader Hammoud', 'user': 'hammh0a', 'type': 'user'}, 'summary': 'We present Hala, a family of Arabic-centric instruction and translation\\nmodels built with our translate-and-tune pipeline. We first compress a strong\\nARleftrightarrowEN teacher to FP8 (yielding sim2times higher\\nthroughput with no quality loss) and use it to create high-fidelity bilingual\\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\\ndata and used to translate high-quality English instruction sets into Arabic,\\nproducing a million-scale corpus tailored to instruction following. We train\\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\\nbalance Arabic specialization with base-model strengths. On Arabic-centric\\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\\n(leq2B) and \"small\" (7-9B) categories, outperforming their bases. We release\\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP.', 'upvotes': 52, 'discussionId': '68cbb58d5a7803ff3be42ecf', 'ai_summary': 'Hala, a family of Arabic-centric instruction and translation models, achieves state-of-the-art results using a translate-and-tune pipeline, slerp merging, and fine-tuning on high-quality bilingual supervision.', 'ai_keywords': ['translate-and-tune pipeline', 'FP8', 'bilingual supervision', 'lightweight language model', 'fine-tuning', 'slerp merging', 'Arabic-centric benchmarks', 'nano category', 'small category']}, 'publishedAt': '2025-09-17T10:19:28.000Z', 'title': 'Hala Technical Report: Building Arabic-Centric Instruction & Translation\\n  Models at Scale', 'summary': 'We present Hala, a family of Arabic-centric instruction and translation\\nmodels built with our translate-and-tune pipeline. We first compress a strong\\nARleftrightarrowEN teacher to FP8 (yielding sim2times higher\\nthroughput with no quality loss) and use it to create high-fidelity bilingual\\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\\ndata and used to translate high-quality English instruction sets into Arabic,\\nproducing a million-scale corpus tailored to instruction following. We train\\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\\nbalance Arabic specialization with base-model strengths. On Arabic-centric\\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\\n(leq2B) and \"small\" (7-9B) categories, outperforming their bases. We release\\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14008.png', 'numComments': 1, 'submittedBy': {'_id': '642b51385bf2355d02a23d15', 'avatarUrl': '/avatars/87985347643b2647555f2453fa4d94fb.svg', 'fullname': 'Hasan Abed Al Kader Hammoud', 'name': 'hammh0a', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.14033', 'authors': [{'_id': '68cb69b55a7803ff3be42dcd', 'name': 'Weijie Yin', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dce', 'name': 'Yongjie Ye', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dcf', 'user': {'_id': '6491b1b2c1741666238f8a0f', 'avatarUrl': '/avatars/9246c9ef06d80bd8628426375c95d4be.svg', 'isPro': False, 'fullname': 'JackShu', 'user': 'Shuhuhuhu', 'type': 'user'}, 'name': 'Fangxun Shu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:26:50.598Z', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dd0', 'name': 'Yue Liao', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dd1', 'name': 'Zijian Kang', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dd2', 'name': 'Hongyuan Dong', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dd3', 'user': {'_id': '64a02fbafab664ac3186f1fd', 'avatarUrl': '/avatars/ccda4d04490d5b456a51d523917e227a.svg', 'isPro': False, 'fullname': 'Haiyang Yu', 'user': 'hyyu20', 'type': 'user'}, 'name': 'Haiyang Yu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:26:48.039Z', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dd4', 'name': 'Dingkang Yang', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dd5', 'user': {'_id': '64d201b1c2bd235422fb1d14', 'avatarUrl': '/avatars/e50581aa66391cedae94e116e759b9ec.svg', 'isPro': False, 'fullname': 'wang', 'user': 'stormthunder', 'type': 'user'}, 'name': 'Jiacong Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:26:53.059Z', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dd6', 'name': 'Han Wang', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dd7', 'name': 'Wenzhuo Liu', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dd8', 'name': 'Xiao Liang', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dd9', 'name': 'Shuicheng Yan', 'hidden': False}, {'_id': '68cb69b55a7803ff3be42dda', 'name': 'Chao Feng', 'hidden': False}], 'publishedAt': '2025-09-17T14:34:02.000Z', 'submittedOnDailyAt': '2025-09-18T00:39:01.459Z', 'title': 'SAIL-VL2 Technical Report', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\\nfor comprehensive multimodal understanding and reasoning. As the successor to\\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\\nparameter scales across diverse image and video benchmarks, demonstrating\\nstrong capabilities from fine-grained perception to complex reasoning. Three\\ncore innovations drive its effectiveness. First, a large-scale data curation\\npipeline with scoring and filtering strategies enhances both quality and\\ndistribution across captioning, OCR, QA, and video data, improving training\\nefficiency. Second, a progressive training framework begins with a powerful\\npre-trained vision encoder (SAIL-ViT), advances through multimodal\\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\\nsystematically strengthens model capabilities. Third, architectural advances\\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\\n106 datasets and achieves state-of-the-art results on challenging reasoning\\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\\nmodels under the 4B parameter scale, while serving as an efficient and\\nextensible foundation for the open-source multimodal community.', 'upvotes': 26, 'discussionId': '68cb69b55a7803ff3be42ddb', 'ai_summary': 'SAIL-VL2, a vision-language foundation model, achieves state-of-the-art performance across diverse benchmarks through data curation, progressive training, and sparse MoE architecture.', 'ai_keywords': ['vision-language foundation model', 'SAIL-VL2', 'SAIL-VL', 'parameter scales', 'image and video benchmarks', 'fine-grained perception', 'complex reasoning', 'large-scale data curation', 'scoring and filtering strategies', 'training efficiency', 'progressive training framework', 'powerful pre-trained vision encoder', 'SAIL-ViT', 'multimodal pre-training', 'thinking-fusion SFT-RL hybrid paradigm', 'dense LLMs', 'efficient sparse Mixture-of-Experts', 'MoE designs', 'challenging reasoning benchmarks', 'MMMU', 'MathVista', 'OpenCompass leaderboard']}, 'publishedAt': '2025-09-17T10:34:02.000Z', 'title': 'SAIL-VL2 Technical Report', 'summary': 'We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\\nfor comprehensive multimodal understanding and reasoning. As the successor to\\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\\nparameter scales across diverse image and video benchmarks, demonstrating\\nstrong capabilities from fine-grained perception to complex reasoning. Three\\ncore innovations drive its effectiveness. First, a large-scale data curation\\npipeline with scoring and filtering strategies enhances both quality and\\ndistribution across captioning, OCR, QA, and video data, improving training\\nefficiency. Second, a progressive training framework begins with a powerful\\npre-trained vision encoder (SAIL-ViT), advances through multimodal\\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\\nsystematically strengthens model capabilities. Third, architectural advances\\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\\n106 datasets and achieves state-of-the-art results on challenging reasoning\\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\\nmodels under the 4B parameter scale, while serving as an efficient and\\nextensible foundation for the open-source multimodal community.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14033.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 106}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.12989', 'authors': [{'_id': '68cbc9354d15e6fae7acb379', 'name': 'Xu Zheng', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb37a', 'name': 'Chenfei Liao', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb37b', 'name': 'Ziqiao Weng', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb37c', 'name': 'Kaiyu Lei', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb37d', 'name': 'Zihao Dongfang', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb37e', 'name': 'Haocong He', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb37f', 'name': 'Yuanhuiyi Lyu', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb380', 'user': {'_id': '65d61d8fd484956b5acc89fe', 'avatarUrl': '/avatars/47954232c90780ffe898a5a445f7fb0a.svg', 'isPro': False, 'fullname': 'Lutao Jiang', 'user': 'LutaoJiang', 'type': 'user'}, 'name': 'Lutao Jiang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:25:42.443Z', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb381', 'name': 'Lu Qi', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb382', 'name': 'Li Chen', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb383', 'name': 'Danda Pani Paudel', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb384', 'name': 'Kailun Yang', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb385', 'name': 'Linfeng Zhang', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb386', 'name': 'Luc Van Gool', 'hidden': False}, {'_id': '68cbc9354d15e6fae7acb387', 'name': 'Xuming Hu', 'hidden': False}], 'publishedAt': '2025-09-16T11:54:37.000Z', 'submittedOnDailyAt': '2025-09-18T07:28:34.074Z', 'title': 'PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era', 'submittedOnDailyBy': {'_id': '6806464ed918f6d2fee2bc8b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg', 'isPro': False, 'fullname': 'Chenfei Liao', 'user': 'Chenfei-Liao', 'type': 'user'}, 'summary': 'Omnidirectional vision, using 360-degree vision to understand the\\nenvironment, has become increasingly critical across domains like robotics,\\nindustrial inspection, and environmental monitoring. Compared to traditional\\npinhole vision, omnidirectional vision provides holistic environmental\\nawareness, significantly enhancing the completeness of scene perception and the\\nreliability of decision-making. However, foundational research in this area has\\nhistorically lagged behind traditional pinhole vision. This talk presents an\\nemerging trend in the embodied AI era: the rapid development of omnidirectional\\nvision, driven by growing industrial demand and academic interest. We highlight\\nrecent breakthroughs in omnidirectional generation, omnidirectional perception,\\nomnidirectional understanding, and related datasets. Drawing on insights from\\nboth academia and industry, we propose an ideal panoramic system architecture\\nin the embodied AI era, PANORAMA, which consists of four key subsystems.\\nMoreover, we offer in-depth opinions related to emerging trends and\\ncross-community impacts at the intersection of panoramic vision and embodied\\nAI, along with the future roadmap and open challenges. This overview\\nsynthesizes state-of-the-art advancements and outlines challenges and\\nopportunities for future research in building robust, general-purpose\\nomnidirectional AI systems in the embodied AI era.', 'upvotes': 20, 'discussionId': '68cbc9364d15e6fae7acb388', 'ai_summary': 'Recent advancements in omnidirectional vision, driven by industrial and academic interest, have led to breakthroughs in generation, perception, and understanding, with the proposal of a new system architecture called PANORAMA.', 'ai_keywords': ['omnidirectional vision', '360-degree vision', 'pinhole vision', 'omnidirectional generation', 'omnidirectional perception', 'omnidirectional understanding', 'panoramic system architecture', 'PANORAMA', 'embodied AI']}, 'publishedAt': '2025-09-16T07:54:37.000Z', 'title': 'PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era', 'summary': 'Omnidirectional vision, using 360-degree vision to understand the\\nenvironment, has become increasingly critical across domains like robotics,\\nindustrial inspection, and environmental monitoring. Compared to traditional\\npinhole vision, omnidirectional vision provides holistic environmental\\nawareness, significantly enhancing the completeness of scene perception and the\\nreliability of decision-making. However, foundational research in this area has\\nhistorically lagged behind traditional pinhole vision. This talk presents an\\nemerging trend in the embodied AI era: the rapid development of omnidirectional\\nvision, driven by growing industrial demand and academic interest. We highlight\\nrecent breakthroughs in omnidirectional generation, omnidirectional perception,\\nomnidirectional understanding, and related datasets. Drawing on insights from\\nboth academia and industry, we propose an ideal panoramic system architecture\\nin the embodied AI era, PANORAMA, which consists of four key subsystems.\\nMoreover, we offer in-depth opinions related to emerging trends and\\ncross-community impacts at the intersection of panoramic vision and embodied\\nAI, along with the future roadmap and open challenges. This overview\\nsynthesizes state-of-the-art advancements and outlines challenges and\\nopportunities for future research in building robust, general-purpose\\nomnidirectional AI systems in the embodied AI era.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.12989.png', 'numComments': 1, 'submittedBy': {'_id': '6806464ed918f6d2fee2bc8b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg', 'fullname': 'Chenfei Liao', 'name': 'Chenfei-Liao', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.14232', 'authors': [{'_id': '68cb67a45a7803ff3be42d1e', 'user': {'_id': '665d4b515fdfe8f923e347a7', 'avatarUrl': '/avatars/d114b24c02dadfca0a8aee104755a8ec.svg', 'isPro': False, 'fullname': 'Zhaokai Wang', 'user': 'wzk1015', 'type': 'user'}, 'name': 'Zhaokai Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:27:10.361Z', 'hidden': False}, {'_id': '68cb67a45a7803ff3be42d1f', 'name': 'Penghao Yin', 'hidden': False}, {'_id': '68cb67a45a7803ff3be42d20', 'name': 'Xiangyu Zhao', 'hidden': False}, {'_id': '68cb67a45a7803ff3be42d21', 'name': 'Changyao Tian', 'hidden': False}, {'_id': '68cb67a45a7803ff3be42d22', 'name': 'Yu Qiao', 'hidden': False}, {'_id': '68cb67a45a7803ff3be42d23', 'name': 'Wenhai Wang', 'hidden': False}, {'_id': '68cb67a45a7803ff3be42d24', 'name': 'Jifeng Dai', 'hidden': False}, {'_id': '68cb67a45a7803ff3be42d25', 'name': 'Gen Luo', 'hidden': False}], 'publishedAt': '2025-09-17T17:59:14.000Z', 'submittedOnDailyAt': '2025-09-18T00:30:14.725Z', 'title': 'GenExam: A Multidisciplinary Text-to-Image Exam', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': \"Exams are a fundamental test of expert-level intelligence and require\\nintegrated understanding, reasoning, and generation. Existing exam-style\\nbenchmarks mainly focus on understanding and reasoning tasks, and current\\ngeneration benchmarks emphasize the illustration of world knowledge and visual\\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\\nand fine-grained scoring points to enable a precise evaluation of semantic\\ncorrectness and visual plausibility. Experiments show that even\\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\\nless than 15% strict scores, and most models yield almost 0%, suggesting the\\ngreat challenge of our benchmark. By framing image generation as an exam,\\nGenExam offers a rigorous assessment of models' ability to integrate knowledge,\\nreasoning, and generation, providing insights on the path to general AGI.\", 'upvotes': 15, 'discussionId': '68cb67a45a7803ff3be42d26', 'githubRepo': 'https://github.com/OpenGVLab/GenExam', 'ai_summary': 'GenExam is a benchmark for evaluating text-to-image generation in exam-style settings across multiple disciplines, highlighting the challenges in integrating knowledge, reasoning, and generation.', 'ai_keywords': ['text-to-image', 'GenExam', 'multidisciplinary', 'exam-style prompts', 'ground-truth images', 'fine-grained scoring', 'semantic correctness', 'visual plausibility', 'GPT-Image-1', 'Gemini-2.5-Flash-Image', 'general AGI'], 'githubStars': 12}, 'publishedAt': '2025-09-17T13:59:14.000Z', 'title': 'GenExam: A Multidisciplinary Text-to-Image Exam', 'summary': \"Exams are a fundamental test of expert-level intelligence and require\\nintegrated understanding, reasoning, and generation. Existing exam-style\\nbenchmarks mainly focus on understanding and reasoning tasks, and current\\ngeneration benchmarks emphasize the illustration of world knowledge and visual\\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\\nand fine-grained scoring points to enable a precise evaluation of semantic\\ncorrectness and visual plausibility. Experiments show that even\\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\\nless than 15% strict scores, and most models yield almost 0%, suggesting the\\ngreat challenge of our benchmark. By framing image generation as an exam,\\nGenExam offers a rigorous assessment of models' ability to integrate knowledge,\\nreasoning, and generation, providing insights on the path to general AGI.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14232.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 106}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.13755', 'authors': [{'_id': '68cb88205a7803ff3be42e43', 'name': 'Zhaoyang Chu', 'hidden': False}, {'_id': '68cb88205a7803ff3be42e44', 'name': 'Yao Wan', 'hidden': False}, {'_id': '68cb88205a7803ff3be42e45', 'name': 'Zhikun Zhang', 'hidden': False}, {'_id': '68cb88205a7803ff3be42e46', 'name': 'Di Wang', 'hidden': False}, {'_id': '68cb88205a7803ff3be42e47', 'name': 'Zhou Yang', 'hidden': False}, {'_id': '68cb88205a7803ff3be42e48', 'name': 'Hongyu Zhang', 'hidden': False}, {'_id': '68cb88205a7803ff3be42e49', 'name': 'Pan Zhou', 'hidden': False}, {'_id': '68cb88205a7803ff3be42e4a', 'name': 'Xuanhua Shi', 'hidden': False}, {'_id': '68cb88205a7803ff3be42e4b', 'name': 'Hai Jin', 'hidden': False}, {'_id': '68cb88205a7803ff3be42e4c', 'name': 'David Lo', 'hidden': False}], 'publishedAt': '2025-09-17T07:12:35.000Z', 'submittedOnDailyAt': '2025-09-18T06:10:10.968Z', 'title': 'Scrub It Out! Erasing Sensitive Memorization in Code Language Models via\\n  Machine Unlearning', 'submittedOnDailyBy': {'_id': '64fb128552e82dd432682b06', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64fb128552e82dd432682b06/GYcOiwa4R3RrgcM2tSuV_.png', 'isPro': False, 'fullname': 'Zhaoyang Chu', 'user': 'chuzy', 'type': 'user'}, 'summary': 'While Code Language Models (CLMs) have demonstrated superior performance in\\nsoftware engineering tasks such as code generation and summarization, recent\\nempirical studies reveal a critical privacy vulnerability: these models exhibit\\nunintended memorization of sensitive training data, enabling verbatim\\nreproduction of confidential information when specifically prompted. To address\\nthis issue, several approaches, including training data de-duplication and\\ndifferential privacy augmentation, have been proposed. However, these methods\\nrequire full-model retraining for deployed CLMs, which incurs substantial\\ncomputational costs. In this paper, we aim to answer the following research\\nquestion: Can sensitive information memorized by CLMs be erased effectively and\\nefficiently?\\n  We conduct a pioneering investigation into erasing sensitive memorization in\\nCLMs through machine unlearning - a post-hoc modification method that removes\\nspecific information from trained models without requiring full retraining.\\nSpecifically, we first quantify the memorization risks of sensitive data within\\nCLM training datasets and curate a high-risk dataset of 50,000 sensitive\\nmemorized samples as unlearning targets. We study two widely used gradient\\nascent-based unlearning approaches: the vanilla and constraint-based methods,\\nand introduce CodeEraser, an advanced variant that selectively unlearns\\nsensitive memorized segments in code while preserving the structural integrity\\nand functional correctness of the surrounding code. Extensive experiments on\\nthree families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,\\nvalidate the effectiveness and efficiency of CodeEraser in erasing targeted\\nsensitive memorization while maintaining model utility.', 'upvotes': 11, 'discussionId': '68cb88205a7803ff3be42e4d', 'githubRepo': 'https://github.com/Zhaoyang-Chu/code-unlearning', 'ai_summary': 'CodeEraser effectively and efficiently removes sensitive memorized information from Code Language Models using machine unlearning techniques without full retraining.', 'ai_keywords': ['Code Language Models', 'CLMs', 'machine unlearning', 'gradient ascent', 'CodeParrot', 'CodeGen-Mono', 'Qwen2.5-Coder', 'CodeEraser'], 'githubStars': 14}, 'publishedAt': '2025-09-17T03:12:35.000Z', 'title': 'Scrub It Out! Erasing Sensitive Memorization in Code Language Models via\\n  Machine Unlearning', 'summary': 'While Code Language Models (CLMs) have demonstrated superior performance in\\nsoftware engineering tasks such as code generation and summarization, recent\\nempirical studies reveal a critical privacy vulnerability: these models exhibit\\nunintended memorization of sensitive training data, enabling verbatim\\nreproduction of confidential information when specifically prompted. To address\\nthis issue, several approaches, including training data de-duplication and\\ndifferential privacy augmentation, have been proposed. However, these methods\\nrequire full-model retraining for deployed CLMs, which incurs substantial\\ncomputational costs. In this paper, we aim to answer the following research\\nquestion: Can sensitive information memorized by CLMs be erased effectively and\\nefficiently?\\n  We conduct a pioneering investigation into erasing sensitive memorization in\\nCLMs through machine unlearning - a post-hoc modification method that removes\\nspecific information from trained models without requiring full retraining.\\nSpecifically, we first quantify the memorization risks of sensitive data within\\nCLM training datasets and curate a high-risk dataset of 50,000 sensitive\\nmemorized samples as unlearning targets. We study two widely used gradient\\nascent-based unlearning approaches: the vanilla and constraint-based methods,\\nand introduce CodeEraser, an advanced variant that selectively unlearns\\nsensitive memorized segments in code while preserving the structural integrity\\nand functional correctness of the surrounding code. Extensive experiments on\\nthree families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,\\nvalidate the effectiveness and efficiency of CodeEraser in erasing targeted\\nsensitive memorization while maintaining model utility.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13755.png', 'numComments': 1, 'submittedBy': {'_id': '64fb128552e82dd432682b06', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64fb128552e82dd432682b06/GYcOiwa4R3RrgcM2tSuV_.png', 'fullname': 'Zhaoyang Chu', 'name': 'chuzy', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2508.14880', 'authors': [{'_id': '68a81aa139413c456c05b22c', 'name': 'Ailing Yu', 'hidden': False}, {'_id': '68a81aa139413c456c05b22d', 'name': 'Lan Yao', 'hidden': False}, {'_id': '68a81aa139413c456c05b22e', 'name': 'Jingnan Liu', 'hidden': False}, {'_id': '68a81aa139413c456c05b22f', 'name': 'Zhe Chen', 'hidden': False}, {'_id': '68a81aa139413c456c05b230', 'name': 'Jiajun Yin', 'hidden': False}, {'_id': '68a81aa139413c456c05b231', 'name': 'Yuan Wang', 'hidden': False}, {'_id': '68a81aa139413c456c05b232', 'name': 'Xinhao Liao', 'hidden': False}, {'_id': '68a81aa139413c456c05b233', 'user': {'_id': '63d7bf58db69ae9b9beaeea3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63d7bf58db69ae9b9beaeea3/xO1nkkmhJGIwRuk6yQDPi.png', 'isPro': False, 'fullname': 'Ye Zhiling', 'user': 'yzlnew', 'type': 'user'}, 'name': 'Zhiling Ye', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-09-18T13:54:35.313Z', 'hidden': False}, {'_id': '68a81aa139413c456c05b234', 'name': 'Ji Li', 'hidden': False}, {'_id': '68a81aa139413c456c05b235', 'name': 'Yun Yue', 'hidden': False}, {'_id': '68a81aa139413c456c05b236', 'name': 'Hansong Xiao', 'hidden': False}, {'_id': '68a81aa139413c456c05b237', 'name': 'Hualei Zhou', 'hidden': False}, {'_id': '68a81aa139413c456c05b238', 'name': 'Chunxiao Guo', 'hidden': False}, {'_id': '68a81aa139413c456c05b239', 'name': 'Peng Wei', 'hidden': False}, {'_id': '68a81aa139413c456c05b23a', 'user': {'_id': '64bbdb049a69e8da48a721b3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/iqrYveaPTvOs_GbLWiTGu.png', 'isPro': False, 'fullname': 'Jinjie Gu', 'user': 'dannygjj', 'type': 'user'}, 'name': 'Jinjie Gu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-09-18T13:51:21.916Z', 'hidden': False}], 'publishedAt': '2025-08-20T17:51:20.000Z', 'submittedOnDailyAt': '2025-09-18T12:18:37.369Z', 'title': 'MedReseacher-R1: Expert-Level Medical Deep Researcher via A\\n  Knowledge-Informed Trajectory Synthesis Framework', 'submittedOnDailyBy': {'_id': '63a369d98c0c89dcae3b8329', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg', 'isPro': True, 'fullname': 'Adina Yakefu', 'user': 'AdinaY', 'type': 'user'}, 'summary': 'Recent developments in Large Language Model (LLM)-based agents have shown\\nimpressive capabilities spanning multiple domains, exemplified by deep research\\nsystems that demonstrate superior performance on complex information-seeking\\nand synthesis tasks. While general-purpose deep research agents have shown\\nimpressive capabilities, they struggle significantly with medical domain\\nchallenges, as evidenced by leading proprietary systems achieving limited\\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\\nframework is constrained by the absence of specialized retrieval tools tailored\\nfor medical contexts.We present a medical deep research agent that addresses\\nthese challenges through two core innovations. First, we develop a novel data\\nsynthesis framework using medical knowledge graphs, extracting the longest\\nchains from subgraphs around rare medical entities to generate complex\\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\\nmedical retrieval engine alongside general-purpose tools, enabling accurate\\nmedical information synthesis. Our approach generates 2100+ diverse\\ntrajectories across 12 medical specialties, each averaging 4.2 tool\\ninteractions.Through a two-stage training paradigm combining supervised\\nfine-tuning and online reinforcement learning with composite rewards, our\\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\\nnew state-of-the-art results on medical benchmarks while maintaining\\ncompetitive performance on general deep research tasks. Our work demonstrates\\nthat strategic domain-specific innovations in architecture, tool design, and\\ntraining data construction can enable smaller open-source models to outperform\\nmuch larger proprietary systems in specialized domains.', 'upvotes': 9, 'discussionId': '68a81aa239413c456c05b23b', 'ai_summary': 'A medical deep research agent using medical knowledge graphs and a custom retrieval engine achieves state-of-the-art performance on medical benchmarks while maintaining competitiveness in general research tasks.', 'ai_keywords': ['Large Language Model (LLM)', 'deep research agents', 'medical domain challenges', 'medical knowledge graphs', 'multi-hop question-answer pairs', 'private medical retrieval engine', 'two-stage training paradigm', 'supervised fine-tuning', 'online reinforcement learning', 'composite rewards', 'MedResearcher-R1-32B']}, 'publishedAt': '2025-08-20T13:51:20.000Z', 'title': 'MedReseacher-R1: Expert-Level Medical Deep Researcher via A\\n  Knowledge-Informed Trajectory Synthesis Framework', 'summary': 'Recent developments in Large Language Model (LLM)-based agents have shown\\nimpressive capabilities spanning multiple domains, exemplified by deep research\\nsystems that demonstrate superior performance on complex information-seeking\\nand synthesis tasks. While general-purpose deep research agents have shown\\nimpressive capabilities, they struggle significantly with medical domain\\nchallenges, as evidenced by leading proprietary systems achieving limited\\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\\nframework is constrained by the absence of specialized retrieval tools tailored\\nfor medical contexts.We present a medical deep research agent that addresses\\nthese challenges through two core innovations. First, we develop a novel data\\nsynthesis framework using medical knowledge graphs, extracting the longest\\nchains from subgraphs around rare medical entities to generate complex\\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\\nmedical retrieval engine alongside general-purpose tools, enabling accurate\\nmedical information synthesis. Our approach generates 2100+ diverse\\ntrajectories across 12 medical specialties, each averaging 4.2 tool\\ninteractions.Through a two-stage training paradigm combining supervised\\nfine-tuning and online reinforcement learning with composite rewards, our\\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\\nnew state-of-the-art results on medical benchmarks while maintaining\\ncompetitive performance on general deep research tasks. Our work demonstrates\\nthat strategic domain-specific innovations in architecture, tool design, and\\ntraining data construction can enable smaller open-source models to outperform\\nmuch larger proprietary systems in specialized domains.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14880.png', 'numComments': 1, 'submittedBy': {'_id': '63a369d98c0c89dcae3b8329', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg', 'fullname': 'Adina Yakefu', 'name': 'AdinaY', 'type': 'user', 'isPro': True, 'isHf': True, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1194}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.14142', 'authors': [{'_id': '68cb67d55a7803ff3be42d28', 'name': 'Peng Xu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d29', 'name': 'Shengwu Xiong', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d2a', 'name': 'Jiajun Zhang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d2b', 'name': 'Yaxiong Chen', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d2c', 'name': 'Bowen Zhou', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d2d', 'name': 'Chen Change Loy', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d2e', 'name': 'David A. Clifton', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d2f', 'name': 'Kyoung Mu Lee', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d30', 'name': 'Luc Van Gool', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d31', 'name': 'Ruiming He', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d32', 'name': 'Ruilin Yao', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d33', 'name': 'Xinwei Long', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d34', 'name': 'Jirui Huang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d35', 'name': 'Kai Tian', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d36', 'name': 'Sa Yang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d37', 'name': 'Yihua Shao', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d38', 'name': 'Jin Feng', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d39', 'name': 'Yue Zhong', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d3a', 'name': 'Jiakai Zhou', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d3b', 'name': 'Cheng Tang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d3c', 'name': 'Tianyu Zou', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d3d', 'name': 'Yifang Zhang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d3e', 'name': 'Junming Liang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d3f', 'name': 'Guoyou Li', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d40', 'name': 'Zhaoxiang Wang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d41', 'name': 'Qiang Zhou', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d42', 'name': 'Yichen Zhao', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d43', 'name': 'Shili Xiong', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d44', 'name': 'Hyeongjin Nam', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d45', 'name': 'Jaerin Lee', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d46', 'name': 'Jaeyoung Chung', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d47', 'name': 'JoonKyu Park', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d48', 'name': 'Junghun Oh', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d49', 'name': 'Kanggeon Lee', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d4a', 'name': 'Wooseok Lee', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d4b', 'name': 'Juneyoung Ro', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d4c', 'name': 'Turghun Osman', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d4d', 'name': 'Can Hu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d4e', 'name': 'Chaoyang Liao', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d4f', 'name': 'Cheng Chen', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d50', 'name': 'Chengcheng Han', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d51', 'name': 'Chenhao Qiu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d52', 'name': 'Chong Peng', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d53', 'name': 'Cong Xu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d54', 'name': 'Dailin Li', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d55', 'name': 'Feiyu Wang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d56', 'name': 'Feng Gao', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d57', 'name': 'Guibo Zhu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d58', 'name': 'Guopeng Tang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d59', 'name': 'Haibo Lu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d5a', 'name': 'Han Fang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d5b', 'name': 'Han Qi', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d5c', 'name': 'Hanxiao Wu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d5d', 'name': 'Haobo Cheng', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d5e', 'name': 'Hongbo Sun', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d5f', 'name': 'Hongyao Chen', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d60', 'name': 'Huayong Hu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d61', 'name': 'Hui Li', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d62', 'name': 'Jiaheng Ma', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d63', 'name': 'Jiang Yu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d64', 'name': 'Jianing Wang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d65', 'name': 'Jie Yang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d66', 'name': 'Jing He', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d67', 'name': 'Jinglin Zhou', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d68', 'name': 'Jingxuan Li', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d69', 'name': 'Josef Kittler', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d6a', 'name': 'Lihao Zheng', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d6b', 'name': 'Linnan Zhao', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d6c', 'name': 'Mengxi Jia', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d6d', 'name': 'Muyang Yan', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d6e', 'name': 'Nguyen Thanh Thien', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d6f', 'name': 'Pu Luo', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d70', 'name': 'Qi Li', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d71', 'name': 'Shien Song', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d72', 'name': 'Shijie Dong', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d73', 'name': 'Shuai Shao', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d74', 'name': 'Shutao Li', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d75', 'name': 'Taofeng Xue', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d76', 'name': 'Tianyang Xu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d77', 'name': 'Tianyi Gao', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d78', 'name': 'Tingting Li', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d79', 'name': 'Wei Zhang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d7a', 'name': 'Weiyang Su', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d7b', 'name': 'Xiaodong Dong', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d7c', 'name': 'Xiao-Jun Wu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d7d', 'name': 'Xiaopeng Zhou', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d7e', 'name': 'Xin Chen', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d7f', 'name': 'Xin Wei', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d80', 'name': 'Xinyi You', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d81', 'name': 'Xudong Kang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d82', 'name': 'Xujie Zhou', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d83', 'name': 'Xusheng Liu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d84', 'name': 'Yanan Wang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d85', 'name': 'Yanbin Huang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d86', 'name': 'Yang Liu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d87', 'name': 'Yang Yang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d88', 'name': 'Yanglin Deng', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d89', 'name': 'Yashu Kang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d8a', 'name': 'Ye Yuan', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d8b', 'name': 'Yi Wen', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d8c', 'name': 'Yicen Tian', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d8d', 'name': 'Yilin Tao', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d8e', 'name': 'Yin Tang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d8f', 'name': 'Yipeng Lin', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d90', 'name': 'Yiqing Wang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d91', 'name': 'Yiting Xi', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d92', 'name': 'Yongkang Yu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d93', 'name': 'Yumei Li', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d94', 'name': 'Yuxin Qin', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d95', 'name': 'Yuying Chen', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d96', 'name': 'Yuzhe Cen', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d97', 'name': 'Zhaofan Zou', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d98', 'name': 'Zhaohong Liu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d99', 'name': 'Zhehao Shen', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d9a', 'name': 'Zhenglin Du', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d9b', 'name': 'Zhengyang Li', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d9c', 'name': 'Zhenni Huang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d9d', 'name': 'Zhenwei Shao', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d9e', 'name': 'Zhilong Song', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42d9f', 'name': 'Zhiyong Feng', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42da0', 'name': 'Zhiyu Wang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42da1', 'name': 'Zhou Yu', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42da2', 'name': 'Ziang Li', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42da3', 'name': 'Zihan Zhai', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42da4', 'name': 'Zijian Zhang', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42da5', 'name': 'Ziyang Peng', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42da6', 'name': 'Ziyun Xiao', 'hidden': False}, {'_id': '68cb67d55a7803ff3be42da7', 'name': 'Zongshu Li', 'hidden': False}], 'publishedAt': '2025-09-17T16:21:34.000Z', 'submittedOnDailyAt': '2025-09-18T00:31:11.292Z', 'title': 'MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\\n  Results, Discussion, and Outlook', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': \"This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\\nto bring together different approaches in multimodal machine learning and LLMs\\nvia a large benchmark. We hope it better allows researchers to follow the\\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\\ntestbeds have boosted the evolution of general-purpose large language models.\\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\\nreleased two tailored datasets Lens and AdsQA as test sets, which support\\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\\nadvertisement videos, respectively. We evaluated 40+ baselines that include\\nboth generalist MLLMs and task-specific models, and opened up three competition\\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\\nindustrial institutions have registered and 40+ valid submissions (out of\\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\\nbaselines and 15+ participants' methods), and rankings are publicly available\\non the MARS2 workshop website and our GitHub organization page\\nhttps://github.com/mars2workshop/, where our updates and announcements of\\nupcoming events will be continuously provided.\", 'upvotes': 6, 'discussionId': '68cb67d65a7803ff3be42da8', 'projectPage': 'https://github.com/mars2workshop/', 'ai_summary': 'The MARS2 2025 Challenge focuses on multimodal reasoning with large language models through real-world and specialized scenarios, evaluating 40+ models across three competition tracks using tailored datasets.', 'ai_keywords': ['multimodal machine learning', 'LLMs', 'MARS2', 'multimodal reasoning', 'large language models', 'testbeds', 'general-purpose large language models', 'real-world scenarios', 'specialized scenarios', 'multimodal reasoning applications', 'MLLMs', 'Visual Grounding in Real-world Scenarios', 'Visual Question Answering with Spatial Awareness', 'Visual Reasoning in Creative Advertisement Videos', 'datasets', 'code sets', 'GitHub organization page']}, 'publishedAt': '2025-09-17T12:21:34.000Z', 'title': 'MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\\n  Results, Discussion, and Outlook', 'summary': \"This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\\nto bring together different approaches in multimodal machine learning and LLMs\\nvia a large benchmark. We hope it better allows researchers to follow the\\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\\ntestbeds have boosted the evolution of general-purpose large language models.\\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\\nreleased two tailored datasets Lens and AdsQA as test sets, which support\\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\\nadvertisement videos, respectively. We evaluated 40+ baselines that include\\nboth generalist MLLMs and task-specific models, and opened up three competition\\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\\nindustrial institutions have registered and 40+ valid submissions (out of\\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\\nbaselines and 15+ participants' methods), and rankings are publicly available\\non the MARS2 workshop website and our GitHub organization page\\nhttps://github.com/mars2workshop/, where our updates and announcements of\\nupcoming events will be continuously provided.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14142.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 106}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.14055', 'authors': [{'_id': '68cb68635a7803ff3be42daa', 'name': 'Gang Cheng', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dab', 'name': 'Xin Gao', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dac', 'name': 'Li Hu', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dad', 'name': 'Siqi Hu', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dae', 'name': 'Mingyang Huang', 'hidden': False}, {'_id': '68cb68635a7803ff3be42daf', 'name': 'Chaonan Ji', 'hidden': False}, {'_id': '68cb68635a7803ff3be42db0', 'name': 'Ju Li', 'hidden': False}, {'_id': '68cb68635a7803ff3be42db1', 'name': 'Dechao Meng', 'hidden': False}, {'_id': '68cb68635a7803ff3be42db2', 'name': 'Jinwei Qi', 'hidden': False}, {'_id': '68cb68635a7803ff3be42db3', 'name': 'Penchong Qiao', 'hidden': False}, {'_id': '68cb68635a7803ff3be42db4', 'name': 'Zhen Shen', 'hidden': False}, {'_id': '68cb68635a7803ff3be42db5', 'name': 'Yafei Song', 'hidden': False}, {'_id': '68cb68635a7803ff3be42db6', 'name': 'Ke Sun', 'hidden': False}, {'_id': '68cb68635a7803ff3be42db7', 'name': 'Linrui Tian', 'hidden': False}, {'_id': '68cb68635a7803ff3be42db8', 'name': 'Feng Wang', 'hidden': False}, {'_id': '68cb68635a7803ff3be42db9', 'name': 'Guangyuan Wang', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dba', 'name': 'Qi Wang', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dbb', 'name': 'Zhongjian Wang', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dbc', 'name': 'Jiayu Xiao', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dbd', 'name': 'Sheng Xu', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dbe', 'name': 'Bang Zhang', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dbf', 'name': 'Peng Zhang', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dc0', 'name': 'Xindi Zhang', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dc1', 'name': 'Zhe Zhang', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dc2', 'name': 'Jingren Zhou', 'hidden': False}, {'_id': '68cb68635a7803ff3be42dc3', 'name': 'Lian Zhuo', 'hidden': False}], 'publishedAt': '2025-09-17T15:00:57.000Z', 'submittedOnDailyAt': '2025-09-18T00:33:25.099Z', 'title': 'Wan-Animate: Unified Character Animation and Replacement with Holistic\\n  Replication', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': \"We introduce Wan-Animate, a unified framework for character animation and\\nreplacement. Given a character image and a reference video, Wan-Animate can\\nanimate the character by precisely replicating the expressions and movements of\\nthe character in the video to generate high-fidelity character videos.\\nAlternatively, it can integrate the animated character into the reference video\\nto replace the original character, replicating the scene's lighting and color\\ntone to achieve seamless environmental integration. Wan-Animate is built upon\\nthe Wan model. To adapt it for character animation tasks, we employ a modified\\ninput paradigm to differentiate between reference conditions and regions for\\ngeneration. This design unifies multiple tasks into a common symbolic\\nrepresentation. We use spatially-aligned skeleton signals to replicate body\\nmotion and implicit facial features extracted from source images to reenact\\nexpressions, enabling the generation of character videos with high\\ncontrollability and expressiveness. Furthermore, to enhance environmental\\nintegration during character replacement, we develop an auxiliary Relighting\\nLoRA. This module preserves the character's appearance consistency while\\napplying the appropriate environmental lighting and color tone. Experimental\\nresults demonstrate that Wan-Animate achieves state-of-the-art performance. We\\nare committed to open-sourcing the model weights and its source code.\", 'upvotes': 6, 'discussionId': '68cb68635a7803ff3be42dc4', 'projectPage': 'https://humanaigc.github.io/wan-animate/', 'ai_summary': 'Wan-Animate is a unified framework for character animation and replacement, using spatially-aligned skeleton signals and implicit facial features to generate high-fidelity character videos with seamless environmental integration.', 'ai_keywords': ['character animation', 'reference video', 'high-fidelity character videos', 'Wan model', 'input paradigm', 'spatially-aligned skeleton signals', 'implicit facial features', 'character replacement', 'environmental integration', 'Relighting LoRA', 'appearance consistency']}, 'publishedAt': '2025-09-17T11:00:57.000Z', 'title': 'Wan-Animate: Unified Character Animation and Replacement with Holistic\\n  Replication', 'summary': \"We introduce Wan-Animate, a unified framework for character animation and\\nreplacement. Given a character image and a reference video, Wan-Animate can\\nanimate the character by precisely replicating the expressions and movements of\\nthe character in the video to generate high-fidelity character videos.\\nAlternatively, it can integrate the animated character into the reference video\\nto replace the original character, replicating the scene's lighting and color\\ntone to achieve seamless environmental integration. Wan-Animate is built upon\\nthe Wan model. To adapt it for character animation tasks, we employ a modified\\ninput paradigm to differentiate between reference conditions and regions for\\ngeneration. This design unifies multiple tasks into a common symbolic\\nrepresentation. We use spatially-aligned skeleton signals to replicate body\\nmotion and implicit facial features extracted from source images to reenact\\nexpressions, enabling the generation of character videos with high\\ncontrollability and expressiveness. Furthermore, to enhance environmental\\nintegration during character replacement, we develop an auxiliary Relighting\\nLoRA. This module preserves the character's appearance consistency while\\napplying the appropriate environmental lighting and color tone. Experimental\\nresults demonstrate that Wan-Animate achieves state-of-the-art performance. We\\nare committed to open-sourcing the model weights and its source code.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14055.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 106}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.13761', 'authors': [{'_id': '68cb674b5a7803ff3be42d0e', 'user': {'_id': '6310ad26f5acafacccf91ac6', 'avatarUrl': '/avatars/b9de109e013f8f181e61c03b6fce47a3.svg', 'isPro': False, 'fullname': 'Chang', 'user': 'JsingMog', 'type': 'user'}, 'name': 'Qikai Chang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:27:12.793Z', 'hidden': False}, {'_id': '68cb674b5a7803ff3be42d0f', 'name': 'Zhenrong Zhang', 'hidden': False}, {'_id': '68cb674b5a7803ff3be42d10', 'name': 'Pengfei Hu', 'hidden': False}, {'_id': '68cb674b5a7803ff3be42d11', 'name': 'Jiefeng Ma', 'hidden': False}, {'_id': '68cb674b5a7803ff3be42d12', 'name': 'Yicheng Pan', 'hidden': False}, {'_id': '68cb674b5a7803ff3be42d13', 'name': 'Jianshu Zhang', 'hidden': False}, {'_id': '68cb674b5a7803ff3be42d14', 'name': 'Jun Du', 'hidden': False}, {'_id': '68cb674b5a7803ff3be42d15', 'name': 'Quan Liu', 'hidden': False}, {'_id': '68cb674b5a7803ff3be42d16', 'name': 'Jianqing Gao', 'hidden': False}], 'publishedAt': '2025-09-17T07:16:12.000Z', 'submittedOnDailyAt': '2025-09-18T00:28:48.602Z', 'title': 'THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\\n  Reasoning', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': \"Large Language Models (LLMs) have made remarkable progress in mathematical\\nreasoning, but still continue to struggle with high-precision tasks like\\nnumerical computation and formal symbolic manipulation. Integrating external\\ntools has emerged as a promising approach to bridge this gap. Despite recent\\nadvances, existing methods struggle with three key challenges: constructing\\ntool-integrated reasoning data, performing fine-grained optimization, and\\nenhancing inference. To overcome these limitations, we propose THOR\\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\\na multi-agent actor-critic-based pipeline for constructing high-quality\\ndatasets of tool-integrated reasoning paths, aligning with the policy and\\ngeneralizing well across diverse models. Second, to perform fine-grained\\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\\nfor both trajectory-level problem solving and step-level code generation. This\\nis motivated by our key insight that the success of an intermediate tool call\\nis a strong predictor of the final answer's correctness. Finally, THOR\\nincorporates a self-correction mechanism that leverages immediate tool feedback\\nto dynamically revise erroneous reasoning paths during inference. Our approach\\ndemonstrates strong generalization across diverse models, performing\\neffectively in both reasoning and non-reasoning models. It further achieves\\nstate-of-the-art performance for models of a similar scale on multiple\\nmathematical benchmarks, while also delivering consistent improvements on code\\nbenchmarks. Our code will be publicly available at\\nhttps://github.com/JingMog/THOR.\", 'upvotes': 6, 'discussionId': '68cb674b5a7803ff3be42d17', 'githubRepo': 'https://github.com/JingMog/THOR', 'ai_summary': 'THOR, a tool-integrated hierarchical optimization framework using RL, enhances mathematical reasoning and code generation by constructing high-quality datasets, optimizing reasoning paths, and correcting errors during inference.', 'ai_keywords': ['Large Language Models', 'mathematical reasoning', 'numerical computation', 'formal symbolic manipulation', 'tool-integrated reasoning', 'multi-agent actor-critic', 'TIRGen', 'hierarchical optimization', 'RL strategy', 'trajectory-level problem solving', 'step-level code generation', 'self-correction mechanism', 'immediate tool feedback', 'reasoning paths', 'generalization', 'mathematical benchmarks', 'code benchmarks'], 'githubStars': 13}, 'publishedAt': '2025-09-17T03:16:12.000Z', 'title': 'THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\\n  Reasoning', 'summary': \"Large Language Models (LLMs) have made remarkable progress in mathematical\\nreasoning, but still continue to struggle with high-precision tasks like\\nnumerical computation and formal symbolic manipulation. Integrating external\\ntools has emerged as a promising approach to bridge this gap. Despite recent\\nadvances, existing methods struggle with three key challenges: constructing\\ntool-integrated reasoning data, performing fine-grained optimization, and\\nenhancing inference. To overcome these limitations, we propose THOR\\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\\na multi-agent actor-critic-based pipeline for constructing high-quality\\ndatasets of tool-integrated reasoning paths, aligning with the policy and\\ngeneralizing well across diverse models. Second, to perform fine-grained\\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\\nfor both trajectory-level problem solving and step-level code generation. This\\nis motivated by our key insight that the success of an intermediate tool call\\nis a strong predictor of the final answer's correctness. Finally, THOR\\nincorporates a self-correction mechanism that leverages immediate tool feedback\\nto dynamically revise erroneous reasoning paths during inference. Our approach\\ndemonstrates strong generalization across diverse models, performing\\neffectively in both reasoning and non-reasoning models. It further achieves\\nstate-of-the-art performance for models of a similar scale on multiple\\nmathematical benchmarks, while also delivering consistent improvements on code\\nbenchmarks. Our code will be publicly available at\\nhttps://github.com/JingMog/THOR.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13761.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 106}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.13523', 'authors': [{'_id': '68cc03433df9ac65e93dc4df', 'name': 'Väinö Hatanpää', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4e0', 'name': 'Eugene Ku', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4e1', 'user': {'_id': '6309242c1a8d77fa3190be5e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6309242c1a8d77fa3190be5e/8lIcbT6UACrP0_WpCuV5p.jpeg', 'isPro': False, 'fullname': 'Jason Stock', 'user': 'stockeh', 'type': 'user'}, 'name': 'Jason Stock', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:25:05.972Z', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4e2', 'name': 'Murali Emani', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4e3', 'user': {'_id': '623da717c29adf5ef618a217', 'avatarUrl': '/avatars/460be23a3e2eb8d344633c3dfdab8a18.svg', 'isPro': False, 'fullname': 'Sam Foreman', 'user': 'samforeman', 'type': 'user'}, 'name': 'Sam Foreman', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:25:03.448Z', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4e4', 'name': 'Chunyong Jung', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4e5', 'user': {'_id': '62ed9b190effdbb91380c8fa', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62ed9b190effdbb91380c8fa/23k3CDaKnWy0oGVwtDRj-.jpeg', 'isPro': False, 'fullname': 'Sandeep Madireddy', 'user': 'Sand33p', 'type': 'user'}, 'name': 'Sandeep Madireddy', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:25:00.870Z', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4e6', 'name': 'Tung Nguyen', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4e7', 'name': 'Varuni Sastry', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4e8', 'name': 'Ray A. O. Sinurat', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4e9', 'name': 'Sam Wheeler', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4ea', 'name': 'Huihuo Zheng', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4eb', 'name': 'Troy Arcomano', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4ec', 'name': 'Venkatram Vishwanath', 'hidden': False}, {'_id': '68cc03433df9ac65e93dc4ed', 'name': 'Rao Kotamarthi', 'hidden': False}], 'publishedAt': '2025-09-16T20:38:29.000Z', 'submittedOnDailyAt': '2025-09-18T11:37:41.179Z', 'title': 'AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions', 'submittedOnDailyBy': {'_id': '623da717c29adf5ef618a217', 'avatarUrl': '/avatars/460be23a3e2eb8d344633c3dfdab8a18.svg', 'isPro': False, 'fullname': 'Sam Foreman', 'user': 'samforeman', 'type': 'user'}, 'summary': 'Generative machine learning offers new opportunities to better understand\\ncomplex Earth system dynamics. Recent diffusion-based methods address spectral\\nbiases and improve ensemble calibration in weather forecasting compared to\\ndeterministic methods, yet have so far proven difficult to scale stably at high\\nresolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin\\ndiffusion transformer to address this gap, and SWiPe, a generalizable technique\\nthat composes window parallelism with sequence and pipeline parallelism to\\nshard window-based transformers without added communication cost or increased\\nglobal batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS\\n(mixed precision) and a peak performance of 11.21 ExaFLOPS with 1 times 1\\npatch size on the 0.25{\\\\deg} ERA5 dataset, achieving 95.5% weak scaling\\nefficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS\\nand remains stable on seasonal scales to 90 days, highlighting the potential of\\nbillion-parameter diffusion models for weather and climate prediction.', 'upvotes': 5, 'discussionId': '68cc03433df9ac65e93dc4ee', 'ai_summary': 'AERIS, a large-scale pixel-level Swin diffusion transformer, addresses scaling issues in high-resolution weather forecasting using SWiPe parallelism, achieving high performance and stability.', 'ai_keywords': ['diffusion-based methods', 'ensemble calibration', 'weather forecasting', 'AERIS', 'Swin diffusion transformer', 'window parallelism', 'sequence parallelism', 'pipeline parallelism', 'SWiPe', 'ExaFLOPS', 'ERA5 dataset', 'weak scaling efficiency', 'strong scaling efficiency', 'IFS ENS', 'seasonal scales']}, 'publishedAt': '2025-09-16T16:38:29.000Z', 'title': 'AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions', 'summary': 'Generative machine learning offers new opportunities to better understand\\ncomplex Earth system dynamics. Recent diffusion-based methods address spectral\\nbiases and improve ensemble calibration in weather forecasting compared to\\ndeterministic methods, yet have so far proven difficult to scale stably at high\\nresolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin\\ndiffusion transformer to address this gap, and SWiPe, a generalizable technique\\nthat composes window parallelism with sequence and pipeline parallelism to\\nshard window-based transformers without added communication cost or increased\\nglobal batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS\\n(mixed precision) and a peak performance of 11.21 ExaFLOPS with 1 times 1\\npatch size on the 0.25{\\\\deg} ERA5 dataset, achieving 95.5% weak scaling\\nefficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS\\nand remains stable on seasonal scales to 90 days, highlighting the potential of\\nbillion-parameter diffusion models for weather and climate prediction.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13523.png', 'numComments': 1, 'submittedBy': {'_id': '623da717c29adf5ef618a217', 'avatarUrl': '/avatars/460be23a3e2eb8d344633c3dfdab8a18.svg', 'fullname': 'Sam Foreman', 'name': 'samforeman', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 12}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.13683', 'authors': [{'_id': '68cb78015a7803ff3be42de0', 'user': {'_id': '62bb1e0f3ff437e49a3088e5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62bb1e0f3ff437e49a3088e5/MWNanci3x5g780xh-704U.png', 'isPro': True, 'fullname': 'Suyuchen Wang', 'user': 'sheryc', 'type': 'user'}, 'name': 'Suyuchen Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:26:42.099Z', 'hidden': False}, {'_id': '68cb78015a7803ff3be42de1', 'name': 'Jinlin Wang', 'hidden': False}, {'_id': '68cb78015a7803ff3be42de2', 'name': 'Xinyu Wang', 'hidden': False}, {'_id': '68cb78015a7803ff3be42de3', 'user': {'_id': '641b26e61911d3be6743e8a4', 'avatarUrl': '/avatars/b56668801ebf3e02274ac61bdb83c78a.svg', 'isPro': False, 'fullname': 'siky L', 'user': 'siky', 'type': 'user'}, 'name': 'Shiqi Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:26:39.471Z', 'hidden': False}, {'_id': '68cb78015a7803ff3be42de4', 'name': 'Xiangru Tang', 'hidden': False}, {'_id': '68cb78015a7803ff3be42de5', 'name': 'Sirui Hong', 'hidden': False}, {'_id': '68cb78015a7803ff3be42de6', 'name': 'Xiao-Wen Chang', 'hidden': False}, {'_id': '68cb78015a7803ff3be42de7', 'name': 'Chenglin Wu', 'hidden': False}, {'_id': '68cb78015a7803ff3be42de8', 'name': 'Bang Liu', 'hidden': False}], 'publishedAt': '2025-09-17T04:28:07.000Z', 'submittedOnDailyAt': '2025-09-18T10:07:57.088Z', 'title': 'Improving Context Fidelity via Native Retrieval-Augmented Reasoning', 'submittedOnDailyBy': {'_id': '62bb1e0f3ff437e49a3088e5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62bb1e0f3ff437e49a3088e5/MWNanci3x5g780xh-704U.png', 'isPro': True, 'fullname': 'Suyuchen Wang', 'user': 'sheryc', 'type': 'user'}, 'summary': \"Large language models (LLMs) often struggle with context fidelity, producing\\ninconsistent answers when responding to questions based on provided\\ninformation. Existing approaches either rely on expensive supervised\\nfine-tuning to generate evidence post-answer or train models to perform web\\nsearches without necessarily improving utilization of the given context. We\\npropose CARE, a novel native retrieval-augmented reasoning framework that\\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\\nprocess with the model's own retrieval capabilities. Our method requires\\nlimited labeled evidence data while significantly enhancing both retrieval\\naccuracy and answer generation performance through strategically retrieved\\nin-context tokens in the reasoning chain. Extensive experiments on multiple\\nreal-world and counterfactual QA benchmarks demonstrate that our approach\\nsubstantially outperforms supervised fine-tuning, traditional\\nretrieval-augmented generation methods, and external retrieval solutions. This\\nwork represents a fundamental advancement in making LLMs more accurate,\\nreliable, and efficient for knowledge-intensive tasks.\", 'upvotes': 4, 'discussionId': '68cb78015a7803ff3be42de9', 'projectPage': 'https://foundationagents.github.io/CARE', 'githubRepo': 'https://github.com/FoundationAgents/CARE', 'ai_summary': 'CARE, a retrieval-augmented reasoning framework, enhances LLMs by integrating in-context evidence, improving retrieval accuracy and answer generation performance.', 'ai_keywords': ['LLMs', 'context fidelity', 'supervised fine-tuning', 'evidence post-answer', 'web searches', 'native retrieval-augmented reasoning framework', 'in-context evidence', 'retrieval capabilities', 'labeled evidence data', 'retrieval accuracy', 'answer generation performance', 'reasoning chain', 'QA benchmarks', 'retrieval-augmented generation methods', 'external retrieval solutions'], 'githubStars': 2}, 'publishedAt': '2025-09-17T00:28:07.000Z', 'title': 'Improving Context Fidelity via Native Retrieval-Augmented Reasoning', 'summary': \"Large language models (LLMs) often struggle with context fidelity, producing\\ninconsistent answers when responding to questions based on provided\\ninformation. Existing approaches either rely on expensive supervised\\nfine-tuning to generate evidence post-answer or train models to perform web\\nsearches without necessarily improving utilization of the given context. We\\npropose CARE, a novel native retrieval-augmented reasoning framework that\\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\\nprocess with the model's own retrieval capabilities. Our method requires\\nlimited labeled evidence data while significantly enhancing both retrieval\\naccuracy and answer generation performance through strategically retrieved\\nin-context tokens in the reasoning chain. Extensive experiments on multiple\\nreal-world and counterfactual QA benchmarks demonstrate that our approach\\nsubstantially outperforms supervised fine-tuning, traditional\\nretrieval-augmented generation methods, and external retrieval solutions. This\\nwork represents a fundamental advancement in making LLMs more accurate,\\nreliable, and efficient for knowledge-intensive tasks.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13683.png', 'numComments': 1, 'submittedBy': {'_id': '62bb1e0f3ff437e49a3088e5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62bb1e0f3ff437e49a3088e5/MWNanci3x5g780xh-704U.png', 'fullname': 'Suyuchen Wang', 'name': 'sheryc', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.13450', 'authors': [{'_id': '68cb65745a7803ff3be42d04', 'name': 'Vincent Siu', 'hidden': False}, {'_id': '68cb65745a7803ff3be42d05', 'user': {'_id': '63c73584f3f2499604a418c3', 'avatarUrl': '/avatars/3b48a2d41631ae853bb0e3c5b92af773.svg', 'isPro': False, 'fullname': 'Nicholas Crispino', 'user': 'ncrispino', 'type': 'user'}, 'name': 'Nicholas Crispino', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:27:17.164Z', 'hidden': False}, {'_id': '68cb65745a7803ff3be42d06', 'name': 'David Park', 'hidden': False}, {'_id': '68cb65745a7803ff3be42d07', 'name': 'Nathan W. Henry', 'hidden': False}, {'_id': '68cb65745a7803ff3be42d08', 'name': 'Zhun Wang', 'hidden': False}, {'_id': '68cb65745a7803ff3be42d09', 'name': 'Yang Liu', 'hidden': False}, {'_id': '68cb65745a7803ff3be42d0a', 'name': 'Dawn Song', 'hidden': False}, {'_id': '68cb65745a7803ff3be42d0b', 'name': 'Chenguang Wang', 'hidden': False}], 'publishedAt': '2025-09-16T18:36:22.000Z', 'submittedOnDailyAt': '2025-09-18T04:03:36.119Z', 'title': 'SteeringControl: Holistic Evaluation of Alignment Steering in LLMs', 'submittedOnDailyBy': {'_id': '63c73584f3f2499604a418c3', 'avatarUrl': '/avatars/3b48a2d41631ae853bb0e3c5b92af773.svg', 'isPro': False, 'fullname': 'Nicholas Crispino', 'user': 'ncrispino', 'type': 'user'}, 'summary': 'We introduce SteeringControl, a benchmark for evaluating representation\\nsteering methods across core alignment objectives--bias, harmful generation,\\nand hallucination--and their effects on secondary behaviors such as sycophancy\\nand commonsense morality. While prior alignment work often highlights\\ntruthfulness or reasoning ability to demonstrate the side effects of\\nrepresentation steering, we find there are many unexplored tradeoffs not yet\\nunderstood in a systematic way. We collect a dataset of safety-relevant primary\\nand secondary behaviors to evaluate steering effectiveness and behavioral\\nentanglement centered around five popular steering methods. To enable this, we\\ncraft a modular steering framework based on unique components that serve as the\\nbuilding blocks of many existing methods. Our results on Qwen-2.5-7B and\\nLlama-3.1-8B find that strong steering performance is dependent on the specific\\ncombination of steering method, model, and targeted behavior, and that severe\\nconcept entanglement can result from poor combinations of these three as well.\\nWe release our code here:\\nhttps://github.com/wang-research-lab/SteeringControl.git.', 'upvotes': 3, 'discussionId': '68cb65755a7803ff3be42d0c', 'ai_summary': 'SteeringControl evaluates representation steering methods across bias, harmful generation, and hallucination, revealing tradeoffs and entanglement effects on secondary behaviors like sycophancy and commonsense morality.', 'ai_keywords': ['representation steering', 'bias', 'harmful generation', 'hallucination', 'sycophancy', 'commonsense morality', 'steering effectiveness', 'behavioral entanglement', 'modular steering framework', 'concept entanglement', 'Qwen-2.5-7B', 'Llama-3.1-8B']}, 'publishedAt': '2025-09-16T14:36:22.000Z', 'title': 'SteeringControl: Holistic Evaluation of Alignment Steering in LLMs', 'summary': 'We introduce SteeringControl, a benchmark for evaluating representation\\nsteering methods across core alignment objectives--bias, harmful generation,\\nand hallucination--and their effects on secondary behaviors such as sycophancy\\nand commonsense morality. While prior alignment work often highlights\\ntruthfulness or reasoning ability to demonstrate the side effects of\\nrepresentation steering, we find there are many unexplored tradeoffs not yet\\nunderstood in a systematic way. We collect a dataset of safety-relevant primary\\nand secondary behaviors to evaluate steering effectiveness and behavioral\\nentanglement centered around five popular steering methods. To enable this, we\\ncraft a modular steering framework based on unique components that serve as the\\nbuilding blocks of many existing methods. Our results on Qwen-2.5-7B and\\nLlama-3.1-8B find that strong steering performance is dependent on the specific\\ncombination of steering method, model, and targeted behavior, and that severe\\nconcept entanglement can result from poor combinations of these three as well.\\nWe release our code here:\\nhttps://github.com/wang-research-lab/SteeringControl.git.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13450.png', 'numComments': 1, 'submittedBy': {'_id': '63c73584f3f2499604a418c3', 'avatarUrl': '/avatars/3b48a2d41631ae853bb0e3c5b92af773.svg', 'fullname': 'Nicholas Crispino', 'name': 'ncrispino', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.14026', 'authors': [{'_id': '68cbd9024d15e6fae7acb3b2', 'user': {'_id': '68b93b3a6c86c127a199ad90', 'avatarUrl': '/avatars/f370d99b240ce5b9e1bfdbd3130d9ee4.svg', 'isPro': False, 'fullname': 'Jiun-Cheng Jiang', 'user': 'Jim137', 'type': 'user'}, 'name': 'Jiun-Cheng Jiang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:25:25.267Z', 'hidden': False}, {'_id': '68cbd9024d15e6fae7acb3b3', 'name': 'Morris Yu-Chao Huang', 'hidden': False}, {'_id': '68cbd9024d15e6fae7acb3b4', 'name': 'Tianlong Chen', 'hidden': False}, {'_id': '68cbd9024d15e6fae7acb3b5', 'name': 'Hsi-Sheng Goan', 'hidden': False}], 'publishedAt': '2025-09-17T14:28:42.000Z', 'submittedOnDailyAt': '2025-09-18T12:24:21.651Z', 'title': 'Quantum Variational Activation Functions Empower Kolmogorov-Arnold\\n  Networks', 'submittedOnDailyBy': {'_id': '68b93b3a6c86c127a199ad90', 'avatarUrl': '/avatars/f370d99b240ce5b9e1bfdbd3130d9ee4.svg', 'isPro': False, 'fullname': 'Jiun-Cheng Jiang', 'user': 'Jim137', 'type': 'user'}, 'summary': 'Variational quantum circuits (VQCs) are central to quantum machine learning,\\nwhile recent progress in Kolmogorov-Arnold networks (KANs) highlights the power\\nof learnable activation functions. We unify these directions by introducing\\nquantum variational activation functions (QVAFs), realized through single-qubit\\ndata re-uploading circuits called DatA Re-Uploading ActivatioNs (DARUANs). We\\nshow that DARUAN with trainable weights in data pre-processing possesses an\\nexponentially growing frequency spectrum with data repetitions, enabling an\\nexponential reduction in parameter size compared with Fourier-based activations\\nwithout loss of expressivity. Embedding DARUAN into KANs yields\\nquantum-inspired KANs (QKANs), which retain the interpretability of KANs while\\nimproving their parameter efficiency, expressivity, and generalization. We\\nfurther introduce two novel techniques to enhance scalability, feasibility and\\ncomputational efficiency, such as layer extension and hybrid QKANs (HQKANs) as\\ndrop-in replacements of multi-layer perceptrons (MLPs) for feed-forward\\nnetworks in large-scale models. We provide theoretical analysis and extensive\\nexperiments on function regression, image classification, and autoregressive\\ngenerative language modeling, demonstrating the efficiency and scalability of\\nQKANs. DARUANs and QKANs offer a promising direction for advancing quantum\\nmachine learning on both noisy intermediate-scale quantum (NISQ) hardware and\\nclassical quantum simulators.', 'upvotes': 1, 'discussionId': '68cbd9024d15e6fae7acb3b6', 'projectPage': 'https://qkan.jimq.cc/', 'githubRepo': 'https://github.com/Jim137/qkan', 'ai_summary': 'Quantum variational activation functions (QVAFs) and quantum-inspired Kolmogorov-Arnold networks (QKANs) enhance parameter efficiency and expressivity in quantum machine learning, offering scalability and improved performance in various tasks.', 'ai_keywords': ['variational quantum circuits', 'Kolmogorov-Arnold networks', 'quantum variational activation functions', 'DARUANs', 'frequency spectrum', 'parameter efficiency', 'expressivity', 'generalization', 'layer extension', 'hybrid QKANs', 'function regression', 'image classification', 'autoregressive generative language modeling', 'NISQ hardware', 'classical quantum simulators'], 'githubStars': 3}, 'publishedAt': '2025-09-17T10:28:42.000Z', 'title': 'Quantum Variational Activation Functions Empower Kolmogorov-Arnold\\n  Networks', 'summary': 'Variational quantum circuits (VQCs) are central to quantum machine learning,\\nwhile recent progress in Kolmogorov-Arnold networks (KANs) highlights the power\\nof learnable activation functions. We unify these directions by introducing\\nquantum variational activation functions (QVAFs), realized through single-qubit\\ndata re-uploading circuits called DatA Re-Uploading ActivatioNs (DARUANs). We\\nshow that DARUAN with trainable weights in data pre-processing possesses an\\nexponentially growing frequency spectrum with data repetitions, enabling an\\nexponential reduction in parameter size compared with Fourier-based activations\\nwithout loss of expressivity. Embedding DARUAN into KANs yields\\nquantum-inspired KANs (QKANs), which retain the interpretability of KANs while\\nimproving their parameter efficiency, expressivity, and generalization. We\\nfurther introduce two novel techniques to enhance scalability, feasibility and\\ncomputational efficiency, such as layer extension and hybrid QKANs (HQKANs) as\\ndrop-in replacements of multi-layer perceptrons (MLPs) for feed-forward\\nnetworks in large-scale models. We provide theoretical analysis and extensive\\nexperiments on function regression, image classification, and autoregressive\\ngenerative language modeling, demonstrating the efficiency and scalability of\\nQKANs. DARUANs and QKANs offer a promising direction for advancing quantum\\nmachine learning on both noisy intermediate-scale quantum (NISQ) hardware and\\nclassical quantum simulators.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14026.png', 'numComments': 1, 'submittedBy': {'_id': '68b93b3a6c86c127a199ad90', 'avatarUrl': '/avatars/f370d99b240ce5b9e1bfdbd3130d9ee4.svg', 'fullname': 'Jiun-Cheng Jiang', 'name': 'Jim137', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.13642', 'authors': [{'_id': '68cb89f75a7803ff3be42e4f', 'user': {'_id': '67160d7ed0aceb0835d7fc12', 'avatarUrl': '/avatars/f4dc2cfffbe5ae2ffcc9410c31759b62.svg', 'isPro': False, 'fullname': 'zrguo', 'user': 'kkwok', 'type': 'user'}, 'name': 'Zirun Guo', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:26:06.027Z', 'hidden': False}, {'_id': '68cb89f75a7803ff3be42e50', 'name': 'Feng Zhang', 'hidden': False}, {'_id': '68cb89f75a7803ff3be42e51', 'name': 'Kai Jia', 'hidden': False}, {'_id': '68cb89f75a7803ff3be42e52', 'name': 'Tao Jin', 'hidden': False}], 'publishedAt': '2025-09-17T02:33:29.000Z', 'submittedOnDailyAt': '2025-09-18T13:35:33.584Z', 'title': 'LLM-I: LLMs are Naturally Interleaved Multimodal Creators', 'submittedOnDailyBy': {'_id': '67160d7ed0aceb0835d7fc12', 'avatarUrl': '/avatars/f4dc2cfffbe5ae2ffcc9410c31759b62.svg', 'isPro': False, 'fullname': 'zrguo', 'user': 'kkwok', 'type': 'user'}, 'summary': 'We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that\\nreframes interleaved image-text generation as a tool-use problem. LLM-I is\\ndesigned to overcome the \"one-tool\" bottleneck of current unified models, which\\nare limited to synthetic imagery and struggle with tasks requiring factual\\ngrounding or programmatic precision. Our framework empowers a central LLM or\\nMLLM agent to intelligently orchestrate a diverse toolkit of specialized visual\\ntools, including online image search, diffusion-based generation, code\\nexecution, and image editing. The agent is trained to select and apply these\\ntools proficiently via a Reinforcement Learning (RL) framework that features a\\nhybrid reward system combining rule-based logic with judgments from LLM and\\nMLLM evaluators. Trained on a diverse new dataset using four different model\\nbackbones, LLM-I demonstrates state-of-the-art performance, outperforming\\nexisting methods by a large margin across four benchmarks. We also introduce a\\nnovel test-time scaling strategy that provides further performance gains.\\nProject Page: https://github.com/ByteDance-BandAI/LLM-I.', 'upvotes': 0, 'discussionId': '68cb89f85a7803ff3be42e53', 'ai_summary': 'LLM-Interleaved (LLM-I) is a flexible framework that uses a central LLM to orchestrate a toolkit of specialized visual tools, achieving state-of-the-art performance in image-text generation through reinforcement learning and a novel scaling strategy.', 'ai_keywords': ['LLM-Interleaved', 'LLM-I', 'interleaved image-text generation', 'tool-use problem', 'unified models', 'synthetic imagery', 'factual grounding', 'programmatic precision', 'diffusion-based generation', 'code execution', 'image editing', 'Reinforcement Learning', 'RL', 'hybrid reward system', 'rule-based logic', 'LLM evaluators', 'MLLM evaluators', 'model backbones', 'state-of-the-art performance', 'benchmarks', 'test-time scaling strategy']}, 'publishedAt': '2025-09-16T22:33:29.000Z', 'title': 'LLM-I: LLMs are Naturally Interleaved Multimodal Creators', 'summary': 'We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that\\nreframes interleaved image-text generation as a tool-use problem. LLM-I is\\ndesigned to overcome the \"one-tool\" bottleneck of current unified models, which\\nare limited to synthetic imagery and struggle with tasks requiring factual\\ngrounding or programmatic precision. Our framework empowers a central LLM or\\nMLLM agent to intelligently orchestrate a diverse toolkit of specialized visual\\ntools, including online image search, diffusion-based generation, code\\nexecution, and image editing. The agent is trained to select and apply these\\ntools proficiently via a Reinforcement Learning (RL) framework that features a\\nhybrid reward system combining rule-based logic with judgments from LLM and\\nMLLM evaluators. Trained on a diverse new dataset using four different model\\nbackbones, LLM-I demonstrates state-of-the-art performance, outperforming\\nexisting methods by a large margin across four benchmarks. We also introduce a\\nnovel test-time scaling strategy that provides further performance gains.\\nProject Page: https://github.com/ByteDance-BandAI/LLM-I.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13642.png', 'numComments': 1, 'submittedBy': {'_id': '67160d7ed0aceb0835d7fc12', 'avatarUrl': '/avatars/f4dc2cfffbe5ae2ffcc9410c31759b62.svg', 'fullname': 'zrguo', 'name': 'kkwok', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.13353', 'authors': [{'_id': '68cb6bfd5a7803ff3be42ddd', 'user': {'_id': '68cb6bb026c197a95a009a60', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Ko-OoQ4wDOCvrqO22J4uu.png', 'isPro': False, 'fullname': 'Muhammad Adnan Shahzad', 'user': 'adnanphp', 'type': 'user'}, 'name': 'Muhammad Adnan Shahzad', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-18T13:26:45.211Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/68cb6bb026c197a95a009a60/YyeZCbP5fc4T8MWbOHCzu.png', 'https://cdn-uploads.huggingface.co/production/uploads/68cb6bb026c197a95a009a60/LnnIoK_kCuOhkO_CESiQh.png', 'https://cdn-uploads.huggingface.co/production/uploads/68cb6bb026c197a95a009a60/vy1RiGozIWp03-E38QrJK.png', 'https://cdn-uploads.huggingface.co/production/uploads/68cb6bb026c197a95a009a60/NGI2N_B7Lj6Y05CYC8LXn.png', 'https://cdn-uploads.huggingface.co/production/uploads/68cb6bb026c197a95a009a60/zMfYrCOtZeY5sfs4wp_mW.png', 'https://cdn-uploads.huggingface.co/production/uploads/68cb6bb026c197a95a009a60/Kf3UX0PNNbl7zj42fpreB.png'], 'publishedAt': '2025-09-14T09:55:00.000Z', 'submittedOnDailyAt': '2025-09-18T12:29:37.332Z', 'title': 'Hybrid Quantum-Classical Model for Image Classification', 'submittedOnDailyBy': {'_id': '68cb6bb026c197a95a009a60', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Ko-OoQ4wDOCvrqO22J4uu.png', 'isPro': False, 'fullname': 'Muhammad Adnan Shahzad', 'user': 'adnanphp', 'type': 'user'}, 'summary': 'This study presents a systematic comparison between hybrid quantum-classical\\nneural networks and purely classical models across three benchmark datasets\\n(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and\\nrobustness. The hybrid models integrate parameterized quantum circuits with\\nclassical deep learning architectures, while the classical counterparts use\\nconventional convolutional neural networks (CNNs). Experiments were conducted\\nover 50 training epochs for each dataset, with evaluations on validation\\naccuracy, test accuracy, training time, computational resource usage, and\\nadversarial robustness (tested with epsilon=0.1 perturbations).Key findings\\ndemonstrate that hybrid models consistently outperform classical models in\\nfinal accuracy, achieving {99.38\\\\% (MNIST), 41.69\\\\% (CIFAR100), and 74.05\\\\%\\n(STL10) validation accuracy, compared to classical benchmarks of 98.21\\\\%,\\n32.25\\\\%, and 63.76\\\\%, respectively. Notably, the hybrid advantage scales with\\ndataset complexity, showing the most significant gains on CIFAR100 (+9.44\\\\%)\\nand STL10 (+10.29\\\\%). Hybrid models also train 5--12times faster (e.g.,\\n21.23s vs. 108.44s per epoch on MNIST) and use 6--32\\\\% fewer parameters} while\\nmaintaining superior generalization to unseen test data.Adversarial robustness\\ntests reveal that hybrid models are significantly more resilient on simpler\\ndatasets (e.g., 45.27\\\\% robust accuracy on MNIST vs. 10.80\\\\% for classical) but\\nshow comparable fragility on complex datasets like CIFAR100 (sim1\\\\%\\nrobustness for both). Resource efficiency analyses indicate that hybrid models\\nconsume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization\\n(9.5\\\\% vs. 23.2\\\\% on average).These results suggest that hybrid\\nquantum-classical architectures offer compelling advantages in accuracy,\\ntraining efficiency, and parameter scalability, particularly for complex vision\\ntasks.', 'upvotes': 0, 'discussionId': '68cb6bfe5a7803ff3be42dde', 'projectPage': 'https://github.com/adnanphp/Hybrid-Quantum-Classical', 'githubRepo': 'https://github.com/adnanphp/Hybrid-Quantum-Classical', 'ai_summary': 'Hybrid quantum-classical neural networks outperform classical models in accuracy, training efficiency, and parameter scalability across various datasets, especially for complex vision tasks.', 'ai_keywords': ['hybrid quantum-classical neural networks', 'parameterized quantum circuits', 'classical deep learning architectures', 'convolutional neural networks', 'validation accuracy', 'test accuracy', 'training time', 'computational resource usage', 'adversarial robustness', 'MNIST', 'CIFAR100', 'STL10'], 'githubStars': 0}, 'publishedAt': '2025-09-14T05:55:00.000Z', 'title': 'Hybrid Quantum-Classical Model for Image Classification', 'summary': 'This study presents a systematic comparison between hybrid quantum-classical\\nneural networks and purely classical models across three benchmark datasets\\n(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and\\nrobustness. The hybrid models integrate parameterized quantum circuits with\\nclassical deep learning architectures, while the classical counterparts use\\nconventional convolutional neural networks (CNNs). Experiments were conducted\\nover 50 training epochs for each dataset, with evaluations on validation\\naccuracy, test accuracy, training time, computational resource usage, and\\nadversarial robustness (tested with epsilon=0.1 perturbations).Key findings\\ndemonstrate that hybrid models consistently outperform classical models in\\nfinal accuracy, achieving {99.38\\\\% (MNIST), 41.69\\\\% (CIFAR100), and 74.05\\\\%\\n(STL10) validation accuracy, compared to classical benchmarks of 98.21\\\\%,\\n32.25\\\\%, and 63.76\\\\%, respectively. Notably, the hybrid advantage scales with\\ndataset complexity, showing the most significant gains on CIFAR100 (+9.44\\\\%)\\nand STL10 (+10.29\\\\%). Hybrid models also train 5--12times faster (e.g.,\\n21.23s vs. 108.44s per epoch on MNIST) and use 6--32\\\\% fewer parameters} while\\nmaintaining superior generalization to unseen test data.Adversarial robustness\\ntests reveal that hybrid models are significantly more resilient on simpler\\ndatasets (e.g., 45.27\\\\% robust accuracy on MNIST vs. 10.80\\\\% for classical) but\\nshow comparable fragility on complex datasets like CIFAR100 (sim1\\\\%\\nrobustness for both). Resource efficiency analyses indicate that hybrid models\\nconsume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization\\n(9.5\\\\% vs. 23.2\\\\% on average).These results suggest that hybrid\\nquantum-classical architectures offer compelling advantages in accuracy,\\ntraining efficiency, and parameter scalability, particularly for complex vision\\ntasks.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/68cb6bb026c197a95a009a60/YyeZCbP5fc4T8MWbOHCzu.png', 'https://cdn-uploads.huggingface.co/production/uploads/68cb6bb026c197a95a009a60/LnnIoK_kCuOhkO_CESiQh.png', 'https://cdn-uploads.huggingface.co/production/uploads/68cb6bb026c197a95a009a60/vy1RiGozIWp03-E38QrJK.png', 'https://cdn-uploads.huggingface.co/production/uploads/68cb6bb026c197a95a009a60/NGI2N_B7Lj6Y05CYC8LXn.png', 'https://cdn-uploads.huggingface.co/production/uploads/68cb6bb026c197a95a009a60/zMfYrCOtZeY5sfs4wp_mW.png', 'https://cdn-uploads.huggingface.co/production/uploads/68cb6bb026c197a95a009a60/Kf3UX0PNNbl7zj42fpreB.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13353.png', 'numComments': 1, 'submittedBy': {'_id': '68cb6bb026c197a95a009a60', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Ko-OoQ4wDOCvrqO22J4uu.png', 'fullname': 'Muhammad Adnan Shahzad', 'name': 'adnanphp', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}"
]