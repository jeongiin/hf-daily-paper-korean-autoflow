[
    "{'paper': {'id': '2509.01106', 'authors': [{'_id': '68b8f795d43cadaf7a688b04', 'name': 'Huang Fang', 'hidden': False}, {'_id': '68b8f795d43cadaf7a688b05', 'name': 'Mengxi Zhang', 'hidden': False}, {'_id': '68b8f795d43cadaf7a688b06', 'name': 'Heng Dong', 'hidden': False}, {'_id': '68b8f795d43cadaf7a688b07', 'name': 'Wei Li', 'hidden': False}, {'_id': '68b8f795d43cadaf7a688b08', 'name': 'Zixuan Wang', 'hidden': False}, {'_id': '68b8f795d43cadaf7a688b09', 'name': 'Qifeng Zhang', 'hidden': False}, {'_id': '68b8f795d43cadaf7a688b0a', 'name': 'Xueyun Tian', 'hidden': False}, {'_id': '68b8f795d43cadaf7a688b0b', 'name': 'Yucheng Hu', 'hidden': False}, {'_id': '68b8f795d43cadaf7a688b0c', 'name': 'Hang Li', 'hidden': False}], 'publishedAt': '2025-09-01T03:53:47.000Z', 'submittedOnDailyAt': '2025-09-04T00:57:07.946Z', 'title': 'Robix: A Unified Model for Robot Interaction, Reasoning and Planning', 'submittedOnDailyBy': {'_id': '63044e025c70c21d0eaf08bc', 'avatarUrl': '/avatars/a2d39973d7fbcbe9d4cce5648b3149c2.svg', 'isPro': False, 'fullname': 'Wei Li', 'user': 'Wiley085', 'type': 'user'}, 'summary': 'We introduce Robix, a unified model that integrates robot reasoning, task\\nplanning, and natural language interaction within a single vision-language\\narchitecture. Acting as the high-level cognitive layer in a hierarchical robot\\nsystem, Robix dynamically generates atomic commands for the low-level\\ncontroller and verbal responses for human interaction, enabling robots to\\nfollow complex instructions, plan long-horizon tasks, and interact naturally\\nwith human within an end-to-end framework. Robix further introduces novel\\ncapabilities such as proactive dialogue, real-time interruption handling, and\\ncontext-aware commonsense reasoning during task execution. At its core, Robix\\nleverages chain-of-thought reasoning and adopts a three-stage training\\nstrategy: (1) continued pretraining to enhance foundational embodied reasoning\\nabilities including 3D spatial understanding, visual grounding, and\\ntask-centric reasoning; (2) supervised finetuning to model human-robot\\ninteraction and task planning as a unified reasoning-action sequence; and (3)\\nreinforcement learning to improve reasoning-action consistency and long-horizon\\ntask coherence. Extensive experiments demonstrate that Robix outperforms both\\nopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in\\ninteractive task execution, demonstrating strong generalization across diverse\\ninstruction types (e.g., open-ended, multi-stage, constrained, invalid, and\\ninterrupted) and various user-involved tasks such as table bussing, grocery\\nshopping, and dietary filtering.', 'upvotes': 29, 'discussionId': '68b8f796d43cadaf7a688b0d', 'projectPage': 'https://robix-seed.github.io/robix/', 'ai_summary': 'Robix, a unified vision-language model, integrates robot reasoning, task planning, and natural language interaction, demonstrating superior performance in interactive task execution through chain-of-thought reasoning and a three-stage training strategy.', 'ai_keywords': ['chain-of-thought reasoning', 'three-stage training strategy', 'continued pretraining', 'supervised finetuning', 'reinforcement learning', 'embodied reasoning', '3D spatial understanding', 'visual grounding', 'task-centric reasoning', 'human-robot interaction', 'task planning', 'reasoning-action sequence', 'reasoning-action consistency', 'long-horizon task coherence']}, 'publishedAt': '2025-08-31T23:53:47.000Z', 'title': 'Robix: A Unified Model for Robot Interaction, Reasoning and Planning', 'summary': 'We introduce Robix, a unified model that integrates robot reasoning, task\\nplanning, and natural language interaction within a single vision-language\\narchitecture. Acting as the high-level cognitive layer in a hierarchical robot\\nsystem, Robix dynamically generates atomic commands for the low-level\\ncontroller and verbal responses for human interaction, enabling robots to\\nfollow complex instructions, plan long-horizon tasks, and interact naturally\\nwith human within an end-to-end framework. Robix further introduces novel\\ncapabilities such as proactive dialogue, real-time interruption handling, and\\ncontext-aware commonsense reasoning during task execution. At its core, Robix\\nleverages chain-of-thought reasoning and adopts a three-stage training\\nstrategy: (1) continued pretraining to enhance foundational embodied reasoning\\nabilities including 3D spatial understanding, visual grounding, and\\ntask-centric reasoning; (2) supervised finetuning to model human-robot\\ninteraction and task planning as a unified reasoning-action sequence; and (3)\\nreinforcement learning to improve reasoning-action consistency and long-horizon\\ntask coherence. Extensive experiments demonstrate that Robix outperforms both\\nopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in\\ninteractive task execution, demonstrating strong generalization across diverse\\ninstruction types (e.g., open-ended, multi-stage, constrained, invalid, and\\ninterrupted) and various user-involved tasks such as table bussing, grocery\\nshopping, and dietary filtering.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01106.png', 'numComments': 1, 'submittedBy': {'_id': '63044e025c70c21d0eaf08bc', 'avatarUrl': '/avatars/a2d39973d7fbcbe9d4cce5648b3149c2.svg', 'fullname': 'Wei Li', 'name': 'Wiley085', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.00375', 'authors': [{'_id': '68b902b5d43cadaf7a688b4c', 'user': {'_id': '6540617c7cadb2d1b42007c5', 'avatarUrl': '/avatars/b1877fd0564c362a0d4a064d4ec43a73.svg', 'isPro': False, 'fullname': 'Ziyi Xia', 'user': 'ZiyiXia', 'type': 'user'}, 'name': 'Ziyi Xia', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-04T08:43:56.322Z', 'hidden': False}, {'_id': '68b902b5d43cadaf7a688b4d', 'name': 'Kun Luo', 'hidden': False}, {'_id': '68b902b5d43cadaf7a688b4e', 'name': 'Hongjin Qian', 'hidden': False}, {'_id': '68b902b5d43cadaf7a688b4f', 'name': 'Zheng Liu', 'hidden': False}], 'publishedAt': '2025-08-30T06:02:56.000Z', 'submittedOnDailyAt': '2025-09-04T01:42:16.942Z', 'title': 'Open Data Synthesis For Deep Research', 'submittedOnDailyBy': {'_id': '6540617c7cadb2d1b42007c5', 'avatarUrl': '/avatars/b1877fd0564c362a0d4a064d4ec43a73.svg', 'isPro': False, 'fullname': 'Ziyi Xia', 'user': 'ZiyiXia', 'type': 'user'}, 'summary': 'Large language models (LLMs) are increasingly expected to go beyond simple\\nfactual queries toward Deep Research-tasks that require decomposing questions\\ninto sub-problems, coordinating multi-step reasoning, and synthesizing evidence\\nfrom diverse sources. We formalize Deep Research tasks with verifiable answers\\nas Hierarchical Constraint Satisfaction Problems (HCSPs), which are\\nfundamentally different from single-constraint, multi-hop, or flat CSP\\nformulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)\\nfail to capture this complexity, while recent synthetic datasets often\\nintroduce shortcut reasoning, knowledge leakage, or lack sufficient structural\\ndepth. To address this gap, we introduce InfoSeek, a scalable framework for\\nsynthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to\\nrecursively build a Research Tree from large-scale webpages, blurring\\nintermediate nodes into valid sub-problems, and converting these trees into\\nnatural language questions that require traversing the full hierarchy. It also\\nenables rapid scaling, yielding over 50K training examples, a curated test set,\\nand reasoning trajectories generated via reject sampling. Experiments show that\\nmodels trained on InfoSeek consistently outperform strong baselines. On a\\nchallenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass\\nmuch larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),\\nwhile achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).\\nBy preserving meta-information such as intermediate steps and retrieval labels,\\nInfoSeek further supports advanced optimization strategies, including compound\\nreward design and trajectory-level exploration. We provide our codes and\\ndatasets in https://github.com/VectorSpaceLab/InfoSeek{this repository}.', 'upvotes': 26, 'discussionId': '68b902b5d43cadaf7a688b50', 'ai_summary': 'InfoSeek is a scalable framework for generating complex Deep Research tasks by synthesizing hierarchical constraint satisfaction problems, enabling models to outperform larger baselines on challenging benchmarks.', 'ai_keywords': ['Hierarchical Constraint Satisfaction Problems', 'HCSPs', 'dual-agent system', 'Research Tree', 'natural language questions', 'reasoning trajectories', 'reject sampling', 'compound reward design', 'trajectory-level exploration']}, 'publishedAt': '2025-08-30T02:02:56.000Z', 'title': 'Open Data Synthesis For Deep Research', 'summary': 'Large language models (LLMs) are increasingly expected to go beyond simple\\nfactual queries toward Deep Research-tasks that require decomposing questions\\ninto sub-problems, coordinating multi-step reasoning, and synthesizing evidence\\nfrom diverse sources. We formalize Deep Research tasks with verifiable answers\\nas Hierarchical Constraint Satisfaction Problems (HCSPs), which are\\nfundamentally different from single-constraint, multi-hop, or flat CSP\\nformulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)\\nfail to capture this complexity, while recent synthetic datasets often\\nintroduce shortcut reasoning, knowledge leakage, or lack sufficient structural\\ndepth. To address this gap, we introduce InfoSeek, a scalable framework for\\nsynthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to\\nrecursively build a Research Tree from large-scale webpages, blurring\\nintermediate nodes into valid sub-problems, and converting these trees into\\nnatural language questions that require traversing the full hierarchy. It also\\nenables rapid scaling, yielding over 50K training examples, a curated test set,\\nand reasoning trajectories generated via reject sampling. Experiments show that\\nmodels trained on InfoSeek consistently outperform strong baselines. On a\\nchallenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass\\nmuch larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),\\nwhile achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).\\nBy preserving meta-information such as intermediate steps and retrieval labels,\\nInfoSeek further supports advanced optimization strategies, including compound\\nreward design and trajectory-level exploration. We provide our codes and\\ndatasets in https://github.com/VectorSpaceLab/InfoSeek{this repository}.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00375.png', 'numComments': 2, 'submittedBy': {'_id': '6540617c7cadb2d1b42007c5', 'avatarUrl': '/avatars/b1877fd0564c362a0d4a064d4ec43a73.svg', 'fullname': 'Ziyi Xia', 'name': 'ZiyiXia', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.03405', 'authors': [{'_id': '68b94c5ed43cadaf7a688c0a', 'name': 'Daniela Gottesman', 'hidden': False}, {'_id': '68b94c5ed43cadaf7a688c0b', 'name': 'Alon Gilae-Dotan', 'hidden': False}, {'_id': '68b94c5ed43cadaf7a688c0c', 'name': 'Ido Cohen', 'hidden': False}, {'_id': '68b94c5ed43cadaf7a688c0d', 'name': 'Yoav Gur-Arieh', 'hidden': False}, {'_id': '68b94c5ed43cadaf7a688c0e', 'name': 'Marius Mosbach', 'hidden': False}, {'_id': '68b94c5ed43cadaf7a688c0f', 'name': 'Ori Yoran', 'hidden': False}, {'_id': '68b94c5ed43cadaf7a688c10', 'user': {'_id': '610b729f9da682cd54ad9adf', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1628140189042-noauth.jpeg', 'isPro': False, 'fullname': 'Mor Geva', 'user': 'mega', 'type': 'user'}, 'name': 'Mor Geva', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-09-04T08:42:49.827Z', 'hidden': False}], 'publishedAt': '2025-09-03T15:31:18.000Z', 'submittedOnDailyAt': '2025-09-04T07:03:36.263Z', 'title': 'LMEnt: A Suite for Analyzing Knowledge in Language Models from\\n  Pretraining Data to Representations', 'submittedOnDailyBy': {'_id': '637f37b5b3cb8158e1725697', 'avatarUrl': '/avatars/18780d63b12aa8a5171130d09e214b25.svg', 'isPro': False, 'fullname': 'Daniela', 'user': 'dhgottesman', 'type': 'user'}, 'summary': 'Language models (LMs) increasingly drive real-world applications that require\\nworld knowledge. However, the internal processes through which models turn data\\ninto representations of knowledge and beliefs about the world, are poorly\\nunderstood. Insights into these processes could pave the way for developing LMs\\nwith knowledge representations that are more consistent, robust, and complete.\\nTo facilitate studying these questions, we present LMEnt, a suite for analyzing\\nknowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a\\nknowledge-rich pretraining corpus, fully annotated with entity mentions, based\\non Wikipedia, (2) an entity-based retrieval method over pretraining data that\\noutperforms previous approaches by as much as 80.4%, and (3) 12 pretrained\\nmodels with up to 1B parameters and 4K intermediate checkpoints, with\\ncomparable performance to popular open-sourced models on knowledge benchmarks.\\nTogether, these resources provide a controlled environment for analyzing\\nconnections between entity mentions in pretraining and downstream performance,\\nand the effects of causal interventions in pretraining data. We show the\\nutility of LMEnt by studying knowledge acquisition across checkpoints, finding\\nthat fact frequency is key, but does not fully explain learning trends. We\\nrelease LMEnt to support studies of knowledge in LMs, including knowledge\\nrepresentations, plasticity, editing, attribution, and learning dynamics.', 'upvotes': 14, 'discussionId': '68b94c5fd43cadaf7a688c11', 'ai_summary': 'LMEnt is a suite for analyzing knowledge acquisition in language models during pretraining, providing annotated corpora, retrieval methods, and pretrained models to study knowledge representations and learning dynamics.', 'ai_keywords': ['language models', 'knowledge acquisition', 'pretraining', 'knowledge-rich pretraining corpus', 'entity mentions', 'entity-based retrieval', 'pretrained models', 'knowledge benchmarks', 'knowledge representations', 'knowledge plasticity', 'knowledge editing', 'knowledge attribution', 'learning dynamics']}, 'publishedAt': '2025-09-03T11:31:18.000Z', 'title': 'LMEnt: A Suite for Analyzing Knowledge in Language Models from\\n  Pretraining Data to Representations', 'summary': 'Language models (LMs) increasingly drive real-world applications that require\\nworld knowledge. However, the internal processes through which models turn data\\ninto representations of knowledge and beliefs about the world, are poorly\\nunderstood. Insights into these processes could pave the way for developing LMs\\nwith knowledge representations that are more consistent, robust, and complete.\\nTo facilitate studying these questions, we present LMEnt, a suite for analyzing\\nknowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a\\nknowledge-rich pretraining corpus, fully annotated with entity mentions, based\\non Wikipedia, (2) an entity-based retrieval method over pretraining data that\\noutperforms previous approaches by as much as 80.4%, and (3) 12 pretrained\\nmodels with up to 1B parameters and 4K intermediate checkpoints, with\\ncomparable performance to popular open-sourced models on knowledge benchmarks.\\nTogether, these resources provide a controlled environment for analyzing\\nconnections between entity mentions in pretraining and downstream performance,\\nand the effects of causal interventions in pretraining data. We show the\\nutility of LMEnt by studying knowledge acquisition across checkpoints, finding\\nthat fact frequency is key, but does not fully explain learning trends. We\\nrelease LMEnt to support studies of knowledge in LMs, including knowledge\\nrepresentations, plasticity, editing, attribution, and learning dynamics.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03405.png', 'numComments': 1, 'submittedBy': {'_id': '637f37b5b3cb8158e1725697', 'avatarUrl': '/avatars/18780d63b12aa8a5171130d09e214b25.svg', 'fullname': 'Daniela', 'name': 'dhgottesman', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2509.01977', 'authors': [{'_id': '68b8f158d43cadaf7a688afb', 'name': 'Dong She', 'hidden': False}, {'_id': '68b8f158d43cadaf7a688afc', 'name': 'Siming Fu', 'hidden': False}, {'_id': '68b8f158d43cadaf7a688afd', 'name': 'Mushui Liu', 'hidden': False}, {'_id': '68b8f158d43cadaf7a688afe', 'name': 'Qiaoqiao Jin', 'hidden': False}, {'_id': '68b8f158d43cadaf7a688aff', 'name': 'Hualiang Wang', 'hidden': False}, {'_id': '68b8f158d43cadaf7a688b00', 'name': 'Mu Liu', 'hidden': False}, {'_id': '68b8f158d43cadaf7a688b01', 'name': 'Jidong Jiang', 'hidden': False}], 'publishedAt': '2025-09-02T05:40:07.000Z', 'submittedOnDailyAt': '2025-09-04T05:05:42.538Z', 'title': 'MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware\\n  Alignment and Disentanglement', 'submittedOnDailyBy': {'_id': '6485dd6d07a2c1915060f603', 'avatarUrl': '/avatars/8594d647359a7d19ab29b8ec91d1444e.svg', 'isPro': False, 'fullname': 'fu', 'user': 'simingfu', 'type': 'user'}, 'summary': 'Multi-subject personalized generation presents unique challenges in\\nmaintaining identity fidelity and semantic coherence when synthesizing images\\nconditioned on multiple reference subjects. Existing methods often suffer from\\nidentity blending and attribute leakage due to inadequate modeling of how\\ndifferent subjects should interact within shared representation spaces. We\\npresent MOSAIC, a representation-centric framework that rethinks multi-subject\\ngeneration through explicit semantic correspondence and orthogonal feature\\ndisentanglement. Our key insight is that multi-subject generation requires\\nprecise semantic alignment at the representation level - knowing exactly which\\nregions in the generated image should attend to which parts of each reference.\\nTo enable this, we introduce SemAlign-MS, a meticulously annotated dataset\\nproviding fine-grained semantic correspondences between multiple reference\\nsubjects and target images, previously unavailable in this domain. Building on\\nthis foundation, we propose the semantic correspondence attention loss to\\nenforce precise point-to-point semantic alignment, ensuring high consistency\\nfrom each reference to its designated regions. Furthermore, we develop the\\nmulti-reference disentanglement loss to push different subjects into orthogonal\\nattention subspaces, preventing feature interference while preserving\\nindividual identity characteristics. Extensive experiments demonstrate that\\nMOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,\\nwhile existing methods typically degrade beyond 3 subjects, MOSAIC maintains\\nhigh fidelity with 4+ reference subjects, opening new possibilities for complex\\nmulti-subject synthesis applications.', 'upvotes': 7, 'discussionId': '68b8f158d43cadaf7a688b02', 'projectPage': 'https://bytedance-fanqie-ai.github.io/MOSAIC/', 'githubRepo': 'https://github.com/bytedance-fanqie-ai/MOSAIC', 'ai_summary': 'MOSAIC framework enhances multi-subject image generation by ensuring precise semantic alignment and orthogonal feature disentanglement, achieving high fidelity even with multiple references.', 'ai_keywords': ['representation-centric framework', 'semantic correspondence', 'orthogonal feature disentanglement', 'SemAlign-MS', 'semantic correspondence attention loss', 'multi-reference disentanglement loss'], 'githubStars': 274}, 'publishedAt': '2025-09-02T01:40:07.000Z', 'title': 'MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware\\n  Alignment and Disentanglement', 'summary': 'Multi-subject personalized generation presents unique challenges in\\nmaintaining identity fidelity and semantic coherence when synthesizing images\\nconditioned on multiple reference subjects. Existing methods often suffer from\\nidentity blending and attribute leakage due to inadequate modeling of how\\ndifferent subjects should interact within shared representation spaces. We\\npresent MOSAIC, a representation-centric framework that rethinks multi-subject\\ngeneration through explicit semantic correspondence and orthogonal feature\\ndisentanglement. Our key insight is that multi-subject generation requires\\nprecise semantic alignment at the representation level - knowing exactly which\\nregions in the generated image should attend to which parts of each reference.\\nTo enable this, we introduce SemAlign-MS, a meticulously annotated dataset\\nproviding fine-grained semantic correspondences between multiple reference\\nsubjects and target images, previously unavailable in this domain. Building on\\nthis foundation, we propose the semantic correspondence attention loss to\\nenforce precise point-to-point semantic alignment, ensuring high consistency\\nfrom each reference to its designated regions. Furthermore, we develop the\\nmulti-reference disentanglement loss to push different subjects into orthogonal\\nattention subspaces, preventing feature interference while preserving\\nindividual identity characteristics. Extensive experiments demonstrate that\\nMOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,\\nwhile existing methods typically degrade beyond 3 subjects, MOSAIC maintains\\nhigh fidelity with 4+ reference subjects, opening new possibilities for complex\\nmulti-subject synthesis applications.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01977.png', 'numComments': 1, 'submittedBy': {'_id': '6485dd6d07a2c1915060f603', 'avatarUrl': '/avatars/8594d647359a7d19ab29b8ec91d1444e.svg', 'fullname': 'fu', 'name': 'simingfu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.00428', 'authors': [{'_id': '68b93229d43cadaf7a688bc6', 'name': 'Xuechao Zou', 'hidden': False}, {'_id': '68b93229d43cadaf7a688bc7', 'name': 'Shun Zhang', 'hidden': False}, {'_id': '68b93229d43cadaf7a688bc8', 'name': 'Xing Fu', 'hidden': False}, {'_id': '68b93229d43cadaf7a688bc9', 'name': 'Yue Li', 'hidden': False}, {'_id': '68b93229d43cadaf7a688bca', 'name': 'Kai Li', 'hidden': False}, {'_id': '68b93229d43cadaf7a688bcb', 'name': 'Yushe Cao', 'hidden': False}, {'_id': '68b93229d43cadaf7a688bcc', 'name': 'Congyan Lang', 'hidden': False}, {'_id': '68b93229d43cadaf7a688bcd', 'name': 'Pin Tao', 'hidden': False}, {'_id': '68b93229d43cadaf7a688bce', 'name': 'Junliang Xing', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/sJrvZF6pJNPec1coRwsLz.jpeg', 'https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/9Chxv_NqnjXFhIVDrKyK3.png', 'https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/Q6nmMSG1YTJDEfG8ySInJ.png'], 'publishedAt': '2025-08-30T09:21:07.000Z', 'submittedOnDailyAt': '2025-09-04T05:17:48.471Z', 'title': 'Mixture of Global and Local Experts with Diffusion Transformer for\\n  Controllable Face Generation', 'submittedOnDailyBy': {'_id': '6617af2beab5eef6b1e8bb9e', 'avatarUrl': '/avatars/d939c02027916331d4c44119565f2ca6.svg', 'isPro': False, 'fullname': 'XavierJiezou', 'user': 'XavierJiezou', 'type': 'user'}, 'summary': 'Controllable face generation poses critical challenges in generative modeling\\ndue to the intricate balance required between semantic controllability and\\nphotorealism. While existing approaches struggle with disentangling semantic\\ncontrols from generation pipelines, we revisit the architectural potential of\\nDiffusion Transformers (DiTs) through the lens of expert specialization. This\\npaper introduces Face-MoGLE, a novel framework featuring: (1)\\nSemantic-decoupled latent modeling through mask-conditioned space\\nfactorization, enabling precise attribute manipulation; (2) A mixture of global\\nand local experts that captures holistic structure and region-level semantics\\nfor fine-grained controllability; (3) A dynamic gating network producing\\ntime-dependent coefficients that evolve with diffusion steps and spatial\\nlocations. Face-MoGLE provides a powerful and flexible solution for\\nhigh-quality, controllable face generation, with strong potential in generative\\nmodeling and security applications. Extensive experiments demonstrate its\\neffectiveness in multimodal and monomodal face generation settings and its\\nrobust zero-shot generalization capability. Project page is available at\\nhttps://github.com/XavierJiezou/Face-MoGLE.', 'upvotes': 7, 'discussionId': '68b93229d43cadaf7a688bcf', 'projectPage': 'https://xavierjiezou.github.io/Face-MoGLE/', 'githubRepo': 'https://github.com/XavierJiezou/Face-MoGLE', 'ai_summary': 'Face-MoGLE, a novel framework using Diffusion Transformers, achieves high-quality, controllable face generation through semantic-decoupled latent modeling, expert specialization, and dynamic gating.', 'ai_keywords': ['Diffusion Transformers', 'Semantic-decoupled latent modeling', 'mask-conditioned space factorization', 'global experts', 'local experts', 'dynamic gating network', 'multimodal face generation', 'monomodal face generation', 'zero-shot generalization'], 'githubStars': 5}, 'publishedAt': '2025-08-30T05:21:07.000Z', 'title': 'Mixture of Global and Local Experts with Diffusion Transformer for\\n  Controllable Face Generation', 'summary': 'Controllable face generation poses critical challenges in generative modeling\\ndue to the intricate balance required between semantic controllability and\\nphotorealism. While existing approaches struggle with disentangling semantic\\ncontrols from generation pipelines, we revisit the architectural potential of\\nDiffusion Transformers (DiTs) through the lens of expert specialization. This\\npaper introduces Face-MoGLE, a novel framework featuring: (1)\\nSemantic-decoupled latent modeling through mask-conditioned space\\nfactorization, enabling precise attribute manipulation; (2) A mixture of global\\nand local experts that captures holistic structure and region-level semantics\\nfor fine-grained controllability; (3) A dynamic gating network producing\\ntime-dependent coefficients that evolve with diffusion steps and spatial\\nlocations. Face-MoGLE provides a powerful and flexible solution for\\nhigh-quality, controllable face generation, with strong potential in generative\\nmodeling and security applications. Extensive experiments demonstrate its\\neffectiveness in multimodal and monomodal face generation settings and its\\nrobust zero-shot generalization capability. Project page is available at\\nhttps://github.com/XavierJiezou/Face-MoGLE.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/sJrvZF6pJNPec1coRwsLz.jpeg', 'https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/9Chxv_NqnjXFhIVDrKyK3.png', 'https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/Q6nmMSG1YTJDEfG8ySInJ.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00428.png', 'numComments': 1, 'submittedBy': {'_id': '6617af2beab5eef6b1e8bb9e', 'avatarUrl': '/avatars/d939c02027916331d4c44119565f2ca6.svg', 'fullname': 'XavierJiezou', 'name': 'XavierJiezou', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2509.02722', 'authors': [{'_id': '68b9940c736018af705e8d58', 'name': 'Delong Chen', 'hidden': False}, {'_id': '68b9940c736018af705e8d59', 'name': 'Theo Moutakanni', 'hidden': False}, {'_id': '68b9940c736018af705e8d5a', 'name': 'Willy Chung', 'hidden': False}, {'_id': '68b9940c736018af705e8d5b', 'name': 'Yejin Bang', 'hidden': False}, {'_id': '68b9940c736018af705e8d5c', 'name': 'Ziwei Ji', 'hidden': False}, {'_id': '68b9940c736018af705e8d5d', 'name': 'Allen Bolourchi', 'hidden': False}, {'_id': '68b9940c736018af705e8d5e', 'name': 'Pascale Fung', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/630491107424d937fa3258be/WFrDJM0y73ZkwHoHHEEGi.mp4'], 'publishedAt': '2025-09-02T18:18:57.000Z', 'submittedOnDailyAt': '2025-09-04T12:20:43.590Z', 'title': 'Planning with Reasoning using Vision Language World Model', 'submittedOnDailyBy': {'_id': '630491107424d937fa3258be', 'avatarUrl': '/avatars/b8bd81bc8544674ee26b78702afdb87c.svg', 'isPro': False, 'fullname': 'Delong Chen', 'user': 'chendelong', 'type': 'user'}, 'summary': 'Effective planning requires strong world models, but high-level world models\\nthat can understand and reason about actions with semantic and temporal\\nabstraction remain largely underdeveloped. We introduce the Vision Language\\nWorld Model (VLWM), a foundation model trained for language-based world\\nmodeling on natural videos. Given visual observations, the VLWM first infers\\nthe overall goal achievements then predicts a trajectory composed of\\ninterleaved actions and world state changes. Those targets are extracted by\\niterative LLM Self-Refine conditioned on compressed future observations\\nrepresented by Tree of Captions. The VLWM learns both an action policy and a\\ndynamics model, which respectively facilitates reactive system-1 plan decoding\\nand reflective system-2 planning via cost minimization. The cost evaluates the\\nsemantic distance between the hypothetical future states given by VLWM\\nroll-outs and the expected goal state, and is measured by a critic model that\\nwe trained in a self-supervised manner. The VLWM achieves state-of-the-art\\nVisual Planning for Assistance (VPA) performance on both benchmark evaluations\\nand our proposed PlannerArena human evaluations, where system-2 improves the\\nElo score by +27% upon system-1. The VLWM models also outperforms strong VLM\\nbaselines on RoboVQA and WorldPrediction benchmark.', 'upvotes': 5, 'discussionId': '68b9940c736018af705e8d5f', 'ai_summary': 'The Vision Language World Model (VLWM) achieves state-of-the-art performance in visual planning by integrating language-based world modeling, action policy learning, and dynamics modeling with semantic and temporal abstraction.', 'ai_keywords': ['Vision Language World Model', 'VLWM', 'language-based world modeling', 'natural videos', 'iterative LLM Self-Refine', 'Tree of Captions', 'action policy', 'dynamics model', 'system-1 plan decoding', 'system-2 planning', 'cost minimization', 'semantic distance', 'critic model', 'self-supervised manner', 'Visual Planning for Assistance', 'VPA', 'PlannerArena', 'Elo score', 'RoboVQA', 'WorldPrediction benchmark']}, 'publishedAt': '2025-09-02T14:18:57.000Z', 'title': 'Planning with Reasoning using Vision Language World Model', 'summary': 'Effective planning requires strong world models, but high-level world models\\nthat can understand and reason about actions with semantic and temporal\\nabstraction remain largely underdeveloped. We introduce the Vision Language\\nWorld Model (VLWM), a foundation model trained for language-based world\\nmodeling on natural videos. Given visual observations, the VLWM first infers\\nthe overall goal achievements then predicts a trajectory composed of\\ninterleaved actions and world state changes. Those targets are extracted by\\niterative LLM Self-Refine conditioned on compressed future observations\\nrepresented by Tree of Captions. The VLWM learns both an action policy and a\\ndynamics model, which respectively facilitates reactive system-1 plan decoding\\nand reflective system-2 planning via cost minimization. The cost evaluates the\\nsemantic distance between the hypothetical future states given by VLWM\\nroll-outs and the expected goal state, and is measured by a critic model that\\nwe trained in a self-supervised manner. The VLWM achieves state-of-the-art\\nVisual Planning for Assistance (VPA) performance on both benchmark evaluations\\nand our proposed PlannerArena human evaluations, where system-2 improves the\\nElo score by +27% upon system-1. The VLWM models also outperforms strong VLM\\nbaselines on RoboVQA and WorldPrediction benchmark.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/630491107424d937fa3258be/WFrDJM0y73ZkwHoHHEEGi.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02722.png', 'numComments': 1, 'submittedBy': {'_id': '630491107424d937fa3258be', 'avatarUrl': '/avatars/b8bd81bc8544674ee26b78702afdb87c.svg', 'fullname': 'Delong Chen', 'name': 'chendelong', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': False}"
]