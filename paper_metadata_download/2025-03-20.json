[
    "{'paper': {'id': '2503.13288', 'authors': [{'_id': '67dbc49d85eacb364e913c38', 'user': {'_id': '64e6cf78ecce34cb442dc889', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg', 'isPro': False, 'fullname': 'Fangzhi Xu', 'user': 'xufangzhi', 'type': 'user'}, 'name': 'Fangzhi Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:44:57.855Z', 'hidden': False}, {'_id': '67dbc49d85eacb364e913c39', 'user': {'_id': '67dbe3d969655e406fda64b8', 'avatarUrl': '/avatars/6053c84e32d0e46dd1e490c493f766ed.svg', 'isPro': False, 'fullname': 'Mei Tuan', 'user': 'Meituannnnnn', 'type': 'user'}, 'name': 'Hang Yan', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-03-20T09:48:32.179Z', 'hidden': False}, {'_id': '67dbc49d85eacb364e913c3a', 'user': {'_id': '637f22fd932a61b89aeeea37', 'avatarUrl': '/avatars/342957f8242d4edaf1d58e1274313afe.svg', 'isPro': False, 'fullname': 'Chang Ma', 'user': 'changma', 'type': 'user'}, 'name': 'Chang Ma', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T11:05:18.661Z', 'hidden': False}, {'_id': '67dbc49d85eacb364e913c3b', 'user': {'_id': '64a7c6e223622f7f189bcbe1', 'avatarUrl': '/avatars/4f13a7ed0d2b8d8dfff7dc650e46450a.svg', 'isPro': False, 'fullname': 'haiteng zhao', 'user': 'haitengzhao', 'type': 'user'}, 'name': 'Haiteng Zhao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T11:05:24.460Z', 'hidden': False}, {'_id': '67dbc49d85eacb364e913c3c', 'name': 'Jun Liu', 'hidden': False}, {'_id': '67dbc49d85eacb364e913c3d', 'user': {'_id': '66ac77011cfb12c087605acb', 'avatarUrl': '/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg', 'isPro': False, 'fullname': 'Lin', 'user': 'Qika', 'type': 'user'}, 'name': 'Qika Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T11:05:34.255Z', 'hidden': False}, {'_id': '67dbc49d85eacb364e913c3e', 'user': {'_id': '6280e830e99dccaac4bbfde5', 'avatarUrl': '/avatars/9242b8d2826ce2f79af9bb794bba2b61.svg', 'isPro': False, 'fullname': 'Zhiyong Wu', 'user': 'zy001', 'type': 'user'}, 'name': 'Zhiyong Wu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T11:05:41.288Z', 'hidden': False}], 'publishedAt': '2025-03-17T15:38:33.000Z', 'submittedOnDailyAt': '2025-03-20T06:08:48.330Z', 'title': 'φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\\n  Exploration and Exploitation', 'submittedOnDailyBy': {'_id': '64e6cf78ecce34cb442dc889', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg', 'isPro': False, 'fullname': 'Fangzhi Xu', 'user': 'xufangzhi', 'type': 'user'}, 'summary': 'Inference-time optimization scales computation to derive deliberate reasoning\\nsteps for effective performance. While previous search-based strategies address\\nthe short-sightedness of auto-regressive generation, the vast search space\\nleads to excessive exploration and insufficient exploitation. To strike an\\nefficient balance to derive the optimal step, we frame the decoding strategy as\\nforesight sampling, leveraging simulated future steps to obtain globally\\noptimal step estimation. Built on it, we propose a novel decoding strategy,\\nnamed phi-Decoding. To provide a precise and expressive estimation of step\\nvalue, phi-Decoding approximates two distributions via foresight and\\nclustering. Sampling from the joint distribution, the optimal steps can be\\nselected for exploitation. To support adaptive computation allocation, we\\npropose in-width and in-depth pruning strategies, featuring a light-weight\\nsolution to achieve inference efficiency. Extensive experiments across seven\\nbenchmarks show phi-Decoding outperforms strong baselines in both\\nperformance and efficiency. Additional analysis demonstrates its generalization\\nacross various LLMs and scalability across a wide range of computing budgets.\\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\\nopen-source PyPI package is coming soon.', 'upvotes': 37, 'discussionId': '67dbc49f85eacb364e913d20', 'githubRepo': 'https://github.com/xufangzhi/phi-Decoding', 'ai_keywords': ['inference-time optimization', 'auto-regressive generation', 'foresight sampling', '$\\\\phi$-Decoding', 'joint distribution', 'in-width and in-depth pruning', 'LLMs (Large Language Models)']}, 'publishedAt': '2025-03-17T11:38:33.000Z', 'title': 'φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\\n  Exploration and Exploitation', 'summary': 'Inference-time optimization scales computation to derive deliberate reasoning\\nsteps for effective performance. While previous search-based strategies address\\nthe short-sightedness of auto-regressive generation, the vast search space\\nleads to excessive exploration and insufficient exploitation. To strike an\\nefficient balance to derive the optimal step, we frame the decoding strategy as\\nforesight sampling, leveraging simulated future steps to obtain globally\\noptimal step estimation. Built on it, we propose a novel decoding strategy,\\nnamed phi-Decoding. To provide a precise and expressive estimation of step\\nvalue, phi-Decoding approximates two distributions via foresight and\\nclustering. Sampling from the joint distribution, the optimal steps can be\\nselected for exploitation. To support adaptive computation allocation, we\\npropose in-width and in-depth pruning strategies, featuring a light-weight\\nsolution to achieve inference efficiency. Extensive experiments across seven\\nbenchmarks show phi-Decoding outperforms strong baselines in both\\nperformance and efficiency. Additional analysis demonstrates its generalization\\nacross various LLMs and scalability across a wide range of computing budgets.\\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\\nopen-source PyPI package is coming soon.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13288.png', 'numComments': 1, 'submittedBy': {'_id': '64e6cf78ecce34cb442dc889', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg', 'fullname': 'Fangzhi Xu', 'name': 'xufangzhi', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 10}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.15265', 'authors': [{'_id': '67db8c4c9e4f93ee46411c1d', 'user': {'_id': '6522e4fbd89bc7773ddc4b58', 'avatarUrl': '/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg', 'isPro': False, 'fullname': 'Ruowen Zhao', 'user': 'zzzrw', 'type': 'user'}, 'name': 'Ruowen Zhao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:45:12.321Z', 'hidden': False}, {'_id': '67db8c4c9e4f93ee46411c1e', 'user': {'_id': '65a420cd90e65dc39a6abe9e', 'avatarUrl': '/avatars/81ac5b749043e899f5017782409f9e28.svg', 'isPro': False, 'fullname': 'yejunliang', 'user': 'yejunliang23', 'type': 'user'}, 'name': 'Junliang Ye', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:45:14.849Z', 'hidden': False}, {'_id': '67db8c4c9e4f93ee46411c1f', 'name': 'Zhengyi Wang', 'hidden': False}, {'_id': '67db8c4c9e4f93ee46411c20', 'user': {'_id': '674fcc541dbfdd4dee12d8e1', 'avatarUrl': '/avatars/f4adbfce8f7611fa5fce5f0f03d61a46.svg', 'isPro': False, 'fullname': 'Guangce Liu', 'user': 'guangce', 'type': 'user'}, 'name': 'Guangce Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T11:06:05.371Z', 'hidden': False}, {'_id': '67db8c4c9e4f93ee46411c21', 'user': {'_id': '6422a9669a00fbe9dfe665a1', 'avatarUrl': '/avatars/288bf5ea18205bcb0bbea29a304ffbbb.svg', 'isPro': False, 'fullname': 'Yiwen Chen', 'user': 'NTU-yiwen', 'type': 'user'}, 'name': 'Yiwen Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T11:06:35.743Z', 'hidden': False}, {'_id': '67db8c4c9e4f93ee46411c22', 'user': {'_id': '63463bc4547c70e4b7d3009f', 'avatarUrl': '/avatars/6e5350fd998f0a7a4143d7504218164a.svg', 'isPro': False, 'fullname': 'Yikai Wang', 'user': 'yikaiwang', 'type': 'user'}, 'name': 'Yikai Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T11:06:45.552Z', 'hidden': False}, {'_id': '67db8c4c9e4f93ee46411c23', 'name': 'Jun Zhu', 'hidden': False}], 'publishedAt': '2025-03-19T14:39:30.000Z', 'submittedOnDailyAt': '2025-03-20T03:22:40.364Z', 'title': 'DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\\n  Learning', 'submittedOnDailyBy': {'_id': '6522e4fbd89bc7773ddc4b58', 'avatarUrl': '/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg', 'isPro': False, 'fullname': 'Ruowen Zhao', 'user': 'zzzrw', 'type': 'user'}, 'summary': 'Triangle meshes play a crucial role in 3D applications for efficient\\nmanipulation and rendering. While auto-regressive methods generate structured\\nmeshes by predicting discrete vertex tokens, they are often constrained by\\nlimited face counts and mesh incompleteness. To address these challenges, we\\npropose DeepMesh, a framework that optimizes mesh generation through two key\\ninnovations: (1) an efficient pre-training strategy incorporating a novel\\ntokenization algorithm, along with improvements in data curation and\\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\\nmesh generation to achieve human preference alignment via Direct Preference\\nOptimization (DPO). We design a scoring standard that combines human evaluation\\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\\ngenerates meshes with intricate details and precise topology, outperforming\\nstate-of-the-art methods in both precision and quality. Project page:\\nhttps://zhaorw02.github.io/DeepMesh/', 'upvotes': 31, 'discussionId': '67db8c519e4f93ee46411d60', 'projectPage': 'https://zhaorw02.github.io/DeepMesh/', 'githubRepo': 'https://github.com/zhaorw02/DeepMesh', 'ai_keywords': ['triangle meshes', 'auto-regressive methods', 'discrete vertex tokens', 'face counts', 'mesh incompleteness', 'DeepMesh', 'tokenization algorithm', 'data curation', 'data processing', 'Reinforcement Learning (RL)', 'Direct Preference Optimization (DPO)', 'human evaluation', '3D metrics', 'point clouds', 'intricate details', 'precise topology', 'state-of-the-art methods', 'precision', 'quality']}, 'publishedAt': '2025-03-19T10:39:30.000Z', 'title': 'DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\\n  Learning', 'summary': 'Triangle meshes play a crucial role in 3D applications for efficient\\nmanipulation and rendering. While auto-regressive methods generate structured\\nmeshes by predicting discrete vertex tokens, they are often constrained by\\nlimited face counts and mesh incompleteness. To address these challenges, we\\npropose DeepMesh, a framework that optimizes mesh generation through two key\\ninnovations: (1) an efficient pre-training strategy incorporating a novel\\ntokenization algorithm, along with improvements in data curation and\\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\\nmesh generation to achieve human preference alignment via Direct Preference\\nOptimization (DPO). We design a scoring standard that combines human evaluation\\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\\ngenerates meshes with intricate details and precise topology, outperforming\\nstate-of-the-art methods in both precision and quality. Project page:\\nhttps://zhaorw02.github.io/DeepMesh/', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15265.png', 'numComments': 2, 'submittedBy': {'_id': '6522e4fbd89bc7773ddc4b58', 'avatarUrl': '/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg', 'fullname': 'Ruowen Zhao', 'name': 'zzzrw', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.15485', 'authors': [{'_id': '67db7dd224fe67fe45b21e63', 'user': {'_id': '6326e20bf0e99f96e024a164', 'avatarUrl': '/avatars/9be111af274cad965f72ed48c71d6122.svg', 'isPro': False, 'fullname': 'Zineng Tang', 'user': 'ZinengTang', 'type': 'user'}, 'name': 'Zineng Tang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T11:06:57.434Z', 'hidden': False}, {'_id': '67db7dd224fe67fe45b21e64', 'name': 'Long Lian', 'hidden': False}, {'_id': '67db7dd224fe67fe45b21e65', 'name': 'Seun Eisape', 'hidden': False}, {'_id': '67db7dd224fe67fe45b21e66', 'name': 'XuDong Wang', 'hidden': False}, {'_id': '67db7dd224fe67fe45b21e67', 'user': {'_id': '667c5764186b27ef806636d3', 'avatarUrl': '/avatars/5c08f0109bc0e350624112c0aff544f6.svg', 'isPro': False, 'fullname': 'Roei Herzig', 'user': 'roeiherz', 'type': 'user'}, 'name': 'Roei Herzig', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:19:56.648Z', 'hidden': False}, {'_id': '67db7dd224fe67fe45b21e68', 'user': {'_id': '6333a9195a032dcd095dda13', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1664329996201-noauth.jpeg', 'isPro': False, 'fullname': 'Adam Yala', 'user': 'yala', 'type': 'user'}, 'name': 'Adam Yala', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:45:44.806Z', 'hidden': False}, {'_id': '67db7dd224fe67fe45b21e69', 'user': {'_id': '6611e6e1188ff298b0dd0b79', 'avatarUrl': '/avatars/3a495283955ec9e06e1829c7eb2cd9a4.svg', 'isPro': False, 'fullname': 'Alane Suhr', 'user': 'alsuhr', 'type': 'user'}, 'name': 'Alane Suhr', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:20:09.062Z', 'hidden': False}, {'_id': '67db7dd224fe67fe45b21e6a', 'user': {'_id': '64cbdf02f103036e23d1c7f3', 'avatarUrl': '/avatars/496069463900dea20929b57381182d39.svg', 'isPro': False, 'fullname': 'Trevor Darrell', 'user': 'trevordarrell', 'type': 'user'}, 'name': 'Trevor Darrell', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:20:14.935Z', 'hidden': False}, {'_id': '67db7dd224fe67fe45b21e6b', 'user': {'_id': '6388f68c43d8b0797a09ff84', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg', 'isPro': False, 'fullname': 'David Chan', 'user': 'davidchan', 'type': 'user'}, 'name': 'David M. Chan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:20:24.455Z', 'hidden': False}], 'publishedAt': '2025-03-19T17:58:57.000Z', 'submittedOnDailyAt': '2025-03-20T01:01:18.127Z', 'title': 'TULIP: Towards Unified Language-Image Pretraining', 'submittedOnDailyBy': {'_id': '6388f68c43d8b0797a09ff84', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg', 'isPro': False, 'fullname': 'David Chan', 'user': 'davidchan', 'type': 'user'}, 'summary': 'Despite the recent success of image-text contrastive models like CLIP and\\nSigLIP, these models often struggle with vision-centric tasks that demand\\nhigh-fidelity image understanding, such as counting, depth estimation, and\\nfine-grained object recognition. These models, by performing language\\nalignment, tend to prioritize high-level semantics over visual understanding,\\nweakening their image understanding. On the other hand, vision-focused models\\nare great at processing visual information but struggle to understand language,\\nlimiting their flexibility for language-driven tasks. In this work, we\\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\\nmodels. Our method leverages generative data augmentation, enhanced image-image\\nand text-text contrastive learning, and image/text reconstruction\\nregularization to learn fine-grained visual features while preserving global\\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\\na 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot\\nclassification, and improving vision-language models, achieving over 3times\\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\\nhttps://tulip-berkeley.github.io', 'upvotes': 23, 'discussionId': '67db7dd424fe67fe45b21ee1', 'projectPage': 'https://tulip-berkeley.github.io/', 'ai_keywords': ['generative data augmentation', 'enhanced image-image and text-text contrastive learning', 'image/text reconstruction regularization', 'fine-grained visual features', 'global semantic alignment', 'zero-shot performance', 'few-shot classification', 'vision-language models']}, 'publishedAt': '2025-03-19T13:58:57.000Z', 'title': 'TULIP: Towards Unified Language-Image Pretraining', 'summary': 'Despite the recent success of image-text contrastive models like CLIP and\\nSigLIP, these models often struggle with vision-centric tasks that demand\\nhigh-fidelity image understanding, such as counting, depth estimation, and\\nfine-grained object recognition. These models, by performing language\\nalignment, tend to prioritize high-level semantics over visual understanding,\\nweakening their image understanding. On the other hand, vision-focused models\\nare great at processing visual information but struggle to understand language,\\nlimiting their flexibility for language-driven tasks. In this work, we\\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\\nmodels. Our method leverages generative data augmentation, enhanced image-image\\nand text-text contrastive learning, and image/text reconstruction\\nregularization to learn fine-grained visual features while preserving global\\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\\na 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot\\nclassification, and improving vision-language models, achieving over 3times\\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\\nhttps://tulip-berkeley.github.io', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15485.png', 'numComments': 1, 'submittedBy': {'_id': '6388f68c43d8b0797a09ff84', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg', 'fullname': 'David Chan', 'name': 'davidchan', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.15475', 'authors': [{'_id': '67db729fa720e711cff4d205', 'name': 'Foundation AI Team', 'hidden': False}, {'_id': '67db729fa720e711cff4d206', 'name': 'Kiran Bhat', 'hidden': False}, {'_id': '67db729fa720e711cff4d207', 'user': {'_id': '6439e28a43fab7b650dad215', 'avatarUrl': '/avatars/f0ac35d4a32750e02461fe9bbd61e6ff.svg', 'isPro': False, 'fullname': 'Nishchaie Khanna', 'user': 'nishchaie-roblox', 'type': 'user'}, 'name': 'Nishchaie Khanna', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:20:51.138Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d208', 'name': 'Karun Channa', 'hidden': False}, {'_id': '67db729fa720e711cff4d209', 'user': {'_id': '63899707ec1f539adc0f9206', 'avatarUrl': '/avatars/b758a7fc9f3125bac23d689856120607.svg', 'isPro': False, 'fullname': 'Tinghui Zhou', 'user': 'tinghuiz', 'type': 'user'}, 'name': 'Tinghui Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:21:01.173Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d20a', 'name': 'Yiheng Zhu', 'hidden': False}, {'_id': '67db729fa720e711cff4d20b', 'user': {'_id': '635c26accb0f36a40bba6b7f', 'avatarUrl': '/avatars/f16603da02a2f262a2e6e42b7b69d801.svg', 'isPro': False, 'fullname': 'Xiaoxia Sun', 'user': 'xiaoxiaroblox', 'type': 'user'}, 'name': 'Xiaoxia Sun', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:21:25.849Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d20c', 'name': 'Charles Shang', 'hidden': False}, {'_id': '67db729fa720e711cff4d20d', 'user': {'_id': '65c2a2460aa2d5313589e89f', 'avatarUrl': '/avatars/705395e754dbf44d26e9457114772df2.svg', 'isPro': False, 'fullname': 'Anirudh Sudarshan', 'user': 'animan123', 'type': 'user'}, 'name': 'Anirudh Sudarshan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:21:37.263Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d20e', 'name': 'Maurice Chu', 'hidden': False}, {'_id': '67db729fa720e711cff4d20f', 'user': {'_id': '633f44efc11d723b1809958b', 'avatarUrl': '/avatars/d70b067deabddae984c3637290b7de99.svg', 'isPro': False, 'fullname': 'Li', 'user': 'Daiqing', 'type': 'user'}, 'name': 'Daiqing Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:22:05.411Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d210', 'user': {'_id': '645d34ecce72244df7b29317', 'avatarUrl': '/avatars/1248933d9f89a15e67086325a8322d5e.svg', 'isPro': False, 'fullname': 'Kangle Deng', 'user': 'kangled', 'type': 'user'}, 'name': 'Kangle Deng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:22:11.753Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d211', 'user': {'_id': '62cd5c43299c0c2e0e437842', 'avatarUrl': '/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg', 'isPro': False, 'fullname': 'Jean-Philippe Fauconnier', 'user': 'j4kn', 'type': 'user'}, 'name': 'Jean-Philippe Fauconnier', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:22:18.262Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d212', 'user': {'_id': '6671bf698171db46e7b6e00f', 'avatarUrl': '/avatars/16d20f61bcbce540704575cd26a30ecb.svg', 'isPro': False, 'fullname': 'Tijmen Verhulsdonck', 'user': 'F1shcalledwanda', 'type': 'user'}, 'name': 'Tijmen Verhulsdonck', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:22:24.222Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d213', 'user': {'_id': '642bb3e44f379cdc3deb26cd', 'avatarUrl': '/avatars/556e792f013bcbe3748d7c9c719070d8.svg', 'isPro': False, 'fullname': 'Maneesh Agrawala', 'user': 'magrawala', 'type': 'user'}, 'name': 'Maneesh Agrawala', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:22:30.095Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d214', 'user': {'_id': '6501ceb6d6583ade3dff5399', 'avatarUrl': '/avatars/07fff5e56b9969642060acc43dd238fe.svg', 'isPro': False, 'fullname': 'Kayvon Fatahalian', 'user': 'kayvonf', 'type': 'user'}, 'name': 'Kayvon Fatahalian', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:22:36.801Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d215', 'user': {'_id': '6303eafb3926de1f7ec3d748', 'avatarUrl': '/avatars/5d23469a5005b00cc110cb0064bf6cee.svg', 'isPro': False, 'fullname': 'Alexander Weiss', 'user': 'abweiss', 'type': 'user'}, 'name': 'Alexander Weiss', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:22:43.610Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d216', 'user': {'_id': '6776731df76978b99ec9e5c3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/DZ22vXYR8ArefQUJt52Mz.png', 'isPro': False, 'fullname': 'Christian Reiser', 'user': 'creiser42', 'type': 'user'}, 'name': 'Christian Reiser', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:22:50.653Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d217', 'user': {'_id': '648b7b123d7ab530fb8fe898', 'avatarUrl': '/avatars/bf6647dbd06b7d20690d676c061cbd10.svg', 'isPro': False, 'fullname': 'Ravi Kiran Chirravuri', 'user': 'coders1122', 'type': 'user'}, 'name': 'Ravi Kiran Chirravuri', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:22:57.065Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d218', 'user': {'_id': '6752101a2f16c25ac9338292', 'avatarUrl': '/avatars/15ec542f5202905bf2f113a5dd736582.svg', 'isPro': False, 'fullname': 'Ravali Kandur', 'user': 'ravali607', 'type': 'user'}, 'name': 'Ravali Kandur', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:23:03.896Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d219', 'name': 'Alejandro Pelaez', 'hidden': False}, {'_id': '67db729fa720e711cff4d21a', 'name': 'Akash Garg', 'hidden': False}, {'_id': '67db729fa720e711cff4d21b', 'name': 'Michael Palleschi', 'hidden': False}, {'_id': '67db729fa720e711cff4d21c', 'name': 'Jessica Wang', 'hidden': False}, {'_id': '67db729fa720e711cff4d21d', 'name': 'Skylar Litz', 'hidden': False}, {'_id': '67db729fa720e711cff4d21e', 'name': 'Leon Liu', 'hidden': False}, {'_id': '67db729fa720e711cff4d21f', 'name': 'Anying Li', 'hidden': False}, {'_id': '67db729fa720e711cff4d220', 'name': 'David Harmon', 'hidden': False}, {'_id': '67db729fa720e711cff4d221', 'name': 'Derek Liu', 'hidden': False}, {'_id': '67db729fa720e711cff4d222', 'name': 'Liangjun Feng', 'hidden': False}, {'_id': '67db729fa720e711cff4d223', 'name': 'Denis Goupil', 'hidden': False}, {'_id': '67db729fa720e711cff4d224', 'name': 'Lukas Kuczynski', 'hidden': False}, {'_id': '67db729fa720e711cff4d225', 'name': 'Jihyun Yoon', 'hidden': False}, {'_id': '67db729fa720e711cff4d226', 'name': 'Naveen Marri', 'hidden': False}, {'_id': '67db729fa720e711cff4d227', 'name': 'Peiye Zhuang', 'hidden': False}, {'_id': '67db729fa720e711cff4d228', 'name': 'Yinan Zhang', 'hidden': False}, {'_id': '67db729fa720e711cff4d229', 'name': 'Brian Yin', 'hidden': False}, {'_id': '67db729fa720e711cff4d22a', 'name': 'Haomiao Jiang', 'hidden': False}, {'_id': '67db729fa720e711cff4d22b', 'user': {'_id': '664eca30e24897b66ca0b54b', 'avatarUrl': '/avatars/16b5e031a36b0d19bd55dae6524e124f.svg', 'isPro': False, 'fullname': 'Marcel van Workum', 'user': 'marcelvanworkum', 'type': 'user'}, 'name': 'Marcel van Workum', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:45:51.777Z', 'hidden': False}, {'_id': '67db729fa720e711cff4d22c', 'name': 'Thomas Lane', 'hidden': False}, {'_id': '67db729fa720e711cff4d22d', 'name': 'Bryce Erickson', 'hidden': False}, {'_id': '67db729fa720e711cff4d22e', 'name': 'Salil Pathare', 'hidden': False}, {'_id': '67db729fa720e711cff4d22f', 'name': 'Kyle Price', 'hidden': False}, {'_id': '67db729fa720e711cff4d230', 'name': 'Anupam Singh', 'hidden': False}, {'_id': '67db729fa720e711cff4d231', 'name': 'David Baszucki', 'hidden': False}], 'publishedAt': '2025-03-19T17:52:17.000Z', 'submittedOnDailyAt': '2025-03-20T00:57:52.833Z', 'title': 'Cube: A Roblox View of 3D Intelligence', 'submittedOnDailyBy': {'_id': '62cd5c43299c0c2e0e437842', 'avatarUrl': '/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg', 'isPro': False, 'fullname': 'Jean-Philippe Fauconnier', 'user': 'j4kn', 'type': 'user'}, 'summary': 'Foundation models trained on vast amounts of data have demonstrated\\nremarkable reasoning and generation capabilities in the domains of text,\\nimages, audio and video. Our goal at Roblox is to build such a foundation model\\nfor 3D intelligence, a model that can support developers in producing all\\naspects of a Roblox experience, from generating 3D objects and scenes to\\nrigging characters for animation to producing programmatic scripts describing\\nobject behaviors. We discuss three key design requirements for such a 3D\\nfoundation model and then present our first step towards building such a model.\\nWe expect that 3D geometric shapes will be a core data type and describe our\\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\\nused in applications for text-to-shape generation, shape-to-text generation and\\ntext-to-scene generation. We demonstrate how these applications can collaborate\\nwith existing large language models (LLMs) to perform scene analysis and\\nreasoning. We conclude with a discussion outlining our path to building a fully\\nunified foundation model for 3D intelligence.', 'upvotes': 17, 'discussionId': '67db72a1a720e711cff4d292', 'githubRepo': 'https://github.com/Roblox/cube', 'ai_keywords': ['3D foundation model', '3D geometric shapes', '3D shape tokenizer', 'text-to-shape generation', 'shape-to-text generation', 'text-to-scene generation', 'large language models (LLMs)', 'scene analysis', 'reasoning', 'unified foundation model']}, 'publishedAt': '2025-03-19T13:52:17.000Z', 'title': 'Cube: A Roblox View of 3D Intelligence', 'summary': 'Foundation models trained on vast amounts of data have demonstrated\\nremarkable reasoning and generation capabilities in the domains of text,\\nimages, audio and video. Our goal at Roblox is to build such a foundation model\\nfor 3D intelligence, a model that can support developers in producing all\\naspects of a Roblox experience, from generating 3D objects and scenes to\\nrigging characters for animation to producing programmatic scripts describing\\nobject behaviors. We discuss three key design requirements for such a 3D\\nfoundation model and then present our first step towards building such a model.\\nWe expect that 3D geometric shapes will be a core data type and describe our\\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\\nused in applications for text-to-shape generation, shape-to-text generation and\\ntext-to-scene generation. We demonstrate how these applications can collaborate\\nwith existing large language models (LLMs) to perform scene analysis and\\nreasoning. We conclude with a discussion outlining our path to building a fully\\nunified foundation model for 3D intelligence.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15475.png', 'numComments': 1, 'submittedBy': {'_id': '62cd5c43299c0c2e0e437842', 'avatarUrl': '/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg', 'fullname': 'Jean-Philippe Fauconnier', 'name': 'j4kn', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.15417', 'authors': [{'_id': '67db8e05842d8b6642a135d0', 'user': {'_id': '6570450a78d7aca0c361a177', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg', 'isPro': False, 'fullname': 'Harold Chen', 'user': 'Harold328', 'type': 'user'}, 'name': 'Harold Haodong Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:45:09.768Z', 'hidden': False}, {'_id': '67db8e05842d8b6642a135d1', 'user': {'_id': '657a776153e2fa36fed33259', 'avatarUrl': '/avatars/f60777abe61bc6743e4cef0ede301295.svg', 'isPro': False, 'fullname': 'Haojian Huang', 'user': 'Jethro37', 'type': 'user'}, 'name': 'Haojian Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:24:10.070Z', 'hidden': False}, {'_id': '67db8e05842d8b6642a135d2', 'user': {'_id': '6581a9e2e4bcbca0322e3608', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IxvVmIg2YQaqa0ZEV6JPa.png', 'isPro': False, 'fullname': 'Xianfeng Wu', 'user': 'Beckham808', 'type': 'user'}, 'name': 'Xianfeng Wu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:24:16.210Z', 'hidden': False}, {'_id': '67db8e05842d8b6642a135d3', 'user': {'_id': '6497ff395b5d43c1c77d2a11', 'avatarUrl': '/avatars/e3704701ed3ea0c1c165001521f40086.svg', 'isPro': False, 'fullname': 'Yexin Liu', 'user': 'AI4VR', 'type': 'user'}, 'name': 'Yexin Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:24:22.201Z', 'hidden': False}, {'_id': '67db8e05842d8b6642a135d4', 'user': {'_id': '66e15e6a9cbd604971999bf2', 'avatarUrl': '/avatars/dc02874bbe33e33c9f0dad4b40e55e01.svg', 'isPro': False, 'fullname': 'Yajing Bai', 'user': 'YajingB', 'type': 'user'}, 'name': 'Yajing Bai', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:24:28.817Z', 'hidden': False}, {'_id': '67db8e05842d8b6642a135d5', 'name': 'Wen-Jie Shu', 'hidden': False}, {'_id': '67db8e05842d8b6642a135d6', 'name': 'Harry Yang', 'hidden': False}, {'_id': '67db8e05842d8b6642a135d7', 'user': {'_id': '65884498ce38d143c435d279', 'avatarUrl': '/avatars/96a917c40680a79a37dcb8fa75014c21.svg', 'isPro': False, 'fullname': 'Ser Nam Lim', 'user': 'sernam', 'type': 'user'}, 'name': 'Ser-Nam Lim', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:24:48.864Z', 'hidden': False}], 'publishedAt': '2025-03-19T16:59:32.000Z', 'submittedOnDailyAt': '2025-03-20T02:10:52.068Z', 'title': 'Temporal Regularization Makes Your Video Generator Stronger', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': 'Temporal quality is a critical aspect of video generation, as it ensures\\nconsistent motion and realistic dynamics across frames. However, achieving high\\ntemporal coherence and diversity remains challenging. In this work, we explore\\ntemporal augmentation in video generation for the first time, and introduce\\nFluxFlow for initial investigation, a strategy designed to enhance temporal\\nquality. Operating at the data level, FluxFlow applies controlled temporal\\nperturbations without requiring architectural modifications. Extensive\\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\\nsignificantly improves temporal coherence and diversity across various video\\ngeneration models, including U-Net, DiT, and AR-based architectures, while\\npreserving spatial fidelity. These findings highlight the potential of temporal\\naugmentation as a simple yet effective approach to advancing video generation\\nquality.', 'upvotes': 17, 'discussionId': '67db8e07842d8b6642a1365f', 'projectPage': 'https://haroldchen19.github.io/FluxFlow/', 'ai_keywords': ['temporal augmentation', 'FluxFlow', 'temporal perturbations', 'temporal quality', 'temporal coherence', 'UCF-101', 'VBench', 'U-Net', 'DiT', 'AR-based architectures', 'spatial fidelity']}, 'publishedAt': '2025-03-19T12:59:32.000Z', 'title': 'Temporal Regularization Makes Your Video Generator Stronger', 'summary': 'Temporal quality is a critical aspect of video generation, as it ensures\\nconsistent motion and realistic dynamics across frames. However, achieving high\\ntemporal coherence and diversity remains challenging. In this work, we explore\\ntemporal augmentation in video generation for the first time, and introduce\\nFluxFlow for initial investigation, a strategy designed to enhance temporal\\nquality. Operating at the data level, FluxFlow applies controlled temporal\\nperturbations without requiring architectural modifications. Extensive\\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\\nsignificantly improves temporal coherence and diversity across various video\\ngeneration models, including U-Net, DiT, and AR-based architectures, while\\npreserving spatial fidelity. These findings highlight the potential of temporal\\naugmentation as a simple yet effective approach to advancing video generation\\nquality.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15417.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 6409}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.14868', 'authors': [{'_id': '67db9f06842d8b6642a5eeaf', 'user': {'_id': '633e6f07309a99325095dd42', 'avatarUrl': '/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg', 'isPro': False, 'fullname': 'Hoigi Seo', 'user': 'Agorium', 'type': 'user'}, 'name': 'Hoigi Seo', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:45:07.668Z', 'hidden': False}, {'_id': '67db9f06842d8b6642a5eeb0', 'name': 'Wongi Jeong', 'hidden': False}, {'_id': '67db9f06842d8b6642a5eeb1', 'user': {'_id': '67076244679d36bc0e3eda5b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0yE8cHzBuz9QA7sE8rQkf.png', 'isPro': False, 'fullname': 'Kyungryeol Lee', 'user': 'hirussell', 'type': 'user'}, 'name': 'Kyungryeol Lee', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:23:58.734Z', 'hidden': False}, {'_id': '67db9f06842d8b6642a5eeb2', 'name': 'Se Young Chun', 'hidden': False}], 'publishedAt': '2025-03-19T03:45:37.000Z', 'submittedOnDailyAt': '2025-03-20T03:25:51.779Z', 'title': 'Efficient Personalization of Quantized Diffusion Model without\\n  Backpropagation', 'submittedOnDailyBy': {'_id': '633e6f07309a99325095dd42', 'avatarUrl': '/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg', 'isPro': False, 'fullname': 'Hoigi Seo', 'user': 'Agorium', 'type': 'user'}, 'summary': 'Diffusion models have shown remarkable performance in image synthesis, but\\nthey demand extensive computational and memory resources for training,\\nfine-tuning and inference. Although advanced quantization techniques have\\nsuccessfully minimized memory usage for inference, training and fine-tuning\\nthese quantized models still require large memory possibly due to\\ndequantization for accurate computation of gradients and/or backpropagation for\\ngradient-based algorithms. However, memory-efficient fine-tuning is\\nparticularly desirable for applications such as personalization that often must\\nbe run on edge devices like mobile phones with private data. In this work, we\\naddress this challenge by quantizing a diffusion model with personalization via\\nTextual Inversion and by leveraging a zeroth-order optimization on\\npersonalization tokens without dequantization so that it does not require\\ngradient and activation storage for backpropagation that consumes considerable\\nmemory. Since a gradient estimation using zeroth-order optimization is quite\\nnoisy for a single or a few images in personalization, we propose to denoise\\nthe estimated gradient by projecting it onto a subspace that is constructed\\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\\ninvestigated the influence of text embedding in image generation, leading to\\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\\nsampling with effective diffusion timesteps. Our method achieves comparable\\nperformance to prior methods in image and text alignment scores for\\npersonalizing Stable Diffusion with only forward passes while reducing training\\nmemory demand up to 8.2times.', 'upvotes': 17, 'discussionId': '67db9f11842d8b6642a5f165', 'projectPage': 'https://ignoww.github.io/ZOODiP_project/', 'githubRepo': 'https://github.com/ignoww/ZOODiP', 'ai_keywords': ['diffusion models', 'image synthesis', 'quantization techniques', 'dequantization', 'gradient-based algorithms', 'memory-efficient fine-tuning', 'Textual Inversion', 'zeroth-order optimization', 'personalization tokens', 'gradient estimation', 'Subspace Gradient', 'subspace projection', 'text embedding', 'Partial Uniform Timestep Sampling', 'diffusion timesteps', 'Stable Diffusion', 'image and text alignment scores']}, 'publishedAt': '2025-03-18T23:45:37.000Z', 'title': 'Efficient Personalization of Quantized Diffusion Model without\\n  Backpropagation', 'summary': 'Diffusion models have shown remarkable performance in image synthesis, but\\nthey demand extensive computational and memory resources for training,\\nfine-tuning and inference. Although advanced quantization techniques have\\nsuccessfully minimized memory usage for inference, training and fine-tuning\\nthese quantized models still require large memory possibly due to\\ndequantization for accurate computation of gradients and/or backpropagation for\\ngradient-based algorithms. However, memory-efficient fine-tuning is\\nparticularly desirable for applications such as personalization that often must\\nbe run on edge devices like mobile phones with private data. In this work, we\\naddress this challenge by quantizing a diffusion model with personalization via\\nTextual Inversion and by leveraging a zeroth-order optimization on\\npersonalization tokens without dequantization so that it does not require\\ngradient and activation storage for backpropagation that consumes considerable\\nmemory. Since a gradient estimation using zeroth-order optimization is quite\\nnoisy for a single or a few images in personalization, we propose to denoise\\nthe estimated gradient by projecting it onto a subspace that is constructed\\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\\ninvestigated the influence of text embedding in image generation, leading to\\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\\nsampling with effective diffusion timesteps. Our method achieves comparable\\nperformance to prior methods in image and text alignment scores for\\npersonalizing Stable Diffusion with only forward passes while reducing training\\nmemory demand up to 8.2times.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14868.png', 'numComments': 1, 'submittedBy': {'_id': '633e6f07309a99325095dd42', 'avatarUrl': '/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg', 'fullname': 'Hoigi Seo', 'name': 'Agorium', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.14891', 'authors': [{'_id': '67dc1aeab91614cb077cbdf4', 'name': 'Honglin Lin', 'hidden': False}, {'_id': '67dc1aeab91614cb077cbdf5', 'name': 'Zhuoshi Pan', 'hidden': False}, {'_id': '67dc1aeab91614cb077cbdf6', 'name': 'Yu Li', 'hidden': False}, {'_id': '67dc1aeab91614cb077cbdf7', 'name': 'Qizhi Pei', 'hidden': False}, {'_id': '67dc1aeab91614cb077cbdf8', 'name': 'Xin Gao', 'hidden': False}, {'_id': '67dc1aeab91614cb077cbdf9', 'name': 'Mengzhang Cai', 'hidden': False}, {'_id': '67dc1aeab91614cb077cbdfa', 'name': 'Conghui He', 'hidden': False}, {'_id': '67dc1aeab91614cb077cbdfb', 'name': 'Lijun Wu', 'hidden': False}], 'publishedAt': '2025-03-19T04:36:35.000Z', 'submittedOnDailyAt': '2025-03-20T12:14:29.841Z', 'title': 'MetaLadder: Ascending Mathematical Solution Quality via\\n  Analogical-Problem Reasoning Transfer', 'submittedOnDailyBy': {'_id': '6397f6081323f19c578f142e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg', 'isPro': False, 'fullname': 'QizhiPei', 'user': 'QizhiPei', 'type': 'user'}, 'summary': 'Large Language Models (LLMs) have demonstrated promising capabilities in\\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\\na vital component in guiding answer generation. Current paradigms typically\\ngenerate CoT and answers directly for a given problem, diverging from human\\nproblem-solving strategies to some extent. Humans often solve problems by\\nrecalling analogous cases and leveraging their solutions to reason about the\\ncurrent task. Inspired by this cognitive process, we propose\\nMetaLadder, a novel framework that explicitly prompts LLMs to recall\\nand reflect on meta-problems, those structurally or semantically analogous\\nproblems, alongside their CoT solutions before addressing the target problem.\\nAdditionally, we introduce a problem-restating mechanism to enhance the model\\'s\\ncomprehension of the target problem by regenerating the original question,\\nwhich further improves reasoning accuracy. Therefore, the model can achieve\\nreasoning transfer from analogical problems, mimicking human-like \"learning\\nfrom examples\" and generalization abilities. Extensive experiments on\\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\\nLLMs\\' problem-solving accuracy, largely outperforming standard CoT-based\\nmethods (10.3\\\\% accuracy gain) and other methods. Our code and data\\nhas been released at https://github.com/LHL3341/MetaLadder.', 'upvotes': 12, 'discussionId': '67dc1aeab91614cb077cbe31', 'githubRepo': 'https://github.com/LHL3341/MetaLadder', 'ai_keywords': ['MetaLadder', 'Chain-of-Thought (CoT)', 'meta-problems', 'problem-restating mechanism', 'reasoning transfer', 'learning from examples', 'generalization abilities']}, 'publishedAt': '2025-03-19T00:36:35.000Z', 'title': 'MetaLadder: Ascending Mathematical Solution Quality via\\n  Analogical-Problem Reasoning Transfer', 'summary': 'Large Language Models (LLMs) have demonstrated promising capabilities in\\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\\na vital component in guiding answer generation. Current paradigms typically\\ngenerate CoT and answers directly for a given problem, diverging from human\\nproblem-solving strategies to some extent. Humans often solve problems by\\nrecalling analogous cases and leveraging their solutions to reason about the\\ncurrent task. Inspired by this cognitive process, we propose\\nMetaLadder, a novel framework that explicitly prompts LLMs to recall\\nand reflect on meta-problems, those structurally or semantically analogous\\nproblems, alongside their CoT solutions before addressing the target problem.\\nAdditionally, we introduce a problem-restating mechanism to enhance the model\\'s\\ncomprehension of the target problem by regenerating the original question,\\nwhich further improves reasoning accuracy. Therefore, the model can achieve\\nreasoning transfer from analogical problems, mimicking human-like \"learning\\nfrom examples\" and generalization abilities. Extensive experiments on\\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\\nLLMs\\' problem-solving accuracy, largely outperforming standard CoT-based\\nmethods (10.3\\\\% accuracy gain) and other methods. Our code and data\\nhas been released at https://github.com/LHL3341/MetaLadder.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14891.png', 'numComments': 1, 'submittedBy': {'_id': '6397f6081323f19c578f142e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg', 'fullname': 'QizhiPei', 'name': 'QizhiPei', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 16}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.12532', 'authors': [{'_id': '67da1df040371958e1732c83', 'user': {'_id': '645b6094bc7518912e1fbc34', 'avatarUrl': '/avatars/5e1a875ba3ce350e71fe7049ca6a44c1.svg', 'isPro': False, 'fullname': 'Fanbin Lu', 'user': 'Fanbin', 'type': 'user'}, 'name': 'Fanbin Lu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:25:02.039Z', 'hidden': False}, {'_id': '67da1df040371958e1732c84', 'user': {'_id': '66371198c8950692a42f19d9', 'avatarUrl': '/avatars/64802642be361df8219f4f645f1335c4.svg', 'isPro': False, 'fullname': 'Zhisheng Zhong', 'user': 'ZhishengZhong', 'type': 'user'}, 'name': 'Zhisheng Zhong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:25:09.861Z', 'hidden': False}, {'_id': '67da1df040371958e1732c85', 'name': 'Ziqin Wei', 'hidden': False}, {'_id': '67da1df040371958e1732c86', 'name': 'Shu Liu', 'hidden': False}, {'_id': '67da1df040371958e1732c87', 'name': 'Chi-Wing Fu', 'hidden': False}, {'_id': '67da1df040371958e1732c88', 'name': 'Jiaya Jia', 'hidden': False}], 'publishedAt': '2025-03-16T14:53:43.000Z', 'submittedOnDailyAt': '2025-03-20T01:38:29.350Z', 'title': 'STEVE: AStep Verification Pipeline for Computer-use Agent Training', 'submittedOnDailyBy': {'_id': '6418554a0956be7233a1023e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png', 'isPro': False, 'fullname': 'zhang yuechen', 'user': 'julianjuaner', 'type': 'user'}, 'summary': 'Developing AI agents to autonomously manipulate graphical user interfaces is\\na long challenging task. Recent advances in data scaling law inspire us to\\ntrain computer-use agents with a scaled instruction set, yet using behavior\\ncloning to train agents still requires immense high-quality trajectories. To\\nmeet the scalability need, we designed STEVE, a step verification pipeline for\\ncomputer-use agent training. First, we establish a large instruction set for\\ncomputer-use agents and collect trajectory data with some suboptimal agents.\\nGPT-4o is used to verify the correctness of each step in the trajectories based\\non the screens before and after the action execution, assigning each step with\\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\\noptimize the agent from the binary stepwise labels. Extensive experiments\\nmanifest that our agent outperforms supervised finetuning by leveraging both\\npositive and negative actions within a trajectory. Also, STEVE enables us to\\ntrain a 7B vision-language model as a computer-use agent, achieving leading\\nperformance in the challenging live desktop environment WinAgentArena with\\ngreat efficiency at a reduced cost. Code and data:\\nhttps://github.com/FanbinLu/STEVE.', 'upvotes': 11, 'discussionId': '67da1df240371958e1732d2f', 'githubRepo': 'https://github.com/FanbinLu/STEVE', 'ai_keywords': ['behavior cloning', 'trajectory data', 'suboptimal agents', 'GPT-4o', 'step verification pipeline', 'correctness verification', 'Kahneman and Tversky Optimization', 'positive actions', 'negative actions', 'vision-language model', 'computer-use agent', 'live desktop environment', 'WinAgentArena']}, 'publishedAt': '2025-03-16T10:53:43.000Z', 'title': 'STEVE: AStep Verification Pipeline for Computer-use Agent Training', 'summary': 'Developing AI agents to autonomously manipulate graphical user interfaces is\\na long challenging task. Recent advances in data scaling law inspire us to\\ntrain computer-use agents with a scaled instruction set, yet using behavior\\ncloning to train agents still requires immense high-quality trajectories. To\\nmeet the scalability need, we designed STEVE, a step verification pipeline for\\ncomputer-use agent training. First, we establish a large instruction set for\\ncomputer-use agents and collect trajectory data with some suboptimal agents.\\nGPT-4o is used to verify the correctness of each step in the trajectories based\\non the screens before and after the action execution, assigning each step with\\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\\noptimize the agent from the binary stepwise labels. Extensive experiments\\nmanifest that our agent outperforms supervised finetuning by leveraging both\\npositive and negative actions within a trajectory. Also, STEVE enables us to\\ntrain a 7B vision-language model as a computer-use agent, achieving leading\\nperformance in the challenging live desktop environment WinAgentArena with\\ngreat efficiency at a reduced cost. Code and data:\\nhttps://github.com/FanbinLu/STEVE.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12532.png', 'numComments': 1, 'submittedBy': {'_id': '6418554a0956be7233a1023e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png', 'fullname': 'zhang yuechen', 'name': 'julianjuaner', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.15354', 'authors': [{'_id': '67dc1db899d011efd7da645e', 'name': 'Yining Lu', 'hidden': False}, {'_id': '67dc1db899d011efd7da645f', 'name': 'Noah Ziems', 'hidden': False}, {'_id': '67dc1db899d011efd7da6460', 'name': 'Hy Dang', 'hidden': False}, {'_id': '67dc1db899d011efd7da6461', 'name': 'Meng Jiang', 'hidden': False}], 'publishedAt': '2025-03-19T15:56:21.000Z', 'submittedOnDailyAt': '2025-03-20T12:27:01.982Z', 'title': 'Optimizing Decomposition for Optimal Claim Verification', 'submittedOnDailyBy': {'_id': '642f742270daaa6e7209a2c8', 'avatarUrl': '/avatars/746fb840c220329f69a17905ea519322.svg', 'isPro': False, 'fullname': 'Yining Lu', 'user': 'ylu610', 'type': 'user'}, 'summary': 'Current research on the Decompose-Then-Verify paradigm for\\nevaluating the factuality of long-form text typically treats decomposition and\\nverification in isolation, overlooking their interactions and potential\\nmisalignment. We find that existing decomposition policies, typically\\nhand-crafted demonstrations, do not align well with downstream verifiers in\\nterms of atomicity -- a novel metric quantifying information density -- leading\\nto suboptimal verification results. We formulate finding the optimal\\ndecomposition policy for optimal verification as a bilevel optimization\\nproblem. To approximate a solution for this strongly NP-hard problem, we\\npropose dynamic decomposition, a reinforcement learning framework that\\nleverages verifier feedback to learn a policy for dynamically decomposing\\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\\ndecomposition outperforms existing decomposition policies, improving\\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\\naverage across varying verifiers, datasets, and atomcities of input claims.', 'upvotes': 10, 'discussionId': '67dc1db999d011efd7da64a7', 'githubRepo': 'https://github.com/yining610/dynamic-decomposition', 'ai_keywords': ['bilevel optimization problem', 'dynamic decomposition', 'reinforcement learning framework']}, 'publishedAt': '2025-03-19T11:56:21.000Z', 'title': 'Optimizing Decomposition for Optimal Claim Verification', 'summary': 'Current research on the Decompose-Then-Verify paradigm for\\nevaluating the factuality of long-form text typically treats decomposition and\\nverification in isolation, overlooking their interactions and potential\\nmisalignment. We find that existing decomposition policies, typically\\nhand-crafted demonstrations, do not align well with downstream verifiers in\\nterms of atomicity -- a novel metric quantifying information density -- leading\\nto suboptimal verification results. We formulate finding the optimal\\ndecomposition policy for optimal verification as a bilevel optimization\\nproblem. To approximate a solution for this strongly NP-hard problem, we\\npropose dynamic decomposition, a reinforcement learning framework that\\nleverages verifier feedback to learn a policy for dynamically decomposing\\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\\ndecomposition outperforms existing decomposition policies, improving\\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\\naverage across varying verifiers, datasets, and atomcities of input claims.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15354.png', 'numComments': 1, 'submittedBy': {'_id': '642f742270daaa6e7209a2c8', 'avatarUrl': '/avatars/746fb840c220329f69a17905ea519322.svg', 'fullname': 'Yining Lu', 'name': 'ylu610', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.11557', 'authors': [{'_id': '67d8889acf12ecfbbb2f109c', 'user': {'_id': '675cae2c127c72c5682a4df6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/675cae2c127c72c5682a4df6/Fpkm1ilmoIpjPR1j8ipSD.png', 'isPro': False, 'fullname': 'jing bi', 'user': 'jing-bi', 'type': 'user'}, 'name': 'Jing Bi', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-18T08:08:56.144Z', 'hidden': False}, {'_id': '67d8889acf12ecfbbb2f109d', 'name': 'Junjia Guo', 'hidden': False}, {'_id': '67d8889acf12ecfbbb2f109e', 'name': 'Susan Liang', 'hidden': False}, {'_id': '67d8889acf12ecfbbb2f109f', 'user': {'_id': '65970f61719cf10fc5c28d85', 'avatarUrl': '/avatars/1cad14d28e9a53f5283a139c856690df.svg', 'isPro': False, 'fullname': 'Guangyu Sun', 'user': 'imguangyu', 'type': 'user'}, 'name': 'Guangyu Sun', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-03-17T20:42:24.004Z', 'hidden': False}, {'_id': '67d8889acf12ecfbbb2f10a0', 'name': 'Luchuan Song', 'hidden': False}, {'_id': '67d8889acf12ecfbbb2f10a1', 'name': 'Yunlong Tang', 'hidden': False}, {'_id': '67d8889acf12ecfbbb2f10a2', 'name': 'Jinxi He', 'hidden': False}, {'_id': '67d8889acf12ecfbbb2f10a3', 'name': 'Jiarui Wu', 'hidden': False}, {'_id': '67d8889acf12ecfbbb2f10a4', 'name': 'Ali Vosoughi', 'hidden': False}, {'_id': '67d8889acf12ecfbbb2f10a5', 'name': 'Chen Chen', 'hidden': False}, {'_id': '67d8889acf12ecfbbb2f10a6', 'name': 'Chenliang Xu', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/675cae2c127c72c5682a4df6/EqgBdRXowuWEve4uYBK4u.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/675cae2c127c72c5682a4df6/JQHAG-UHUMDX9hHHM9_Jk.png'], 'publishedAt': '2025-03-14T16:26:11.000Z', 'submittedOnDailyAt': '2025-03-20T13:11:14.839Z', 'title': 'VERIFY: A Benchmark of Visual Explanation and Reasoning for\\n  Investigating Multimodal Reasoning Fidelity', 'submittedOnDailyBy': {'_id': '675cae2c127c72c5682a4df6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/675cae2c127c72c5682a4df6/Fpkm1ilmoIpjPR1j8ipSD.png', 'isPro': False, 'fullname': 'jing bi', 'user': 'jing-bi', 'type': 'user'}, 'summary': 'Visual reasoning is central to human cognition, enabling individuals to\\ninterpret and abstractly understand their environment. Although recent\\nMultimodal Large Language Models (MLLMs) have demonstrated impressive\\nperformance across language and vision-language tasks, existing benchmarks\\nprimarily measure recognition-based skills and inadequately assess true visual\\nreasoning capabilities. To bridge this critical gap, we introduce VERIFY, a\\nbenchmark explicitly designed to isolate and rigorously evaluate the visual\\nreasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to\\nreason primarily from visual information, providing minimal textual context to\\nreduce reliance on domain-specific knowledge and linguistic biases. Each\\nproblem is accompanied by a human-annotated reasoning path, making it the first\\nto provide in-depth evaluation of model decision-making processes.\\nAdditionally, we propose novel metrics that assess visual reasoning fidelity\\nbeyond mere accuracy, highlighting critical imbalances in current model\\nreasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers\\nsignificant limitations, underscoring the need for a balanced and holistic\\napproach to both perception and reasoning. For more teaser and testing, visit\\nour project page (https://verify-eqh.pages.dev/).', 'upvotes': 10, 'discussionId': '67d8889dcf12ecfbbb2f1192', 'projectPage': 'https://proj-verify.jing.vision/', 'ai_keywords': ['Multimodal Large Language Models (MLLMs)', 'visual reasoning capabilities', 'recognition-based skills', 'benchmark', 'reasoning path', 'visual reasoning fidelity']}, 'publishedAt': '2025-03-14T12:26:11.000Z', 'title': 'VERIFY: A Benchmark of Visual Explanation and Reasoning for\\n  Investigating Multimodal Reasoning Fidelity', 'summary': 'Visual reasoning is central to human cognition, enabling individuals to\\ninterpret and abstractly understand their environment. Although recent\\nMultimodal Large Language Models (MLLMs) have demonstrated impressive\\nperformance across language and vision-language tasks, existing benchmarks\\nprimarily measure recognition-based skills and inadequately assess true visual\\nreasoning capabilities. To bridge this critical gap, we introduce VERIFY, a\\nbenchmark explicitly designed to isolate and rigorously evaluate the visual\\nreasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to\\nreason primarily from visual information, providing minimal textual context to\\nreduce reliance on domain-specific knowledge and linguistic biases. Each\\nproblem is accompanied by a human-annotated reasoning path, making it the first\\nto provide in-depth evaluation of model decision-making processes.\\nAdditionally, we propose novel metrics that assess visual reasoning fidelity\\nbeyond mere accuracy, highlighting critical imbalances in current model\\nreasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers\\nsignificant limitations, underscoring the need for a balanced and holistic\\napproach to both perception and reasoning. For more teaser and testing, visit\\nour project page (https://verify-eqh.pages.dev/).', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/675cae2c127c72c5682a4df6/EqgBdRXowuWEve4uYBK4u.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/675cae2c127c72c5682a4df6/JQHAG-UHUMDX9hHHM9_Jk.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11557.png', 'numComments': 1, 'submittedBy': {'_id': '675cae2c127c72c5682a4df6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/675cae2c127c72c5682a4df6/Fpkm1ilmoIpjPR1j8ipSD.png', 'fullname': 'jing bi', 'name': 'jing-bi', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.15264', 'authors': [{'_id': '67dbbe8fafd5251fc6b55730', 'user': {'_id': '66c598b536a75deef8bb21c0', 'avatarUrl': '/avatars/b3721202d01d3c947c36f0beae43ea1c.svg', 'isPro': False, 'fullname': 'Hengrui Kang', 'user': 'khr0516', 'type': 'user'}, 'name': 'Hengrui Kang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:45:05.833Z', 'hidden': False}, {'_id': '67dbbe8fafd5251fc6b55731', 'name': 'Siwei Wen', 'hidden': False}, {'_id': '67dbbe8fafd5251fc6b55732', 'user': {'_id': '653b8c3e97a4d71d950e2f20', 'avatarUrl': '/avatars/b68880022e14556d0be58c69615db3be.svg', 'isPro': False, 'fullname': 'Zichen Wen', 'user': 'zichenwen', 'type': 'user'}, 'name': 'Zichen Wen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:45:04.004Z', 'hidden': False}, {'_id': '67dbbe8fafd5251fc6b55733', 'user': {'_id': '66978ee0b8656f6506b4acb2', 'avatarUrl': '/avatars/298acb8222e189fce4368985ee5374a1.svg', 'isPro': False, 'fullname': 'Junyan Ye', 'user': 'Yejy53', 'type': 'user'}, 'name': 'Junyan Ye', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:27:18.626Z', 'hidden': False}, {'_id': '67dbbe8fafd5251fc6b55734', 'user': {'_id': '66d5b56c77a026c3d2086a79', 'avatarUrl': '/avatars/45da07fd82fd455955faa05b27a6393f.svg', 'isPro': False, 'fullname': 'Weijia Li', 'user': 'liweijia', 'type': 'user'}, 'name': 'Weijia Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:27:12.349Z', 'hidden': False}, {'_id': '67dbbe8fafd5251fc6b55735', 'user': {'_id': '65f95363ca387c9d45a2d2ad', 'avatarUrl': '/avatars/8e2e8ff5f3b8fbd6d24f0f8947ca5ef4.svg', 'isPro': False, 'fullname': 'Peilin Feng', 'user': 'Sssunset', 'type': 'user'}, 'name': 'Peilin Feng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:27:04.917Z', 'hidden': False}, {'_id': '67dbbe8fafd5251fc6b55736', 'user': {'_id': '646325085897b675c65aea0f', 'avatarUrl': '/avatars/28ce7388f9318b49bdd0a5594c0f6732.svg', 'isPro': False, 'fullname': 'Baichuan Zhou', 'user': 'bczhou', 'type': 'user'}, 'name': 'Baichuan Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:26:58.914Z', 'hidden': False}, {'_id': '67dbbe8fafd5251fc6b55737', 'name': 'Bin Wang', 'hidden': False}, {'_id': '67dbbe8fafd5251fc6b55738', 'user': {'_id': '636317ed80c1a705a6eff396', 'avatarUrl': '/avatars/3db090e101b916d9256d0d3e043db71d.svg', 'isPro': False, 'fullname': 'Dahua Lin', 'user': 'lindahua', 'type': 'user'}, 'name': 'Dahua Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:26:52.306Z', 'hidden': False}, {'_id': '67dbbe8fafd5251fc6b55739', 'user': {'_id': '642ec9831d1737803dc1c30a', 'avatarUrl': '/avatars/c9ded838bad09004c15a27200e66a108.svg', 'isPro': False, 'fullname': 'linfeng zhang', 'user': 'linfengZ', 'type': 'user'}, 'name': 'Linfeng Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:26:43.997Z', 'hidden': False}, {'_id': '67dbbe8fafd5251fc6b5573a', 'user': {'_id': '63f9fca8d4349b157a109eec', 'avatarUrl': '/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg', 'isPro': False, 'fullname': 'Conghui He', 'user': 'conghui', 'type': 'user'}, 'name': 'Conghui He', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:26:34.087Z', 'hidden': False}], 'publishedAt': '2025-03-19T14:37:21.000Z', 'submittedOnDailyAt': '2025-03-20T05:54:43.430Z', 'title': 'LEGION: Learning to Ground and Explain for Synthetic Image Detection', 'submittedOnDailyBy': {'_id': '653b8c3e97a4d71d950e2f20', 'avatarUrl': '/avatars/b68880022e14556d0be58c69615db3be.svg', 'isPro': False, 'fullname': 'Zichen Wen', 'user': 'zichenwen', 'type': 'user'}, 'summary': 'The rapid advancements in generative technology have emerged as a\\ndouble-edged sword. While offering powerful tools that enhance convenience,\\nthey also pose significant social concerns. As defenders, current synthetic\\nimage detection methods often lack artifact-level textual interpretability and\\nare overly focused on image manipulation detection, and current datasets\\nusually suffer from outdated generators and a lack of fine-grained annotations.\\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\\nfeatures 4 distinct image content types, 3 categories of artifacts, and\\nfine-grained annotations covering pixel-level segmentation, detailed textual\\nexplanations, and artifact category labels. Furthermore, we propose LEGION\\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\\nlarge language model (MLLM)-based image forgery analysis framework that\\nintegrates artifact detection, segmentation, and explanation. Building upon\\nthis capability, we further explore LEGION as a controller, integrating it into\\nimage refinement pipelines to guide the generation of higher-quality and more\\nrealistic images. Extensive experiments show that LEGION outperforms existing\\nmethods across multiple benchmarks, particularly surpassing the second-best\\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\\nMoreover, the refined images generated under its guidance exhibit stronger\\nalignment with human preferences. The code, model, and dataset will be\\nreleased.', 'upvotes': 8, 'discussionId': '67dbbe92afd5251fc6b55825', 'projectPage': 'https://opendatalab.github.io/LEGION', 'githubRepo': 'https://github.com/opendatalab/LEGION', 'ai_keywords': ['SynthScars', 'LEGION', 'multimodal large language model', 'image forgery analysis framework', 'artifact detection', 'segmentation', 'explanation', 'mIoU', 'F1 score']}, 'publishedAt': '2025-03-19T10:37:21.000Z', 'title': 'LEGION: Learning to Ground and Explain for Synthetic Image Detection', 'summary': 'The rapid advancements in generative technology have emerged as a\\ndouble-edged sword. While offering powerful tools that enhance convenience,\\nthey also pose significant social concerns. As defenders, current synthetic\\nimage detection methods often lack artifact-level textual interpretability and\\nare overly focused on image manipulation detection, and current datasets\\nusually suffer from outdated generators and a lack of fine-grained annotations.\\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\\nfeatures 4 distinct image content types, 3 categories of artifacts, and\\nfine-grained annotations covering pixel-level segmentation, detailed textual\\nexplanations, and artifact category labels. Furthermore, we propose LEGION\\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\\nlarge language model (MLLM)-based image forgery analysis framework that\\nintegrates artifact detection, segmentation, and explanation. Building upon\\nthis capability, we further explore LEGION as a controller, integrating it into\\nimage refinement pipelines to guide the generation of higher-quality and more\\nrealistic images. Extensive experiments show that LEGION outperforms existing\\nmethods across multiple benchmarks, particularly surpassing the second-best\\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\\nMoreover, the refined images generated under its guidance exhibit stronger\\nalignment with human preferences. The code, model, and dataset will be\\nreleased.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15264.png', 'numComments': 1, 'submittedBy': {'_id': '653b8c3e97a4d71d950e2f20', 'avatarUrl': '/avatars/b68880022e14556d0be58c69615db3be.svg', 'fullname': 'Zichen Wen', 'name': 'zichenwen', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.14505', 'authors': [{'_id': '67db13f71956dcedf0b4d357', 'user': {'_id': '635a6dd21668c4ead3ed19fa', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg', 'isPro': False, 'fullname': 'Susung Hong', 'user': 'susunghong', 'type': 'user'}, 'name': 'Susung Hong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:25:56.403Z', 'hidden': False}, {'_id': '67db13f71956dcedf0b4d358', 'user': {'_id': '66d20e411ba71ac4c0488132', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kkqDAFT15lx4wrJVXiFGJ.png', 'isPro': False, 'fullname': 'Ira Kemelmacher-Shlizerman', 'user': 'kemelmi', 'type': 'user'}, 'name': 'Ira Kemelmacher-Shlizerman', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:26:06.351Z', 'hidden': False}, {'_id': '67db13f71956dcedf0b4d359', 'name': 'Brian Curless', 'hidden': False}, {'_id': '67db13f71956dcedf0b4d35a', 'name': 'Steven M. Seitz', 'hidden': False}], 'publishedAt': '2025-03-18T17:59:58.000Z', 'submittedOnDailyAt': '2025-03-20T02:39:46.392Z', 'title': 'MusicInfuser: Making Video Diffusion Listen and Dance', 'submittedOnDailyBy': {'_id': '635a6dd21668c4ead3ed19fa', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg', 'isPro': False, 'fullname': 'Susung Hong', 'user': 'susunghong', 'type': 'user'}, 'summary': 'We introduce MusicInfuser, an approach for generating high-quality dance\\nvideos that are synchronized to a specified music track. Rather than attempting\\nto design and train a new multimodal audio-video model, we show how existing\\nvideo diffusion models can be adapted to align with musical inputs by\\nintroducing lightweight music-video cross-attention and a low-rank adapter.\\nUnlike prior work requiring motion capture data, our approach fine-tunes only\\non dance videos. MusicInfuser achieves high-quality music-driven video\\ngeneration while preserving the flexibility and generative capabilities of the\\nunderlying models. We introduce an evaluation framework using Video-LLMs to\\nassess multiple dimensions of dance generation quality. The project page and\\ncode are available at https://susunghong.github.io/MusicInfuser.', 'upvotes': 8, 'discussionId': '67db13fc1956dcedf0b4d470', 'ai_keywords': ['video diffusion models', 'multimodal audio-video model', 'music-video cross-attention', 'low-rank adapter', 'dance videos', 'motion capture data', 'music-driven video generation', 'Video-LLMs']}, 'publishedAt': '2025-03-18T13:59:58.000Z', 'title': 'MusicInfuser: Making Video Diffusion Listen and Dance', 'summary': 'We introduce MusicInfuser, an approach for generating high-quality dance\\nvideos that are synchronized to a specified music track. Rather than attempting\\nto design and train a new multimodal audio-video model, we show how existing\\nvideo diffusion models can be adapted to align with musical inputs by\\nintroducing lightweight music-video cross-attention and a low-rank adapter.\\nUnlike prior work requiring motion capture data, our approach fine-tunes only\\non dance videos. MusicInfuser achieves high-quality music-driven video\\ngeneration while preserving the flexibility and generative capabilities of the\\nunderlying models. We introduce an evaluation framework using Video-LLMs to\\nassess multiple dimensions of dance generation quality. The project page and\\ncode are available at https://susunghong.github.io/MusicInfuser.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14505.png', 'numComments': 1, 'submittedBy': {'_id': '635a6dd21668c4ead3ed19fa', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg', 'fullname': 'Susung Hong', 'name': 'susunghong', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.12769', 'authors': [{'_id': '67d8ded81a1b6ae91f79eb18', 'user': {'_id': '67067633351e0c16a5c27497', 'avatarUrl': '/avatars/356aa3431198c8931b820a714bcfb19d.svg', 'isPro': False, 'fullname': 'Shenghao Fu', 'user': 'fushh7', 'type': 'user'}, 'name': 'Shenghao Fu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:46:26.291Z', 'hidden': False}, {'_id': '67d8ded81a1b6ae91f79eb19', 'user': {'_id': '66a097801a26a2350395edc7', 'avatarUrl': '/avatars/1e7e127cb7222df7d56e5bfda6bab519.svg', 'isPro': False, 'fullname': 'Qize Yang', 'user': 'PhilipC', 'type': 'user'}, 'name': 'Qize Yang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:46:23.971Z', 'hidden': False}, {'_id': '67d8ded81a1b6ae91f79eb1a', 'user': {'_id': '644fe6a9e1d7a97f3b66e906', 'avatarUrl': '/avatars/ad1a45f0b1c8a4d03ba87f2a3ce5a8f8.svg', 'isPro': False, 'fullname': 'Yuanming-Li', 'user': 'Lymann', 'type': 'user'}, 'name': 'Yuan-Ming Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:46:20.167Z', 'hidden': False}, {'_id': '67d8ded81a1b6ae91f79eb1b', 'user': {'_id': '66b02f7405e2b2771bb431db', 'avatarUrl': '/avatars/9d1ecb38e6cb2f0be33a5c1938bb1253.svg', 'isPro': False, 'fullname': 'Yi-Xing Peng', 'user': 'maybetomorrow', 'type': 'user'}, 'name': 'Yi-Xing Peng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:46:18.304Z', 'hidden': False}, {'_id': '67d8ded81a1b6ae91f79eb1c', 'name': 'Kun-Yu Lin', 'hidden': False}, {'_id': '67d8ded81a1b6ae91f79eb1d', 'name': 'Xihan Wei', 'hidden': False}, {'_id': '67d8ded81a1b6ae91f79eb1e', 'name': 'Jian-Fang Hu', 'hidden': False}, {'_id': '67d8ded81a1b6ae91f79eb1f', 'name': 'Xiaohua Xie', 'hidden': False}, {'_id': '67d8ded81a1b6ae91f79eb20', 'name': 'Wei-Shi Zheng', 'hidden': False}], 'publishedAt': '2025-03-17T03:05:31.000Z', 'submittedOnDailyAt': '2025-03-20T00:48:47.112Z', 'title': 'ViSpeak: Visual Instruction Feedback in Streaming Videos', 'submittedOnDailyBy': {'_id': '6686044047f2a33570e59e31', 'avatarUrl': '/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg', 'isPro': False, 'fullname': 'Jiaxing Zhao', 'user': 'StarJiaxing', 'type': 'user'}, 'summary': 'Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\\noffline video understanding. Instead, streaming video understanding poses great\\nchallenges to recent models due to its time-sensitive, omni-modal and\\ninteractive characteristics. In this work, we aim to extend the streaming video\\nunderstanding from a new perspective and propose a novel task named Visual\\nInstruction Feedback in which models should be aware of visual contents and\\nlearn to extract instructions from them. For example, when users wave their\\nhands to agents, agents should recognize the gesture and start conversations\\nwith welcome information. Thus, following instructions in visual modality\\ngreatly enhances user-agent interactions. To facilitate research, we define\\nseven key subtasks highly relevant to visual modality and collect the\\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\\nunderstanding LMM with GPT-4o-level performance on various streaming video\\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\\nViSpeak is equipped with basic visual instruction feedback ability, serving as\\na solid baseline for future research.', 'upvotes': 7, 'discussionId': '67d8ded91a1b6ae91f79eb5c', 'ai_keywords': ['Large Multi-modal Models (LMMs)', 'streaming video understanding', 'Visual Instruction Feedback', 'visual contents', 'instructions', 'gesture recognition', 'user-agent interactions', 'subtasks', 'ViSpeak-Instruct dataset', 'ViSpeak-Bench', 'ViSpeak model', 'GPT-4o-level performance', 'streaming video understanding benchmarks', 'finetuning']}, 'publishedAt': '2025-03-16T23:05:31.000Z', 'title': 'ViSpeak: Visual Instruction Feedback in Streaming Videos', 'summary': 'Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\\noffline video understanding. Instead, streaming video understanding poses great\\nchallenges to recent models due to its time-sensitive, omni-modal and\\ninteractive characteristics. In this work, we aim to extend the streaming video\\nunderstanding from a new perspective and propose a novel task named Visual\\nInstruction Feedback in which models should be aware of visual contents and\\nlearn to extract instructions from them. For example, when users wave their\\nhands to agents, agents should recognize the gesture and start conversations\\nwith welcome information. Thus, following instructions in visual modality\\ngreatly enhances user-agent interactions. To facilitate research, we define\\nseven key subtasks highly relevant to visual modality and collect the\\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\\nunderstanding LMM with GPT-4o-level performance on various streaming video\\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\\nViSpeak is equipped with basic visual instruction feedback ability, serving as\\na solid baseline for future research.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12769.png', 'numComments': 1, 'submittedBy': {'_id': '6686044047f2a33570e59e31', 'avatarUrl': '/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg', 'fullname': 'Jiaxing Zhao', 'name': 'StarJiaxing', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 13}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.11227', 'authors': [{'_id': '67da533bb443470b7908a048', 'user': {'_id': '658be7fe135580745c510323', 'avatarUrl': '/avatars/830e5cec4565efdc23226a86a0fcef0e.svg', 'isPro': False, 'fullname': 'Jian Zhang', 'user': 'VentureZJ', 'type': 'user'}, 'name': 'Jian Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-19T09:44:13.546Z', 'hidden': False}, {'_id': '67da533bb443470b7908a049', 'name': 'Bifan Wei', 'hidden': False}, {'_id': '67da533bb443470b7908a04a', 'name': 'Shihao Qi', 'hidden': False}, {'_id': '67da533bb443470b7908a04b', 'name': 'haiping Zhu', 'hidden': False}, {'_id': '67da533bb443470b7908a04c', 'name': 'Jun Liu', 'hidden': False}, {'_id': '67da533bb443470b7908a04d', 'user': {'_id': '66ac77011cfb12c087605acb', 'avatarUrl': '/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg', 'isPro': False, 'fullname': 'Lin', 'user': 'Qika', 'type': 'user'}, 'name': 'Qika Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:28:09.851Z', 'hidden': False}], 'publishedAt': '2025-03-14T09:23:22.000Z', 'submittedOnDailyAt': '2025-03-20T06:15:24.085Z', 'title': 'GKG-LLM: A Unified Framework for Generalized Knowledge Graph\\n  Construction', 'submittedOnDailyBy': {'_id': '658be7fe135580745c510323', 'avatarUrl': '/avatars/830e5cec4565efdc23226a86a0fcef0e.svg', 'isPro': False, 'fullname': 'Jian Zhang', 'user': 'VentureZJ', 'type': 'user'}, 'summary': 'The construction of Generalized Knowledge Graph (GKG), including knowledge\\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\\nfor various natural language processing tasks. Current studies typically\\nconstruct these types of graph separately, overlooking holistic insights and\\npotential unification that could be beneficial in computing resources and usage\\nperspectives. However, a key challenge in developing a unified framework for\\nGKG is obstacles arising from task-specific differences. In this study, we\\npropose a unified framework for constructing generalized knowledge graphs to\\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\\nacross the three types of graphs, categorizing them into in-sample,\\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\\nknowledge from the three types of graphs into the Large Language Models.\\nExtensive experiments show that our proposed model improves the construction of\\nall three graph types across in-domain, OOD and counter-task data.', 'upvotes': 7, 'discussionId': '67da533db443470b7908a0e6', 'ai_keywords': ['knowledge graph', 'event knowledge graph', 'commonsense knowledge graph', 'natural language processing', 'unified framework', 'in-sample data', 'counter-task data', 'out-of-distribution data', 'three-stage curriculum learning fine-tuning framework', 'Large Language Models']}, 'publishedAt': '2025-03-14T05:23:22.000Z', 'title': 'GKG-LLM: A Unified Framework for Generalized Knowledge Graph\\n  Construction', 'summary': 'The construction of Generalized Knowledge Graph (GKG), including knowledge\\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\\nfor various natural language processing tasks. Current studies typically\\nconstruct these types of graph separately, overlooking holistic insights and\\npotential unification that could be beneficial in computing resources and usage\\nperspectives. However, a key challenge in developing a unified framework for\\nGKG is obstacles arising from task-specific differences. In this study, we\\npropose a unified framework for constructing generalized knowledge graphs to\\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\\nacross the three types of graphs, categorizing them into in-sample,\\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\\nknowledge from the three types of graphs into the Large Language Models.\\nExtensive experiments show that our proposed model improves the construction of\\nall three graph types across in-domain, OOD and counter-task data.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11227.png', 'numComments': 1, 'submittedBy': {'_id': '658be7fe135580745c510323', 'avatarUrl': '/avatars/830e5cec4565efdc23226a86a0fcef0e.svg', 'fullname': 'Jian Zhang', 'name': 'VentureZJ', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.13360', 'authors': [{'_id': '67d8e21dea26d6d743f2adde', 'user': {'_id': '6623975c728f756224d4b768', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg', 'isPro': False, 'fullname': 'Allen Sun', 'user': 'Allen8', 'type': 'user'}, 'name': 'Hai-Long Sun', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T10:46:16.064Z', 'hidden': False}, {'_id': '67d8e21dea26d6d743f2addf', 'user': {'_id': '64b75c9c479b934973de0e98', 'avatarUrl': '/avatars/99a02865ddc6af27a77fcf7701f9f666.svg', 'isPro': False, 'fullname': 'Zhun Sun', 'user': 'zhunsun', 'type': 'user'}, 'name': 'Zhun Sun', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:28:46.228Z', 'hidden': False}, {'_id': '67d8e21dea26d6d743f2ade0', 'user': {'_id': '631952b6f18d0b5d999ca397', 'avatarUrl': '/avatars/eb87dfc142a7602f8fb888f7b7b60d38.svg', 'isPro': False, 'fullname': 'Peng', 'user': 'Houwen', 'type': 'user'}, 'name': 'Houwen Peng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:28:40.023Z', 'hidden': False}, {'_id': '67d8e21dea26d6d743f2ade1', 'user': {'_id': '67ab41b0ef4586b3ea65e7ed', 'avatarUrl': '/avatars/4fa69250c6657624276a9af2aea6cd89.svg', 'isPro': False, 'fullname': 'Han-Jia Ye', 'user': 'han-jia', 'type': 'user'}, 'name': 'Han-Jia Ye', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:28:54.161Z', 'hidden': False}], 'publishedAt': '2025-03-17T16:45:12.000Z', 'submittedOnDailyAt': '2025-03-20T04:52:23.426Z', 'title': 'Mitigating Visual Forgetting via Take-along Visual Conditioning for\\n  Multi-modal Long CoT Reasoning', 'submittedOnDailyBy': {'_id': '6623975c728f756224d4b768', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg', 'isPro': False, 'fullname': 'Allen Sun', 'user': 'Allen8', 'type': 'user'}, 'summary': \"Recent advancements in Large Language Models (LLMs) have demonstrated\\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\\nto advanced, product-oriented solutions like OpenAI o1. During our\\nre-implementation of this model, we noticed that in multimodal tasks requiring\\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\\nmaintain focus on the visual information, in other words, MLLMs suffer from a\\ngradual decline in attention to visual information as reasoning progresses,\\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\\nduring long-chain reasoning. Concretely, we truncate the reasoning process\\nmidway, then re-complete the reasoning process with the input image removed. We\\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\\nmodel's textual outputs dominate the following reasoning process. Motivated by\\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\\nimage input to critical reasoning stages and compresses redundant visual tokens\\nvia dynamic pruning. This methodology helps the model retain attention to the\\nvisual components throughout the reasoning. Our approach achieves\\nstate-of-the-art performance on average across five mathematical reasoning\\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\\nenhancing multimodal reasoning systems.\", 'upvotes': 5, 'discussionId': '67d8e21eea26d6d743f2ae50', 'ai_keywords': ['Chain-of-Thought (CoT)', 'OpenAI o1', 'Multimodal LLMs (MLLMs)', 'attention to visual information', 'text-over-relied outputs', 'ablate image inputs', 'long-chain reasoning', \"MathVista's test-hard subset\", 'Take-along Visual Conditioning (TVC)', 'critical reasoning stages', 'dynamic pruning', 'multimodal reasoning systems']}, 'publishedAt': '2025-03-17T12:45:12.000Z', 'title': 'Mitigating Visual Forgetting via Take-along Visual Conditioning for\\n  Multi-modal Long CoT Reasoning', 'summary': \"Recent advancements in Large Language Models (LLMs) have demonstrated\\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\\nto advanced, product-oriented solutions like OpenAI o1. During our\\nre-implementation of this model, we noticed that in multimodal tasks requiring\\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\\nmaintain focus on the visual information, in other words, MLLMs suffer from a\\ngradual decline in attention to visual information as reasoning progresses,\\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\\nduring long-chain reasoning. Concretely, we truncate the reasoning process\\nmidway, then re-complete the reasoning process with the input image removed. We\\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\\nmodel's textual outputs dominate the following reasoning process. Motivated by\\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\\nimage input to critical reasoning stages and compresses redundant visual tokens\\nvia dynamic pruning. This methodology helps the model retain attention to the\\nvisual components throughout the reasoning. Our approach achieves\\nstate-of-the-art performance on average across five mathematical reasoning\\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\\nenhancing multimodal reasoning systems.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13360.png', 'numComments': 1, 'submittedBy': {'_id': '6623975c728f756224d4b768', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg', 'fullname': 'Allen Sun', 'name': 'Allen8', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.12963', 'authors': [{'_id': '67da8735cae81b233ffd2927', 'user': {'_id': '67da84f499b60bd76c91fd83', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5iUk_mhBg0lLbUbODEybH.png', 'isPro': False, 'fullname': 'Chaolong Yang', 'user': 'ChaolongYang', 'type': 'user'}, 'name': 'Chaolong Yang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T11:00:42.125Z', 'hidden': False}, {'_id': '67da8735cae81b233ffd2928', 'user': {'_id': '65d6adbe654f85ff0bb0c246', 'avatarUrl': '/avatars/a11b086ae1422b8d64d56d2c5825d3c0.svg', 'isPro': False, 'fullname': 'kai yao', 'user': 'KaiserYaoJM', 'type': 'user'}, 'name': 'Kai Yao', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-03-19T08:58:30.094Z', 'hidden': False}, {'_id': '67da8735cae81b233ffd2929', 'user': {'_id': '673aef01836947961d636a15', 'avatarUrl': '/avatars/3d45b5048d7a55a8a31a928081d95d8a.svg', 'isPro': False, 'fullname': 'Yuyao Yan', 'user': 'Ritayy', 'type': 'user'}, 'name': 'Yuyao Yan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:29:13.789Z', 'hidden': False}, {'_id': '67da8735cae81b233ffd292a', 'user': {'_id': '6773b0515c88cb6ab34cc8fd', 'avatarUrl': '/avatars/5d4c4156463a7d69d88e579bf14215d0.svg', 'isPro': False, 'fullname': 'chenru jiang', 'user': 'chenrujiang', 'type': 'user'}, 'name': 'Chenru Jiang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:29:20.450Z', 'hidden': False}, {'_id': '67da8735cae81b233ffd292b', 'user': {'_id': '672094a12174ca42d583e7bc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/OYcCwVQwR0oDQcVsMr22R.png', 'isPro': False, 'fullname': 'Weiguang Zhao', 'user': 'weiguangzhao', 'type': 'user'}, 'name': 'Weiguang Zhao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-20T11:05:14.366Z', 'hidden': False}, {'_id': '67da8735cae81b233ffd292c', 'name': 'Jie Sun', 'hidden': False}, {'_id': '67da8735cae81b233ffd292d', 'name': 'Guangliang Cheng', 'hidden': False}, {'_id': '67da8735cae81b233ffd292e', 'name': 'Yifei Zhang', 'hidden': False}, {'_id': '67da8735cae81b233ffd292f', 'name': 'Bin Dong', 'hidden': False}, {'_id': '67da8735cae81b233ffd2930', 'name': 'Kaizhu Huang', 'hidden': False}], 'publishedAt': '2025-03-17T09:18:31.000Z', 'submittedOnDailyAt': '2025-03-20T09:31:48.626Z', 'title': 'Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based\\n  Spatiotemporal Diffusion for Audio-driven Talking Portrait', 'submittedOnDailyBy': {'_id': '67da84f499b60bd76c91fd83', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5iUk_mhBg0lLbUbODEybH.png', 'isPro': False, 'fullname': 'Chaolong Yang', 'user': 'ChaolongYang', 'type': 'user'}, 'summary': 'Audio-driven single-image talking portrait generation plays a crucial role in\\nvirtual reality, digital human creation, and filmmaking. Existing approaches\\nare generally categorized into keypoint-based and image-based methods.\\nKeypoint-based methods effectively preserve character identity but struggle to\\ncapture fine facial details due to the fixed points limitation of the 3D\\nMorphable Model. Moreover, traditional generative networks face challenges in\\nestablishing causality between audio and keypoints on limited datasets,\\nresulting in low pose diversity. In contrast, image-based approaches produce\\nhigh-quality portraits with diverse details using the diffusion network but\\nincur identity distortion and expensive computational costs. In this work, we\\npropose KDTalker, the first framework to combine unsupervised implicit 3D\\nkeypoint with a spatiotemporal diffusion model. Leveraging unsupervised\\nimplicit 3D keypoints, KDTalker adapts facial information densities, allowing\\nthe diffusion process to model diverse head poses and capture fine facial\\ndetails flexibly. The custom-designed spatiotemporal attention mechanism\\nensures accurate lip synchronization, producing temporally consistent,\\nhigh-quality animations while enhancing computational efficiency. Experimental\\nresults demonstrate that KDTalker achieves state-of-the-art performance\\nregarding lip synchronization accuracy, head pose diversity, and execution\\nefficiency.Our codes are available at https://github.com/chaolongy/KDTalker.', 'upvotes': 5, 'discussionId': '67da8736cae81b233ffd29b9', 'ai_keywords': ['KDTalker', 'unsupervised implicit 3D keypoint', 'spatiotemporal diffusion model', 'spatiotemporal attention mechanism', 'lip synchronization', 'temporally consistent', 'computational efficiency']}, 'publishedAt': '2025-03-17T05:18:31.000Z', 'title': 'Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based\\n  Spatiotemporal Diffusion for Audio-driven Talking Portrait', 'summary': 'Audio-driven single-image talking portrait generation plays a crucial role in\\nvirtual reality, digital human creation, and filmmaking. Existing approaches\\nare generally categorized into keypoint-based and image-based methods.\\nKeypoint-based methods effectively preserve character identity but struggle to\\ncapture fine facial details due to the fixed points limitation of the 3D\\nMorphable Model. Moreover, traditional generative networks face challenges in\\nestablishing causality between audio and keypoints on limited datasets,\\nresulting in low pose diversity. In contrast, image-based approaches produce\\nhigh-quality portraits with diverse details using the diffusion network but\\nincur identity distortion and expensive computational costs. In this work, we\\npropose KDTalker, the first framework to combine unsupervised implicit 3D\\nkeypoint with a spatiotemporal diffusion model. Leveraging unsupervised\\nimplicit 3D keypoints, KDTalker adapts facial information densities, allowing\\nthe diffusion process to model diverse head poses and capture fine facial\\ndetails flexibly. The custom-designed spatiotemporal attention mechanism\\nensures accurate lip synchronization, producing temporally consistent,\\nhigh-quality animations while enhancing computational efficiency. Experimental\\nresults demonstrate that KDTalker achieves state-of-the-art performance\\nregarding lip synchronization accuracy, head pose diversity, and execution\\nefficiency.Our codes are available at https://github.com/chaolongy/KDTalker.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12963.png', 'numComments': 1, 'submittedBy': {'_id': '67da84f499b60bd76c91fd83', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5iUk_mhBg0lLbUbODEybH.png', 'fullname': 'Chaolong Yang', 'name': 'ChaolongYang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.15055', 'authors': [{'_id': '67db9586a2f164ac51f84c72', 'user': {'_id': '641ee9fe632a1ec42caf1fa6', 'avatarUrl': '/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg', 'isPro': False, 'fullname': 'Arina Razmyslovich', 'user': 'lavriz', 'type': 'user'}, 'name': 'Arina Razmyslovich', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-03-20T04:13:32.101Z', 'hidden': False}, {'_id': '67db9586a2f164ac51f84c73', 'user': {'_id': '6786af3f8c6e6a8a0fb39b45', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/OBfFNaBuLyuEEuuszJfHj.png', 'isPro': False, 'fullname': 'Kseniia Murasheva', 'user': 'Kseniamorph', 'type': 'user'}, 'name': 'Kseniia Murasheva', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:29:50.922Z', 'hidden': False}, {'_id': '67db9586a2f164ac51f84c74', 'name': 'Sofia Sedlova', 'hidden': False}, {'_id': '67db9586a2f164ac51f84c75', 'user': {'_id': '64aab6e5d4d43713ceb83fb1', 'avatarUrl': '/avatars/3c636ed07e42c9883b0f7ad70b74f156.svg', 'isPro': False, 'fullname': 'Julien Capitaine', 'user': 'DrKapichu', 'type': 'user'}, 'name': 'Julien Capitaine', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:30:01.959Z', 'hidden': False}, {'_id': '67db9586a2f164ac51f84c76', 'name': 'Eugene Dmitriev', 'hidden': False}], 'publishedAt': '2025-03-19T09:46:54.000Z', 'submittedOnDailyAt': '2025-03-20T02:46:26.189Z', 'title': 'ELTEX: A Framework for Domain-Driven Synthetic Data Generation', 'submittedOnDailyBy': {'_id': '641ee9fe632a1ec42caf1fa6', 'avatarUrl': '/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg', 'isPro': False, 'fullname': 'Arina Razmyslovich', 'user': 'lavriz', 'type': 'user'}, 'summary': \"We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\\nfor generating high-quality synthetic training data in specialized domains.\\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\\ntheir performance in specialized domains like cybersecurity remains limited by\\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\\nby systematically integrating explicit domain indicator extraction with dynamic\\nprompting to preserve critical domain knowledge throughout the generation\\nprocess. We demonstrate ELTEX's effectiveness in the context of\\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\\nvarious combinations of real and ELTEX-generated data. Our results show that\\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\\nboth standard classification metrics and uncertainty calibration, while\\nrequiring significantly fewer computational resources. We release a curated\\nsynthetic dataset of social media texts for cyberattack detection in\\nblockchain. Our work demonstrates that domain-driven synthetic data generation\\ncan effectively bridge the performance gap between resource-efficient models\\nand larger architectures in specialized domains.\", 'upvotes': 1, 'discussionId': '67db958fa2f164ac51f84f51', 'githubRepo': 'https://github.com/1712n/eltex', 'ai_keywords': ['ELTEX', 'domain-driven framework', 'high-quality synthetic training data', 'Large Language Models (LLMs)', 'cohort indicator extraction', 'dynamic prompting', 'critical domain knowledge', 'blockchain-related cyberattack detection', 'Gemma-2B', 'performance competitive', 'GPT-4', 'standard classification metrics', 'uncertainty calibration', 'computational resources', 'synthetic dataset', 'social media texts', 'domain-driven synthetic data generation', 'performance gap', 'resource-efficient models', 'larger architectures']}, 'publishedAt': '2025-03-19T05:46:54.000Z', 'title': 'ELTEX: A Framework for Domain-Driven Synthetic Data Generation', 'summary': \"We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\\nfor generating high-quality synthetic training data in specialized domains.\\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\\ntheir performance in specialized domains like cybersecurity remains limited by\\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\\nby systematically integrating explicit domain indicator extraction with dynamic\\nprompting to preserve critical domain knowledge throughout the generation\\nprocess. We demonstrate ELTEX's effectiveness in the context of\\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\\nvarious combinations of real and ELTEX-generated data. Our results show that\\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\\nboth standard classification metrics and uncertainty calibration, while\\nrequiring significantly fewer computational resources. We release a curated\\nsynthetic dataset of social media texts for cyberattack detection in\\nblockchain. Our work demonstrates that domain-driven synthetic data generation\\ncan effectively bridge the performance gap between resource-efficient models\\nand larger architectures in specialized domains.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15055.png', 'numComments': 1, 'submittedBy': {'_id': '641ee9fe632a1ec42caf1fa6', 'avatarUrl': '/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg', 'fullname': 'Arina Razmyslovich', 'name': 'lavriz', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.13517', 'authors': [{'_id': '67dc13a0b0451eaa2c5e298a', 'name': 'Hao Cui', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e298b', 'name': 'Zahra Shamsi', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e298c', 'name': 'Gowoon Cheon', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e298d', 'name': 'Xuejian Ma', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e298e', 'user': {'_id': '668ec66fd76c5aee9ba82a57', 'avatarUrl': '/avatars/4ba953faaf05c483615d2ea1ae0b5572.svg', 'isPro': False, 'fullname': 'Shutong Li', 'user': 'LigeiaE', 'type': 'user'}, 'name': 'Shutong Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:31:25.035Z', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e298f', 'name': 'Maria Tikhanovskaya', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e2990', 'user': {'_id': '6751fd0c907b6ca2f6c71179', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/75-MmaXqwW_HFMDoHntkU.jpeg', 'isPro': False, 'fullname': 'Peter Norgaard', 'user': 'PNorgMagician', 'type': 'user'}, 'name': 'Peter Norgaard', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:31:16.168Z', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e2991', 'name': 'Nayantara Mudur', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e2992', 'user': {'_id': '6251ade7183aa426692442a0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6251ade7183aa426692442a0/1nP_mQqgcrSPgV1Cf2nMP.jpeg', 'isPro': False, 'fullname': 'Martyna Plomecka', 'user': 'Martynaplomecka', 'type': 'user'}, 'name': 'Martyna Plomecka', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:31:47.096Z', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e2993', 'user': {'_id': '667c456320d68cc5814ebc04', 'avatarUrl': '/avatars/4cb3e893f36aac3442e46afae9d725c7.svg', 'isPro': False, 'fullname': 'Paul Raccuglia', 'user': 'praccu-google', 'type': 'user'}, 'name': 'Paul Raccuglia', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:31:53.506Z', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e2994', 'name': 'Yasaman Bahri', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e2995', 'user': {'_id': '66a7bb1d5f395fc0c6913f77', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sDFjAHabkD_SEYACVD5YU.jpeg', 'isPro': False, 'fullname': 'Victor V. Albert', 'user': 'valbert4', 'type': 'user'}, 'name': 'Victor V. Albert', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:32:04.907Z', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e2996', 'name': 'Pranesh Srinivasan', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e2997', 'user': {'_id': '64664b4ecf550af36eb40363', 'avatarUrl': '/avatars/bf2b1ebbeac37f943578d5c93c834746.svg', 'isPro': False, 'fullname': 'Haining Pan', 'user': 'hainingpan', 'type': 'user'}, 'name': 'Haining Pan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:31:02.332Z', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e2998', 'name': 'Philippe Faist', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e2999', 'name': 'Brian Rohr', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e299a', 'name': 'Michael J. Statt', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e299b', 'user': {'_id': '6386a8ed06858a85f595b533', 'avatarUrl': '/avatars/bdcd45db1dbc86d2f817bc682578cde3.svg', 'isPro': False, 'fullname': 'Dan Morris', 'user': 'danmorris427', 'type': 'user'}, 'name': 'Dan Morris', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-20T13:30:48.407Z', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e299c', 'name': 'Drew Purves', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e299d', 'name': 'Elise Kleeman', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e299e', 'name': 'Ruth Alcantara', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e299f', 'name': 'Matthew Abraham', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e29a0', 'name': 'Muqthar Mohammad', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e29a1', 'name': 'Ean Phing VanLee', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e29a2', 'name': 'Chenfei Jiang', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e29a3', 'name': 'Elizabeth Dorfman', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e29a4', 'name': 'Eun-Ah Kim', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e29a5', 'name': 'Michael P Brenner', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e29a6', 'name': 'Viren Jain', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e29a7', 'name': 'Sameera Ponda', 'hidden': False}, {'_id': '67dc13a0b0451eaa2c5e29a8', 'user': {'_id': '646125a5933afb0106a9dabb', 'avatarUrl': '/avatars/ccef359e4442f560a87fd66228e54c8b.svg', 'isPro': False, 'fullname': 'Subhashini', 'user': 'vsubhashini', 'type': 'user'}, 'name': 'Subhashini Venugopalan', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-03-20T13:15:39.949Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/646125a5933afb0106a9dabb/3PHAJKzNd2gQpNwQQYtdf.png'], 'publishedAt': '2025-03-14T17:53:03.000Z', 'submittedOnDailyAt': '2025-03-20T11:42:32.021Z', 'title': 'CURIE: Evaluating LLMs On Multitask Scientific Long Context\\n  Understanding and Reasoning', 'submittedOnDailyBy': {'_id': '646125a5933afb0106a9dabb', 'avatarUrl': '/avatars/ccef359e4442f560a87fd66228e54c8b.svg', 'isPro': False, 'fullname': 'Subhashini', 'user': 'vsubhashini', 'type': 'user'}, 'summary': 'Scientific problem-solving involves synthesizing information while applying\\nexpert knowledge. We introduce CURIE, a scientific long-Context\\nUnderstanding,Reasoning and Information Extraction benchmark to measure the\\npotential of Large Language Models (LLMs) in scientific problem-solving and\\nassisting scientists in realistic workflows. This benchmark introduces ten\\nchallenging tasks with a total of 580 problems and solution pairs curated by\\nexperts in six disciplines - materials science, condensed matter physics,\\nquantum computing, geospatial analysis, biodiversity, and proteins - covering\\nboth experimental and theoretical work-flows in science. We evaluate a range of\\nclosed and open LLMs on tasks in CURIE which requires domain expertise,\\ncomprehension of long in-context information,and multi-step reasoning. While\\nGemini Flash 2.0 and Claude-3 show consistent high comprehension across\\ndomains, the popular GPT-4o and command-R+ fail dramatically on protein\\nsequencing tasks. With the best performance at 32% there is much room for\\nimprovement for all models. We hope that insights gained from CURIE can guide\\nthe future development of LLMs in sciences. Evaluation code and data are in\\nhttps://github.com/google/curie', 'upvotes': 1, 'discussionId': '67dc13a4b0451eaa2c5e2ada', 'projectPage': 'https://github.com/google/curie', 'githubRepo': 'https://github.com/google/curie', 'ai_keywords': ['Large Language Models (LLMs)', 'scientific long-Context Understanding, Reasoning and Information Extraction benchmark (CURIE)', 'domain expertise', 'multi-step reasoning', 'aa (amino acid) sequence tasks', 'protein sequencing tasks']}, 'publishedAt': '2025-03-14T13:53:03.000Z', 'title': 'CURIE: Evaluating LLMs On Multitask Scientific Long Context\\n  Understanding and Reasoning', 'summary': 'Scientific problem-solving involves synthesizing information while applying\\nexpert knowledge. We introduce CURIE, a scientific long-Context\\nUnderstanding,Reasoning and Information Extraction benchmark to measure the\\npotential of Large Language Models (LLMs) in scientific problem-solving and\\nassisting scientists in realistic workflows. This benchmark introduces ten\\nchallenging tasks with a total of 580 problems and solution pairs curated by\\nexperts in six disciplines - materials science, condensed matter physics,\\nquantum computing, geospatial analysis, biodiversity, and proteins - covering\\nboth experimental and theoretical work-flows in science. We evaluate a range of\\nclosed and open LLMs on tasks in CURIE which requires domain expertise,\\ncomprehension of long in-context information,and multi-step reasoning. While\\nGemini Flash 2.0 and Claude-3 show consistent high comprehension across\\ndomains, the popular GPT-4o and command-R+ fail dramatically on protein\\nsequencing tasks. With the best performance at 32% there is much room for\\nimprovement for all models. We hope that insights gained from CURIE can guide\\nthe future development of LLMs in sciences. Evaluation code and data are in\\nhttps://github.com/google/curie', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/646125a5933afb0106a9dabb/3PHAJKzNd2gQpNwQQYtdf.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13517.png', 'numComments': 1, 'submittedBy': {'_id': '646125a5933afb0106a9dabb', 'avatarUrl': '/avatars/ccef359e4442f560a87fd66228e54c8b.svg', 'fullname': 'Subhashini', 'name': 'vsubhashini', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}"
]