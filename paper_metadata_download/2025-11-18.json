[
    "{'paper': {'id': '2511.13612', 'authors': [{'_id': '691bfdc96bfd5965c0fd3951', 'user': {'_id': '65352acb7139c5dd8d9a8590', 'avatarUrl': '/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg', 'isPro': False, 'fullname': 'JiachengChen', 'user': 'JC-Chen', 'type': 'user'}, 'name': 'Jiacheng Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:07:04.293Z', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3952', 'name': 'Qianjia Cheng', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3953', 'name': 'Fangchen Yu', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3954', 'name': 'Haiyuan Wan', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3955', 'name': 'Yuchen Zhang', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3956', 'name': 'Shenghe Zheng', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3957', 'name': 'Junchi Yao', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3958', 'name': 'Qingyang Zhang', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3959', 'name': 'Haonan He', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd395a', 'name': 'Yun Luo', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd395b', 'name': 'Yufeng Zhao', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd395c', 'name': 'Futing Wang', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd395d', 'name': 'Li Sheng', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd395e', 'name': 'Chengxing Xie', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd395f', 'name': 'Yuxin Zuo', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3960', 'name': 'Yizhuo Li', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3961', 'name': 'Wenxauan Zeng', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3962', 'name': 'Yulun Wu', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3963', 'user': {'_id': '6731caae58ba0f0984c188ea', 'avatarUrl': '/avatars/445275cca29f97c5f8754d6c27075887.svg', 'isPro': False, 'fullname': 'Rui Huang', 'user': 'Rui1121', 'type': 'user'}, 'name': 'Rui Huang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:07:02.211Z', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3964', 'name': 'Dongzhan Zhou', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3965', 'name': 'Kai Chen', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3966', 'name': 'Yu Qiao', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3967', 'name': 'Lei Bai', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3968', 'name': 'Yu Cheng', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd3969', 'name': 'Ning Ding', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd396a', 'name': 'Bowen Zhou', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd396b', 'name': 'Peng Ye', 'hidden': False}, {'_id': '691bfdc96bfd5965c0fd396c', 'name': 'Ganqu Cui', 'hidden': False}], 'publishedAt': '2025-11-17T17:18:13.000Z', 'submittedOnDailyAt': '2025-11-18T02:37:44.601Z', 'title': 'P1: Mastering Physics Olympiads with Reinforcement Learning', 'submittedOnDailyBy': {'_id': '65352acb7139c5dd8d9a8590', 'avatarUrl': '/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg', 'isPro': False, 'fullname': 'JiachengChen', 'user': 'JC-Chen', 'type': 'user'}, 'summary': 'Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.', 'upvotes': 102, 'discussionId': '691bfdca6bfd5965c0fd396d', 'projectPage': 'https://prime-rl.github.io/P1/', 'githubRepo': 'https://github.com/PRIME-RL/P1', 'githubStars': 44}, 'publishedAt': '2025-11-17T12:18:13.000Z', 'title': 'P1: Mastering Physics Olympiads with Reinforcement Learning', 'summary': 'Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13612.png', 'numComments': 3, 'submittedBy': {'_id': '65352acb7139c5dd8d9a8590', 'avatarUrl': '/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg', 'fullname': 'JiachengChen', 'name': 'JC-Chen', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2511.12609', 'authors': [{'_id': '691be5fc6bfd5965c0fd37d4', 'name': 'Yunxin Li', 'hidden': False}, {'_id': '691be5fc6bfd5965c0fd37d5', 'user': {'_id': '64d9da538767727dff1e8f19', 'avatarUrl': '/avatars/aaad38795007c6cbcb94c7eed1706e51.svg', 'isPro': False, 'fullname': 'Xinyu Chen', 'user': 'Ghaser', 'type': 'user'}, 'name': 'Xinyu Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:08:12.997Z', 'hidden': False}, {'_id': '691be5fc6bfd5965c0fd37d6', 'name': 'Shenyuan Jiang', 'hidden': False}, {'_id': '691be5fc6bfd5965c0fd37d7', 'user': {'_id': '652fb8bcc9dd2692a25ef2e3', 'avatarUrl': '/avatars/461e6cc1c3441cde18192b080b0b8576.svg', 'isPro': False, 'fullname': 'Haoyuan Shi', 'user': 'MrSunshy', 'type': 'user'}, 'name': 'Haoyuan Shi', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:08:19.326Z', 'hidden': False}, {'_id': '691be5fc6bfd5965c0fd37d8', 'name': 'Zhenyu Liu', 'hidden': False}, {'_id': '691be5fc6bfd5965c0fd37d9', 'name': 'Xuanyu Zhang', 'hidden': False}, {'_id': '691be5fc6bfd5965c0fd37da', 'name': 'Nanhao Deng', 'hidden': False}, {'_id': '691be5fc6bfd5965c0fd37db', 'name': 'Zhenran Xu', 'hidden': False}, {'_id': '691be5fc6bfd5965c0fd37dc', 'name': 'Yicheng Ma', 'hidden': False}, {'_id': '691be5fc6bfd5965c0fd37dd', 'name': 'Meishan Zhang', 'hidden': False}, {'_id': '691be5fc6bfd5965c0fd37de', 'name': 'Baotian Hu', 'hidden': False}, {'_id': '691be5fc6bfd5965c0fd37df', 'name': 'Min Zhang', 'hidden': False}], 'publishedAt': '2025-11-16T14:10:55.000Z', 'submittedOnDailyAt': '2025-11-18T01:14:55.926Z', 'title': 'Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data', 'submittedOnDailyBy': {'_id': '62fdb01bc1588e1d4c6c1a7c', 'avatarUrl': '/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg', 'isPro': False, 'fullname': 'Yunxin Li', 'user': 'YunxinLi', 'type': 'user'}, 'summary': \"We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.\", 'upvotes': 83, 'discussionId': '691be5fc6bfd5965c0fd37e0', 'projectPage': 'https://idealistxy.github.io/Uni-MoE-v2.github.io/', 'githubRepo': 'https://github.com/HITsz-TMG/Uni-MoE', 'githubStars': 825, 'organization': {'_id': '629867d7f2bf8bd3e468706e', 'name': 'HIT-TMG', 'fullname': 'HITsz-Text and Multimodal Generative Intelligence Group(TMG)', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1658735061824-62986540f2bf8bd3e468622a.png'}}, 'publishedAt': '2025-11-16T09:10:55.000Z', 'title': 'Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data', 'summary': \"We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.12609.png', 'numComments': 1, 'submittedBy': {'_id': '62fdb01bc1588e1d4c6c1a7c', 'avatarUrl': '/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg', 'fullname': 'Yunxin Li', 'name': 'YunxinLi', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 12}, 'organization': {'_id': '629867d7f2bf8bd3e468706e', 'name': 'HIT-TMG', 'fullname': 'HITsz-Text and Multimodal Generative Intelligence Group(TMG)', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1658735061824-62986540f2bf8bd3e468622a.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2511.11793', 'authors': [{'_id': '691be81b6bfd5965c0fd37e2', 'name': 'MiroMind Team', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37e3', 'name': 'Song Bai', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37e4', 'name': 'Lidong Bing', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37e5', 'name': 'Carson Chen', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37e6', 'name': 'Guanzheng Chen', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37e7', 'name': 'Yuntao Chen', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37e8', 'name': 'Zhe Chen', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37e9', 'name': 'Ziyi Chen', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37ea', 'name': 'Jifeng Dai', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37eb', 'name': 'Xuan Dong', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37ec', 'name': 'Yue Deng', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37ed', 'name': 'Yunjie Fu', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37ee', 'name': 'Junqi Ge', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37ef', 'name': 'Chenxia Han', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37f0', 'name': 'Tammy Huang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37f1', 'name': 'Zhenhang Huang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37f2', 'name': 'Jerry Jiao', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37f3', 'name': 'Shilei Jiang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37f4', 'name': 'Tianyu Jiao', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37f5', 'name': 'Xiaoqi Jian', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37f6', 'name': 'Lei Lei', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37f7', 'name': 'Ruilin Li', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37f8', 'name': 'Ryan Luo', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37f9', 'name': 'Tiantong Li', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37fa', 'name': 'Xiang Lin', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37fb', 'name': 'Ziyuan Liu', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37fc', 'name': 'Zhiqi Li', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37fd', 'name': 'Jie Ni', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37fe', 'name': 'Qiang Ren', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd37ff', 'name': 'Pax Sun', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3800', 'name': 'Shiqian Su', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3801', 'name': 'Chenxin Tao', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3802', 'name': 'Bin Wang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3803', 'name': 'Hellen Wang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3804', 'name': 'Haonan Wang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3805', 'name': 'James Wang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3806', 'name': 'Jin Wang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3807', 'name': 'Jojo Wang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3808', 'name': 'Letian Wang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3809', 'name': 'Shizun Wang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd380a', 'name': 'Weizhi Wang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd380b', 'name': 'Zixuan Wang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd380c', 'name': 'Jinfan Xu', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd380d', 'name': 'Sen Xing', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd380e', 'name': 'Chenyu Yang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd380f', 'name': 'Hai Ye', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3810', 'name': 'Jiaheng Yu', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3811', 'name': 'Yue Yu', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3812', 'name': 'Muyan Zhong', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3813', 'name': 'Tianchen Zhao', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3814', 'name': 'Xizhou Zhu', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3815', 'name': 'Yanpeng Zhou', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3816', 'name': 'Yifan Zhang', 'hidden': False}, {'_id': '691be81b6bfd5965c0fd3817', 'name': 'Zhi Zhu', 'hidden': False}], 'publishedAt': '2025-11-14T18:52:07.000Z', 'submittedOnDailyAt': '2025-11-18T02:00:07.077Z', 'title': 'MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.', 'upvotes': 65, 'discussionId': '691be81b6bfd5965c0fd3818', 'projectPage': 'https://dr.miromind.ai/', 'githubRepo': 'https://github.com/MiroMindAI/MiroThinker', 'githubStars': 742}, 'publishedAt': '2025-11-14T13:52:07.000Z', 'title': 'MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling', 'summary': 'We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11793.png', 'numComments': 2, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 162}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2511.13647', 'authors': [{'_id': '691bf0036bfd5965c0fd38e6', 'user': {'_id': '686fc1fb779dd387eb29a46b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/686fc1fb779dd387eb29a46b/jdBko2vuxOB9HGAoryMZS.jpeg', 'isPro': False, 'fullname': 'Chunshi Wang', 'user': 'AiEson2', 'type': 'user'}, 'name': 'Chunshi Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:07:33.977Z', 'hidden': False}, {'_id': '691bf0036bfd5965c0fd38e7', 'user': {'_id': '65a420cd90e65dc39a6abe9e', 'avatarUrl': '/avatars/81ac5b749043e899f5017782409f9e28.svg', 'isPro': False, 'fullname': 'yejunliang', 'user': 'yejunliang23', 'type': 'user'}, 'name': 'Junliang Ye', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:07:38.454Z', 'hidden': False}, {'_id': '691bf0036bfd5965c0fd38e8', 'name': 'Yunhan Yang', 'hidden': False}, {'_id': '691bf0036bfd5965c0fd38e9', 'name': 'Yang Li', 'hidden': False}, {'_id': '691bf0036bfd5965c0fd38ea', 'name': 'Zizhuo Lin', 'hidden': False}, {'_id': '691bf0036bfd5965c0fd38eb', 'name': 'Jun Zhu', 'hidden': False}, {'_id': '691bf0036bfd5965c0fd38ec', 'name': 'Zhuo Chen', 'hidden': False}, {'_id': '691bf0036bfd5965c0fd38ed', 'name': 'Yawei Luo', 'hidden': False}, {'_id': '691bf0036bfd5965c0fd38ee', 'name': 'Chunchao Guo', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/eQD2nZGEZhwmzc6HIhQRq.png'], 'publishedAt': '2025-11-17T17:59:52.000Z', 'submittedOnDailyAt': '2025-11-18T01:35:04.756Z', 'title': 'Part-X-MLLM: Part-aware 3D Multimodal Large Language Model', 'submittedOnDailyBy': {'_id': '65a420cd90e65dc39a6abe9e', 'avatarUrl': '/avatars/81ac5b749043e899f5017782409f9e28.svg', 'isPro': False, 'fullname': 'yejunliang', 'user': 'yejunliang23', 'type': 'user'}, 'summary': 'We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/', 'upvotes': 61, 'discussionId': '691bf0036bfd5965c0fd38ef', 'projectPage': 'https://chunshi.wang/Part-X-MLLM/', 'organization': {'_id': '6645f953c39288df638dbdd5', 'name': 'Tencent-Hunyuan', 'fullname': 'Tencent Hunyuan', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png'}}, 'publishedAt': '2025-11-17T12:59:52.000Z', 'title': 'Part-X-MLLM: Part-aware 3D Multimodal Large Language Model', 'summary': 'We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/eQD2nZGEZhwmzc6HIhQRq.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13647.png', 'numComments': 1, 'submittedBy': {'_id': '65a420cd90e65dc39a6abe9e', 'avatarUrl': '/avatars/81ac5b749043e899f5017782409f9e28.svg', 'fullname': 'yejunliang', 'name': 'yejunliang23', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}, 'organization': {'_id': '6645f953c39288df638dbdd5', 'name': 'Tencent-Hunyuan', 'fullname': 'Tencent Hunyuan', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2511.13254', 'authors': [{'_id': '691c1c836bfd5965c0fd39e4', 'user': {'_id': '6419b34a9a27800807c34a63', 'avatarUrl': '/avatars/ee1391a9a153bae0dd04323b1fa5b5d6.svg', 'isPro': False, 'fullname': 'Shalini M', 'user': 'shalinimaiti', 'type': 'user'}, 'name': 'Shalini Maiti', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:06:47.486Z', 'hidden': False}, {'_id': '691c1c836bfd5965c0fd39e5', 'user': {'_id': '6687ee79eee600e418404cc9', 'avatarUrl': '/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg', 'isPro': False, 'fullname': 'Amar Budhiraja', 'user': 'ambud26', 'type': 'user'}, 'name': 'Amar Budhiraja', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:06:49.892Z', 'hidden': False}, {'_id': '691c1c836bfd5965c0fd39e6', 'name': 'Bhavul Gauri', 'hidden': False}, {'_id': '691c1c836bfd5965c0fd39e7', 'user': {'_id': '691c6b9b660c15d270b5838a', 'avatarUrl': '/avatars/2dc40e079bd8b7af7ae4a4ac43acc552.svg', 'isPro': False, 'fullname': 'Gaurav Chaurasia', 'user': 'gchauras', 'type': 'user'}, 'name': 'Gaurav Chaurasia', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:06:45.558Z', 'hidden': False}, {'_id': '691c1c836bfd5965c0fd39e8', 'name': 'Anton Protopopov', 'hidden': False}, {'_id': '691c1c836bfd5965c0fd39e9', 'name': 'Alexis Audran-Reiss', 'hidden': False}, {'_id': '691c1c836bfd5965c0fd39ea', 'name': 'Michael Slater', 'hidden': False}, {'_id': '691c1c836bfd5965c0fd39eb', 'name': 'Despoina Magka', 'hidden': False}, {'_id': '691c1c836bfd5965c0fd39ec', 'name': 'Tatiana Shavrina', 'hidden': False}, {'_id': '691c1c836bfd5965c0fd39ed', 'name': 'Roberta Raileanu', 'hidden': False}, {'_id': '691c1c836bfd5965c0fd39ee', 'name': 'Yoram Bachrach', 'hidden': False}], 'publishedAt': '2025-11-17T11:13:34.000Z', 'submittedOnDailyAt': '2025-11-18T04:47:00.815Z', 'title': 'Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance', 'submittedOnDailyBy': {'_id': '6687ee79eee600e418404cc9', 'avatarUrl': '/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg', 'isPro': False, 'fullname': 'Amar Budhiraja', 'user': 'ambud26', 'type': 'user'}, 'summary': 'Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.', 'upvotes': 55, 'discussionId': '691c1c846bfd5965c0fd39fc', 'organization': {'_id': '5e63d8713071d5be688861b8', 'name': 'facebook', 'fullname': 'AI at Meta', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png'}}, 'publishedAt': '2025-11-17T06:13:34.000Z', 'title': 'Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance', 'summary': 'Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13254.png', 'numComments': 2, 'submittedBy': {'_id': '6687ee79eee600e418404cc9', 'avatarUrl': '/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg', 'fullname': 'Amar Budhiraja', 'name': 'ambud26', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'organization': {'_id': '5e63d8713071d5be688861b8', 'name': 'facebook', 'fullname': 'AI at Meta', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2511.11653', 'authors': [{'_id': '691be8946bfd5965c0fd381a', 'name': 'Duolin Sun', 'hidden': False}, {'_id': '691be8946bfd5965c0fd381b', 'name': 'Meixiu Long', 'hidden': False}, {'_id': '691be8946bfd5965c0fd381c', 'user': {'_id': '63f87b14b0ae1748524a8f50', 'avatarUrl': '/avatars/e6543d75d115bd34edbd80f322457b75.svg', 'isPro': False, 'fullname': 'dan', 'user': 'prayerdan', 'type': 'user'}, 'name': 'Dan Yang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:08:00.142Z', 'hidden': False}, {'_id': '691be8946bfd5965c0fd381d', 'name': 'Yihan Jiao', 'hidden': False}, {'_id': '691be8946bfd5965c0fd381e', 'name': 'Zhehao Tan', 'hidden': False}, {'_id': '691be8946bfd5965c0fd381f', 'name': 'Jie Feng', 'hidden': False}, {'_id': '691be8946bfd5965c0fd3820', 'name': 'Junjie Wang', 'hidden': False}, {'_id': '691be8946bfd5965c0fd3821', 'name': 'Yue Shen', 'hidden': False}, {'_id': '691be8946bfd5965c0fd3822', 'name': 'Peng Wei', 'hidden': False}, {'_id': '691be8946bfd5965c0fd3823', 'name': 'Jian Wang', 'hidden': False}, {'_id': '691be8946bfd5965c0fd3824', 'name': 'Jinjie Gu', 'hidden': False}], 'publishedAt': '2025-11-10T15:25:31.000Z', 'submittedOnDailyAt': '2025-11-18T01:28:16.607Z', 'title': 'GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning', 'submittedOnDailyBy': {'_id': '63f87b14b0ae1748524a8f50', 'avatarUrl': '/avatars/e6543d75d115bd34edbd80f322457b75.svg', 'isPro': False, 'fullname': 'dan', 'user': 'prayerdan', 'type': 'user'}, 'summary': 'Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.', 'upvotes': 40, 'discussionId': '691be8956bfd5965c0fd3825', 'githubRepo': 'https://github.com/AQ-MedAI/Diver', 'githubStars': 205, 'organization': {'_id': '68a4291ddb54c3044cf32600', 'name': 'AQ-MedAI', 'fullname': 'AQ', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/6826eb9bf609dcc6d6c6df52/mOPPxrtf_etoO1rlLBtnV.jpeg'}}, 'publishedAt': '2025-11-10T10:25:31.000Z', 'title': 'GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning', 'summary': 'Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11653.png', 'numComments': 6, 'submittedBy': {'_id': '63f87b14b0ae1748524a8f50', 'avatarUrl': '/avatars/e6543d75d115bd34edbd80f322457b75.svg', 'fullname': 'dan', 'name': 'prayerdan', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}, 'organization': {'_id': '68a4291ddb54c3044cf32600', 'name': 'AQ-MedAI', 'fullname': 'AQ', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/6826eb9bf609dcc6d6c6df52/mOPPxrtf_etoO1rlLBtnV.jpeg'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2511.13704', 'authors': [{'_id': '691bf40e6bfd5965c0fd392c', 'user': {'_id': '6570450a78d7aca0c361a177', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/MX7jHhTQwLs-BvYIu5rqb.jpeg', 'isPro': False, 'fullname': 'Harold Chen', 'user': 'Harold328', 'type': 'user'}, 'name': 'Harold Haodong Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:07:10.644Z', 'hidden': False}, {'_id': '691bf40e6bfd5965c0fd392d', 'user': {'_id': '66ea643899af9ac3463639b1', 'avatarUrl': '/avatars/252d470e761a57834dee3dbc60dfefed.svg', 'isPro': False, 'fullname': 'Disen Lan', 'user': 'landisen', 'type': 'user'}, 'name': 'Disen Lan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:07:17.030Z', 'hidden': False}, {'_id': '691bf40e6bfd5965c0fd392e', 'name': 'Wen-Jie Shu', 'hidden': False}, {'_id': '691bf40e6bfd5965c0fd392f', 'name': 'Qingyang Liu', 'hidden': False}, {'_id': '691bf40e6bfd5965c0fd3930', 'name': 'Zihan Wang', 'hidden': False}, {'_id': '691bf40e6bfd5965c0fd3931', 'name': 'Sirui Chen', 'hidden': False}, {'_id': '691bf40e6bfd5965c0fd3932', 'name': 'Wenkai Cheng', 'hidden': False}, {'_id': '691bf40e6bfd5965c0fd3933', 'name': 'Kanghao Chen', 'hidden': False}, {'_id': '691bf40e6bfd5965c0fd3934', 'name': 'Hongfei Zhang', 'hidden': False}, {'_id': '691bf40e6bfd5965c0fd3935', 'user': {'_id': '674aa9af9494dd9106006c27', 'avatarUrl': '/avatars/2f04bb009983ec0ade738aa7941cf6dc.svg', 'isPro': False, 'fullname': 'ggbond', 'user': 'zhangzixin02', 'type': 'user'}, 'name': 'Zixin Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:07:14.604Z', 'hidden': False}, {'_id': '691bf40e6bfd5965c0fd3936', 'name': 'Rongjin Guo', 'hidden': False}, {'_id': '691bf40e6bfd5965c0fd3937', 'name': 'Yu Cheng', 'hidden': False}, {'_id': '691bf40e6bfd5965c0fd3938', 'name': 'Ying-Cong Chen', 'hidden': False}], 'publishedAt': '2025-11-17T18:52:44.000Z', 'submittedOnDailyAt': '2025-11-18T01:53:38.680Z', 'title': 'TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models', 'submittedOnDailyBy': {'_id': '674aa9af9494dd9106006c27', 'avatarUrl': '/avatars/2f04bb009983ec0ade738aa7941cf6dc.svg', 'isPro': False, 'fullname': 'ggbond', 'user': 'zhangzixin02', 'type': 'user'}, 'summary': \"The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.\", 'upvotes': 38, 'discussionId': '691bf40f6bfd5965c0fd3939', 'projectPage': 'https://haroldchen19.github.io/TiViBench-Page/', 'githubRepo': 'https://github.com/EnVision-Research/TiViBench', 'githubStars': 41}, 'publishedAt': '2025-11-17T13:52:44.000Z', 'title': 'TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models', 'summary': \"The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13704.png', 'numComments': 3, 'submittedBy': {'_id': '674aa9af9494dd9106006c27', 'avatarUrl': '/avatars/2f04bb009983ec0ade738aa7941cf6dc.svg', 'fullname': 'ggbond', 'name': 'zhangzixin02', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2511.09611', 'authors': [{'_id': '6916fa1a931c8d43efc57d66', 'user': {'_id': '64e357dd825f4133e7427bf8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64e357dd825f4133e7427bf8/HwaWhINrzkbXG6SHG2oyf.jpeg', 'isPro': False, 'fullname': 'tyfeld', 'user': 'tyfeld', 'type': 'user'}, 'name': 'Ye Tian', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:09:31.571Z', 'hidden': False}, {'_id': '6916fa1a931c8d43efc57d67', 'user': {'_id': '64fde4e252e82dd432b74ce9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg', 'isPro': False, 'fullname': 'Ling Yang', 'user': 'Lingaaaaaaa', 'type': 'user'}, 'name': 'Ling Yang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-17T10:31:39.834Z', 'hidden': False}, {'_id': '6916fa1a931c8d43efc57d68', 'user': {'_id': '68370f225b8acaa67dfe76ff', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/68370f225b8acaa67dfe76ff/iNLiNEsEqR6R2i-epe9Nq.jpeg', 'isPro': False, 'fullname': '杨炅凡', 'user': 'Gokottaw434', 'type': 'user'}, 'name': 'Jiongfan Yang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:09:26.359Z', 'hidden': False}, {'_id': '6916fa1a931c8d43efc57d69', 'name': 'Anran Wang', 'hidden': False}, {'_id': '6916fa1a931c8d43efc57d6a', 'name': 'Yu Tian', 'hidden': False}, {'_id': '6916fa1a931c8d43efc57d6b', 'name': 'Jiani Zheng', 'hidden': False}, {'_id': '6916fa1a931c8d43efc57d6c', 'name': 'Haochen Wang', 'hidden': False}, {'_id': '6916fa1a931c8d43efc57d6d', 'name': 'Zhiyang Teng', 'hidden': False}, {'_id': '6916fa1a931c8d43efc57d6e', 'name': 'Zhuochen Wang', 'hidden': False}, {'_id': '6916fa1a931c8d43efc57d6f', 'name': 'Yinjie Wang', 'hidden': False}, {'_id': '6916fa1a931c8d43efc57d70', 'name': 'Yunhai Tong', 'hidden': False}, {'_id': '6916fa1a931c8d43efc57d71', 'name': 'Mengdi Wang', 'hidden': False}, {'_id': '6916fa1a931c8d43efc57d72', 'name': 'Xiangtai Li', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64fde4e252e82dd432b74ce9/1kEAqmfprWpR7h4c0aBof.mp4'], 'publishedAt': '2025-11-12T18:58:21.000Z', 'submittedOnDailyAt': '2025-11-18T00:16:10.548Z', 'title': 'MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation', 'submittedOnDailyBy': {'_id': '64fde4e252e82dd432b74ce9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg', 'isPro': False, 'fullname': 'Ling Yang', 'user': 'Lingaaaaaaa', 'type': 'user'}, 'summary': 'While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\\\\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel', 'upvotes': 37, 'discussionId': '6916fa1a931c8d43efc57d73', 'projectPage': 'https://tyfeld.github.io/mmadaparellel.github.io/', 'githubRepo': 'https://github.com/tyfeld/MMaDA-Parallel', 'ai_summary': 'A parallel multimodal diffusion framework, MMaDA-Parallel, enhances cross-modal alignment and semantic consistency in thinking-aware image synthesis by addressing error propagation issues in sequential approaches.', 'ai_keywords': ['thinking-aware generation', 'sequential', 'autoregressive', 'ParaBench', 'diffusion framework', 'parallel multimodal diffusion', 'MMaDA-Parallel', 'continuous', 'bidirectional interaction', 'supervised finetuning', 'Parallel Reinforcement Learning', 'ParaRL', 'semantic rewards', 'cross-modal alignment', 'semantic consistency', 'Output Alignment', 'Bagel'], 'githubStars': 45, 'organization': {'_id': '653b817d32c97d0655575872', 'name': 'ByteDance', 'fullname': 'ByteDance', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png'}}, 'publishedAt': '2025-11-12T13:58:21.000Z', 'title': 'MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation', 'summary': 'While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\\\\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64fde4e252e82dd432b74ce9/1kEAqmfprWpR7h4c0aBof.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.09611.png', 'numComments': 2, 'submittedBy': {'_id': '64fde4e252e82dd432b74ce9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg', 'fullname': 'Ling Yang', 'name': 'Lingaaaaaaa', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 12}, 'organization': {'_id': '653b817d32c97d0655575872', 'name': 'ByteDance', 'fullname': 'ByteDance', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2511.13648', 'authors': [{'_id': '691bf1746bfd5965c0fd390d', 'user': {'_id': '65af6f6b52e1b2aae437af2e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg', 'isPro': False, 'fullname': 'Ziang Cao', 'user': 'Caoza', 'type': 'user'}, 'name': 'Ziang Cao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:07:29.228Z', 'hidden': False}, {'_id': '691bf1746bfd5965c0fd390e', 'name': 'Fangzhou Hong', 'hidden': False}, {'_id': '691bf1746bfd5965c0fd390f', 'name': 'Zhaoxi Chen', 'hidden': False}, {'_id': '691bf1746bfd5965c0fd3910', 'name': 'Liang Pan', 'hidden': False}, {'_id': '691bf1746bfd5965c0fd3911', 'name': 'Ziwei Liu', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/t_KUb2UHHyR8OZqM-9bK-.mp4'], 'publishedAt': '2025-11-17T17:59:53.000Z', 'submittedOnDailyAt': '2025-11-18T01:39:50.445Z', 'title': 'PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': '3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.', 'upvotes': 29, 'discussionId': '691bf1756bfd5965c0fd3912', 'projectPage': 'https://physx-anything.github.io/', 'githubRepo': 'https://github.com/ziangcao0312/PhysX-Anything', 'githubStars': 84}, 'publishedAt': '2025-11-17T12:59:53.000Z', 'title': 'PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image', 'summary': '3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/t_KUb2UHHyR8OZqM-9bK-.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13648.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 162}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2511.11332', 'authors': [{'_id': '691b09b66bfd5965c0fd367d', 'name': 'Chaoyun Zhang', 'hidden': False}, {'_id': '691b09b66bfd5965c0fd367e', 'name': 'Liqun Li', 'hidden': False}, {'_id': '691b09b66bfd5965c0fd367f', 'name': 'He Huang', 'hidden': False}, {'_id': '691b09b66bfd5965c0fd3680', 'name': 'Chiming Ni', 'hidden': False}, {'_id': '691b09b66bfd5965c0fd3681', 'name': 'Bo Qiao', 'hidden': False}, {'_id': '691b09b66bfd5965c0fd3682', 'name': 'Si Qin', 'hidden': False}, {'_id': '691b09b66bfd5965c0fd3683', 'name': 'Yu Kang', 'hidden': False}, {'_id': '691b09b66bfd5965c0fd3684', 'name': 'Minghua Ma', 'hidden': False}, {'_id': '691b09b66bfd5965c0fd3685', 'name': 'Qingwei Lin', 'hidden': False}, {'_id': '691b09b66bfd5965c0fd3686', 'name': 'Saravan Rajmohan', 'hidden': False}, {'_id': '691b09b66bfd5965c0fd3687', 'name': 'Dongmei Zhang', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/654dbac9938fbf1e696be8aa/I0zgaHnbkVFlFCK7fEx1-.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/654dbac9938fbf1e696be8aa/a5zQwYjmhqQXoHem4cKrr.png'], 'publishedAt': '2025-11-14T14:05:31.000Z', 'submittedOnDailyAt': '2025-11-18T00:17:05.397Z', 'title': 'UFO^3: Weaving the Digital Agent Galaxy', 'submittedOnDailyBy': {'_id': '654dbac9938fbf1e696be8aa', 'avatarUrl': '/avatars/b3c4035c48169c1bfb04a439fce3499f.svg', 'isPro': True, 'fullname': 'Chaoyun Zhang', 'user': 'vyokky', 'type': 'user'}, 'summary': 'Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO^3, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO^3 models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.\\n  We evaluate UFO^3 on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO^3 achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO^3 achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.', 'upvotes': 17, 'discussionId': '691b09b66bfd5965c0fd368a', 'projectPage': 'https://microsoft.github.io/UFO/', 'githubRepo': 'https://github.com/microsoft/UFO/', 'ai_summary': 'UFO$^3$ unifies heterogeneous devices into a single orchestration fabric, enabling seamless task collaboration and dynamic optimization across distributed environments.', 'ai_keywords': ['TaskConstellation', 'TaskStars', 'TaskStarLines', 'DAG', 'Constellation Orchestrator', 'Agent Interaction Protocol', 'NebulaBench', 'parallelism', 'end-to-end latency', 'fault-injection', 'adaptive computing fabric', 'ubiquitous computing'], 'githubStars': 7720, 'organization': {'_id': '5e6485f787403103f9f1055e', 'name': 'microsoft', 'fullname': 'Microsoft', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png'}}, 'publishedAt': '2025-11-14T09:05:31.000Z', 'title': 'UFO^3: Weaving the Digital Agent Galaxy', 'summary': 'Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO^3, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO^3 models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.\\n  We evaluate UFO^3 on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO^3 achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO^3 achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/654dbac9938fbf1e696be8aa/I0zgaHnbkVFlFCK7fEx1-.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/654dbac9938fbf1e696be8aa/a5zQwYjmhqQXoHem4cKrr.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11332.png', 'numComments': 2, 'submittedBy': {'_id': '654dbac9938fbf1e696be8aa', 'avatarUrl': '/avatars/b3c4035c48169c1bfb04a439fce3499f.svg', 'fullname': 'Chaoyun Zhang', 'name': 'vyokky', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'organization': {'_id': '5e6485f787403103f9f1055e', 'name': 'microsoft', 'fullname': 'Microsoft', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2511.13655', 'authors': [{'_id': '691bf0fb6bfd5965c0fd38f1', 'name': 'Henry Herzog', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38f2', 'name': 'Favyen Bastani', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38f3', 'name': 'Yawen Zhang', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38f4', 'name': 'Gabriel Tseng', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38f5', 'name': 'Joseph Redmon', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38f6', 'name': 'Hadrien Sablon', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38f7', 'name': 'Ryan Park', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38f8', 'name': 'Jacob Morrison', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38f9', 'name': 'Alexandra Buraczynski', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38fa', 'name': 'Karen Farley', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38fb', 'name': 'Joshua Hansen', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38fc', 'name': 'Andrew Howe', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38fd', 'name': 'Patrick Alan Johnson', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38fe', 'name': 'Mark Otterlee', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd38ff', 'name': 'Ted Schmitt', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd3900', 'name': 'Hunter Pitelka', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd3901', 'name': 'Stephen Daspit', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd3902', 'name': 'Rachel Ratner', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd3903', 'name': 'Christopher Wilhelm', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd3904', 'name': 'Sebastian Wood', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd3905', 'name': 'Mike Jacobi', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd3906', 'name': 'Hannah Kerner', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd3907', 'name': 'Evan Shelhamer', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd3908', 'name': 'Ali Farhadi', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd3909', 'name': 'Ranjay Krishna', 'hidden': False}, {'_id': '691bf0fb6bfd5965c0fd390a', 'name': 'Patrick Beukema', 'hidden': False}], 'publishedAt': '2025-11-17T18:06:26.000Z', 'submittedOnDailyAt': '2025-11-18T01:37:55.642Z', 'title': 'OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': \"Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at https://github.com/allenai/olmoearth_pretrain{https://github.com/allenai/olmoearth_pretrain}.\", 'upvotes': 6, 'discussionId': '691bf0fb6bfd5965c0fd390b', 'projectPage': 'https://olmoearth.allenai.org/', 'githubRepo': 'https://github.com/allenai/olmoearth_pretrain', 'githubStars': 80, 'organization': {'_id': '5e70f3648ce3c604d78fe132', 'name': 'allenai', 'fullname': 'Ai2', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png'}}, 'publishedAt': '2025-11-17T13:06:26.000Z', 'title': 'OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation', 'summary': \"Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at https://github.com/allenai/olmoearth_pretrain{https://github.com/allenai/olmoearth_pretrain}.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13655.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 162}, 'organization': {'_id': '5e70f3648ce3c604d78fe132', 'name': 'allenai', 'fullname': 'Ai2', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2511.12710', 'authors': [{'_id': '691c7827fecc9dfeafbf46c1', 'name': 'Yunhao Chen', 'hidden': False}, {'_id': '691c7827fecc9dfeafbf46c2', 'user': {'_id': '677d32cf28895e2124065a63', 'avatarUrl': '/avatars/897fa9fb3b3adc6e60094e3b505f94bc.svg', 'isPro': False, 'fullname': 'Xin Wang', 'user': 'xinwang22', 'type': 'user'}, 'name': 'Xin Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:07:51.263Z', 'hidden': False}, {'_id': '691c7827fecc9dfeafbf46c3', 'name': 'Juncheng Li', 'hidden': False}, {'_id': '691c7827fecc9dfeafbf46c4', 'name': 'Yixu Wang', 'hidden': False}, {'_id': '691c7827fecc9dfeafbf46c5', 'name': 'Jie Li', 'hidden': False}, {'_id': '691c7827fecc9dfeafbf46c6', 'name': 'Yan Teng', 'hidden': False}, {'_id': '691c7827fecc9dfeafbf46c7', 'name': 'Yingchun Wang', 'hidden': False}, {'_id': '691c7827fecc9dfeafbf46c8', 'name': 'Xingjun Ma', 'hidden': False}], 'publishedAt': '2025-11-16T17:52:07.000Z', 'submittedOnDailyAt': '2025-11-18T11:17:48.011Z', 'title': 'Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs', 'submittedOnDailyBy': {'_id': '635f7b4af72ae36c3e3223be', 'avatarUrl': '/avatars/afd8967e2e0dbddc0cf5692ec5673ba1.svg', 'isPro': False, 'fullname': 'Yunhao Chen', 'user': 'dongdongunique', 'type': 'user'}, 'summary': 'Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce EvoSynth, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\\\\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.', 'upvotes': 5, 'discussionId': '691c7827fecc9dfeafbf46c9'}, 'publishedAt': '2025-11-16T12:52:07.000Z', 'title': 'Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs', 'summary': 'Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce EvoSynth, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\\\\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.12710.png', 'numComments': 1, 'submittedBy': {'_id': '635f7b4af72ae36c3e3223be', 'avatarUrl': '/avatars/afd8967e2e0dbddc0cf5692ec5673ba1.svg', 'fullname': 'Yunhao Chen', 'name': 'dongdongunique', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2511.13646', 'authors': [{'_id': '691bf72f6bfd5965c0fd3941', 'user': {'_id': '645ac350c35da9c7afd82379', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645ac350c35da9c7afd82379/nRNoSz0QA2yJmqK3C3F95.jpeg', 'isPro': False, 'fullname': 'Chunqiu Steven Xia', 'user': 'nevetsaix', 'type': 'user'}, 'name': 'Chunqiu Steven Xia', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:07:07.969Z', 'hidden': False}, {'_id': '691bf72f6bfd5965c0fd3942', 'name': 'Zhe Wang', 'hidden': False}, {'_id': '691bf72f6bfd5965c0fd3943', 'name': 'Yan Yang', 'hidden': False}, {'_id': '691bf72f6bfd5965c0fd3944', 'name': 'Yuxiang Wei', 'hidden': False}, {'_id': '691bf72f6bfd5965c0fd3945', 'name': 'Lingming Zhang', 'hidden': False}], 'publishedAt': '2025-11-17T17:58:18.000Z', 'submittedOnDailyAt': '2025-11-18T02:04:03.244Z', 'title': 'Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.', 'upvotes': 4, 'discussionId': '691bf72f6bfd5965c0fd3946'}, 'publishedAt': '2025-11-17T12:58:18.000Z', 'title': 'Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?', 'summary': 'Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13646.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 162}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2511.09809', 'authors': [{'_id': '691bdb1a6bfd5965c0fd37a6', 'name': 'Konstantinos M. Dafnis', 'hidden': False}, {'_id': '691bdb1a6bfd5965c0fd37a7', 'name': 'Dimitris N. Metaxas', 'hidden': False}], 'publishedAt': '2025-11-12T23:25:58.000Z', 'submittedOnDailyAt': '2025-11-18T00:12:55.813Z', 'title': 'Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models', 'submittedOnDailyBy': {'_id': '653c4555e337eff8f5c9e930', 'avatarUrl': '/avatars/a03a9e017fc01995a30ff8d8f1657b42.svg', 'isPro': False, 'fullname': 'Konstantinos Dafnis', 'user': 'kdafnis', 'type': 'user'}, 'summary': 'Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.', 'upvotes': 3, 'discussionId': '691bdb1a6bfd5965c0fd37a8', 'organization': {'_id': '64f9febe35a7fc7d4fe404d1', 'name': 'Rutgers-CBIM', 'fullname': 'Rutgers Center for Computational Biomedicine Imaging and Modeling'}}, 'publishedAt': '2025-11-12T18:25:58.000Z', 'title': 'Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models', 'summary': 'Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.09809.png', 'numComments': 1, 'submittedBy': {'_id': '653c4555e337eff8f5c9e930', 'avatarUrl': '/avatars/a03a9e017fc01995a30ff8d8f1657b42.svg', 'fullname': 'Konstantinos Dafnis', 'name': 'kdafnis', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'organization': {'_id': '64f9febe35a7fc7d4fe404d1', 'name': 'Rutgers-CBIM', 'fullname': 'Rutgers Center for Computational Biomedicine Imaging and Modeling'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2511.12997', 'authors': [{'_id': '691bf7a86bfd5965c0fd3948', 'name': 'Genglin Liu', 'hidden': False}, {'_id': '691bf7a86bfd5965c0fd3949', 'name': 'Shijie Geng', 'hidden': False}, {'_id': '691bf7a86bfd5965c0fd394a', 'name': 'Sha Li', 'hidden': False}, {'_id': '691bf7a86bfd5965c0fd394b', 'name': 'Hejie Cui', 'hidden': False}, {'_id': '691bf7a86bfd5965c0fd394c', 'name': 'Sarah Zhang', 'hidden': False}, {'_id': '691bf7a86bfd5965c0fd394d', 'name': 'Xin Liu', 'hidden': False}, {'_id': '691bf7a86bfd5965c0fd394e', 'name': 'Tianyi Liu', 'hidden': False}], 'publishedAt': '2025-11-17T05:38:50.000Z', 'submittedOnDailyAt': '2025-11-18T02:07:31.429Z', 'title': 'WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.', 'upvotes': 2, 'discussionId': '691bf7a86bfd5965c0fd394f'}, 'publishedAt': '2025-11-17T00:38:50.000Z', 'title': 'WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance', 'summary': 'Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.12997.png', 'numComments': 0, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 162}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2511.11407', 'authors': [{'_id': '691c14fd6bfd5965c0fd39ba', 'user': {'_id': '66e01d290b0271487147fdc9', 'avatarUrl': '/avatars/2dfb01fba8b781a15555f52f8b92d06c.svg', 'isPro': False, 'fullname': 'Manyu Li', 'user': 'ieellee', 'type': 'user'}, 'name': 'Manyu Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:06:59.028Z', 'hidden': False}, {'_id': '691c14fd6bfd5965c0fd39bb', 'name': 'Ruian He', 'hidden': False}, {'_id': '691c14fd6bfd5965c0fd39bc', 'name': 'Chenxi Ma', 'hidden': False}, {'_id': '691c14fd6bfd5965c0fd39bd', 'name': 'Weimin Tan', 'hidden': False}, {'_id': '691c14fd6bfd5965c0fd39be', 'name': 'Bo Yan', 'hidden': False}], 'publishedAt': '2025-11-14T15:35:43.000Z', 'submittedOnDailyAt': '2025-11-18T04:11:54.659Z', 'title': 'MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model', 'submittedOnDailyBy': {'_id': '66e01d290b0271487147fdc9', 'avatarUrl': '/avatars/2dfb01fba8b781a15555f52f8b92d06c.svg', 'isPro': False, 'fullname': 'Manyu Li', 'user': 'ieellee', 'type': 'user'}, 'summary': \"Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.\", 'upvotes': 2, 'discussionId': '691c14fd6bfd5965c0fd39bf', 'githubRepo': 'https://github.com/ieellee/MicroVQA-PlusPlus'}, 'publishedAt': '2025-11-14T10:35:43.000Z', 'title': 'MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model', 'summary': \"Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11407.png', 'numComments': 1, 'submittedBy': {'_id': '66e01d290b0271487147fdc9', 'avatarUrl': '/avatars/2dfb01fba8b781a15555f52f8b92d06c.svg', 'fullname': 'Manyu Li', 'name': 'ieellee', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2511.02767', 'authors': [{'_id': '691c759cfecc9dfeafbf46ba', 'user': {'_id': '6301af7259ab5d9dc0a15060', 'avatarUrl': '/avatars/0bcda98801877332893a1de169000737.svg', 'isPro': False, 'fullname': 'Tyler Zhu', 'user': 'tyleryzhu', 'type': 'user'}, 'name': 'Tyler Zhu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-11-18T14:06:30.540Z', 'hidden': False}, {'_id': '691c759cfecc9dfeafbf46bb', 'name': 'Tengda Han', 'hidden': False}, {'_id': '691c759cfecc9dfeafbf46bc', 'name': 'Leonidas Guibas', 'hidden': False}, {'_id': '691c759cfecc9dfeafbf46bd', 'name': 'Viorica Pătrăucean', 'hidden': False}, {'_id': '691c759cfecc9dfeafbf46be', 'name': 'Maks Ovsjanikov', 'hidden': False}], 'publishedAt': '2025-11-04T17:52:14.000Z', 'submittedOnDailyAt': '2025-11-18T11:04:28.705Z', 'title': 'Dynamic Reflections: Probing Video Representations with Text Alignment', 'submittedOnDailyBy': {'_id': '6301af7259ab5d9dc0a15060', 'avatarUrl': '/avatars/0bcda98801877332893a1de169000737.svg', 'isPro': False, 'fullname': 'Tyler Zhu', 'user': 'tyleryzhu', 'type': 'user'}, 'summary': 'The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at https://video-prh.github.io/', 'upvotes': 1, 'discussionId': '691c759dfecc9dfeafbf46bf', 'organization': {'_id': '60f6cbb2852126bac698c89e', 'name': 'deepmind', 'fullname': 'Deepmind', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg'}}, 'publishedAt': '2025-11-04T12:52:14.000Z', 'title': 'Dynamic Reflections: Probing Video Representations with Text Alignment', 'summary': 'The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at https://video-prh.github.io/', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02767.png', 'numComments': 1, 'submittedBy': {'_id': '6301af7259ab5d9dc0a15060', 'avatarUrl': '/avatars/0bcda98801877332893a1de169000737.svg', 'fullname': 'Tyler Zhu', 'name': 'tyleryzhu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'organization': {'_id': '60f6cbb2852126bac698c89e', 'name': 'deepmind', 'fullname': 'Deepmind', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2511.10628', 'authors': [{'_id': '6916f8f93b377710cdf19a4b', 'name': 'Jiang Liu', 'hidden': False}, {'_id': '6916f8f93b377710cdf19a4c', 'name': 'Jialian Wu', 'hidden': False}, {'_id': '6916f8f93b377710cdf19a4d', 'name': 'Xiaodong Yu', 'hidden': False}, {'_id': '6916f8f93b377710cdf19a4e', 'name': 'Yusheng Su', 'hidden': False}, {'_id': '6916f8f93b377710cdf19a4f', 'name': 'Prakamya Mishra', 'hidden': False}, {'_id': '6916f8f93b377710cdf19a50', 'name': 'Gowtham Ramesh', 'hidden': False}, {'_id': '6916f8f93b377710cdf19a51', 'name': 'Sudhanshu Ranjan', 'hidden': False}, {'_id': '6916f8f93b377710cdf19a52', 'name': 'Chaitanya Manem', 'hidden': False}, {'_id': '6916f8f93b377710cdf19a53', 'name': 'Ximeng Sun', 'hidden': False}, {'_id': '6916f8f93b377710cdf19a54', 'name': 'Ze Wang', 'hidden': False}, {'_id': '6916f8f93b377710cdf19a55', 'name': 'Pratik Prabhanjan Brahma', 'hidden': False}, {'_id': '6916f8f93b377710cdf19a56', 'name': 'Zicheng Liu', 'hidden': False}, {'_id': '6916f8f93b377710cdf19a57', 'name': 'Emad Barsoum', 'hidden': False}], 'publishedAt': '2025-11-13T18:52:46.000Z', 'submittedOnDailyAt': '2025-11-18T07:47:53.652Z', 'title': 'Instella: Fully Open Language Models with Stellar Performance', 'submittedOnDailyBy': {'_id': '65bb2a8400e143d672b170c9', 'avatarUrl': '/avatars/e1d745d12f0a4c8dae95d2f6983d5201.svg', 'isPro': False, 'fullname': 'Prakamya Mishra', 'user': 'Prakamya', 'type': 'user'}, 'summary': 'Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.', 'upvotes': 0, 'discussionId': '6916f8fa3b377710cdf19a58', 'githubRepo': 'https://github.com/AMD-AGI/Instella', 'ai_summary': 'Instella, a family of fully open large language models, achieves state-of-the-art performance using open data and is competitive with leading open-weight models, with specialized variants for long context and mathematical reasoning.', 'ai_keywords': ['large language models', 'AMD Instinct MI300X GPUs', 'large-scale pre-training', 'general-purpose instruction tuning', 'alignment with human preferences', 'Instella', 'Instella-Long', 'Instella-Math', 'supervised fine-tuning', 'reinforcement learning'], 'githubStars': 294, 'organization': {'_id': '641d8a1cfe2172ccc01c265e', 'name': 'amd', 'fullname': 'AMD', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/65ff7616871b36bf84150cda/mBg2Z4Vcdt5bkjxbYJFej.png'}}, 'publishedAt': '2025-11-13T13:52:46.000Z', 'title': 'Instella: Fully Open Language Models with Stellar Performance', 'summary': 'Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.10628.png', 'numComments': 1, 'submittedBy': {'_id': '65bb2a8400e143d672b170c9', 'avatarUrl': '/avatars/e1d745d12f0a4c8dae95d2f6983d5201.svg', 'fullname': 'Prakamya Mishra', 'name': 'Prakamya', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'organization': {'_id': '641d8a1cfe2172ccc01c265e', 'name': 'amd', 'fullname': 'AMD', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/65ff7616871b36bf84150cda/mBg2Z4Vcdt5bkjxbYJFej.png'}, 'isAuthorParticipating': False}"
]