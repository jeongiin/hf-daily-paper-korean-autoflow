[
    "{'paper': {'id': '2503.20215', 'authors': [{'_id': '67e4f2507e97884ba4205660', 'name': 'Jin Xu', 'hidden': False}, {'_id': '67e4f2507e97884ba4205661', 'user': {'_id': '661e577cbac5d981f883b743', 'avatarUrl': '/avatars/95e55e9707a6b55594c264081202d7f4.svg', 'isPro': False, 'fullname': 'GuoZhifang', 'user': 'ZhifangGuo', 'type': 'user'}, 'name': 'Zhifang Guo', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:52:32.600Z', 'hidden': False}, {'_id': '67e4f2507e97884ba4205662', 'user': {'_id': '6594f06ac04427eb38444bce', 'avatarUrl': '/avatars/b13fbf589b25eff038deb3fa12d95871.svg', 'isPro': False, 'fullname': 'Jinzheng He', 'user': 'jinzheng-he', 'type': 'user'}, 'name': 'Jinzheng He', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:52:48.707Z', 'hidden': False}, {'_id': '67e4f2507e97884ba4205663', 'name': 'Hangrui Hu', 'hidden': False}, {'_id': '67e4f2507e97884ba4205664', 'name': 'Ting He', 'hidden': False}, {'_id': '67e4f2507e97884ba4205665', 'user': {'_id': '63451cf0a05b51f7ded25505', 'avatarUrl': '/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg', 'isPro': False, 'fullname': 'shuai bai', 'user': 'bluelike', 'type': 'user'}, 'name': 'Shuai Bai', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:53:02.818Z', 'hidden': False}, {'_id': '67e4f2507e97884ba4205666', 'user': {'_id': '6461d675681b2e19b6acb5a5', 'avatarUrl': '/avatars/0d95d65d30f6672ec09dc92155324d7f.svg', 'isPro': False, 'fullname': 'Keqin Chen', 'user': 'chenkq', 'type': 'user'}, 'name': 'Keqin Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:53:19.770Z', 'hidden': False}, {'_id': '67e4f2507e97884ba4205667', 'user': {'_id': '649a3ba9342f14148357c367', 'avatarUrl': '/avatars/81a769fa38b7384f382ff3cc10d6d624.svg', 'isPro': False, 'fullname': 'Jialin Wang', 'user': 'JialinWang', 'type': 'user'}, 'name': 'Jialin Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:53:26.467Z', 'hidden': False}, {'_id': '67e4f2507e97884ba4205668', 'name': 'Yang Fan', 'hidden': False}, {'_id': '67e4f2507e97884ba4205669', 'user': {'_id': '6712930f0fac3235c56edf5b', 'avatarUrl': '/avatars/cafe7cb56ce7c3b2572f5f2d0b89357a.svg', 'isPro': False, 'fullname': 'kai dang', 'user': '1vk5i', 'type': 'user'}, 'name': 'Kai Dang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:53:33.105Z', 'hidden': False}, {'_id': '67e4f2507e97884ba420566a', 'name': 'Bin Zhang', 'hidden': False}, {'_id': '67e4f2507e97884ba420566b', 'name': 'Xiong Wang', 'hidden': False}, {'_id': '67e4f2507e97884ba420566c', 'user': {'_id': '62c6a751a71b40cf26f359a8', 'avatarUrl': '/avatars/49abd2e71946035452c316d703baaac6.svg', 'isPro': False, 'fullname': 'Yunfei Chu', 'user': 'faychu', 'type': 'user'}, 'name': 'Yunfei Chu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:53:51.503Z', 'hidden': False}, {'_id': '67e4f2507e97884ba420566d', 'user': {'_id': '620760a26e3b7210c2ff1943', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg', 'isPro': False, 'fullname': 'Junyang Lin', 'user': 'JustinLin610', 'type': 'user'}, 'name': 'Junyang Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:53:44.277Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/659e513ea9bc1f60189ac148/3pDelIehF3CWGPS0jrZvN.png'], 'publishedAt': '2025-03-26T04:17:55.000Z', 'submittedOnDailyAt': '2025-03-27T05:14:09.555Z', 'title': 'Qwen2.5-Omni Technical Report', 'submittedOnDailyBy': {'_id': '659e513ea9bc1f60189ac148', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/659e513ea9bc1f60189ac148/DBDDqjGTQ0SvjWyuYu7py.jpeg', 'isPro': False, 'fullname': 'YuanjunLv', 'user': 'Bakerbunker', 'type': 'user'}, 'summary': \"In this report, we present Qwen2.5-Omni, an end-to-end multimodal model\\ndesigned to perceive diverse modalities, including text, images, audio, and\\nvideo, while simultaneously generating text and natural speech responses in a\\nstreaming manner. To enable the streaming of multimodal information inputs,\\nboth audio and visual encoders utilize a block-wise processing approach. To\\nsynchronize the timestamps of video inputs with audio, we organize the audio\\nand video sequentially in an interleaved manner and propose a novel position\\nembedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently\\ngenerate text and speech while avoiding interference between the two\\nmodalities, we propose Thinker-Talker architecture. In this framework,\\nThinker functions as a large language model tasked with text generation, while\\nTalker is a dual-track autoregressive model that directly utilizes the hidden\\nrepresentations from the Thinker to produce audio tokens as output. Both the\\nThinker and Talker models are designed to be trained and inferred in an\\nend-to-end manner. For decoding audio tokens in a streaming manner, we\\nintroduce a sliding-window DiT that restricts the receptive field, aiming to\\nreduce the initial package delay. Qwen2.5-Omni is comparable with the similarly\\nsized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni\\nachieves state-of-the-art performance on multimodal benchmarks like Omni-Bench.\\nNotably, Qwen2.5-Omni's performance in end-to-end speech instruction following\\nis comparable to its capabilities with text inputs, as evidenced by benchmarks\\nsuch as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming\\nTalker outperforms most existing streaming and non-streaming alternatives in\\nrobustness and naturalness.\", 'upvotes': 37, 'discussionId': '67e4f2527e97884ba42056df', 'projectPage': 'https://qwenlm.github.io/blog/qwen2.5-omni/', 'githubRepo': 'https://github.com/QwenLM/Qwen2.5-Omni', 'ai_keywords': ['multimodal model', 'block-wise processing', 'interleaved manner', 'TMRoPE (Time-aligned Multimodal RoPE)', 'position embedding', 'Thinker-Talker architecture', 'large language model', 'dual-track autoregressive model', 'end-to-end manner', 'sliding-window DiT', 'receptive field', 'initial package delay', 'Omni-Bench', 'MMLU', 'GSM8K', 'end-to-end speech instruction following']}, 'publishedAt': '2025-03-26T00:17:55.000Z', 'title': 'Qwen2.5-Omni Technical Report', 'summary': \"In this report, we present Qwen2.5-Omni, an end-to-end multimodal model\\ndesigned to perceive diverse modalities, including text, images, audio, and\\nvideo, while simultaneously generating text and natural speech responses in a\\nstreaming manner. To enable the streaming of multimodal information inputs,\\nboth audio and visual encoders utilize a block-wise processing approach. To\\nsynchronize the timestamps of video inputs with audio, we organize the audio\\nand video sequentially in an interleaved manner and propose a novel position\\nembedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently\\ngenerate text and speech while avoiding interference between the two\\nmodalities, we propose Thinker-Talker architecture. In this framework,\\nThinker functions as a large language model tasked with text generation, while\\nTalker is a dual-track autoregressive model that directly utilizes the hidden\\nrepresentations from the Thinker to produce audio tokens as output. Both the\\nThinker and Talker models are designed to be trained and inferred in an\\nend-to-end manner. For decoding audio tokens in a streaming manner, we\\nintroduce a sliding-window DiT that restricts the receptive field, aiming to\\nreduce the initial package delay. Qwen2.5-Omni is comparable with the similarly\\nsized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni\\nachieves state-of-the-art performance on multimodal benchmarks like Omni-Bench.\\nNotably, Qwen2.5-Omni's performance in end-to-end speech instruction following\\nis comparable to its capabilities with text inputs, as evidenced by benchmarks\\nsuch as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming\\nTalker outperforms most existing streaming and non-streaming alternatives in\\nrobustness and naturalness.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/659e513ea9bc1f60189ac148/3pDelIehF3CWGPS0jrZvN.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20215.png', 'numComments': 2, 'submittedBy': {'_id': '659e513ea9bc1f60189ac148', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/659e513ea9bc1f60189ac148/DBDDqjGTQ0SvjWyuYu7py.jpeg', 'fullname': 'YuanjunLv', 'name': 'Bakerbunker', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.19757', 'authors': [{'_id': '67e3e1e20706b07bfb2713d6', 'user': {'_id': '643fa1c318afbc4d1f3e5e59', 'avatarUrl': '/avatars/f8f35355902b4cda72e9c6d768322fae.svg', 'isPro': False, 'fullname': 'Zhi Hou', 'user': 'zhihou', 'type': 'user'}, 'name': 'Zhi Hou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:43:49.995Z', 'hidden': False}, {'_id': '67e3e1e20706b07bfb2713d7', 'user': {'_id': '64c9e86a6a26cddbecd9bae2', 'avatarUrl': '/avatars/61a84989dbbc1898ebcba3236dbed039.svg', 'isPro': False, 'fullname': 'Tianyi Zhang', 'user': 'TianyiZhang0213', 'type': 'user'}, 'name': 'Tianyi Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:44:21.118Z', 'hidden': False}, {'_id': '67e3e1e20706b07bfb2713d8', 'name': 'Yuwen Xiong', 'hidden': False}, {'_id': '67e3e1e20706b07bfb2713d9', 'user': {'_id': '66ab30dfd456f0408b93f27b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66ab30dfd456f0408b93f27b/nps4Kni_eOExO5Z92RiiF.jpeg', 'isPro': False, 'fullname': 'Haonan Duan', 'user': 'robot-haonan', 'type': 'user'}, 'name': 'Haonan Duan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-27T09:04:53.498Z', 'hidden': False}, {'_id': '67e3e1e20706b07bfb2713da', 'user': {'_id': '648a1e44fe11ebd7489c289c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WwMcD9PK0gxIu2I0n0QyD.jpeg', 'isPro': False, 'fullname': 'Hengjun Pu', 'user': 'MIASANMIA', 'type': 'user'}, 'name': 'Hengjun Pu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:45:01.725Z', 'hidden': False}, {'_id': '67e3e1e20706b07bfb2713db', 'user': {'_id': '66b9a5bb32be421cd8538cd6', 'avatarUrl': '/avatars/a1f7c0fe3ed4741017db713b4e6d47c8.svg', 'isPro': False, 'fullname': 'Ronglei Tong', 'user': 'TTTTTony', 'type': 'user'}, 'name': 'Ronglei Tong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:45:07.673Z', 'hidden': False}, {'_id': '67e3e1e20706b07bfb2713dc', 'user': {'_id': '679165b9c7f527ef3619504e', 'avatarUrl': '/avatars/f3e6ce5fc3d05c8632d8b208f55c2987.svg', 'isPro': False, 'fullname': 'Chengyang Zhao', 'user': 'chengyzhao', 'type': 'user'}, 'name': 'Chengyang Zhao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:45:13.906Z', 'hidden': False}, {'_id': '67e3e1e20706b07bfb2713dd', 'user': {'_id': '64ae2359179421d320b1694b', 'avatarUrl': '/avatars/c387a75191005bcaa473091de5383a10.svg', 'isPro': False, 'fullname': 'Xizhou Zhu', 'user': 'Einsiedler', 'type': 'user'}, 'name': 'Xizhou Zhu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:45:20.614Z', 'hidden': False}, {'_id': '67e3e1e20706b07bfb2713de', 'name': 'Yu Qiao', 'hidden': False}, {'_id': '67e3e1e20706b07bfb2713df', 'user': {'_id': '64686f7172d9180d4ac8b4e4', 'avatarUrl': '/avatars/db67dd6c4b2b41054ddcce5a18ade6f8.svg', 'isPro': False, 'fullname': 'Jifeng Dai', 'user': 'daijifeng', 'type': 'user'}, 'name': 'Jifeng Dai', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:45:27.039Z', 'hidden': False}, {'_id': '67e3e1e20706b07bfb2713e0', 'user': {'_id': '632dab84fdb35759ea6646a0', 'avatarUrl': '/avatars/857b0b4d115aa5ab2f143e60b0e4edc6.svg', 'isPro': False, 'fullname': 'Yuntao Chen', 'user': 'YuntaoChen', 'type': 'user'}, 'name': 'Yuntao Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:45:40.200Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/643fa1c318afbc4d1f3e5e59/T0twf6ibM7Htfq525gf3L.mp4'], 'publishedAt': '2025-03-25T15:19:56.000Z', 'submittedOnDailyAt': '2025-03-27T01:27:55.746Z', 'title': 'Dita: Scaling Diffusion Transformer for Generalist\\n  Vision-Language-Action Policy', 'submittedOnDailyBy': {'_id': '643fa1c318afbc4d1f3e5e59', 'avatarUrl': '/avatars/f8f35355902b4cda72e9c6d768322fae.svg', 'isPro': False, 'fullname': 'Zhi Hou', 'user': 'zhihou', 'type': 'user'}, 'summary': \"While recent vision-language-action models trained on diverse robot datasets\\nexhibit promising generalization capabilities with limited in-domain data,\\ntheir reliance on compact action heads to predict discretized or continuous\\nactions constrains adaptability to heterogeneous action spaces. We present\\nDita, a scalable framework that leverages Transformer architectures to directly\\ndenoise continuous action sequences through a unified multimodal diffusion\\nprocess. Departing from prior methods that condition denoising on fused\\nembeddings via shallow networks, Dita employs in-context conditioning --\\nenabling fine-grained alignment between denoised actions and raw visual tokens\\nfrom historical observations. This design explicitly models action deltas and\\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\\nTransformer's scalability, Dita effectively integrates cross-embodiment\\ndatasets across diverse camera perspectives, observation scenes, tasks, and\\naction spaces. Such synergy enhances robustness against various variances and\\nfacilitates the successful execution of long-horizon tasks. Evaluations across\\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\\nsimulation. Notably, Dita achieves robust real-world adaptation to\\nenvironmental variances and complex long-horizon tasks through 10-shot\\nfinetuning, using only third-person camera inputs. The architecture establishes\\na versatile, lightweight and open-source baseline for generalist robot policy\\nlearning. Project Page: https://robodita.github.io.\", 'upvotes': 37, 'discussionId': '67e3e1e40706b07bfb2714cd', 'projectPage': 'https://robodita.github.io', 'githubRepo': 'https://github.com/RoboDita/Dita', 'ai_keywords': ['Transformer architectures', 'multimodal diffusion process', 'in-context conditioning', 'action deltas', 'environmental nuances', 'cross-embodiment datasets', 'long-horizon tasks', '10-shot finetuning', 'third-person camera inputs', 'generalist robot policy learning']}, 'publishedAt': '2025-03-25T11:19:56.000Z', 'title': 'Dita: Scaling Diffusion Transformer for Generalist\\n  Vision-Language-Action Policy', 'summary': \"While recent vision-language-action models trained on diverse robot datasets\\nexhibit promising generalization capabilities with limited in-domain data,\\ntheir reliance on compact action heads to predict discretized or continuous\\nactions constrains adaptability to heterogeneous action spaces. We present\\nDita, a scalable framework that leverages Transformer architectures to directly\\ndenoise continuous action sequences through a unified multimodal diffusion\\nprocess. Departing from prior methods that condition denoising on fused\\nembeddings via shallow networks, Dita employs in-context conditioning --\\nenabling fine-grained alignment between denoised actions and raw visual tokens\\nfrom historical observations. This design explicitly models action deltas and\\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\\nTransformer's scalability, Dita effectively integrates cross-embodiment\\ndatasets across diverse camera perspectives, observation scenes, tasks, and\\naction spaces. Such synergy enhances robustness against various variances and\\nfacilitates the successful execution of long-horizon tasks. Evaluations across\\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\\nsimulation. Notably, Dita achieves robust real-world adaptation to\\nenvironmental variances and complex long-horizon tasks through 10-shot\\nfinetuning, using only third-person camera inputs. The architecture establishes\\na versatile, lightweight and open-source baseline for generalist robot policy\\nlearning. Project Page: https://robodita.github.io.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/643fa1c318afbc4d1f3e5e59/T0twf6ibM7Htfq525gf3L.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19757.png', 'numComments': 1, 'submittedBy': {'_id': '643fa1c318afbc4d1f3e5e59', 'avatarUrl': '/avatars/f8f35355902b4cda72e9c6d768322fae.svg', 'fullname': 'Zhi Hou', 'name': 'zhihou', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.19990', 'authors': [{'_id': '67e4d3df7e97884ba4150ec0', 'user': {'_id': '662516d72419feed62fb3a0a', 'avatarUrl': '/avatars/24c4157829e70a4e346aa984885aa5ad.svg', 'isPro': False, 'fullname': 'Dian', 'user': 'KexianTang', 'type': 'user'}, 'name': 'Kexian Tang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:46:12.696Z', 'hidden': False}, {'_id': '67e4d3df7e97884ba4150ec1', 'user': {'_id': '64a6ae0e0437599198cf3a98', 'avatarUrl': '/avatars/6635432cc0589ba12dc170cad6986d6d.svg', 'isPro': False, 'fullname': 'Junyao Gao', 'user': 'favourisnotyou', 'type': 'user'}, 'name': 'Junyao Gao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-27T09:03:15.061Z', 'hidden': False}, {'_id': '67e4d3df7e97884ba4150ec2', 'user': {'_id': '63d4b843df01ef426a0f79fb', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1676365795587-63d4b843df01ef426a0f79fb.jpeg', 'isPro': False, 'fullname': 'Yanhong Zeng', 'user': 'zengyh1900', 'type': 'user'}, 'name': 'Yanhong Zeng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:46:24.775Z', 'hidden': False}, {'_id': '67e4d3df7e97884ba4150ec3', 'user': {'_id': '63ee1379190ddd6214efd73a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png', 'isPro': False, 'fullname': 'HAODONG DUAN', 'user': 'KennyUTC', 'type': 'user'}, 'name': 'Haodong Duan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-27T09:00:33.733Z', 'hidden': False}, {'_id': '67e4d3df7e97884ba4150ec4', 'name': 'Yanan Sun', 'hidden': False}, {'_id': '67e4d3df7e97884ba4150ec5', 'user': {'_id': '62fb2a9dc95d426ff8f74c8d', 'avatarUrl': '/avatars/25c1a68ee7b7d0cc7e9f56bde37f4914.svg', 'isPro': False, 'fullname': 'Zhening Xing', 'user': 'Leoxing', 'type': 'user'}, 'name': 'Zhening Xing', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-27T09:00:36.341Z', 'hidden': False}, {'_id': '67e4d3df7e97884ba4150ec6', 'user': {'_id': '6385f8598b5acae8d24caf16', 'avatarUrl': '/avatars/9d261f95d24e882157b987b8827098be.svg', 'isPro': False, 'fullname': 'liuwenran', 'user': 'lwrshi1965', 'type': 'user'}, 'name': 'Wenran Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:46:45.866Z', 'hidden': False}, {'_id': '67e4d3df7e97884ba4150ec7', 'user': {'_id': '6414230a0fcefcf72e5085dd', 'avatarUrl': '/avatars/3a38dc8c84b0f27af846184d1c19f6ef.svg', 'isPro': False, 'fullname': 'Kaifeng Lyu', 'user': 'vfleaking', 'type': 'user'}, 'name': 'Kaifeng Lyu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:46:52.341Z', 'hidden': False}, {'_id': '67e4d3df7e97884ba4150ec8', 'name': 'Kai Chen', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/kjq_V0to2uTR2RSR3TyfV.png'], 'publishedAt': '2025-03-25T18:21:07.000Z', 'submittedOnDailyAt': '2025-03-27T03:00:01.698Z', 'title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'submittedOnDailyBy': {'_id': '63ee1379190ddd6214efd73a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png', 'isPro': False, 'fullname': 'HAODONG DUAN', 'user': 'KennyUTC', 'type': 'user'}, 'summary': \"Multi-step spatial reasoning entails understanding and reasoning about\\nspatial relationships across multiple sequential steps, which is crucial for\\ntackling complex real-world applications, such as robotic manipulation,\\nautonomous navigation, and automated assembly. To assess how well current\\nMultimodal Large Language Models (MLLMs) have acquired this fundamental\\ncapability, we introduce LEGO-Puzzles, a scalable benchmark designed\\nto evaluate both spatial understanding and sequential\\nreasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100\\ncarefully curated visual question-answering (VQA) samples spanning 11 distinct\\ntasks, ranging from basic spatial understanding to complex multi-step\\nreasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of\\nstate-of-the-art MLLMs and uncover significant limitations in their spatial\\nreasoning capabilities: even the most powerful MLLMs can answer only about half\\nof the test cases, whereas human participants achieve over 90\\\\% accuracy. In\\naddition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images\\nfollowing assembly illustrations. Our experiments show that only\\nGemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these\\ninstructions, while other MLLMs either replicate the input image or generate\\ncompletely irrelevant outputs. Overall, LEGO-Puzzles exposes critical\\ndeficiencies in existing MLLMs' spatial understanding and sequential reasoning\\ncapabilities, and underscores the need for further advancements in multimodal\\nspatial reasoning.\", 'upvotes': 24, 'discussionId': '67e4d3e07e97884ba4150f2b', 'ai_keywords': ['Multimodal Large Language Models (MLLMs)', 'LEGO-Puzzles', 'visual question-answering (VQA)', 'spatial understanding', 'sequential reasoning', 'Gemini-2.0-Flash', 'GPT-4o']}, 'publishedAt': '2025-03-25T14:21:07.000Z', 'title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'summary': \"Multi-step spatial reasoning entails understanding and reasoning about\\nspatial relationships across multiple sequential steps, which is crucial for\\ntackling complex real-world applications, such as robotic manipulation,\\nautonomous navigation, and automated assembly. To assess how well current\\nMultimodal Large Language Models (MLLMs) have acquired this fundamental\\ncapability, we introduce LEGO-Puzzles, a scalable benchmark designed\\nto evaluate both spatial understanding and sequential\\nreasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100\\ncarefully curated visual question-answering (VQA) samples spanning 11 distinct\\ntasks, ranging from basic spatial understanding to complex multi-step\\nreasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of\\nstate-of-the-art MLLMs and uncover significant limitations in their spatial\\nreasoning capabilities: even the most powerful MLLMs can answer only about half\\nof the test cases, whereas human participants achieve over 90\\\\% accuracy. In\\naddition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images\\nfollowing assembly illustrations. Our experiments show that only\\nGemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these\\ninstructions, while other MLLMs either replicate the input image or generate\\ncompletely irrelevant outputs. Overall, LEGO-Puzzles exposes critical\\ndeficiencies in existing MLLMs' spatial understanding and sequential reasoning\\ncapabilities, and underscores the need for further advancements in multimodal\\nspatial reasoning.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/kjq_V0to2uTR2RSR3TyfV.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19990.png', 'numComments': 1, 'submittedBy': {'_id': '63ee1379190ddd6214efd73a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png', 'fullname': 'HAODONG DUAN', 'name': 'KennyUTC', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 24}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.20314', 'authors': [{'_id': '67e4b65a080a33e3955b340c', 'name': 'WanTeam', 'hidden': False}, {'_id': '67e4b65a080a33e3955b340e', 'user': {'_id': '63f1f1727ddf724fbcbc9c7e', 'avatarUrl': '/avatars/9e0516d9b1036c23c78f313c79872f55.svg', 'isPro': False, 'fullname': 'Ang Wang', 'user': 'ang-annng', 'type': 'user'}, 'name': 'Ang Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:47:28.144Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b340f', 'user': {'_id': '64755ff5a51711a3b59118af', 'avatarUrl': '/avatars/2e899088902db94e785107c3ec2abe85.svg', 'isPro': False, 'fullname': 'Baole Ai', 'user': 'baoleai', 'type': 'user'}, 'name': 'Baole Ai', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:47:49.260Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3410', 'name': 'Bin Wen', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3411', 'user': {'_id': '6458970cab9a44f42f620a80', 'avatarUrl': '/avatars/f9779b0621c931f922440fec95342444.svg', 'isPro': False, 'fullname': 'chaojie mao', 'user': 'chaojiemao', 'type': 'user'}, 'name': 'Chaojie Mao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:48:02.730Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3412', 'user': {'_id': '66592c72f4124d863fd55574', 'avatarUrl': '/avatars/98f0d5e6ba3728e8a1164aa5188a3298.svg', 'isPro': False, 'fullname': 'Chenwei Xie', 'user': 'chenweix7', 'type': 'user'}, 'name': 'Chen-Wei Xie', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:48:10.933Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3413', 'name': 'Di Chen', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3414', 'name': 'Feiwu Yu', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3415', 'user': {'_id': '67a73767282aa06f7bcaeeb1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/J28OVrPhD0xYulWMgICmW.png', 'isPro': False, 'fullname': 'Haiming Zhao', 'user': 'HermanZ', 'type': 'user'}, 'name': 'Haiming Zhao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:48:26.135Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3416', 'user': {'_id': '651441e92c5da979038df5ee', 'avatarUrl': '/avatars/85cdafcccb522eced50dc9e4770b630a.svg', 'isPro': False, 'fullname': 'Jianxiao Yang', 'user': 'Jianxiao0203', 'type': 'user'}, 'name': 'Jianxiao Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:48:33.714Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3417', 'user': {'_id': '6274b866f978441a764b30f6', 'avatarUrl': '/avatars/953b1ff82f63e371a7358a85d68304cd.svg', 'isPro': False, 'fullname': 'jianyuan.zengjy', 'user': 'filwsyl', 'type': 'user'}, 'name': 'Jianyuan Zeng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:48:40.108Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3418', 'name': 'Jiayu Wang', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3419', 'user': {'_id': '66f0e0262aee3cb7e981bbac', 'avatarUrl': '/avatars/f8f1e70469b5e047dc6e0e9dec6c5bc1.svg', 'isPro': False, 'fullname': 'Jingfeng Zhang', 'user': 'jingfengzhang', 'type': 'user'}, 'name': 'Jingfeng Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:48:58.316Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b341a', 'user': {'_id': '602f88f5e8149a962412a667', 'avatarUrl': '/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg', 'isPro': False, 'fullname': 'Zhou', 'user': 'Jingren', 'type': 'user'}, 'name': 'Jingren Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:49:09.146Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b341b', 'user': {'_id': '627c93b2bec91eb1720b8bad', 'avatarUrl': '/avatars/89c31c71aa5027543ed5be0471fe1109.svg', 'isPro': False, 'fullname': 'Jinkai Wang', 'user': 'zwsjink', 'type': 'user'}, 'name': 'Jinkai Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:49:15.680Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b341c', 'user': {'_id': '6465941d0e6c7618f615675b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6465941d0e6c7618f615675b/W4EHqlCucz_bojFLFEeV_.jpeg', 'isPro': False, 'fullname': 'Jixuan Chen', 'user': 'Mayome', 'type': 'user'}, 'name': 'Jixuan Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:49:25.437Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b341d', 'name': 'Kai Zhu', 'hidden': False}, {'_id': '67e4b65a080a33e3955b341e', 'name': 'Kang Zhao', 'hidden': False}, {'_id': '67e4b65a080a33e3955b341f', 'name': 'Keyu Yan', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3420', 'name': 'Lianghua Huang', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3421', 'user': {'_id': '63b4ec15103617b0a5b3101e', 'avatarUrl': '/avatars/e6faad833b31ad5d892faccf621e7a34.svg', 'isPro': False, 'fullname': 'Mengyang Feng', 'user': 'archerfmy', 'type': 'user'}, 'name': 'Mengyang Feng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:50:01.919Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3422', 'user': {'_id': '66eae63f533fd44f8a8ca60b', 'avatarUrl': '/avatars/38cecb4c80cc7a6e63028fcb572e3a22.svg', 'isPro': False, 'fullname': 'Zhang Ningyi', 'user': 'ZhangNy', 'type': 'user'}, 'name': 'Ningyi Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:50:13.628Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3423', 'name': 'Pandeng Li', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3424', 'user': {'_id': '64c5182771947b03ffee931c', 'avatarUrl': '/avatars/478f4e06ac1bced092dde0f11963a975.svg', 'isPro': False, 'fullname': 'Wupingyu', 'user': 'wpy1999', 'type': 'user'}, 'name': 'Pingyu Wu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:50:38.625Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3425', 'user': {'_id': '642e3bcb958faf258a40e89c', 'avatarUrl': '/avatars/213501def37dc53032cee17e37fcc4c1.svg', 'isPro': False, 'fullname': 'Ruihang Chu', 'user': 'Ruihang', 'type': 'user'}, 'name': 'Ruihang Chu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:50:46.771Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3426', 'user': {'_id': '6790e2b74932687e24024b4a', 'avatarUrl': '/avatars/951f55648490e1f520483a3e425621dd.svg', 'isPro': False, 'fullname': 'Ruili', 'user': 'RuiliFeng', 'type': 'user'}, 'name': 'Ruili Feng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:51:03.191Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3427', 'name': 'Shiwei Zhang', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3428', 'user': {'_id': '62bbf42ac9633b01802a6d45', 'avatarUrl': '/avatars/0fee1462d228f5e7f22d5c240900a3ad.svg', 'isPro': False, 'fullname': 'Siyang Sun', 'user': 'sunsiyang', 'type': 'user'}, 'name': 'Siyang Sun', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:51:10.461Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3429', 'name': 'Tao Fang', 'hidden': False}, {'_id': '67e4b65a080a33e3955b342a', 'name': 'Tianxing Wang', 'hidden': False}, {'_id': '67e4b65a080a33e3955b342b', 'name': 'Tianyi Gui', 'hidden': False}, {'_id': '67e4b65a080a33e3955b342c', 'name': 'Tingyu Weng', 'hidden': False}, {'_id': '67e4b65a080a33e3955b342d', 'name': 'Tong Shen', 'hidden': False}, {'_id': '67e4b65a080a33e3955b342e', 'name': 'Wei Lin', 'hidden': False}, {'_id': '67e4b65a080a33e3955b342f', 'name': 'Wei Wang', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3430', 'name': 'Wei Wang', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3431', 'user': {'_id': '623c6253389748c9f72ca287', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1654828369523-623c6253389748c9f72ca287.jpeg', 'isPro': False, 'fullname': 'wenmeng zhou', 'user': 'wenmengzhou', 'type': 'user'}, 'name': 'Wenmeng Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:51:38.310Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3432', 'user': {'_id': '644240b1251730a7ee243ef3', 'avatarUrl': '/avatars/c4ca99739e2b6f3d3d0ca83ecc54766a.svg', 'isPro': False, 'fullname': 'wente.wang', 'user': 'shiftc', 'type': 'user'}, 'name': 'Wente Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:51:46.041Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3433', 'user': {'_id': '64af91eb5c17fe25cfcbebc3', 'avatarUrl': '/avatars/ffc6e7b6a40300e05e66f544264dddbc.svg', 'isPro': False, 'fullname': 'Wenting Shen', 'user': 'SeventeenSSS', 'type': 'user'}, 'name': 'Wenting Shen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:51:53.298Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3434', 'name': 'Wenyuan Yu', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3435', 'user': {'_id': '642e19b26748dd4f8eea1321', 'avatarUrl': '/avatars/a534e61c21d2fb3c7a4c4d4dba98fafb.svg', 'isPro': False, 'fullname': 'Xianzhong Shi', 'user': 'itutor', 'type': 'user'}, 'name': 'Xianzhong Shi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:51:19.514Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3436', 'user': {'_id': '65105ab08c4b535a97052fe8', 'avatarUrl': '/avatars/a97862045a26a74ca33d1a47b6a1f2b4.svg', 'isPro': False, 'fullname': 'xiaominghuang', 'user': 'xiaominghuang', 'type': 'user'}, 'name': 'Xiaoming Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:52:03.599Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3437', 'name': 'Xin Xu', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3438', 'name': 'Yan Kou', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3439', 'name': 'Yangyu Lv', 'hidden': False}, {'_id': '67e4b65a080a33e3955b343a', 'name': 'Yifei Li', 'hidden': False}, {'_id': '67e4b65a080a33e3955b343b', 'user': {'_id': '67d39e61943a965360fbbc0c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-JwILFmblPdd6Sv28c1J7.png', 'isPro': False, 'fullname': 'yijing liu', 'user': '86diphda', 'type': 'user'}, 'name': 'Yijing Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:52:18.647Z', 'hidden': False}, {'_id': '67e4b65a080a33e3955b343c', 'name': 'Yiming Wang', 'hidden': False}, {'_id': '67e4b65a080a33e3955b343d', 'name': 'Yingya Zhang', 'hidden': False}, {'_id': '67e4b65a080a33e3955b343e', 'name': 'Yitong Huang', 'hidden': False}, {'_id': '67e4b65a080a33e3955b343f', 'name': 'Yong Li', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3440', 'name': 'You Wu', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3441', 'name': 'Yu Liu', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3442', 'name': 'Yulin Pan', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3443', 'name': 'Yun Zheng', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3444', 'name': 'Yuntao Hong', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3445', 'name': 'Yupeng Shi', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3446', 'name': 'Yutong Feng', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3447', 'name': 'Zeyinzi Jiang', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3448', 'name': 'Zhen Han', 'hidden': False}, {'_id': '67e4b65a080a33e3955b3449', 'name': 'Zhi-Fan Wu', 'hidden': False}, {'_id': '67e4b65a080a33e3955b344a', 'name': 'Ziyu Liu', 'hidden': False}], 'publishedAt': '2025-03-26T08:25:43.000Z', 'submittedOnDailyAt': '2025-03-27T00:52:37.426Z', 'title': 'Wan: Open and Advanced Large-Scale Video Generative Models', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': \"This report presents Wan, a comprehensive and open suite of video foundation\\nmodels designed to push the boundaries of video generation. Built upon the\\nmainstream diffusion transformer paradigm, Wan achieves significant\\nadvancements in generative capabilities through a series of innovations,\\nincluding our novel VAE, scalable pre-training strategies, large-scale data\\ncuration, and automated evaluation metrics. These contributions collectively\\nenhance the model's performance and versatility. Specifically, Wan is\\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\\ntrained on a vast dataset comprising billions of images and videos,\\ndemonstrates the scaling laws of video generation with respect to both data and\\nmodel size. It consistently outperforms the existing open-source models as well\\nas state-of-the-art commercial solutions across multiple internal and external\\nbenchmarks, demonstrating a clear and significant performance superiority.\\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\\nparameters, for efficiency and effectiveness respectively. It also covers\\nmultiple downstream applications, including image-to-video, instruction-guided\\nvideo editing, and personal video generation, encompassing up to eight tasks.\\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\\nincluding source code and all models, with the goal of fostering the growth of\\nthe video generation community. This openness seeks to significantly expand the\\ncreative possibilities of video production in the industry and provide academia\\nwith high-quality video foundation models. All the code and models are\\navailable at https://github.com/Wan-Video/Wan2.1.\", 'upvotes': 23, 'discussionId': '67e4b663080a33e3955b371a', 'ai_keywords': ['diffusion transformer', 'VAE', 'large-scale data curation', 'automated evaluation metrics', 'scaling laws', 'image-to-video', 'instruction-guided video editing', 'personal video generation']}, 'publishedAt': '2025-03-26T04:25:43.000Z', 'title': 'Wan: Open and Advanced Large-Scale Video Generative Models', 'summary': \"This report presents Wan, a comprehensive and open suite of video foundation\\nmodels designed to push the boundaries of video generation. Built upon the\\nmainstream diffusion transformer paradigm, Wan achieves significant\\nadvancements in generative capabilities through a series of innovations,\\nincluding our novel VAE, scalable pre-training strategies, large-scale data\\ncuration, and automated evaluation metrics. These contributions collectively\\nenhance the model's performance and versatility. Specifically, Wan is\\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\\ntrained on a vast dataset comprising billions of images and videos,\\ndemonstrates the scaling laws of video generation with respect to both data and\\nmodel size. It consistently outperforms the existing open-source models as well\\nas state-of-the-art commercial solutions across multiple internal and external\\nbenchmarks, demonstrating a clear and significant performance superiority.\\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\\nparameters, for efficiency and effectiveness respectively. It also covers\\nmultiple downstream applications, including image-to-video, instruction-guided\\nvideo editing, and personal video generation, encompassing up to eight tasks.\\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\\nincluding source code and all models, with the goal of fostering the growth of\\nthe video generation community. This openness seeks to significantly expand the\\ncreative possibilities of video production in the industry and provide academia\\nwith high-quality video foundation models. All the code and models are\\navailable at https://github.com/Wan-Video/Wan2.1.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20314.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': True, 'isMod': False, 'followerCount': 6482}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.20201', 'authors': [{'_id': '67e4b04c8c0347025bd0fe84', 'user': {'_id': '6109bc89e84ad84682a69754', 'avatarUrl': '/avatars/067aac8784320d4e8e875379dc4cc209.svg', 'isPro': False, 'fullname': 'Salaheddin Alzubi', 'user': 'salzubi401', 'type': 'user'}, 'name': 'Salaheddin Alzubi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:54:03.915Z', 'hidden': False}, {'_id': '67e4b04c8c0347025bd0fe85', 'user': {'_id': '673f945e6cd62dbd4b02790d', 'avatarUrl': '/avatars/3742e4e6b88d4f8b78d5c5308f55773e.svg', 'isPro': False, 'fullname': 'Creston Brooks', 'user': 'cabxyz', 'type': 'user'}, 'name': 'Creston Brooks', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-03-27T01:56:28.853Z', 'hidden': False}, {'_id': '67e4b04c8c0347025bd0fe86', 'user': {'_id': '666619508a270cedd594e55e', 'avatarUrl': '/avatars/79bb2b09a663cae555140ec9379f05d9.svg', 'isPro': False, 'fullname': 'Purva Chiniya', 'user': 'pchiniya', 'type': 'user'}, 'name': 'Purva Chiniya', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:54:10.184Z', 'hidden': False}, {'_id': '67e4b04c8c0347025bd0fe87', 'name': 'Edoardo Contente', 'hidden': False}, {'_id': '67e4b04c8c0347025bd0fe88', 'name': 'Chiara von Gerlach', 'hidden': False}, {'_id': '67e4b04c8c0347025bd0fe89', 'user': {'_id': '62296d3f2df798b7e951e475', 'avatarUrl': '/avatars/661c23416c3d418e2996f9b9a024db82.svg', 'isPro': False, 'fullname': 'Lucas Irwin', 'user': 'ljirwin', 'type': 'user'}, 'name': 'Lucas Irwin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:54:26.927Z', 'hidden': False}, {'_id': '67e4b04c8c0347025bd0fe8a', 'name': 'Yihan Jiang', 'hidden': False}, {'_id': '67e4b04c8c0347025bd0fe8b', 'user': {'_id': '67759bf644ceb61f96739324', 'avatarUrl': '/avatars/44cb431a9cbc73e060aff7d90435c42d.svg', 'isPro': False, 'fullname': 'Arda Kaz', 'user': 'speedyarda', 'type': 'user'}, 'name': 'Arda Kaz', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:54:57.570Z', 'hidden': False}, {'_id': '67e4b04c8c0347025bd0fe8c', 'user': {'_id': '64b98bcf842aa47891bc0f63', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/L-smrOCQ3MXtvnISJqmxJ.png', 'isPro': False, 'fullname': 'Windsor Nguyen', 'user': 'windsornguyen', 'type': 'user'}, 'name': 'Windsor Nguyen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:54:50.840Z', 'hidden': False}, {'_id': '67e4b04c8c0347025bd0fe8d', 'user': {'_id': '6756dffd88428044e2ddbdd9', 'avatarUrl': '/avatars/41dbb83c68b56546cdf8e34379faf6b3.svg', 'isPro': False, 'fullname': 'Sewoong Oh', 'user': 'sewoong79', 'type': 'user'}, 'name': 'Sewoong Oh', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:55:20.604Z', 'hidden': False}, {'_id': '67e4b04c8c0347025bd0fe8e', 'user': {'_id': '65f86cc77b704590d4a5439f', 'avatarUrl': '/avatars/1a828cf755839f058241fb19ca83341f.svg', 'isPro': False, 'fullname': 'Himanshu Tyagi', 'user': 'HimanshuTyagi', 'type': 'user'}, 'name': 'Himanshu Tyagi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:55:27.139Z', 'hidden': False}, {'_id': '67e4b04c8c0347025bd0fe8f', 'name': 'Pramod Viswanath', 'hidden': False}], 'publishedAt': '2025-03-26T03:51:32.000Z', 'submittedOnDailyAt': '2025-03-27T00:26:59.804Z', 'title': 'Open Deep Search: Democratizing Search with Open-source Reasoning Agents', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': \"We introduce Open Deep Search (ODS) to close the increasing gap between the\\nproprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and\\nOpenAI's GPT-4o Search Preview, and their open-source counterparts. The main\\ninnovation introduced in ODS is to augment the reasoning capabilities of the\\nlatest open-source LLMs with reasoning agents that can judiciously use web\\nsearch tools to answer queries. Concretely, ODS consists of two components that\\nwork with a base LLM chosen by the user: Open Search Tool and Open Reasoning\\nAgent. Open Reasoning Agent interprets the given task and completes it by\\norchestrating a sequence of actions that includes calling tools, one of which\\nis the Open Search Tool. Open Search Tool is a novel web search tool that\\noutperforms proprietary counterparts. Together with powerful open-source\\nreasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses\\nthe existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES.\\nFor example, on the FRAMES evaluation benchmark, ODS improves the best existing\\nbaseline of the recently released GPT-4o Search Preview by 9.7% in accuracy.\\nODS is a general framework for seamlessly augmenting any LLMs -- for example,\\nDeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search\\nand reasoning capabilities to achieve state-of-the-art performance: 88.3% on\\nSimpleQA and 75.3% on FRAMES.\", 'upvotes': 19, 'discussionId': '67e4b04c8c0347025bd0fed2', 'ai_keywords': ['LLMs', 'reasoning agents', 'web search tools', 'Open Search Tool', 'Open Reasoning Agent', 'DeepSeek-R1', 'SimpleQA', 'FRAMES', 'GPT-4o Search Preview']}, 'publishedAt': '2025-03-25T23:51:32.000Z', 'title': 'Open Deep Search: Democratizing Search with Open-source Reasoning Agents', 'summary': \"We introduce Open Deep Search (ODS) to close the increasing gap between the\\nproprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and\\nOpenAI's GPT-4o Search Preview, and their open-source counterparts. The main\\ninnovation introduced in ODS is to augment the reasoning capabilities of the\\nlatest open-source LLMs with reasoning agents that can judiciously use web\\nsearch tools to answer queries. Concretely, ODS consists of two components that\\nwork with a base LLM chosen by the user: Open Search Tool and Open Reasoning\\nAgent. Open Reasoning Agent interprets the given task and completes it by\\norchestrating a sequence of actions that includes calling tools, one of which\\nis the Open Search Tool. Open Search Tool is a novel web search tool that\\noutperforms proprietary counterparts. Together with powerful open-source\\nreasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses\\nthe existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES.\\nFor example, on the FRAMES evaluation benchmark, ODS improves the best existing\\nbaseline of the recently released GPT-4o Search Preview by 9.7% in accuracy.\\nODS is a general framework for seamlessly augmenting any LLMs -- for example,\\nDeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search\\nand reasoning capabilities to achieve state-of-the-art performance: 88.3% on\\nSimpleQA and 75.3% on FRAMES.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20201.png', 'numComments': 2, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': True, 'isMod': False, 'followerCount': 6482}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.20240', 'authors': [{'_id': '67e4ae6787c92169aa3caa74', 'user': {'_id': '66435efdc26b490acc85079b', 'avatarUrl': '/avatars/16e0ee25734516d4295abe0fcc0e26a9.svg', 'isPro': False, 'fullname': 'Prin Phunyaphibarn', 'user': 'prinphunya', 'type': 'user'}, 'name': 'Prin Phunyaphibarn', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-27T09:03:34.055Z', 'hidden': False}, {'_id': '67e4ae6787c92169aa3caa75', 'user': {'_id': '6342796a0875f2c99cfd313b', 'avatarUrl': '/avatars/98575092404c4197b20c929a6499a015.svg', 'isPro': False, 'fullname': 'Yuseung \"Phillip\" Lee', 'user': 'phillipinseoul', 'type': 'user'}, 'name': 'Phillip Y. Lee', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-27T09:03:32.144Z', 'hidden': False}, {'_id': '67e4ae6787c92169aa3caa76', 'name': 'Jaihoon Kim', 'hidden': False}, {'_id': '67e4ae6787c92169aa3caa77', 'user': {'_id': '631f432b5ba8c026340a7890', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/631f432b5ba8c026340a7890/9PK7A_TRMpugwYjCsNBf1.jpeg', 'isPro': False, 'fullname': 'Minhyuk Sung', 'user': 'Minhyuk', 'type': 'user'}, 'name': 'Minhyuk Sung', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:47:11.255Z', 'hidden': False}], 'publishedAt': '2025-03-26T05:11:38.000Z', 'submittedOnDailyAt': '2025-03-27T00:22:30.335Z', 'title': 'Unconditional Priors Matter! Improving Conditional Generation of\\n  Fine-Tuned Diffusion Models', 'submittedOnDailyBy': {'_id': '6342796a0875f2c99cfd313b', 'avatarUrl': '/avatars/98575092404c4197b20c929a6499a015.svg', 'isPro': False, 'fullname': 'Yuseung \"Phillip\" Lee', 'user': 'phillipinseoul', 'type': 'user'}, 'summary': 'Classifier-Free Guidance (CFG) is a fundamental technique in training\\nconditional diffusion models. The common practice for CFG-based training is to\\nuse a single network to learn both conditional and unconditional noise\\nprediction, with a small dropout rate for conditioning. However, we observe\\nthat the joint learning of unconditional noise with limited bandwidth in\\ntraining results in poor priors for the unconditional case. More importantly,\\nthese poor unconditional noise predictions become a serious reason for\\ndegrading the quality of conditional generation. Inspired by the fact that most\\nCFG-based conditional models are trained by fine-tuning a base model with\\nbetter unconditional generation, we first show that simply replacing the\\nunconditional noise in CFG with that predicted by the base model can\\nsignificantly improve conditional generation. Furthermore, we show that a\\ndiffusion model other than the one the fine-tuned model was trained on can be\\nused for unconditional noise replacement. We experimentally verify our claim\\nwith a range of CFG-based conditional models for both image and video\\ngeneration, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and\\nInstructPix2Pix.', 'upvotes': 18, 'discussionId': '67e4ae6a87c92169aa3cabc2', 'ai_keywords': ['Classifier-Free Guidance (CFG)', 'conditional diffusion models', 'noise prediction', 'dropout rate', 'priors', 'fine-tuning', 'base model', 'unconditional generation', 'variance scaling', 'Zero-1-to-3', 'Versatile Diffusion', 'DiT', 'DynamiCrafter', 'InstructPix2Pix']}, 'publishedAt': '2025-03-26T01:11:38.000Z', 'title': 'Unconditional Priors Matter! Improving Conditional Generation of\\n  Fine-Tuned Diffusion Models', 'summary': 'Classifier-Free Guidance (CFG) is a fundamental technique in training\\nconditional diffusion models. The common practice for CFG-based training is to\\nuse a single network to learn both conditional and unconditional noise\\nprediction, with a small dropout rate for conditioning. However, we observe\\nthat the joint learning of unconditional noise with limited bandwidth in\\ntraining results in poor priors for the unconditional case. More importantly,\\nthese poor unconditional noise predictions become a serious reason for\\ndegrading the quality of conditional generation. Inspired by the fact that most\\nCFG-based conditional models are trained by fine-tuning a base model with\\nbetter unconditional generation, we first show that simply replacing the\\nunconditional noise in CFG with that predicted by the base model can\\nsignificantly improve conditional generation. Furthermore, we show that a\\ndiffusion model other than the one the fine-tuned model was trained on can be\\nused for unconditional noise replacement. We experimentally verify our claim\\nwith a range of CFG-based conditional models for both image and video\\ngeneration, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and\\nInstructPix2Pix.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20240.png', 'numComments': 1, 'submittedBy': {'_id': '6342796a0875f2c99cfd313b', 'avatarUrl': '/avatars/98575092404c4197b20c929a6499a015.svg', 'fullname': 'Yuseung \"Phillip\" Lee', 'name': 'phillipinseoul', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19480', 'authors': [{'_id': '67e3693eebafaa1efbed08d2', 'user': {'_id': '67d30d9ae45dc43004b31425', 'avatarUrl': '/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg', 'isPro': False, 'fullname': 'Shijie Ma', 'user': 'msj9817', 'type': 'user'}, 'name': 'Shijie Ma', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-26T20:44:37.274Z', 'hidden': False}, {'_id': '67e3693eebafaa1efbed08d3', 'user': {'_id': '6455cc8f654d8bccae50e4d4', 'avatarUrl': '/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg', 'isPro': False, 'fullname': 'Yuying Ge', 'user': 'tttoaster', 'type': 'user'}, 'name': 'Yuying Ge', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:55:42.975Z', 'hidden': False}, {'_id': '67e3693eebafaa1efbed08d4', 'name': 'Teng Wang', 'hidden': False}, {'_id': '67e3693eebafaa1efbed08d5', 'user': {'_id': '67bc21106a3e748d80d11dc7', 'avatarUrl': '/avatars/fbc7e76a3266a3c06d03e85db96a51cf.svg', 'isPro': False, 'fullname': 'yuxin guo', 'user': 'aether25', 'type': 'user'}, 'name': 'Yuxin Guo', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:56:02.897Z', 'hidden': False}, {'_id': '67e3693eebafaa1efbed08d6', 'user': {'_id': '640e9762b03f4cd29f58d982', 'avatarUrl': '/avatars/81da37d628163fe3e094b247c7c3a3b5.svg', 'isPro': False, 'fullname': 'Yixiao Ge', 'user': 'yxgeee', 'type': 'user'}, 'name': 'Yixiao Ge', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:56:08.653Z', 'hidden': False}, {'_id': '67e3693eebafaa1efbed08d7', 'user': {'_id': '63ca3ddc04c979828310bfcb', 'avatarUrl': '/avatars/615e0d8622950b4408b40d550f02a894.svg', 'isPro': False, 'fullname': 'Ying Shan', 'user': 'yshan2u', 'type': 'user'}, 'name': 'Ying Shan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:56:14.275Z', 'hidden': False}], 'publishedAt': '2025-03-25T09:15:34.000Z', 'submittedOnDailyAt': '2025-03-27T01:26:37.430Z', 'title': 'GenHancer: Imperfect Generative Models are Secretly Strong\\n  Vision-Centric Enhancers', 'submittedOnDailyBy': {'_id': '67d30d9ae45dc43004b31425', 'avatarUrl': '/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg', 'isPro': False, 'fullname': 'Shijie Ma', 'user': 'msj9817', 'type': 'user'}, 'summary': \"The synergy between generative and discriminative models receives growing\\nattention. While discriminative Contrastive Language-Image Pre-Training (CLIP)\\nexcels in high-level semantics, it struggles with perceiving fine-grained\\nvisual details. Generally, to enhance representations, generative models take\\nCLIP's visual features as conditions for reconstruction. However, the\\nunderlying principle remains underexplored. In this work, we empirically found\\nthat visually perfect generations are not always optimal for representation\\nenhancement. The essence lies in effectively extracting fine-grained knowledge\\nfrom generative models while mitigating irrelevant information. To explore\\ncritical factors, we delve into three aspects: (1) Conditioning mechanisms: We\\nfound that even a small number of local tokens can drastically reduce the\\ndifficulty of reconstruction, leading to collapsed training. We thus conclude\\nthat utilizing only global visual tokens as conditions is the most effective\\nstrategy. (2) Denoising configurations: We observed that end-to-end training\\nintroduces extraneous information. To address this, we propose a two-stage\\ntraining strategy to prioritize learning useful visual knowledge. Additionally,\\nwe demonstrate that lightweight denoisers can yield remarkable improvements.\\n(3) Generation paradigms: We explore both continuous and discrete denoisers\\nwith desirable outcomes, validating the versatility of our method. Through our\\nin-depth explorations, we have finally arrived at an effective method, namely\\nGenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark,\\ne.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into\\nmultimodal large language models for better vision-centric performance. All the\\nmodels and codes are made publicly available.\", 'upvotes': 12, 'discussionId': '67e36940ebafaa1efbed0951', 'projectPage': 'https://mashijie1028.github.io/GenHancer/', 'githubRepo': 'https://github.com/mashijie1028/GenHancer', 'ai_keywords': ['generative', 'discriminative', 'Contrastive Language-Image Pre-Training (CLIP)', 'visual features', 'reconstruction', 'local tokens', 'global visual tokens', 'Conditioning mechanisms', 'end-to-end training', 'denoising configurations', 'two-stage training', 'lightweight denoisers', 'continuous denoisers', 'discrete denoisers', 'Generation paradigms', 'GenHancer', 'MMVP-VLM benchmark', 'OpenAICLIP', 'multimodal large language models', 'vision-centric performance']}, 'publishedAt': '2025-03-25T05:15:34.000Z', 'title': 'GenHancer: Imperfect Generative Models are Secretly Strong\\n  Vision-Centric Enhancers', 'summary': \"The synergy between generative and discriminative models receives growing\\nattention. While discriminative Contrastive Language-Image Pre-Training (CLIP)\\nexcels in high-level semantics, it struggles with perceiving fine-grained\\nvisual details. Generally, to enhance representations, generative models take\\nCLIP's visual features as conditions for reconstruction. However, the\\nunderlying principle remains underexplored. In this work, we empirically found\\nthat visually perfect generations are not always optimal for representation\\nenhancement. The essence lies in effectively extracting fine-grained knowledge\\nfrom generative models while mitigating irrelevant information. To explore\\ncritical factors, we delve into three aspects: (1) Conditioning mechanisms: We\\nfound that even a small number of local tokens can drastically reduce the\\ndifficulty of reconstruction, leading to collapsed training. We thus conclude\\nthat utilizing only global visual tokens as conditions is the most effective\\nstrategy. (2) Denoising configurations: We observed that end-to-end training\\nintroduces extraneous information. To address this, we propose a two-stage\\ntraining strategy to prioritize learning useful visual knowledge. Additionally,\\nwe demonstrate that lightweight denoisers can yield remarkable improvements.\\n(3) Generation paradigms: We explore both continuous and discrete denoisers\\nwith desirable outcomes, validating the versatility of our method. Through our\\nin-depth explorations, we have finally arrived at an effective method, namely\\nGenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark,\\ne.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into\\nmultimodal large language models for better vision-centric performance. All the\\nmodels and codes are made publicly available.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19480.png', 'numComments': 1, 'submittedBy': {'_id': '67d30d9ae45dc43004b31425', 'avatarUrl': '/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg', 'fullname': 'Shijie Ma', 'name': 'msj9817', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.20672', 'authors': [{'_id': '67e4b82c672b3d9c9cb42c70', 'name': 'Yuyang Peng', 'hidden': False}, {'_id': '67e4b82c672b3d9c9cb42c71', 'name': 'Shishi Xiao', 'hidden': False}, {'_id': '67e4b82c672b3d9c9cb42c72', 'user': {'_id': '66bf00ca5b4e241fe266059d', 'avatarUrl': '/avatars/f3eedfecf5baa8e2ac80d37abe42c63f.svg', 'isPro': False, 'fullname': 'Keming Wu', 'user': 'wukeming11', 'type': 'user'}, 'name': 'Keming Wu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:57:20.576Z', 'hidden': False}, {'_id': '67e4b82c672b3d9c9cb42c73', 'user': {'_id': '672894ff1905afcdc9132fc6', 'avatarUrl': '/avatars/78297eadad816c45d680aa70cea3b973.svg', 'isPro': False, 'fullname': 'QISHENG LIAO', 'user': 'Marseclipse', 'type': 'user'}, 'name': 'Qisheng Liao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:57:14.624Z', 'hidden': False}, {'_id': '67e4b82c672b3d9c9cb42c74', 'user': {'_id': '64ba249e5c4deebf69aa17fd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/h7YdQe9wEr1TTJfP1ALhb.jpeg', 'isPro': False, 'fullname': 'chen', 'user': 'bohanChen', 'type': 'user'}, 'name': 'Bohan Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:57:07.459Z', 'hidden': False}, {'_id': '67e4b82c672b3d9c9cb42c75', 'user': {'_id': '6298fd95b58e71e2ac9f3ad8', 'avatarUrl': '/avatars/7d34644d537bc5c17cf1e4ce4095355c.svg', 'isPro': False, 'fullname': 'Kevin Lin', 'user': 'kevinlin311tw', 'type': 'user'}, 'name': 'Kevin Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:57:01.691Z', 'hidden': False}, {'_id': '67e4b82c672b3d9c9cb42c76', 'name': 'Danqing Huang', 'hidden': False}, {'_id': '67e4b82c672b3d9c9cb42c77', 'name': 'Ji Li', 'hidden': False}, {'_id': '67e4b82c672b3d9c9cb42c78', 'user': {'_id': '631f108bb45367a05fe74260', 'avatarUrl': '/avatars/c20da6800b643728062712d9a2771648.svg', 'isPro': False, 'fullname': 'Researcher', 'user': 'YuanYuhui', 'type': 'user'}, 'name': 'Yuhui Yuan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:56:39.210Z', 'hidden': False}], 'publishedAt': '2025-03-26T16:04:57.000Z', 'submittedOnDailyAt': '2025-03-27T01:00:23.038Z', 'title': 'BizGen: Advancing Article-level Visual Text Rendering for Infographics\\n  Generation', 'submittedOnDailyBy': {'_id': '62333a88fd7bb4a39b92d387', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png', 'isPro': False, 'fullname': 'Alex Jinpeng Wang', 'user': 'Awiny', 'type': 'user'}, 'summary': 'Recently, state-of-the-art text-to-image generation models, such as Flux and\\nIdeogram 2.0, have made significant progress in sentence-level visual text\\nrendering. In this paper, we focus on the more challenging scenarios of\\narticle-level visual text rendering and address a novel task of generating\\nhigh-quality business content, including infographics and slides, based on user\\nprovided article-level descriptive prompts and ultra-dense layouts. The\\nfundamental challenges are twofold: significantly longer context lengths and\\nthe scarcity of high-quality business content data.\\n  In contrast to most previous works that focus on a limited number of\\nsub-regions and sentence-level prompts, ensuring precise adherence to\\nultra-dense layouts with tens or even hundreds of sub-regions in business\\ncontent is far more challenging. We make two key technical contributions: (i)\\nthe construction of scalable, high-quality business content dataset, i.e.,\\nInfographics-650K, equipped with ultra-dense layouts and prompts by\\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\\nand (ii) a layout-guided cross attention scheme, which injects tens of\\nregion-wise prompts into a set of cropped region latent space according to the\\nultra-dense layouts, and refine each sub-regions flexibly during inference\\nusing a layout conditional CFG.\\n  We demonstrate the strong results of our system compared to previous SOTA\\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\\nconduct thorough ablation experiments to verify the effectiveness of each\\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\\nthe broader community to advance the progress of business content generation.', 'upvotes': 11, 'discussionId': '67e4b831672b3d9c9cb42ebb', 'projectPage': 'https://bizgen-msra.github.io/', 'githubRepo': 'https://github.com/1230young/bizgen', 'ai_keywords': ['scalable, high-quality business content dataset', 'Infographics-650K', 'layer-wise retrieval-augmented infographic generation scheme', 'layout-guided cross attention scheme', 'cropped region latent space', 'layout conditional CFG', 'BizEval prompt set', 'ablation experiments']}, 'publishedAt': '2025-03-26T12:04:57.000Z', 'title': 'BizGen: Advancing Article-level Visual Text Rendering for Infographics\\n  Generation', 'summary': 'Recently, state-of-the-art text-to-image generation models, such as Flux and\\nIdeogram 2.0, have made significant progress in sentence-level visual text\\nrendering. In this paper, we focus on the more challenging scenarios of\\narticle-level visual text rendering and address a novel task of generating\\nhigh-quality business content, including infographics and slides, based on user\\nprovided article-level descriptive prompts and ultra-dense layouts. The\\nfundamental challenges are twofold: significantly longer context lengths and\\nthe scarcity of high-quality business content data.\\n  In contrast to most previous works that focus on a limited number of\\nsub-regions and sentence-level prompts, ensuring precise adherence to\\nultra-dense layouts with tens or even hundreds of sub-regions in business\\ncontent is far more challenging. We make two key technical contributions: (i)\\nthe construction of scalable, high-quality business content dataset, i.e.,\\nInfographics-650K, equipped with ultra-dense layouts and prompts by\\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\\nand (ii) a layout-guided cross attention scheme, which injects tens of\\nregion-wise prompts into a set of cropped region latent space according to the\\nultra-dense layouts, and refine each sub-regions flexibly during inference\\nusing a layout conditional CFG.\\n  We demonstrate the strong results of our system compared to previous SOTA\\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\\nconduct thorough ablation experiments to verify the effectiveness of each\\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\\nthe broader community to advance the progress of business content generation.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20672.png', 'numComments': 2, 'submittedBy': {'_id': '62333a88fd7bb4a39b92d387', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png', 'fullname': 'Alex Jinpeng Wang', 'name': 'Awiny', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.20020', 'authors': [{'_id': '67e4b288fa81c69f446da710', 'name': 'Gemini Robotics Team', 'hidden': False}, {'_id': '67e4b288fa81c69f446da711', 'user': {'_id': '61fd7ac3fbafe89f48101d83', 'avatarUrl': '/avatars/cb2c86a04574498e71d6c447c2b289c1.svg', 'isPro': False, 'fullname': 'Saminda Abeyruwan', 'user': 'saminda', 'type': 'user'}, 'name': 'Saminda Abeyruwan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:58:16.630Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da712', 'name': 'Joshua Ainslie', 'hidden': False}, {'_id': '67e4b288fa81c69f446da713', 'user': {'_id': '6253ee39e4e98393660b5c35', 'avatarUrl': '/avatars/9c032f6a0729bfe5c16b3affe190834d.svg', 'isPro': False, 'fullname': 'Jean-Baptiste Alayrac', 'user': 'jalayrac', 'type': 'user'}, 'name': 'Jean-Baptiste Alayrac', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:58:27.846Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da714', 'user': {'_id': '63f47a88121f9894707465ed', 'avatarUrl': '/avatars/d85d409d19068aea02a2532b587dd1ef.svg', 'isPro': False, 'fullname': 'Montserrat Gonzalez Arenas', 'user': 'montse90', 'type': 'user'}, 'name': 'Montserrat Gonzalez Arenas', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:58:33.087Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da715', 'user': {'_id': '66140283cf3fef4fa812e92f', 'avatarUrl': '/avatars/033586adc3931d6c85bf9e84220992b4.svg', 'isPro': False, 'fullname': 'Travis Armstrong', 'user': 'TravisAStrong', 'type': 'user'}, 'name': 'Travis Armstrong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:58:39.411Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da716', 'user': {'_id': '653ad36e5f1703225b266b7b', 'avatarUrl': '/avatars/170e6b54a9859c7fca0289a09654c47f.svg', 'isPro': False, 'fullname': 'Ashwin Balakrishna', 'user': 'abalakrishna123', 'type': 'user'}, 'name': 'Ashwin Balakrishna', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:58:46.130Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da717', 'name': 'Robert Baruch', 'hidden': False}, {'_id': '67e4b288fa81c69f446da718', 'name': 'Maria Bauza', 'hidden': False}, {'_id': '67e4b288fa81c69f446da719', 'name': 'Michiel Blokzijl', 'hidden': False}, {'_id': '67e4b288fa81c69f446da71a', 'name': 'Steven Bohez', 'hidden': False}, {'_id': '67e4b288fa81c69f446da71b', 'name': 'Konstantinos Bousmalis', 'hidden': False}, {'_id': '67e4b288fa81c69f446da71c', 'name': 'Anthony Brohan', 'hidden': False}, {'_id': '67e4b288fa81c69f446da71d', 'name': 'Thomas Buschmann', 'hidden': False}, {'_id': '67e4b288fa81c69f446da71e', 'name': 'Arunkumar Byravan', 'hidden': False}, {'_id': '67e4b288fa81c69f446da71f', 'name': 'Serkan Cabi', 'hidden': False}, {'_id': '67e4b288fa81c69f446da720', 'name': 'Ken Caluwaerts', 'hidden': False}, {'_id': '67e4b288fa81c69f446da721', 'name': 'Federico Casarini', 'hidden': False}, {'_id': '67e4b288fa81c69f446da722', 'name': 'Oscar Chang', 'hidden': False}, {'_id': '67e4b288fa81c69f446da723', 'name': 'Jose Enrique Chen', 'hidden': False}, {'_id': '67e4b288fa81c69f446da724', 'name': 'Xi Chen', 'hidden': False}, {'_id': '67e4b288fa81c69f446da725', 'name': 'Hao-Tien Lewis Chiang', 'hidden': False}, {'_id': '67e4b288fa81c69f446da726', 'name': 'Krzysztof Choromanski', 'hidden': False}, {'_id': '67e4b288fa81c69f446da727', 'name': \"David D'Ambrosio\", 'hidden': False}, {'_id': '67e4b288fa81c69f446da728', 'name': 'Sudeep Dasari', 'hidden': False}, {'_id': '67e4b288fa81c69f446da729', 'name': 'Todor Davchev', 'hidden': False}, {'_id': '67e4b288fa81c69f446da72a', 'name': 'Coline Devin', 'hidden': False}, {'_id': '67e4b288fa81c69f446da72b', 'user': {'_id': '62dac377f014388f908974f4', 'avatarUrl': '/avatars/39bf0ca206441575a45f577060cdd8bc.svg', 'isPro': False, 'fullname': 'Norman Di Palo', 'user': 'normandipalo', 'type': 'user'}, 'name': 'Norman Di Palo', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:00:09.232Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da72c', 'user': {'_id': '64d89da3bab152b24713108e', 'avatarUrl': '/avatars/22e10f9a13b0d18b3a3b1f5281c7124d.svg', 'isPro': False, 'fullname': 'Tianli Ding', 'user': 'Tding', 'type': 'user'}, 'name': 'Tianli Ding', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:00:18.146Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da72d', 'user': {'_id': '63b7d36469b7bd7324f9f438', 'avatarUrl': '/avatars/67b1864c378102b1cf2de571cce7bf9a.svg', 'isPro': False, 'fullname': 'Adil Dostmohamed', 'user': 'adild', 'type': 'user'}, 'name': 'Adil Dostmohamed', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:00:29.517Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da72e', 'user': {'_id': '67225875b46c703941fa7967', 'avatarUrl': '/avatars/7c89fbdd9a135210209bcd0cbfe7988a.svg', 'isPro': False, 'fullname': 'Danny Driess', 'user': 'dannydriess', 'type': 'user'}, 'name': 'Danny Driess', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:00:37.044Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da72f', 'user': {'_id': '63c9bd445fdc575773c732fe', 'avatarUrl': '/avatars/def472d1ab3fbf751225357c0932ae7e.svg', 'isPro': False, 'fullname': 'Yilun Du', 'user': 'yilundu', 'type': 'user'}, 'name': 'Yilun Du', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:00:43.648Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da730', 'name': 'Debidatta Dwibedi', 'hidden': False}, {'_id': '67e4b288fa81c69f446da731', 'user': {'_id': '66f6e9a737473469e871cae8', 'avatarUrl': '/avatars/8c247380cee5d879aac204299963d3a7.svg', 'isPro': False, 'fullname': 'Michael Elabd', 'user': 'michaelelabd', 'type': 'user'}, 'name': 'Michael Elabd', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:00:55.368Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da732', 'name': 'Claudio Fantacci', 'hidden': False}, {'_id': '67e4b288fa81c69f446da733', 'name': 'Cody Fong', 'hidden': False}, {'_id': '67e4b288fa81c69f446da734', 'name': 'Erik Frey', 'hidden': False}, {'_id': '67e4b288fa81c69f446da735', 'name': 'Chuyuan Fu', 'hidden': False}, {'_id': '67e4b288fa81c69f446da736', 'name': 'Marissa Giustina', 'hidden': False}, {'_id': '67e4b288fa81c69f446da737', 'name': 'Keerthana Gopalakrishnan', 'hidden': False}, {'_id': '67e4b288fa81c69f446da738', 'name': 'Laura Graesser', 'hidden': False}, {'_id': '67e4b288fa81c69f446da739', 'name': 'Leonard Hasenclever', 'hidden': False}, {'_id': '67e4b288fa81c69f446da73a', 'name': 'Nicolas Heess', 'hidden': False}, {'_id': '67e4b288fa81c69f446da73b', 'name': 'Brandon Hernaez', 'hidden': False}, {'_id': '67e4b288fa81c69f446da73c', 'name': 'Alexander Herzog', 'hidden': False}, {'_id': '67e4b288fa81c69f446da73d', 'name': 'R. Alex Hofer', 'hidden': False}, {'_id': '67e4b288fa81c69f446da73e', 'name': 'Jan Humplik', 'hidden': False}, {'_id': '67e4b288fa81c69f446da73f', 'name': 'Atil Iscen', 'hidden': False}, {'_id': '67e4b288fa81c69f446da740', 'name': 'Mithun George Jacob', 'hidden': False}, {'_id': '67e4b288fa81c69f446da741', 'name': 'Deepali Jain', 'hidden': False}, {'_id': '67e4b288fa81c69f446da742', 'name': 'Ryan Julian', 'hidden': False}, {'_id': '67e4b288fa81c69f446da743', 'user': {'_id': '64b7bf04a5018e3c7ca2ecda', 'avatarUrl': '/avatars/4a434c344f68f2915c6e823262e62946.svg', 'isPro': False, 'fullname': 'Dmitry Kalashnikov', 'user': 'dmitry-kalashnikov', 'type': 'user'}, 'name': 'Dmitry Kalashnikov', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:01:27.098Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da744', 'name': 'M. Emre Karagozler', 'hidden': False}, {'_id': '67e4b288fa81c69f446da745', 'name': 'Stefani Karp', 'hidden': False}, {'_id': '67e4b288fa81c69f446da746', 'name': 'Chase Kew', 'hidden': False}, {'_id': '67e4b288fa81c69f446da747', 'name': 'Jerad Kirkland', 'hidden': False}, {'_id': '67e4b288fa81c69f446da748', 'name': 'Sean Kirmani', 'hidden': False}, {'_id': '67e4b288fa81c69f446da749', 'name': 'Yuheng Kuang', 'hidden': False}, {'_id': '67e4b288fa81c69f446da74a', 'user': {'_id': '631fa7e9124782a19efd20f2', 'avatarUrl': '/avatars/456ef70cdb2a6a1f79a078746e96034a.svg', 'isPro': False, 'fullname': 'Thomas Lampe', 'user': 'tlampe', 'type': 'user'}, 'name': 'Thomas Lampe', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:01:36.252Z', 'hidden': False}, {'_id': '67e4b288fa81c69f446da74b', 'name': 'Antoine Laurens', 'hidden': False}, {'_id': '67e4b288fa81c69f446da74c', 'name': 'Isabel Leal', 'hidden': False}, {'_id': '67e4b288fa81c69f446da74d', 'name': 'Alex X. Lee', 'hidden': False}, {'_id': '67e4b288fa81c69f446da74e', 'name': 'Tsang-Wei Edward Lee', 'hidden': False}, {'_id': '67e4b288fa81c69f446da74f', 'name': 'Jacky Liang', 'hidden': False}, {'_id': '67e4b288fa81c69f446da750', 'name': 'Yixin Lin', 'hidden': False}, {'_id': '67e4b288fa81c69f446da751', 'name': 'Sharath Maddineni', 'hidden': False}, {'_id': '67e4b288fa81c69f446da752', 'name': 'Anirudha Majumdar', 'hidden': False}, {'_id': '67e4b288fa81c69f446da753', 'name': 'Assaf Hurwitz Michaely', 'hidden': False}, {'_id': '67e4b288fa81c69f446da754', 'name': 'Robert Moreno', 'hidden': False}, {'_id': '67e4b288fa81c69f446da755', 'name': 'Michael Neunert', 'hidden': False}, {'_id': '67e4b288fa81c69f446da756', 'name': 'Francesco Nori', 'hidden': False}, {'_id': '67e4b288fa81c69f446da757', 'name': 'Carolina Parada', 'hidden': False}, {'_id': '67e4b288fa81c69f446da758', 'name': 'Emilio Parisotto', 'hidden': False}, {'_id': '67e4b288fa81c69f446da759', 'name': 'Peter Pastor', 'hidden': False}, {'_id': '67e4b288fa81c69f446da75a', 'name': 'Acorn Pooley', 'hidden': False}, {'_id': '67e4b288fa81c69f446da75b', 'name': 'Kanishka Rao', 'hidden': False}, {'_id': '67e4b288fa81c69f446da75c', 'name': 'Krista Reymann', 'hidden': False}, {'_id': '67e4b288fa81c69f446da75d', 'name': 'Dorsa Sadigh', 'hidden': False}, {'_id': '67e4b288fa81c69f446da75e', 'name': 'Stefano Saliceti', 'hidden': False}, {'_id': '67e4b288fa81c69f446da75f', 'name': 'Pannag Sanketi', 'hidden': False}, {'_id': '67e4b288fa81c69f446da760', 'name': 'Pierre Sermanet', 'hidden': False}, {'_id': '67e4b288fa81c69f446da761', 'name': 'Dhruv Shah', 'hidden': False}, {'_id': '67e4b288fa81c69f446da762', 'name': 'Mohit Sharma', 'hidden': False}, {'_id': '67e4b288fa81c69f446da763', 'name': 'Kathryn Shea', 'hidden': False}, {'_id': '67e4b288fa81c69f446da764', 'name': 'Charles Shu', 'hidden': False}, {'_id': '67e4b288fa81c69f446da765', 'name': 'Vikas Sindhwani', 'hidden': False}, {'_id': '67e4b288fa81c69f446da766', 'name': 'Sumeet Singh', 'hidden': False}, {'_id': '67e4b288fa81c69f446da767', 'name': 'Radu Soricut', 'hidden': False}, {'_id': '67e4b288fa81c69f446da768', 'name': 'Jost Tobias Springenberg', 'hidden': False}, {'_id': '67e4b288fa81c69f446da769', 'name': 'Rachel Sterneck', 'hidden': False}, {'_id': '67e4b288fa81c69f446da76a', 'name': 'Razvan Surdulescu', 'hidden': False}, {'_id': '67e4b288fa81c69f446da76b', 'name': 'Jie Tan', 'hidden': False}, {'_id': '67e4b288fa81c69f446da76c', 'name': 'Jonathan Tompson', 'hidden': False}, {'_id': '67e4b288fa81c69f446da76d', 'name': 'Vincent Vanhoucke', 'hidden': False}, {'_id': '67e4b288fa81c69f446da76e', 'name': 'Jake Varley', 'hidden': False}, {'_id': '67e4b288fa81c69f446da76f', 'name': 'Grace Vesom', 'hidden': False}, {'_id': '67e4b288fa81c69f446da770', 'name': 'Giulia Vezzani', 'hidden': False}, {'_id': '67e4b288fa81c69f446da771', 'name': 'Oriol Vinyals', 'hidden': False}, {'_id': '67e4b288fa81c69f446da772', 'name': 'Ayzaan Wahid', 'hidden': False}, {'_id': '67e4b288fa81c69f446da773', 'name': 'Stefan Welker', 'hidden': False}, {'_id': '67e4b288fa81c69f446da774', 'name': 'Paul Wohlhart', 'hidden': False}, {'_id': '67e4b288fa81c69f446da775', 'name': 'Fei Xia', 'hidden': False}, {'_id': '67e4b288fa81c69f446da776', 'name': 'Ted Xiao', 'hidden': False}, {'_id': '67e4b288fa81c69f446da777', 'name': 'Annie Xie', 'hidden': False}, {'_id': '67e4b288fa81c69f446da778', 'name': 'Jinyu Xie', 'hidden': False}, {'_id': '67e4b288fa81c69f446da779', 'name': 'Peng Xu', 'hidden': False}, {'_id': '67e4b288fa81c69f446da77a', 'name': 'Sichun Xu', 'hidden': False}, {'_id': '67e4b288fa81c69f446da77b', 'name': 'Ying Xu', 'hidden': False}, {'_id': '67e4b288fa81c69f446da77c', 'name': 'Zhuo Xu', 'hidden': False}, {'_id': '67e4b288fa81c69f446da77d', 'name': 'Yuxiang Yang', 'hidden': False}, {'_id': '67e4b288fa81c69f446da77e', 'name': 'Rui Yao', 'hidden': False}, {'_id': '67e4b288fa81c69f446da77f', 'name': 'Sergey Yaroshenko', 'hidden': False}, {'_id': '67e4b288fa81c69f446da780', 'name': 'Wenhao Yu', 'hidden': False}, {'_id': '67e4b288fa81c69f446da781', 'name': 'Wentao Yuan', 'hidden': False}, {'_id': '67e4b288fa81c69f446da782', 'name': 'Jingwei Zhang', 'hidden': False}, {'_id': '67e4b288fa81c69f446da783', 'name': 'Tingnan Zhang', 'hidden': False}, {'_id': '67e4b288fa81c69f446da784', 'name': 'Allan Zhou', 'hidden': False}, {'_id': '67e4b288fa81c69f446da785', 'name': 'Yuxiang Zhou', 'hidden': False}], 'publishedAt': '2025-03-25T19:02:56.000Z', 'submittedOnDailyAt': '2025-03-27T00:36:27.703Z', 'title': 'Gemini Robotics: Bringing AI into the Physical World', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': \"Recent advancements in large multimodal models have led to the emergence of\\nremarkable generalist capabilities in digital domains, yet their translation to\\nphysical agents such as robots remains a significant challenge. This report\\nintroduces a new family of AI models purposefully designed for robotics and\\nbuilt upon the foundation of Gemini 2.0. We present Gemini Robotics, an\\nadvanced Vision-Language-Action (VLA) generalist model capable of directly\\ncontrolling robots. Gemini Robotics executes smooth and reactive movements to\\ntackle a wide range of complex manipulation tasks while also being robust to\\nvariations in object types and positions, handling unseen environments as well\\nas following diverse, open vocabulary instructions. We show that with\\nadditional fine-tuning, Gemini Robotics can be specialized to new capabilities\\nincluding solving long-horizon, highly dexterous tasks, learning new\\nshort-horizon tasks from as few as 100 demonstrations and adapting to\\ncompletely novel robot embodiments. This is made possible because Gemini\\nRobotics builds on top of the Gemini Robotics-ER model, the second model we\\nintroduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends\\nGemini's multimodal reasoning capabilities into the physical world, with\\nenhanced spatial and temporal understanding. This enables capabilities relevant\\nto robotics including object detection, pointing, trajectory and grasp\\nprediction, as well as multi-view correspondence and 3D bounding box\\npredictions. We show how this novel combination can support a variety of\\nrobotics applications. We also discuss and address important safety\\nconsiderations related to this new class of robotics foundation models. The\\nGemini Robotics family marks a substantial step towards developing\\ngeneral-purpose robots that realizes AI's potential in the physical world.\", 'upvotes': 9, 'discussionId': '67e4b28cfa81c69f446da8c7', 'ai_keywords': ['Vision-Language-Action (VLA) generalist model', 'multimodal model', 'multimodal reasoning capabilities', 'fine-tuning', 'long-horizon, highly dexterous tasks', 'short-horizon tasks', 'robot embodiments', 'embodied reasoning', 'object detection', 'pointing', 'trajectory prediction', 'grasp prediction', 'multi-view correspondence', '3D bounding box predictions', 'robotics applications', 'safety considerations', 'robotics foundation models', 'general-purpose robots']}, 'publishedAt': '2025-03-25T15:02:56.000Z', 'title': 'Gemini Robotics: Bringing AI into the Physical World', 'summary': \"Recent advancements in large multimodal models have led to the emergence of\\nremarkable generalist capabilities in digital domains, yet their translation to\\nphysical agents such as robots remains a significant challenge. This report\\nintroduces a new family of AI models purposefully designed for robotics and\\nbuilt upon the foundation of Gemini 2.0. We present Gemini Robotics, an\\nadvanced Vision-Language-Action (VLA) generalist model capable of directly\\ncontrolling robots. Gemini Robotics executes smooth and reactive movements to\\ntackle a wide range of complex manipulation tasks while also being robust to\\nvariations in object types and positions, handling unseen environments as well\\nas following diverse, open vocabulary instructions. We show that with\\nadditional fine-tuning, Gemini Robotics can be specialized to new capabilities\\nincluding solving long-horizon, highly dexterous tasks, learning new\\nshort-horizon tasks from as few as 100 demonstrations and adapting to\\ncompletely novel robot embodiments. This is made possible because Gemini\\nRobotics builds on top of the Gemini Robotics-ER model, the second model we\\nintroduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends\\nGemini's multimodal reasoning capabilities into the physical world, with\\nenhanced spatial and temporal understanding. This enables capabilities relevant\\nto robotics including object detection, pointing, trajectory and grasp\\nprediction, as well as multi-view correspondence and 3D bounding box\\npredictions. We show how this novel combination can support a variety of\\nrobotics applications. We also discuss and address important safety\\nconsiderations related to this new class of robotics foundation models. The\\nGemini Robotics family marks a substantial step towards developing\\ngeneral-purpose robots that realizes AI's potential in the physical world.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20020.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': True, 'isMod': False, 'followerCount': 6482}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.20757', 'authors': [{'_id': '67e4c08fd9b7021d4a600fa4', 'user': {'_id': '662b4e3bc709a61df840fda1', 'avatarUrl': '/avatars/fc73c63a4e1f8fbb084ec43ec9af0af0.svg', 'isPro': False, 'fullname': 'Hu Yunhai', 'user': 'AlexCCtop', 'type': 'user'}, 'name': 'Yunhai Hu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:57:51.052Z', 'hidden': False}, {'_id': '67e4c08fd9b7021d4a600fa5', 'user': {'_id': '62f662bcc58915315c4eccea', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg', 'isPro': True, 'fullname': 'Yilun', 'user': 'yilunzhao', 'type': 'user'}, 'name': 'Yilun Zhao', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-03-27T03:05:54.275Z', 'hidden': False}, {'_id': '67e4c08fd9b7021d4a600fa6', 'user': {'_id': '660103ec4ae78d4ded4633fc', 'avatarUrl': '/avatars/efce106d70f5d092bf44d0638aa49984.svg', 'isPro': False, 'fullname': 'CHEN Zhao', 'user': 'chenzhao', 'type': 'user'}, 'name': 'Chen Zhao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:58:04.608Z', 'hidden': False}, {'_id': '67e4c08fd9b7021d4a600fa7', 'user': {'_id': '5f5ba21188f57f65f951f255', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png', 'isPro': False, 'fullname': 'Arman Cohan', 'user': 'armanc', 'type': 'user'}, 'name': 'Arman Cohan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T09:57:57.092Z', 'hidden': False}], 'publishedAt': '2025-03-26T17:46:08.000Z', 'submittedOnDailyAt': '2025-03-27T01:36:43.674Z', 'title': 'MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\\n  Search', 'submittedOnDailyBy': {'_id': '62f662bcc58915315c4eccea', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg', 'isPro': True, 'fullname': 'Yilun', 'user': 'yilunzhao', 'type': 'user'}, 'summary': 'We introduce MCTS-RAG, a novel approach that enhances the reasoning\\ncapabilities of small language models on knowledge-intensive tasks by\\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\\nintegrates retrieval and reasoning through an iterative decision-making\\nprocess. Unlike standard RAG methods, which typically retrieve information\\nindependently from reasoning and thus integrate knowledge suboptimally, or\\nconventional MCTS reasoning, which depends solely on internal model knowledge\\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\\nretrieval. This integrated approach enhances decision-making, reduces\\nhallucinations, and ensures improved factual accuracy and response consistency.\\nThe experimental results on multiple reasoning and knowledge-intensive datasets\\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\\nGPT-4o by effectively scaling inference-time compute, setting a new standard\\nfor reasoning in small-scale models.', 'upvotes': 6, 'discussionId': '67e4c092d9b7021d4a60108b', 'ai_keywords': ['MCTS-RAG', 'retrieval-augmented generation (RAG)', 'Monte Carlo Tree Search (MCTS)', 'reasoning paths', 'iterative decision-making', 'knowledge suboptimally', 'structured reasoning', 'adaptive retrieval', 'decision-making', 'hallucinations', 'factual accuracy', 'response consistency', 'ComplexWebQA', 'GPQA', 'FoolMeTwice', 'small-scale LMs', 'frontier LLMs (GPT-4o)', 'scaling inference-time compute']}, 'publishedAt': '2025-03-26T13:46:08.000Z', 'title': 'MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\\n  Search', 'summary': 'We introduce MCTS-RAG, a novel approach that enhances the reasoning\\ncapabilities of small language models on knowledge-intensive tasks by\\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\\nintegrates retrieval and reasoning through an iterative decision-making\\nprocess. Unlike standard RAG methods, which typically retrieve information\\nindependently from reasoning and thus integrate knowledge suboptimally, or\\nconventional MCTS reasoning, which depends solely on internal model knowledge\\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\\nretrieval. This integrated approach enhances decision-making, reduces\\nhallucinations, and ensures improved factual accuracy and response consistency.\\nThe experimental results on multiple reasoning and knowledge-intensive datasets\\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\\nGPT-4o by effectively scaling inference-time compute, setting a new standard\\nfor reasoning in small-scale models.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20757.png', 'numComments': 1, 'submittedBy': {'_id': '62f662bcc58915315c4eccea', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg', 'fullname': 'Yilun', 'name': 'yilunzhao', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 12}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.19950', 'authors': [{'_id': '67e4b27cfe1f5acc68028de9', 'user': {'_id': '6399c67bf78f75ae73146760', 'avatarUrl': '/avatars/d1c3b0eff67b598a1ad58e297382957f.svg', 'isPro': False, 'fullname': 'CHEN Han', 'user': 'Concyclics', 'type': 'user'}, 'name': 'Han Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-27T09:03:29.747Z', 'hidden': False}, {'_id': '67e4b27cfe1f5acc68028dea', 'user': {'_id': '650ccf6a36ac7eba06ea1cfa', 'avatarUrl': '/avatars/50374fca4f6cc2cf7a6601cd8d3f725b.svg', 'isPro': False, 'fullname': 'Zicong Jiang', 'user': 'Zicong99', 'type': 'user'}, 'name': 'Zicong Jiang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:02:00.596Z', 'hidden': True}, {'_id': '67e4b27cfe1f5acc68028deb', 'user': {'_id': '64b781c5da8017900e7b8b25', 'avatarUrl': '/avatars/0db9a83b0908cc6b9417360ed77fcc1a.svg', 'isPro': False, 'fullname': 'zining zhang', 'user': 'deciding', 'type': 'user'}, 'name': 'Zining Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:02:07.675Z', 'hidden': False}, {'_id': '67e4b27cfe1f5acc68028dec', 'name': 'Bingsheng He', 'hidden': False}, {'_id': '67e4b27cfe1f5acc68028ded', 'name': 'Pingyi Luo', 'hidden': False}, {'_id': '67e4b27cfe1f5acc68028dee', 'name': 'Mian Lu', 'hidden': False}, {'_id': '67e4b27cfe1f5acc68028def', 'name': 'Yuqiang Chen', 'hidden': False}], 'publishedAt': '2025-03-25T16:24:45.000Z', 'submittedOnDailyAt': '2025-03-27T02:17:05.033Z', 'title': 'LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\\n  Accuracy Preservation', 'submittedOnDailyBy': {'_id': '6399c67bf78f75ae73146760', 'avatarUrl': '/avatars/d1c3b0eff67b598a1ad58e297382957f.svg', 'isPro': False, 'fullname': 'CHEN Han', 'user': 'Concyclics', 'type': 'user'}, 'summary': \"We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\\nCache in large language model (LLM) inference, delivering substantial memory\\nsavings while preserving superior performance. Previous methods either assume\\nthat later tokens are more important or attempt to predict important tokens\\nbased on earlier attention patterns. Both approaches, however, can result in\\nperformance bottlenecks or frequent mispredictions.\\n  LogQuant takes a different approach. By applying a log-based filtering\\nmechanism, it selectively compresses the KV Cache across the entire context,\\nachieving better performance with the same or even reduced memory footprint\\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\\nand boosts batch size by 60% without increasing memory consumption. For\\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\\nby 40% to 200% at the same compression ratio, outperforming comparable\\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\\nlike Python's transformers library. Implementation can be available in\\nhttps://github.com/Concyclics/LogQuantKV.\", 'upvotes': 5, 'discussionId': '67e4b27efe1f5acc68028e72', 'githubRepo': 'https://github.com/Concyclics/LogQuantKV', 'ai_keywords': ['KV Cache', 'large language model (LLM)', 'token', 'attention pattern', 'log-based filtering mechanism', 'throughput', 'batch size', 'accuracy', 'Math Completion', 'Code Completion', 'compression ratio', 'transformers library']}, 'publishedAt': '2025-03-25T12:24:45.000Z', 'title': 'LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\\n  Accuracy Preservation', 'summary': \"We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\\nCache in large language model (LLM) inference, delivering substantial memory\\nsavings while preserving superior performance. Previous methods either assume\\nthat later tokens are more important or attempt to predict important tokens\\nbased on earlier attention patterns. Both approaches, however, can result in\\nperformance bottlenecks or frequent mispredictions.\\n  LogQuant takes a different approach. By applying a log-based filtering\\nmechanism, it selectively compresses the KV Cache across the entire context,\\nachieving better performance with the same or even reduced memory footprint\\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\\nand boosts batch size by 60% without increasing memory consumption. For\\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\\nby 40% to 200% at the same compression ratio, outperforming comparable\\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\\nlike Python's transformers library. Implementation can be available in\\nhttps://github.com/Concyclics/LogQuantKV.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19950.png', 'numComments': 1, 'submittedBy': {'_id': '6399c67bf78f75ae73146760', 'avatarUrl': '/avatars/d1c3b0eff67b598a1ad58e297382957f.svg', 'fullname': 'CHEN Han', 'name': 'Concyclics', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.19462', 'authors': [{'_id': '67e3641cd8da46951f860d84', 'user': {'_id': '645b8bf6438d6cfbe1ae47ae', 'avatarUrl': '/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg', 'isPro': False, 'fullname': 'Haiyu Zhang', 'user': 'aejion', 'type': 'user'}, 'name': 'Haiyu Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:02:45.776Z', 'hidden': False}, {'_id': '67e3641cd8da46951f860d85', 'user': {'_id': '643e943a70c6a27621eb1c89', 'avatarUrl': '/avatars/73ec521ab5ba84cc7908c52c0acef6ef.svg', 'isPro': False, 'fullname': 'Xinyuan Chen', 'user': 'AriaChen', 'type': 'user'}, 'name': 'Xinyuan Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:02:58.635Z', 'hidden': False}, {'_id': '67e3641cd8da46951f860d86', 'user': {'_id': '63201256c6b20f03c829c4b8', 'avatarUrl': '/avatars/a42092119777d65e60b12eb5ba0e45f1.svg', 'isPro': False, 'fullname': 'Yaohui Wang', 'user': 'YaohuiW', 'type': 'user'}, 'name': 'Yaohui Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:03:18.769Z', 'hidden': False}, {'_id': '67e3641cd8da46951f860d87', 'user': {'_id': '65d5ec74cd05bc1eaa125040', 'avatarUrl': '/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg', 'isPro': False, 'fullname': 'Xihui Liu', 'user': 'XihuiLiu', 'type': 'user'}, 'name': 'Xihui Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:03:26.468Z', 'hidden': False}, {'_id': '67e3641cd8da46951f860d88', 'name': 'Yunhong Wang', 'hidden': False}, {'_id': '67e3641cd8da46951f860d89', 'name': 'Yu Qiao', 'hidden': False}], 'publishedAt': '2025-03-25T08:52:07.000Z', 'submittedOnDailyAt': '2025-03-27T00:39:48.103Z', 'title': 'AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset', 'submittedOnDailyBy': {'_id': '645b8bf6438d6cfbe1ae47ae', 'avatarUrl': '/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg', 'isPro': False, 'fullname': 'Haiyu Zhang', 'user': 'aejion', 'type': 'user'}, 'summary': 'Diffusion models have achieved remarkable progress in the field of video\\ngeneration. However, their iterative denoising nature requires a large number\\nof inference steps to generate a video, which is slow and computationally\\nexpensive. In this paper, we begin with a detailed analysis of the challenges\\npresent in existing diffusion distillation methods and propose a novel\\nefficient method, namely AccVideo, to reduce the inference steps for\\naccelerating video diffusion models with synthetic dataset. We leverage the\\npretrained video diffusion model to generate multiple valid denoising\\ntrajectories as our synthetic dataset, which eliminates the use of useless data\\npoints during distillation. Based on the synthetic dataset, we design a\\ntrajectory-based few-step guidance that utilizes key data points from the\\ndenoising trajectories to learn the noise-to-video mapping, enabling video\\ngeneration in fewer steps. Furthermore, since the synthetic dataset captures\\nthe data distribution at each diffusion timestep, we introduce an adversarial\\ntraining strategy to align the output distribution of the student model with\\nthat of our synthetic dataset, thereby enhancing the video quality. Extensive\\nexperiments demonstrate that our model achieves 8.5x improvements in generation\\nspeed compared to the teacher model while maintaining comparable performance.\\nCompared to previous accelerating methods, our approach is capable of\\ngenerating videos with higher quality and resolution, i.e., 5-seconds,\\n720x1280, 24fps.', 'upvotes': 5, 'discussionId': '67e3641ed8da46951f860e12', 'ai_keywords': ['diffusion models', 'video generation', 'iterative denoising', 'inference steps', 'diffusion distillation', 'AccVideo', 'synthetic dataset', 'pretrained video diffusion model', 'denoising trajectories', 'trajectory-based few-step guidance', 'noise-to-video mapping', 'adversarial training strategy']}, 'publishedAt': '2025-03-25T04:52:07.000Z', 'title': 'AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset', 'summary': 'Diffusion models have achieved remarkable progress in the field of video\\ngeneration. However, their iterative denoising nature requires a large number\\nof inference steps to generate a video, which is slow and computationally\\nexpensive. In this paper, we begin with a detailed analysis of the challenges\\npresent in existing diffusion distillation methods and propose a novel\\nefficient method, namely AccVideo, to reduce the inference steps for\\naccelerating video diffusion models with synthetic dataset. We leverage the\\npretrained video diffusion model to generate multiple valid denoising\\ntrajectories as our synthetic dataset, which eliminates the use of useless data\\npoints during distillation. Based on the synthetic dataset, we design a\\ntrajectory-based few-step guidance that utilizes key data points from the\\ndenoising trajectories to learn the noise-to-video mapping, enabling video\\ngeneration in fewer steps. Furthermore, since the synthetic dataset captures\\nthe data distribution at each diffusion timestep, we introduce an adversarial\\ntraining strategy to align the output distribution of the student model with\\nthat of our synthetic dataset, thereby enhancing the video quality. Extensive\\nexperiments demonstrate that our model achieves 8.5x improvements in generation\\nspeed compared to the teacher model while maintaining comparable performance.\\nCompared to previous accelerating methods, our approach is capable of\\ngenerating videos with higher quality and resolution, i.e., 5-seconds,\\n720x1280, 24fps.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19462.png', 'numComments': 1, 'submittedBy': {'_id': '645b8bf6438d6cfbe1ae47ae', 'avatarUrl': '/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg', 'fullname': 'Haiyu Zhang', 'name': 'aejion', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.20271', 'authors': [{'_id': '67e4dc6a38e4d1444c71ce70', 'user': {'_id': '604ae011caabafacfa48e3de', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg', 'isPro': False, 'fullname': 'Haoqin Tu', 'user': 'PahaII', 'type': 'user'}, 'name': 'Haoqin Tu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:17:04.325Z', 'hidden': False}, {'_id': '67e4dc6a38e4d1444c71ce71', 'user': {'_id': '6625e4e960286184cd2a3cc8', 'avatarUrl': '/avatars/894c4ba1dbf776c5db62e03cde07f9f8.svg', 'isPro': False, 'fullname': 'Weitao Feng', 'user': 'Helicopt', 'type': 'user'}, 'name': 'Weitao Feng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:17:11.346Z', 'hidden': False}, {'_id': '67e4dc6a38e4d1444c71ce72', 'user': {'_id': '65ca20731082be5463d0945c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xpySHGNH7_yyH58gUyvvZ.jpeg', 'isPro': False, 'fullname': 'Hardy Chen', 'user': 'alihiker', 'type': 'user'}, 'name': 'Hardy Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:17:20.107Z', 'hidden': False}, {'_id': '67e4dc6a38e4d1444c71ce73', 'name': 'Hui Liu', 'hidden': False}, {'_id': '67e4dc6a38e4d1444c71ce74', 'user': {'_id': '6465f6467ff8fcbef7d22513', 'avatarUrl': '/avatars/07992835c235fbb07016a0ea4f1d61cb.svg', 'isPro': False, 'fullname': 'Xianfeng Tang', 'user': 'xianft', 'type': 'user'}, 'name': 'Xianfeng Tang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:17:40.389Z', 'hidden': False}, {'_id': '67e4dc6a38e4d1444c71ce75', 'user': {'_id': '645eb61da3c5cd8a16efffff', 'avatarUrl': '/avatars/9112bfeed598dfabf9e077e69e09ecc9.svg', 'isPro': False, 'fullname': 'Cihang Xie', 'user': 'cihangxie', 'type': 'user'}, 'name': 'Cihang Xie', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:17:34.681Z', 'hidden': False}], 'publishedAt': '2025-03-26T06:38:31.000Z', 'submittedOnDailyAt': '2025-03-27T06:03:39.751Z', 'title': 'ViLBench: A Suite for Vision-Language Process Reward Modeling', 'submittedOnDailyBy': {'_id': '604ae011caabafacfa48e3de', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg', 'isPro': False, 'fullname': 'Haoqin Tu', 'user': 'PahaII', 'type': 'user'}, 'summary': \"Process-supervised reward models serve as a fine-grained function that\\nprovides detailed step-wise feedback to model responses, facilitating effective\\nselection of reasoning trajectories for complex tasks. Despite its advantages,\\nevaluation on PRMs remains less explored, especially in the multimodal domain.\\nTo address this gap, this paper first benchmarks current vision large language\\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\\nsuperior VLLMs do not necessarily yield better rewarding performance. To\\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\\npromising pathway towards bridging the gap between general VLLMs and reward\\nmodels -- by collecting 73.6K vision-language process reward data using an\\nenhanced tree-search algorithm, our 3B model is able to achieve an average\\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\\nand data.\", 'upvotes': 4, 'discussionId': '67e4dc6b38e4d1444c71cee2', 'ai_keywords': ['vision large language models (VLLMs)', 'output reward models (ORMs)', 'process reward models (PRMs)', 'vision-language benchmarks', 'Chain-of-Thought (CoT)', 'vision-language process reward data', 'tree-search algorithm', 'ViLBench']}, 'publishedAt': '2025-03-26T02:38:31.000Z', 'title': 'ViLBench: A Suite for Vision-Language Process Reward Modeling', 'summary': \"Process-supervised reward models serve as a fine-grained function that\\nprovides detailed step-wise feedback to model responses, facilitating effective\\nselection of reasoning trajectories for complex tasks. Despite its advantages,\\nevaluation on PRMs remains less explored, especially in the multimodal domain.\\nTo address this gap, this paper first benchmarks current vision large language\\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\\nsuperior VLLMs do not necessarily yield better rewarding performance. To\\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\\npromising pathway towards bridging the gap between general VLLMs and reward\\nmodels -- by collecting 73.6K vision-language process reward data using an\\nenhanced tree-search algorithm, our 3B model is able to achieve an average\\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\\nand data.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20271.png', 'numComments': 1, 'submittedBy': {'_id': '604ae011caabafacfa48e3de', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg', 'fullname': 'Haoqin Tu', 'name': 'PahaII', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.19846', 'authors': [{'_id': '67e4c449672b3d9c9cb8aa4b', 'user': {'_id': '67e4c444692ba54248a6b337', 'avatarUrl': '/avatars/7695a30d185ccb2354129c72538e9180.svg', 'isPro': False, 'fullname': 'Aaron Serianni', 'user': 'serianni', 'type': 'user'}, 'name': 'Aaron Serianni', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-27T09:03:27.573Z', 'hidden': False}, {'_id': '67e4c449672b3d9c9cb8aa4c', 'user': {'_id': '6301af7259ab5d9dc0a15060', 'avatarUrl': '/avatars/0bcda98801877332893a1de169000737.svg', 'isPro': False, 'fullname': 'Tyler Zhu', 'user': 'tyleryzhu', 'type': 'user'}, 'name': 'Tyler Zhu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:16:06.286Z', 'hidden': False}, {'_id': '67e4c449672b3d9c9cb8aa4d', 'name': 'Olga Russakovsky', 'hidden': False}, {'_id': '67e4c449672b3d9c9cb8aa4e', 'name': 'Vikram V. Ramaswamy', 'hidden': False}], 'publishedAt': '2025-03-25T17:11:39.000Z', 'submittedOnDailyAt': '2025-03-27T01:57:35.164Z', 'title': 'Attention IoU: Examining Biases in CelebA using Attention Maps', 'submittedOnDailyBy': {'_id': '67e4c444692ba54248a6b337', 'avatarUrl': '/avatars/7695a30d185ccb2354129c72538e9180.svg', 'isPro': False, 'fullname': 'Aaron Serianni', 'user': 'serianni', 'type': 'user'}, 'summary': \"Computer vision models have been shown to exhibit and amplify biases across a\\nwide array of datasets and tasks. Existing methods for quantifying bias in\\nclassification models primarily focus on dataset distribution and model\\nperformance on subgroups, overlooking the internal workings of a model. We\\nintroduce the Attention-IoU (Attention Intersection over Union) metric and\\nrelated scores, which use attention maps to reveal biases within a model's\\ninternal representations and identify image features potentially causing the\\nbiases. First, we validate Attention-IoU on the synthetic Waterbirds dataset,\\nshowing that the metric accurately measures model bias. We then analyze the\\nCelebA dataset, finding that Attention-IoU uncovers correlations beyond\\naccuracy disparities. Through an investigation of individual attributes through\\nthe protected attribute of Male, we examine the distinct ways biases are\\nrepresented in CelebA. Lastly, by subsampling the training set to change\\nattribute correlations, we demonstrate that Attention-IoU reveals potential\\nconfounding variables not present in dataset labels.\", 'upvotes': 4, 'discussionId': '67e4c44a672b3d9c9cb8aaae', 'ai_keywords': ['Attention-IoU', 'attention maps', 'internal representations', 'image features', 'Waterbirds dataset', 'CelebA dataset', 'accuracy disparities', 'protected attribute', 'attribute correlations', 'confounding variables']}, 'publishedAt': '2025-03-25T13:11:39.000Z', 'title': 'Attention IoU: Examining Biases in CelebA using Attention Maps', 'summary': \"Computer vision models have been shown to exhibit and amplify biases across a\\nwide array of datasets and tasks. Existing methods for quantifying bias in\\nclassification models primarily focus on dataset distribution and model\\nperformance on subgroups, overlooking the internal workings of a model. We\\nintroduce the Attention-IoU (Attention Intersection over Union) metric and\\nrelated scores, which use attention maps to reveal biases within a model's\\ninternal representations and identify image features potentially causing the\\nbiases. First, we validate Attention-IoU on the synthetic Waterbirds dataset,\\nshowing that the metric accurately measures model bias. We then analyze the\\nCelebA dataset, finding that Attention-IoU uncovers correlations beyond\\naccuracy disparities. Through an investigation of individual attributes through\\nthe protected attribute of Male, we examine the distinct ways biases are\\nrepresented in CelebA. Lastly, by subsampling the training set to change\\nattribute correlations, we demonstrate that Attention-IoU reveals potential\\nconfounding variables not present in dataset labels.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19846.png', 'numComments': 0, 'submittedBy': {'_id': '67e4c444692ba54248a6b337', 'avatarUrl': '/avatars/7695a30d185ccb2354129c72538e9180.svg', 'fullname': 'Aaron Serianni', 'name': 'serianni', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.20756', 'authors': [{'_id': '67e4b0b850ca38886d7e78d0', 'user': {'_id': '6466549fa1a19b0623fe868f', 'avatarUrl': '/avatars/8f8c81e3db782df2b47833ee61d845e2.svg', 'isPro': False, 'fullname': 'Chenxi Wang', 'user': 'Chenxiwang', 'type': 'user'}, 'name': 'Chenxi Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:20:08.223Z', 'hidden': False}, {'_id': '67e4b0b850ca38886d7e78d1', 'user': {'_id': '669663472d25bd04e9af1d66', 'avatarUrl': '/avatars/8b11d5d79d1b8b205baa498a942f573c.svg', 'isPro': False, 'fullname': 'Jizhan Fang', 'user': 'JizhanFang', 'type': 'user'}, 'name': 'Jizhan Fang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:20:16.102Z', 'hidden': False}, {'_id': '67e4b0b850ca38886d7e78d2', 'user': {'_id': '64c073417255ff868721fb54', 'avatarUrl': '/avatars/c2a00114598aa89f071b937f0af4077f.svg', 'isPro': False, 'fullname': 'Xiang CHEN', 'user': 'xiangchen-dvi', 'type': 'user'}, 'name': 'Xiang Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:20:23.885Z', 'hidden': False}, {'_id': '67e4b0b850ca38886d7e78d3', 'user': {'_id': '626e9144afb8cb8f7a5cb503', 'avatarUrl': '/avatars/7e9df47dd4e3cb6df193878e561cc2bc.svg', 'isPro': False, 'fullname': 'Bozhong Tian', 'user': 'bozhong', 'type': 'user'}, 'name': 'Bozhong Tian', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:20:29.614Z', 'hidden': False}, {'_id': '67e4b0b850ca38886d7e78d4', 'user': {'_id': '6549caee44e75a7de4fee2fa', 'avatarUrl': '/avatars/5aea69671eb1299aaaa948d888b4b64f.svg', 'isPro': False, 'fullname': 'Xu Ziwen', 'user': 'xzwnlp', 'type': 'user'}, 'name': 'Ziwen Xu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:20:40.540Z', 'hidden': False}, {'_id': '67e4b0b850ca38886d7e78d5', 'user': {'_id': '67c06592ebc2e682b2628bde', 'avatarUrl': '/avatars/1032c0319eded956d2dd461b0e1b4ddb.svg', 'isPro': False, 'fullname': 'Huajun Chen', 'user': 'HuajunChen', 'type': 'user'}, 'name': 'Huajun Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:20:55.588Z', 'hidden': False}, {'_id': '67e4b0b850ca38886d7e78d6', 'user': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'isPro': False, 'fullname': 'Ningyu Zhang', 'user': 'Ningyu', 'type': 'user'}, 'name': 'Ningyu Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:21:07.281Z', 'hidden': False}], 'publishedAt': '2025-03-26T17:45:29.000Z', 'submittedOnDailyAt': '2025-03-27T00:28:51.612Z', 'title': 'ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\\n  Systems', 'submittedOnDailyBy': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'isPro': False, 'fullname': 'Ningyu Zhang', 'user': 'Ningyu', 'type': 'user'}, 'summary': \"Recent advancements in Large Multimodal Models (LMMs) have shown promise in\\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\\nhindered by challenges such as misunderstanding of traffic knowledge, complex\\nroad conditions, and diverse states of vehicle. To address these challenges, we\\npropose the use of Knowledge Editing, which enables targeted modifications to a\\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\\nwhich includes various real-world scenarios, multiple data types, and\\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\\nderive several interesting conclusions. We hope that our work will contribute\\nto the further advancement of knowledge editing applications in the field of\\nautonomous driving. Code and data are available in\\nhttps://github.com/zjunlp/EasyEdit.\", 'upvotes': 3, 'discussionId': '67e4b0bb50ca38886d7e79d0', 'ai_keywords': ['Knowledge Editing', 'ADS-Edit', 'multimodal knowledge editing dataset', 'autonomous driving systems']}, 'publishedAt': '2025-03-26T13:45:29.000Z', 'title': 'ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\\n  Systems', 'summary': \"Recent advancements in Large Multimodal Models (LMMs) have shown promise in\\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\\nhindered by challenges such as misunderstanding of traffic knowledge, complex\\nroad conditions, and diverse states of vehicle. To address these challenges, we\\npropose the use of Knowledge Editing, which enables targeted modifications to a\\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\\nwhich includes various real-world scenarios, multiple data types, and\\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\\nderive several interesting conclusions. We hope that our work will contribute\\nto the further advancement of knowledge editing applications in the field of\\nautonomous driving. Code and data are available in\\nhttps://github.com/zjunlp/EasyEdit.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20756.png', 'numComments': 1, 'submittedBy': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'fullname': 'Ningyu Zhang', 'name': 'Ningyu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 20}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.20198', 'authors': [{'_id': '67e4b98039509b0149142daa', 'user': {'_id': '62333a88fd7bb4a39b92d387', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png', 'isPro': False, 'fullname': 'Alex Jinpeng Wang', 'user': 'Awiny', 'type': 'user'}, 'name': 'Alex Jinpeng Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:16:22.358Z', 'hidden': False}, {'_id': '67e4b98039509b0149142dab', 'user': {'_id': '63db16fff03c3d71ef397206', 'avatarUrl': '/avatars/bfb7e0d730b7d03302799d5d2828d97d.svg', 'isPro': False, 'fullname': 'Linjie Li', 'user': 'linjieli222', 'type': 'user'}, 'name': 'Linjie Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:16:38.209Z', 'hidden': False}, {'_id': '67e4b98039509b0149142dac', 'user': {'_id': '630713411801ecc7d2592a7c', 'avatarUrl': '/avatars/fb36f69f03421c3a2a7f72ba0858fa60.svg', 'isPro': False, 'fullname': 'Zhengyuan Yang', 'user': 'zyang39', 'type': 'user'}, 'name': 'Zhengyuan Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:16:44.117Z', 'hidden': False}, {'_id': '67e4b98039509b0149142dad', 'user': {'_id': '6413521d4e5305c14f22e110', 'avatarUrl': '/avatars/a6f8d0573e678f79bc3c0b7897b818ce.svg', 'isPro': False, 'fullname': 'Lijuan Wang', 'user': 'Lijuan', 'type': 'user'}, 'name': 'Lijuan Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:16:50.812Z', 'hidden': False}, {'_id': '67e4b98039509b0149142dae', 'name': 'Min Li', 'hidden': False}], 'publishedAt': '2025-03-26T03:44:25.000Z', 'submittedOnDailyAt': '2025-03-27T01:06:03.239Z', 'title': 'Beyond Words: Advancing Long-Text Image Generation via Multimodal\\n  Autoregressive Models', 'submittedOnDailyBy': {'_id': '62333a88fd7bb4a39b92d387', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png', 'isPro': False, 'fullname': 'Alex Jinpeng Wang', 'user': 'Awiny', 'type': 'user'}, 'summary': 'Recent advancements in autoregressive and diffusion models have led to strong\\nperformance in image generation with short scene text words. However,\\ngenerating coherent, long-form text in images, such as paragraphs in slides or\\ndocuments, remains a major challenge for current generative models. We present\\nthe first work specifically focused on long text image generation, addressing a\\ncritical gap in existing text-to-image systems that typically handle only brief\\nphrases or single sentences. Through comprehensive analysis of state-of-the-art\\nautoregressive generation models, we identify the image tokenizer as a critical\\nbottleneck in text generating quality. To address this, we introduce a novel\\ntext-focused, binary tokenizer optimized for capturing detailed scene text\\nfeatures. Leveraging our tokenizer, we develop \\\\ModelName, a multimodal\\nautoregressive model that excels in generating high-quality long-text images\\nwith unprecedented fidelity. Our model offers robust controllability, enabling\\ncustomization of text properties such as font style, size, color, and\\nalignment. Extensive experiments demonstrate that \\\\ModelName~significantly\\noutperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E\\n3~dalle3 in generating long text accurately, consistently, and flexibly.\\nBeyond its technical achievements, \\\\ModelName~opens up exciting opportunities\\nfor innovative applications like interleaved document and PowerPoint\\ngeneration, establishing a new frontier in long-text image generating.', 'upvotes': 3, 'discussionId': '67e4b98439509b0149142f3c', 'ai_keywords': ['autoregressive models', 'diffusion models', 'image tokenizer', 'binary tokenizer', 'multimodal autoregressive model', 'font style', 'text properties', 'size', 'color', 'alignment', 'SD3.5 Large', 'GPT4o', 'DALL-E 3', 'long-text image generation']}, 'publishedAt': '2025-03-25T23:44:25.000Z', 'title': 'Beyond Words: Advancing Long-Text Image Generation via Multimodal\\n  Autoregressive Models', 'summary': 'Recent advancements in autoregressive and diffusion models have led to strong\\nperformance in image generation with short scene text words. However,\\ngenerating coherent, long-form text in images, such as paragraphs in slides or\\ndocuments, remains a major challenge for current generative models. We present\\nthe first work specifically focused on long text image generation, addressing a\\ncritical gap in existing text-to-image systems that typically handle only brief\\nphrases or single sentences. Through comprehensive analysis of state-of-the-art\\nautoregressive generation models, we identify the image tokenizer as a critical\\nbottleneck in text generating quality. To address this, we introduce a novel\\ntext-focused, binary tokenizer optimized for capturing detailed scene text\\nfeatures. Leveraging our tokenizer, we develop \\\\ModelName, a multimodal\\nautoregressive model that excels in generating high-quality long-text images\\nwith unprecedented fidelity. Our model offers robust controllability, enabling\\ncustomization of text properties such as font style, size, color, and\\nalignment. Extensive experiments demonstrate that \\\\ModelName~significantly\\noutperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E\\n3~dalle3 in generating long text accurately, consistently, and flexibly.\\nBeyond its technical achievements, \\\\ModelName~opens up exciting opportunities\\nfor innovative applications like interleaved document and PowerPoint\\ngeneration, establishing a new frontier in long-text image generating.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20198.png', 'numComments': 1, 'submittedBy': {'_id': '62333a88fd7bb4a39b92d387', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png', 'fullname': 'Alex Jinpeng Wang', 'name': 'Awiny', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.17358', 'authors': [{'_id': '67e3b41d0fa8f886db6b323a', 'user': {'_id': '649f019c59c1ae90dbe37b8e', 'avatarUrl': '/avatars/a2021fcd1342ad6d1360acdef34c9185.svg', 'isPro': False, 'fullname': 'Jerred Chen', 'user': 'jerredchen', 'type': 'user'}, 'name': 'Jerred Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:13:31.357Z', 'hidden': False}, {'_id': '67e3b41d0fa8f886db6b323b', 'user': {'_id': '6305ee63d70693fdf1c7dbb8', 'avatarUrl': '/avatars/0e81ed3757b4e65be82063b538c3fe49.svg', 'isPro': False, 'fullname': 'Ronald Clark', 'user': 'r0nn13', 'type': 'user'}, 'name': 'Ronald Clark', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:13:57.988Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/jIZXKaHvYvl_B_Tg_SAeY.jpeg', 'https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/cn0aTY48FezRlyiSMberj.mp4'], 'publishedAt': '2025-03-21T17:58:56.000Z', 'submittedOnDailyAt': '2025-03-27T07:45:14.887Z', 'title': 'Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred\\n  Image', 'submittedOnDailyBy': {'_id': '6305ee63d70693fdf1c7dbb8', 'avatarUrl': '/avatars/0e81ed3757b4e65be82063b538c3fe49.svg', 'isPro': False, 'fullname': 'Ronald Clark', 'user': 'r0nn13', 'type': 'user'}, 'summary': 'In many robotics and VR/AR applications, fast camera motions cause a high\\nlevel of motion blur, causing existing camera pose estimation methods to fail.\\nIn this work, we propose a novel framework that leverages motion blur as a rich\\ncue for motion estimation rather than treating it as an unwanted artifact. Our\\napproach works by predicting a dense motion flow field and a monocular depth\\nmap directly from a single motion-blurred image. We then recover the\\ninstantaneous camera velocity by solving a linear least squares problem under\\nthe small motion assumption. In essence, our method produces an IMU-like\\nmeasurement that robustly captures fast and aggressive camera movements. To\\ntrain our model, we construct a large-scale dataset with realistic synthetic\\nmotion blur derived from ScanNet++v2 and further refine our model by training\\nend-to-end on real data using our fully differentiable pipeline. Extensive\\nevaluations on real-world benchmarks demonstrate that our method achieves\\nstate-of-the-art angular and translational velocity estimates, outperforming\\ncurrent methods like MASt3R and COLMAP.', 'upvotes': 3, 'discussionId': '67e3b4200fa8f886db6b3328', 'projectPage': 'https://jerredchen.github.io/image-as-imu/', 'ai_keywords': ['motion blur', 'camera pose estimation', 'motion flow field', 'monocular depth map', 'linear least squares problem', 'small motion assumption', 'IMU-like measurement', 'ScanNet++v2', 'fully differentiable pipeline', 'real-world benchmarks', 'angular velocity estimates', 'translational velocity estimates']}, 'publishedAt': '2025-03-21T13:58:56.000Z', 'title': 'Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred\\n  Image', 'summary': 'In many robotics and VR/AR applications, fast camera motions cause a high\\nlevel of motion blur, causing existing camera pose estimation methods to fail.\\nIn this work, we propose a novel framework that leverages motion blur as a rich\\ncue for motion estimation rather than treating it as an unwanted artifact. Our\\napproach works by predicting a dense motion flow field and a monocular depth\\nmap directly from a single motion-blurred image. We then recover the\\ninstantaneous camera velocity by solving a linear least squares problem under\\nthe small motion assumption. In essence, our method produces an IMU-like\\nmeasurement that robustly captures fast and aggressive camera movements. To\\ntrain our model, we construct a large-scale dataset with realistic synthetic\\nmotion blur derived from ScanNet++v2 and further refine our model by training\\nend-to-end on real data using our fully differentiable pipeline. Extensive\\nevaluations on real-world benchmarks demonstrate that our method achieves\\nstate-of-the-art angular and translational velocity estimates, outperforming\\ncurrent methods like MASt3R and COLMAP.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/jIZXKaHvYvl_B_Tg_SAeY.jpeg', 'https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/cn0aTY48FezRlyiSMberj.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17358.png', 'numComments': 1, 'submittedBy': {'_id': '6305ee63d70693fdf1c7dbb8', 'avatarUrl': '/avatars/0e81ed3757b4e65be82063b538c3fe49.svg', 'fullname': 'Ronald Clark', 'name': 'r0nn13', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.20220', 'authors': [{'_id': '67e4b035af7d0551dc377e13', 'name': 'Weijie Guo', 'hidden': False}, {'_id': '67e4b035af7d0551dc377e14', 'user': {'_id': '661434dd0dcdede49138cd7c', 'avatarUrl': '/avatars/35f08bf08c5e9edfb3c78e280af718cb.svg', 'isPro': False, 'fullname': 'Guofeng Zhang', 'user': 'guofeng1123', 'type': 'user'}, 'name': 'Guofeng Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:15:46.953Z', 'hidden': False}, {'_id': '67e4b035af7d0551dc377e15', 'user': {'_id': '625f81afe1994410eef1c36a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg', 'isPro': False, 'fullname': 'Wufei Ma', 'user': 'wufeim', 'type': 'user'}, 'name': 'Wufei Ma', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:15:53.618Z', 'hidden': False}, {'_id': '67e4b035af7d0551dc377e16', 'name': 'Alan Yuille', 'hidden': False}], 'publishedAt': '2025-03-26T04:23:53.000Z', 'submittedOnDailyAt': '2025-03-27T00:26:48.304Z', 'title': 'DINeMo: Learning Neural Mesh Models with no 3D Annotations', 'submittedOnDailyBy': {'_id': '625f81afe1994410eef1c36a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg', 'isPro': False, 'fullname': 'Wufei Ma', 'user': 'wufeim', 'type': 'user'}, 'summary': 'Category-level 3D/6D pose estimation is a crucial step towards comprehensive\\n3D scene understanding, which would enable a broad range of applications in\\nrobotics and embodied AI. Recent works explored neural mesh models that\\napproach a range of 2D and 3D tasks from an analysis-by-synthesis perspective.\\nDespite the largely enhanced robustness to partial occlusion and domain shifts,\\nthese methods depended heavily on 3D annotations for part-contrastive learning,\\nwhich confines them to a narrow set of categories and hinders efficient\\nscaling. In this work, we present DINeMo, a novel neural mesh model that is\\ntrained with no 3D annotations by leveraging pseudo-correspondence obtained\\nfrom large visual foundation models. We adopt a bidirectional\\npseudo-correspondence generation method, which produce pseudo correspondence\\nutilize both local appearance features and global context information.\\nExperimental results on car datasets demonstrate that our DINeMo outperforms\\nprevious zero- and few-shot 3D pose estimation by a wide margin, narrowing the\\ngap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively\\nand efficiently when incorporating more unlabeled images during training, which\\ndemonstrate the advantages over supervised learning methods that rely on 3D\\nannotations. Our project page is available at\\nhttps://analysis-by-synthesis.github.io/DINeMo/.', 'upvotes': 2, 'discussionId': '67e4b038af7d0551dc377f07', 'projectPage': 'https://analysis-by-synthesis.github.io/DINeMo/', 'ai_keywords': ['neural mesh models', 'analysis-by-synthesis', 'robustness', 'partial occlusion', 'domain shifts', 'part-contrastive learning', 'pseudo-correspondence', 'visual foundation models', 'bidirectional pseudo-correspondence generation', 'local appearance features', 'global context information', 'zero-shot', 'few-shot', 'fully-supervised methods']}, 'publishedAt': '2025-03-26T00:23:53.000Z', 'title': 'DINeMo: Learning Neural Mesh Models with no 3D Annotations', 'summary': 'Category-level 3D/6D pose estimation is a crucial step towards comprehensive\\n3D scene understanding, which would enable a broad range of applications in\\nrobotics and embodied AI. Recent works explored neural mesh models that\\napproach a range of 2D and 3D tasks from an analysis-by-synthesis perspective.\\nDespite the largely enhanced robustness to partial occlusion and domain shifts,\\nthese methods depended heavily on 3D annotations for part-contrastive learning,\\nwhich confines them to a narrow set of categories and hinders efficient\\nscaling. In this work, we present DINeMo, a novel neural mesh model that is\\ntrained with no 3D annotations by leveraging pseudo-correspondence obtained\\nfrom large visual foundation models. We adopt a bidirectional\\npseudo-correspondence generation method, which produce pseudo correspondence\\nutilize both local appearance features and global context information.\\nExperimental results on car datasets demonstrate that our DINeMo outperforms\\nprevious zero- and few-shot 3D pose estimation by a wide margin, narrowing the\\ngap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively\\nand efficiently when incorporating more unlabeled images during training, which\\ndemonstrate the advantages over supervised learning methods that rely on 3D\\nannotations. Our project page is available at\\nhttps://analysis-by-synthesis.github.io/DINeMo/.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20220.png', 'numComments': 1, 'submittedBy': {'_id': '625f81afe1994410eef1c36a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg', 'fullname': 'Wufei Ma', 'name': 'wufeim', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.19953', 'authors': [{'_id': '67e4b46cc90e5edf25f581f8', 'name': 'Stefan Stojanov', 'hidden': False}, {'_id': '67e4b46cc90e5edf25f581f9', 'user': {'_id': '6303e4bb0547362a22a62274', 'avatarUrl': '/avatars/91c9922913e45410ceed77f2e4971588.svg', 'isPro': False, 'fullname': 'David Wendt', 'user': 'kmeisthax', 'type': 'user'}, 'name': 'David Wendt', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:14:24.479Z', 'hidden': False}, {'_id': '67e4b46cc90e5edf25f581fa', 'name': 'Seungwoo Kim', 'hidden': False}, {'_id': '67e4b46cc90e5edf25f581fb', 'name': 'Rahul Venkatesh', 'hidden': False}, {'_id': '67e4b46cc90e5edf25f581fc', 'name': 'Kevin Feigelis', 'hidden': False}, {'_id': '67e4b46cc90e5edf25f581fd', 'name': 'Jiajun Wu', 'hidden': False}, {'_id': '67e4b46cc90e5edf25f581fe', 'name': 'Daniel LK Yamins', 'hidden': False}], 'publishedAt': '2025-03-25T17:58:52.000Z', 'submittedOnDailyAt': '2025-03-27T00:44:41.359Z', 'title': 'Self-Supervised Learning of Motion Concepts by Optimizing\\n  Counterfactuals', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': \"Estimating motion in videos is an essential computer vision problem with many\\ndownstream applications, including controllable video generation and robotics.\\nCurrent solutions are primarily trained using synthetic data or require tuning\\nof situation-specific heuristics, which inherently limits these models'\\ncapabilities in real-world contexts. Despite recent developments in large-scale\\nself-supervised learning from videos, leveraging such representations for\\nmotion estimation remains relatively underexplored. In this work, we develop\\nOpt-CWM, a self-supervised technique for flow and occlusion estimation from a\\npre-trained next-frame prediction model. Opt-CWM works by learning to optimize\\ncounterfactual probes that extract motion information from a base video model,\\navoiding the need for fixed heuristics while training on unrestricted video\\ninputs. We achieve state-of-the-art performance for motion estimation on\\nreal-world videos while requiring no labeled data.\", 'upvotes': 2, 'discussionId': '67e4b46ec90e5edf25f582db', 'ai_keywords': ['self-supervised learning', 'flow estimation', 'occlusion estimation', 'next-frame prediction model', 'counterfactual probes', 'motion estimation']}, 'publishedAt': '2025-03-25T13:58:52.000Z', 'title': 'Self-Supervised Learning of Motion Concepts by Optimizing\\n  Counterfactuals', 'summary': \"Estimating motion in videos is an essential computer vision problem with many\\ndownstream applications, including controllable video generation and robotics.\\nCurrent solutions are primarily trained using synthetic data or require tuning\\nof situation-specific heuristics, which inherently limits these models'\\ncapabilities in real-world contexts. Despite recent developments in large-scale\\nself-supervised learning from videos, leveraging such representations for\\nmotion estimation remains relatively underexplored. In this work, we develop\\nOpt-CWM, a self-supervised technique for flow and occlusion estimation from a\\npre-trained next-frame prediction model. Opt-CWM works by learning to optimize\\ncounterfactual probes that extract motion information from a base video model,\\navoiding the need for fixed heuristics while training on unrestricted video\\ninputs. We achieve state-of-the-art performance for motion estimation on\\nreal-world videos while requiring no labeled data.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19953.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': True, 'isMod': False, 'followerCount': 6482}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.16870', 'authors': [{'_id': '67e4c860136c8a867191e52e', 'user': {'_id': '6627ff2a3b4fbc8420a416c3', 'avatarUrl': '/avatars/de3aafdaf5563fe25edcdb92b394474f.svg', 'isPro': False, 'fullname': 'AR', 'user': 'Anshumann', 'type': 'user'}, 'name': 'Anshumann', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:03:45.930Z', 'hidden': False}, {'_id': '67e4c860136c8a867191e52f', 'user': {'_id': '61765fe0b0715831eab6d465', 'avatarUrl': '/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg', 'isPro': False, 'fullname': 'Mohd Abbas Zaidi', 'user': 'ya-mehdi', 'type': 'user'}, 'name': 'Mohd Abbas Zaidi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:03:57.163Z', 'hidden': False}, {'_id': '67e4c860136c8a867191e530', 'name': 'Akhil Kedia', 'hidden': False}, {'_id': '67e4c860136c8a867191e531', 'user': {'_id': '65d61eebee922bc7a777d5a6', 'avatarUrl': '/avatars/180efd503ef412b4dc728e6aa477c16e.svg', 'isPro': False, 'fullname': 'Jinwoo Ahn', 'user': 'AndrewAhn', 'type': 'user'}, 'name': 'Jinwoo Ahn', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:04:07.687Z', 'hidden': False}, {'_id': '67e4c860136c8a867191e532', 'user': {'_id': '629059cdb90dde28ef5cbb30', 'avatarUrl': '/avatars/451b0e8999447b3a2f03378fe98c0661.svg', 'isPro': False, 'fullname': 'Taehwak Kwon', 'user': 'Rock222', 'type': 'user'}, 'name': 'Taehwak Kwon', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:04:23.731Z', 'hidden': False}, {'_id': '67e4c860136c8a867191e533', 'user': {'_id': '6354137306d707b332451421', 'avatarUrl': '/avatars/46770f32702e3ad08f91faeef9e4ea6e.svg', 'isPro': False, 'fullname': 'Kangwook Lee', 'user': 'kw1jjang', 'type': 'user'}, 'name': 'Kangwook Lee', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:04:33.400Z', 'hidden': False}, {'_id': '67e4c860136c8a867191e534', 'user': {'_id': '653b6e7232bd4db35d981615', 'avatarUrl': '/avatars/0fbf0c5d502b840bf26baf8c420c7593.svg', 'isPro': False, 'fullname': 'Haejun Lee', 'user': 'haejunlee', 'type': 'user'}, 'name': 'Haejun Lee', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:04:39.155Z', 'hidden': False}, {'_id': '67e4c860136c8a867191e535', 'user': {'_id': '64b4be0665a7e15eac085878', 'avatarUrl': '/avatars/dd19ec2987fb0735457c6492b53aacfe.svg', 'isPro': False, 'fullname': 'Joo-hyung Lee', 'user': 'snrbs17', 'type': 'user'}, 'name': 'Joohyung Lee', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-03-27T10:04:46.702Z', 'hidden': False}], 'publishedAt': '2025-03-21T05:58:18.000Z', 'submittedOnDailyAt': '2025-03-27T02:09:27.795Z', 'title': 'Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs', 'submittedOnDailyBy': {'_id': '61765fe0b0715831eab6d465', 'avatarUrl': '/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg', 'isPro': False, 'fullname': 'Mohd Abbas Zaidi', 'user': 'ya-mehdi', 'type': 'user'}, 'summary': \"Knowledge distillation can be a cost-effective technique to distill knowledge\\nin Large Language Models, if the teacher output logits can be pre-computed and\\ncached. However, successfully applying this to pre-training remains largely\\nunexplored. In this work, we prove that naive approaches for sparse knowledge\\ndistillation such as caching Top-K probabilities, while intuitive, provide\\nbiased estimates of teacher probability distribution to the student, resulting\\nin suboptimal performance and calibration. We propose an\\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\\nwhich provides unbiased estimates, preserves the gradient in expectation, and\\nrequires storing significantly sparser logits. Our method enables faster\\ntraining of student models with marginal overhead (<10%) compared to\\ncross-entropy based training, while maintaining competitive performance\\ncompared to full distillation, across a range of model sizes from 300M to 3B.\", 'upvotes': 2, 'discussionId': '67e4c861136c8a867191e58c', 'ai_keywords': ['Knowledge distillation', 'Large Language Models', 'teacher output logits', 'pre-computed', 'cached', 'sparse knowledge distillation', 'Top-K probabilities', 'biased estimates', 'teacher probability distribution', 'student', 'suboptimal performance', 'calibration', 'importance-sampling-based method', 'Random Sampling Knowledge Distillation', 'unbiased estimates', 'gradient in expectation', 'storing significantly sparser logits', 'cross-entropy based training', 'competitive performance', 'model sizes']}, 'publishedAt': '2025-03-21T01:58:18.000Z', 'title': 'Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs', 'summary': \"Knowledge distillation can be a cost-effective technique to distill knowledge\\nin Large Language Models, if the teacher output logits can be pre-computed and\\ncached. However, successfully applying this to pre-training remains largely\\nunexplored. In this work, we prove that naive approaches for sparse knowledge\\ndistillation such as caching Top-K probabilities, while intuitive, provide\\nbiased estimates of teacher probability distribution to the student, resulting\\nin suboptimal performance and calibration. We propose an\\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\\nwhich provides unbiased estimates, preserves the gradient in expectation, and\\nrequires storing significantly sparser logits. Our method enables faster\\ntraining of student models with marginal overhead (<10%) compared to\\ncross-entropy based training, while maintaining competitive performance\\ncompared to full distillation, across a range of model sizes from 300M to 3B.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16870.png', 'numComments': 1, 'submittedBy': {'_id': '61765fe0b0715831eab6d465', 'avatarUrl': '/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg', 'fullname': 'Mohd Abbas Zaidi', 'name': 'ya-mehdi', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.18929', 'authors': [{'_id': '67e5691d4240c0b10d6126ea', 'name': 'Brian R. Bartoldson', 'hidden': False}, {'_id': '67e5691d4240c0b10d6126eb', 'name': 'Siddarth Venkatraman', 'hidden': False}, {'_id': '67e5691d4240c0b10d6126ec', 'name': 'James Diffenderfer', 'hidden': False}, {'_id': '67e5691d4240c0b10d6126ed', 'name': 'Moksh Jain', 'hidden': False}, {'_id': '67e5691d4240c0b10d6126ee', 'name': 'Tal Ben-Nun', 'hidden': False}, {'_id': '67e5691d4240c0b10d6126ef', 'name': 'Seanie Lee', 'hidden': False}, {'_id': '67e5691d4240c0b10d6126f0', 'name': 'Minsu Kim', 'hidden': False}, {'_id': '67e5691d4240c0b10d6126f1', 'name': 'Johan Obando-Ceron', 'hidden': False}, {'_id': '67e5691d4240c0b10d6126f2', 'name': 'Yoshua Bengio', 'hidden': False}, {'_id': '67e5691d4240c0b10d6126f3', 'name': 'Bhavya Kailkhura', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6478f30ea68454566353ef95/1sIa7cKmzWjFwUSershwX.png'], 'publishedAt': '2025-03-24T17:51:39.000Z', 'submittedOnDailyAt': '2025-03-27T13:38:44.968Z', 'title': 'Trajectory Balance with Asynchrony: Decoupling Exploration and Learning\\n  for Fast, Scalable LLM Post-Training', 'submittedOnDailyBy': {'_id': '6478f30ea68454566353ef95', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6478f30ea68454566353ef95/hX5NiVPyQbK8TIBKnT4J1.jpeg', 'isPro': False, 'fullname': 'Johan Samir Obando Ceron', 'user': 'johanobandoc', 'type': 'user'}, 'summary': 'Reinforcement learning (RL) is a critical component of large language model\\n(LLM) post-training. However, existing on-policy algorithms used for\\npost-training are inherently incompatible with the use of experience replay\\nbuffers, which can be populated scalably by distributed off-policy actors to\\nenhance exploration as compute increases. We propose efficiently obtaining this\\nbenefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a\\nmassively scalable LLM RL system. In contrast to existing approaches, TBA uses\\na larger fraction of compute on search, constantly generating off-policy data\\nfor a central replay buffer. A training node simultaneously samples data from\\nthis buffer based on reward or recency to update the policy using Trajectory\\nBalance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA\\noffers three key advantages: (1) decoupled training and search, speeding up\\ntraining wall-clock time by 4x or more; (2) improved diversity through\\nlarge-scale off-policy sampling; and (3) scalable search for sparse reward\\nsettings. On mathematical reasoning, preference-tuning, and automated\\nred-teaming (diverse and representative post-training tasks), TBA produces\\nspeed and performance improvements over strong baselines.', 'upvotes': 1, 'discussionId': '67e5691f4240c0b10d612780', 'projectPage': 'https://tba-llm.github.io'}, 'publishedAt': '2025-03-24T13:51:39.000Z', 'title': 'Trajectory Balance with Asynchrony: Decoupling Exploration and Learning\\n  for Fast, Scalable LLM Post-Training', 'summary': 'Reinforcement learning (RL) is a critical component of large language model\\n(LLM) post-training. However, existing on-policy algorithms used for\\npost-training are inherently incompatible with the use of experience replay\\nbuffers, which can be populated scalably by distributed off-policy actors to\\nenhance exploration as compute increases. We propose efficiently obtaining this\\nbenefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a\\nmassively scalable LLM RL system. In contrast to existing approaches, TBA uses\\na larger fraction of compute on search, constantly generating off-policy data\\nfor a central replay buffer. A training node simultaneously samples data from\\nthis buffer based on reward or recency to update the policy using Trajectory\\nBalance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA\\noffers three key advantages: (1) decoupled training and search, speeding up\\ntraining wall-clock time by 4x or more; (2) improved diversity through\\nlarge-scale off-policy sampling; and (3) scalable search for sparse reward\\nsettings. On mathematical reasoning, preference-tuning, and automated\\nred-teaming (diverse and representative post-training tasks), TBA produces\\nspeed and performance improvements over strong baselines.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6478f30ea68454566353ef95/1sIa7cKmzWjFwUSershwX.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18929.png', 'numComments': 1, 'submittedBy': {'_id': '6478f30ea68454566353ef95', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6478f30ea68454566353ef95/hX5NiVPyQbK8TIBKnT4J1.jpeg', 'fullname': 'Johan Samir Obando Ceron', 'name': 'johanobandoc', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.15893', 'authors': [{'_id': '67e55e19c7f04e09b025a261', 'user': {'_id': '64060b49a577649430bf6974', 'avatarUrl': '/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg', 'isPro': False, 'fullname': 'Jiawei Wang', 'user': 'Jarvis1111', 'type': 'user'}, 'name': 'Jiawei Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-27T14:25:07.694Z', 'hidden': False}, {'_id': '67e55e19c7f04e09b025a262', 'name': 'Kai Hu', 'hidden': False}, {'_id': '67e55e19c7f04e09b025a263', 'name': 'Qiang Huo', 'hidden': False}], 'publishedAt': '2025-03-20T06:44:47.000Z', 'submittedOnDailyAt': '2025-03-27T12:57:36.450Z', 'title': 'UniHDSA: A Unified Relation Prediction Approach for Hierarchical\\n  Document Structure Analysis', 'submittedOnDailyBy': {'_id': '64060b49a577649430bf6974', 'avatarUrl': '/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg', 'isPro': False, 'fullname': 'Jiawei Wang', 'user': 'Jarvis1111', 'type': 'user'}, 'summary': \"Document structure analysis, aka document layout analysis, is crucial for\\nunderstanding both the physical layout and logical structure of documents,\\nserving information retrieval, document summarization, knowledge extraction,\\netc. Hierarchical Document Structure Analysis (HDSA) specifically aims to\\nrestore the hierarchical structure of documents created using authoring\\nsoftware with hierarchical schemas. Previous research has primarily followed\\ntwo approaches: one focuses on tackling specific subtasks of HDSA in isolation,\\nsuch as table detection or reading order prediction, while the other adopts a\\nunified framework that uses multiple branches or modules, each designed to\\naddress a distinct task. In this work, we propose a unified relation prediction\\napproach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as\\nrelation prediction problems and consolidates relation prediction labels into a\\nunified label space. This allows a single relation prediction module to handle\\nmultiple tasks simultaneously, whether at a page-level or document-level\\nstructure analysis. To validate the effectiveness of UniHDSA, we develop a\\nmultimodal end-to-end system based on Transformer architectures. Extensive\\nexperimental results demonstrate that our approach achieves state-of-the-art\\nperformance on a hierarchical document structure analysis benchmark,\\nComp-HRDoc, and competitive results on a large-scale document layout analysis\\ndataset, DocLayNet, effectively illustrating the superiority of our method\\nacross all sub-tasks. The Comp-HRDoc benchmark and UniHDSA's configurations are\\npublicly available at https://github.com/microsoft/CompHRDoc.\", 'upvotes': 1, 'discussionId': '67e55e1bc7f04e09b025a32b', 'githubRepo': 'https://github.com/microsoft/CompHRDoc'}, 'publishedAt': '2025-03-20T02:44:47.000Z', 'title': 'UniHDSA: A Unified Relation Prediction Approach for Hierarchical\\n  Document Structure Analysis', 'summary': \"Document structure analysis, aka document layout analysis, is crucial for\\nunderstanding both the physical layout and logical structure of documents,\\nserving information retrieval, document summarization, knowledge extraction,\\netc. Hierarchical Document Structure Analysis (HDSA) specifically aims to\\nrestore the hierarchical structure of documents created using authoring\\nsoftware with hierarchical schemas. Previous research has primarily followed\\ntwo approaches: one focuses on tackling specific subtasks of HDSA in isolation,\\nsuch as table detection or reading order prediction, while the other adopts a\\nunified framework that uses multiple branches or modules, each designed to\\naddress a distinct task. In this work, we propose a unified relation prediction\\napproach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as\\nrelation prediction problems and consolidates relation prediction labels into a\\nunified label space. This allows a single relation prediction module to handle\\nmultiple tasks simultaneously, whether at a page-level or document-level\\nstructure analysis. To validate the effectiveness of UniHDSA, we develop a\\nmultimodal end-to-end system based on Transformer architectures. Extensive\\nexperimental results demonstrate that our approach achieves state-of-the-art\\nperformance on a hierarchical document structure analysis benchmark,\\nComp-HRDoc, and competitive results on a large-scale document layout analysis\\ndataset, DocLayNet, effectively illustrating the superiority of our method\\nacross all sub-tasks. The Comp-HRDoc benchmark and UniHDSA's configurations are\\npublicly available at https://github.com/microsoft/CompHRDoc.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15893.png', 'numComments': 1, 'submittedBy': {'_id': '64060b49a577649430bf6974', 'avatarUrl': '/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg', 'fullname': 'Jiawei Wang', 'name': 'Jarvis1111', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.10997', 'authors': [{'_id': '67e5528c3a023bdcabe85a47', 'user': {'_id': '64b94d61d2176850e0d2cef2', 'avatarUrl': '/avatars/b76903cc64b61d652b12b31d8832697d.svg', 'isPro': False, 'fullname': 'Aashish Anantha Ramakrishnan', 'user': 'aashananth', 'type': 'user'}, 'name': 'Aashish Anantha Ramakrishnan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-27T14:25:14.389Z', 'hidden': False}, {'_id': '67e5528c3a023bdcabe85a48', 'user': {'_id': '65198e4a5ae78fd448f17f68', 'avatarUrl': '/avatars/8ff77732afa9e0ab05ade2be45739130.svg', 'isPro': False, 'fullname': 'Aadarsh', 'user': 'aadarsh-ram', 'type': 'user'}, 'name': 'Aadarsh Anantha Ramakrishnan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-27T14:25:16.660Z', 'hidden': False}, {'_id': '67e5528c3a023bdcabe85a49', 'name': 'Dongwon Lee', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65198e4a5ae78fd448f17f68/JaVZyQzlm4E4IXbEOqECh.jpeg'], 'publishedAt': '2025-03-14T01:45:38.000Z', 'submittedOnDailyAt': '2025-03-27T13:08:16.954Z', 'title': 'RONA: Pragmatically Diverse Image Captioning with Coherence Relations', 'submittedOnDailyBy': {'_id': '65198e4a5ae78fd448f17f68', 'avatarUrl': '/avatars/8ff77732afa9e0ab05ade2be45739130.svg', 'isPro': False, 'fullname': 'Aadarsh', 'user': 'aadarsh-ram', 'type': 'user'}, 'summary': 'Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally\\ngenerate diverse image captions by employing syntactic and semantic variations\\nto describe image components. However, human-written captions prioritize\\nconveying a central message alongside visual descriptions using pragmatic cues.\\nTo enhance pragmatic diversity, it is essential to explore alternative ways of\\ncommunicating these messages in conjunction with visual content. To address\\nthis challenge, we propose RONA, a novel prompting strategy for Multi-modal\\nLarge Language Models (MLLM) that leverages Coherence Relations as an axis for\\nvariation. We demonstrate that RONA generates captions with better overall\\ndiversity and ground-truth alignment, compared to MLLM baselines across\\nmultiple domains. Our code is available at: https://github.com/aashish2000/RONA', 'upvotes': 1, 'discussionId': '67e5528d3a023bdcabe85aaa', 'githubRepo': 'https://github.com/aashish2000/RONA', 'ai_keywords': ['Multi-modal Large Language Models (MLLM)', 'Coherence Relations', 'prompting strategy']}, 'publishedAt': '2025-03-13T21:45:38.000Z', 'title': 'RONA: Pragmatically Diverse Image Captioning with Coherence Relations', 'summary': 'Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally\\ngenerate diverse image captions by employing syntactic and semantic variations\\nto describe image components. However, human-written captions prioritize\\nconveying a central message alongside visual descriptions using pragmatic cues.\\nTo enhance pragmatic diversity, it is essential to explore alternative ways of\\ncommunicating these messages in conjunction with visual content. To address\\nthis challenge, we propose RONA, a novel prompting strategy for Multi-modal\\nLarge Language Models (MLLM) that leverages Coherence Relations as an axis for\\nvariation. We demonstrate that RONA generates captions with better overall\\ndiversity and ground-truth alignment, compared to MLLM baselines across\\nmultiple domains. Our code is available at: https://github.com/aashish2000/RONA', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65198e4a5ae78fd448f17f68/JaVZyQzlm4E4IXbEOqECh.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10997.png', 'numComments': 1, 'submittedBy': {'_id': '65198e4a5ae78fd448f17f68', 'avatarUrl': '/avatars/8ff77732afa9e0ab05ade2be45739130.svg', 'fullname': 'Aadarsh', 'name': 'aadarsh-ram', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.20641', 'authors': [{'_id': '67e5608e345a214efb382ad5', 'name': 'Han Wu', 'hidden': False}, {'_id': '67e5608e345a214efb382ad6', 'name': 'Yuxuan Yao', 'hidden': False}, {'_id': '67e5608e345a214efb382ad7', 'name': 'Shuqi Liu', 'hidden': False}, {'_id': '67e5608e345a214efb382ad8', 'name': 'Zehua Liu', 'hidden': False}, {'_id': '67e5608e345a214efb382ad9', 'name': 'Xiaojin Fu', 'hidden': False}, {'_id': '67e5608e345a214efb382ada', 'name': 'Xiongwei Han', 'hidden': False}, {'_id': '67e5608e345a214efb382adb', 'name': 'Xing Li', 'hidden': False}, {'_id': '67e5608e345a214efb382adc', 'name': 'Hui-Ling Zhen', 'hidden': False}, {'_id': '67e5608e345a214efb382add', 'name': 'Tao Zhong', 'hidden': False}, {'_id': '67e5608e345a214efb382ade', 'name': 'Mingxuan Yuan', 'hidden': False}], 'publishedAt': '2025-03-26T15:34:37.000Z', 'submittedOnDailyAt': '2025-03-27T13:02:54.043Z', 'title': 'Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging', 'submittedOnDailyBy': {'_id': '654c47faa40678750adb5c0a', 'avatarUrl': '/avatars/37ae3d6c866d31e387e772685e9c7fa3.svg', 'isPro': False, 'fullname': 'Han Wu', 'user': 'hahahawu', 'type': 'user'}, 'summary': \"The transition from System 1 to System 2 reasoning in large language models\\n(LLMs) has marked significant advancements in handling complex tasks through\\ndeliberate, iterative thinking. However, this progress often comes at the cost\\nof efficiency, as models tend to overthink, generating redundant reasoning\\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\\nreasoning has emerged as a promising solution to this challenge, aiming to\\nbalance reasoning depth with practical efficiency. While existing approaches,\\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\\nengineering, have shown potential, they are either computationally expensive or\\nunstable. Model merging, on the other hand, offers a cost-effective and robust\\nalternative by integrating the quick-thinking capabilities of System 1 models\\nwith the methodical reasoning of System 2 models. In this work, we present a\\ncomprehensive empirical study on model merging for L2S reasoning, exploring\\ndiverse methodologies, including task-vector-based, SVD-based, and\\nactivation-informed merging. Our experiments reveal that model merging can\\nreduce average response length by up to 55% while preserving or even improving\\nbaseline performance. We also identify a strong correlation between model scale\\nand merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.\\nFurthermore, we investigate the merged model's ability to self-critique and\\nself-correct, as well as its adaptive response length based on task complexity.\\nOur findings highlight model merging as a highly efficient and effective\\nparadigm for L2S reasoning, offering a practical solution to the overthinking\\nproblem while maintaining the robustness of System 2 reasoning. This work can\\nbe found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.\", 'upvotes': 0, 'discussionId': '67e5608f345a214efb382b66'}, 'publishedAt': '2025-03-26T11:34:37.000Z', 'title': 'Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging', 'summary': \"The transition from System 1 to System 2 reasoning in large language models\\n(LLMs) has marked significant advancements in handling complex tasks through\\ndeliberate, iterative thinking. However, this progress often comes at the cost\\nof efficiency, as models tend to overthink, generating redundant reasoning\\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\\nreasoning has emerged as a promising solution to this challenge, aiming to\\nbalance reasoning depth with practical efficiency. While existing approaches,\\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\\nengineering, have shown potential, they are either computationally expensive or\\nunstable. Model merging, on the other hand, offers a cost-effective and robust\\nalternative by integrating the quick-thinking capabilities of System 1 models\\nwith the methodical reasoning of System 2 models. In this work, we present a\\ncomprehensive empirical study on model merging for L2S reasoning, exploring\\ndiverse methodologies, including task-vector-based, SVD-based, and\\nactivation-informed merging. Our experiments reveal that model merging can\\nreduce average response length by up to 55% while preserving or even improving\\nbaseline performance. We also identify a strong correlation between model scale\\nand merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.\\nFurthermore, we investigate the merged model's ability to self-critique and\\nself-correct, as well as its adaptive response length based on task complexity.\\nOur findings highlight model merging as a highly efficient and effective\\nparadigm for L2S reasoning, offering a practical solution to the overthinking\\nproblem while maintaining the robustness of System 2 reasoning. This work can\\nbe found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20641.png', 'numComments': 1, 'submittedBy': {'_id': '654c47faa40678750adb5c0a', 'avatarUrl': '/avatars/37ae3d6c866d31e387e772685e9c7fa3.svg', 'fullname': 'Han Wu', 'name': 'hahahawu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.17970', 'authors': [{'_id': '67e52a53c17e104ac467acad', 'name': 'Yang Luo', 'hidden': False}, {'_id': '67e52a53c17e104ac467acae', 'name': 'Shiru Wang', 'hidden': False}, {'_id': '67e52a53c17e104ac467acaf', 'name': 'Jun Liu', 'hidden': False}, {'_id': '67e52a53c17e104ac467acb0', 'name': 'Jiaxuan Xiao', 'hidden': False}, {'_id': '67e52a53c17e104ac467acb1', 'name': 'Rundong Xue', 'hidden': False}, {'_id': '67e52a53c17e104ac467acb2', 'user': {'_id': '64ec877bb93654d4ca5c92e9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg', 'isPro': False, 'fullname': 'Zeyu Zhang', 'user': 'SteveZeyuZhang', 'type': 'user'}, 'name': 'Zeyu Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-27T14:25:30.015Z', 'hidden': False}, {'_id': '67e52a53c17e104ac467acb3', 'name': 'Hao Zhang', 'hidden': False}, {'_id': '67e52a53c17e104ac467acb4', 'name': 'Yu Lu', 'hidden': False}, {'_id': '67e52a53c17e104ac467acb5', 'name': 'Yang Zhao', 'hidden': False}, {'_id': '67e52a53c17e104ac467acb6', 'name': 'Yutong Xie', 'hidden': False}], 'publishedAt': '2025-03-23T07:37:24.000Z', 'submittedOnDailyAt': '2025-03-27T09:08:11.199Z', 'title': 'PathoHR: Breast Cancer Survival Prediction on High-Resolution\\n  Pathological Images', 'submittedOnDailyBy': {'_id': '64ec877bb93654d4ca5c92e9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg', 'isPro': False, 'fullname': 'Zeyu Zhang', 'user': 'SteveZeyuZhang', 'type': 'user'}, 'summary': \"Breast cancer survival prediction in computational pathology presents a\\nremarkable challenge due to tumor heterogeneity. For instance, different\\nregions of the same tumor in the pathology image can show distinct\\nmorphological and molecular characteristics. This makes it difficult to extract\\nrepresentative features from whole slide images (WSIs) that truly reflect the\\ntumor's aggressive potential and likely survival outcomes. In this paper, we\\npresent PathoHR, a novel pipeline for accurate breast cancer survival\\nprediction that enhances any size of pathological images to enable more\\neffective feature learning. Our approach entails (1) the incorporation of a\\nplug-and-play high-resolution Vision Transformer (ViT) to enhance patch-wise\\nWSI representation, enabling more detailed and comprehensive feature\\nextraction, (2) the systematic evaluation of multiple advanced similarity\\nmetrics for comparing WSI-extracted features, optimizing the representation\\nlearning process to better capture tumor characteristics, (3) the demonstration\\nthat smaller image patches enhanced follow the proposed pipeline can achieve\\nequivalent or superior prediction accuracy compared to raw larger patches,\\nwhile significantly reducing computational overhead. Experimental findings\\nvalid that PathoHR provides the potential way of integrating enhanced image\\nresolution with optimized feature learning to advance computational pathology,\\noffering a promising direction for more accurate and efficient breast cancer\\nsurvival prediction. Code will be available at\\nhttps://github.com/AIGeeksGroup/PathoHR.\", 'upvotes': 0, 'discussionId': '67e52a54c17e104ac467ad3b', 'githubRepo': 'https://github.com/AIGeeksGroup/PathoHR', 'ai_keywords': ['Vision Transformer (ViT)', 'high-resolution enhancement', 'patch-wise WSI representation', 'similarity metrics', 'feature learning', 'tumor heterogeneity', 'whole slide images (WSIs)']}, 'publishedAt': '2025-03-23T03:37:24.000Z', 'title': 'PathoHR: Breast Cancer Survival Prediction on High-Resolution\\n  Pathological Images', 'summary': \"Breast cancer survival prediction in computational pathology presents a\\nremarkable challenge due to tumor heterogeneity. For instance, different\\nregions of the same tumor in the pathology image can show distinct\\nmorphological and molecular characteristics. This makes it difficult to extract\\nrepresentative features from whole slide images (WSIs) that truly reflect the\\ntumor's aggressive potential and likely survival outcomes. In this paper, we\\npresent PathoHR, a novel pipeline for accurate breast cancer survival\\nprediction that enhances any size of pathological images to enable more\\neffective feature learning. Our approach entails (1) the incorporation of a\\nplug-and-play high-resolution Vision Transformer (ViT) to enhance patch-wise\\nWSI representation, enabling more detailed and comprehensive feature\\nextraction, (2) the systematic evaluation of multiple advanced similarity\\nmetrics for comparing WSI-extracted features, optimizing the representation\\nlearning process to better capture tumor characteristics, (3) the demonstration\\nthat smaller image patches enhanced follow the proposed pipeline can achieve\\nequivalent or superior prediction accuracy compared to raw larger patches,\\nwhile significantly reducing computational overhead. Experimental findings\\nvalid that PathoHR provides the potential way of integrating enhanced image\\nresolution with optimized feature learning to advance computational pathology,\\noffering a promising direction for more accurate and efficient breast cancer\\nsurvival prediction. Code will be available at\\nhttps://github.com/AIGeeksGroup/PathoHR.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17970.png', 'numComments': 1, 'submittedBy': {'_id': '64ec877bb93654d4ca5c92e9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg', 'fullname': 'Zeyu Zhang', 'name': 'SteveZeyuZhang', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}"
]