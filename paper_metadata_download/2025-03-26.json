[
    "{'paper': {'id': '2503.19325', 'authors': [{'_id': '67e35f6fc9d8214b5e1c64c3', 'name': 'Yuchao Gu', 'hidden': False}, {'_id': '67e35f6fc9d8214b5e1c64c4', 'name': 'Weijia Mao', 'hidden': False}, {'_id': '67e35f6fc9d8214b5e1c64c5', 'name': 'Mike Zheng Shou', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63021630a35b21bd8a53305a/SL0MQs7OvQpNlGBhroTW3.png'], 'publishedAt': '2025-03-25T03:38:06.000Z', 'submittedOnDailyAt': '2025-03-26T00:37:14.940Z', 'title': 'Long-Context Autoregressive Video Modeling with Next-Frame Prediction', 'submittedOnDailyBy': {'_id': '63021630a35b21bd8a53305a', 'avatarUrl': '/avatars/7a7e8b39749eda61e57d8a1908726558.svg', 'isPro': True, 'fullname': 'Gu Yuchao', 'user': 'guyuchao', 'type': 'user'}, 'summary': 'Long-context autoregressive modeling has significantly advanced language\\ngeneration, but video generation still struggles to fully utilize extended\\ntemporal contexts. To investigate long-context video modeling, we introduce\\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\\nmodeling. Just as language models learn causal dependencies between tokens\\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\\nframes, achieving better convergence than Token AR and video diffusion\\ntransformers. Building on FAR, we observe that long-context vision modeling\\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\\ntemporal decay for remote context and fails to extrapolate well to long video\\nsequences. Additionally, training on long videos is computationally expensive,\\nas vision tokens grow much faster than language tokens. To tackle these issues,\\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\\nan test-time technique that adds flexible temporal decay to RoPE, enabling\\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\\nshort-term context modeling, where a high-resolution short-term context window\\nensures fine-grained temporal consistency, while an unlimited long-term context\\nwindow encodes long-range information using fewer tokens. With this approach,\\nwe can train on long video sequences with a manageable token context length. We\\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\\nlong-video generation, providing a simple yet effective baseline for video\\nautoregressive modeling.', 'upvotes': 56, 'discussionId': '67e35f72c9d8214b5e1c659b', 'ai_keywords': ['Frame AutoRegressive (FAR)', 'Token AR', 'video autoregressive modeling', 'visual redundancy', 'RoPE (Rotary Position Embedding)', 'temporal decay', 'FlexRoPE', 'long short-term context modeling', 'high-resolution short-term context window', 'long-term context window', 'state-of-the-art performance', 'video generation']}, 'publishedAt': '2025-03-24T23:38:06.000Z', 'title': 'Long-Context Autoregressive Video Modeling with Next-Frame Prediction', 'summary': 'Long-context autoregressive modeling has significantly advanced language\\ngeneration, but video generation still struggles to fully utilize extended\\ntemporal contexts. To investigate long-context video modeling, we introduce\\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\\nmodeling. Just as language models learn causal dependencies between tokens\\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\\nframes, achieving better convergence than Token AR and video diffusion\\ntransformers. Building on FAR, we observe that long-context vision modeling\\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\\ntemporal decay for remote context and fails to extrapolate well to long video\\nsequences. Additionally, training on long videos is computationally expensive,\\nas vision tokens grow much faster than language tokens. To tackle these issues,\\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\\nan test-time technique that adds flexible temporal decay to RoPE, enabling\\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\\nshort-term context modeling, where a high-resolution short-term context window\\nensures fine-grained temporal consistency, while an unlimited long-term context\\nwindow encodes long-range information using fewer tokens. With this approach,\\nwe can train on long video sequences with a manageable token context length. We\\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\\nlong-video generation, providing a simple yet effective baseline for video\\nautoregressive modeling.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63021630a35b21bd8a53305a/SL0MQs7OvQpNlGBhroTW3.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19325.png', 'numComments': 1, 'submittedBy': {'_id': '63021630a35b21bd8a53305a', 'avatarUrl': '/avatars/7a7e8b39749eda61e57d8a1908726558.svg', 'fullname': 'Gu Yuchao', 'name': 'guyuchao', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.18931', 'authors': [{'_id': '67e25c4d1908043170bd551d', 'user': {'_id': '64651db3611ae99d14d392ea', 'avatarUrl': '/avatars/b818dc0dddc999758ab5737d5053e8c3.svg', 'isPro': False, 'fullname': 'cyt', 'user': 'Row11n', 'type': 'user'}, 'name': 'Yitong Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-25T08:18:45.692Z', 'hidden': False}, {'_id': '67e25c4d1908043170bd551e', 'name': 'Lingchen Meng', 'hidden': False}, {'_id': '67e25c4d1908043170bd551f', 'name': 'Wujian Peng', 'hidden': False}, {'_id': '67e25c4d1908043170bd5520', 'name': 'Zuxuan Wu', 'hidden': False}, {'_id': '67e25c4d1908043170bd5521', 'name': 'Yu-Gang Jiang', 'hidden': False}], 'publishedAt': '2025-03-24T17:52:47.000Z', 'submittedOnDailyAt': '2025-03-26T01:10:42.553Z', 'title': 'CoMP: Continual Multimodal Pre-training for Vision Foundation Models', 'submittedOnDailyBy': {'_id': '64651db3611ae99d14d392ea', 'avatarUrl': '/avatars/b818dc0dddc999758ab5737d5053e8c3.svg', 'isPro': False, 'fullname': 'cyt', 'user': 'Row11n', 'type': 'user'}, 'summary': 'Pre-trained Vision Foundation Models (VFMs) provide strong visual\\nrepresentations for a wide range of applications. In this paper, we continually\\npre-train prevailing VFMs in a multimodal manner such that they can\\neffortlessly process visual inputs of varying sizes and produce visual\\nrepresentations that are more aligned with language representations, regardless\\nof their original pre-training process. To this end, we introduce CoMP, a\\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\\nRotary Position Embedding to support native resolution continual pre-training,\\nand an Alignment Loss between visual and textual features through language\\nprototypes to align multimodal representations. By three-stage training, our\\nVFMs achieve remarkable improvements not only in multimodal understanding but\\nalso in other downstream tasks such as classification and segmentation.\\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\\nmIoU on ADE20K under frozen chunk evaluation.', 'upvotes': 23, 'discussionId': '67e25c4f1908043170bd55a8', 'projectPage': 'https://slimm-x.github.io/comp/', 'githubRepo': 'https://github.com/SliMM-X/CoMP-MM', 'ai_keywords': ['Vision Foundation Models (VFMs)', 'Continual Rotary Position Embedding', 'Alignment Loss', 'language prototypes', 'multimodal pre-training pipeline', 'three-stage training', 'multimodal understanding', 'classification', 'segmentation', 'ChartQA', 'DocVQA', 'LLM', 'ImageNet-1K', 'ADE20K', 'frozen chunk evaluation']}, 'publishedAt': '2025-03-24T13:52:47.000Z', 'title': 'CoMP: Continual Multimodal Pre-training for Vision Foundation Models', 'summary': 'Pre-trained Vision Foundation Models (VFMs) provide strong visual\\nrepresentations for a wide range of applications. In this paper, we continually\\npre-train prevailing VFMs in a multimodal manner such that they can\\neffortlessly process visual inputs of varying sizes and produce visual\\nrepresentations that are more aligned with language representations, regardless\\nof their original pre-training process. To this end, we introduce CoMP, a\\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\\nRotary Position Embedding to support native resolution continual pre-training,\\nand an Alignment Loss between visual and textual features through language\\nprototypes to align multimodal representations. By three-stage training, our\\nVFMs achieve remarkable improvements not only in multimodal understanding but\\nalso in other downstream tasks such as classification and segmentation.\\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\\nmIoU on ADE20K under frozen chunk evaluation.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18931.png', 'numComments': 1, 'submittedBy': {'_id': '64651db3611ae99d14d392ea', 'avatarUrl': '/avatars/b818dc0dddc999758ab5737d5053e8c3.svg', 'fullname': 'cyt', 'name': 'Row11n', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.19385', 'authors': [{'_id': '67e36241d8da46951f858026', 'name': 'Jaihoon Kim', 'hidden': False}, {'_id': '67e36241d8da46951f858027', 'name': 'Taehoon Yoon', 'hidden': False}, {'_id': '67e36241d8da46951f858028', 'name': 'Jisung Hwang', 'hidden': False}, {'_id': '67e36241d8da46951f858029', 'name': 'Minhyuk Sung', 'hidden': False}], 'publishedAt': '2025-03-25T06:30:45.000Z', 'submittedOnDailyAt': '2025-03-26T00:49:38.583Z', 'title': 'Inference-Time Scaling for Flow Models via Stochastic Generation and\\n  Rollover Budget Forcing', 'submittedOnDailyBy': {'_id': '6342796a0875f2c99cfd313b', 'avatarUrl': '/avatars/98575092404c4197b20c929a6499a015.svg', 'isPro': False, 'fullname': 'Yuseung \"Phillip\" Lee', 'user': 'phillipinseoul', 'type': 'user'}, 'summary': 'We propose an inference-time scaling approach for pretrained flow models.\\nRecently, inference-time scaling has gained significant attention in LLMs and\\ndiffusion models, improving sample quality or better aligning outputs with user\\npreferences by leveraging additional computation. For diffusion models,\\nparticle sampling has allowed more efficient scaling due to the stochasticity\\nat intermediate denoising steps. On the contrary, while flow models have gained\\npopularity as an alternative to diffusion models--offering faster generation\\nand high-quality outputs in state-of-the-art image and video generative\\nmodels--efficient inference-time scaling methods used for diffusion models\\ncannot be directly applied due to their deterministic generative process. To\\nenable efficient inference-time scaling for flow models, we propose three key\\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\\nInterpolant conversion, broadening the search space and enhancing sample\\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\\ncomputational resources across timesteps to maximize budget utilization. Our\\nexperiments show that SDE-based generation, particularly variance-preserving\\n(VP) interpolant-based generation, improves the performance of particle\\nsampling methods for inference-time scaling in flow models. Additionally, we\\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\\nall previous inference-time scaling approaches.', 'upvotes': 21, 'discussionId': '67e36245d8da46951f85802c', 'ai_keywords': ['flow models', 'inference-time scaling', 'LLMs', 'diffusion models', 'sample quality', 'user preferences', 'particle sampling', 'stochasticity', 'denoising steps', 'generative process', 'SDE-based generation', 'interpolant conversion', 'sample diversity', 'Rollover Budget Forcing (RBF)', 'adaptive allocation', 'computational resources', 'timesteps', 'budget utilization', 'variance-preserving (VP)', 'VP interpolant-based generation']}, 'publishedAt': '2025-03-25T02:30:45.000Z', 'title': 'Inference-Time Scaling for Flow Models via Stochastic Generation and\\n  Rollover Budget Forcing', 'summary': 'We propose an inference-time scaling approach for pretrained flow models.\\nRecently, inference-time scaling has gained significant attention in LLMs and\\ndiffusion models, improving sample quality or better aligning outputs with user\\npreferences by leveraging additional computation. For diffusion models,\\nparticle sampling has allowed more efficient scaling due to the stochasticity\\nat intermediate denoising steps. On the contrary, while flow models have gained\\npopularity as an alternative to diffusion models--offering faster generation\\nand high-quality outputs in state-of-the-art image and video generative\\nmodels--efficient inference-time scaling methods used for diffusion models\\ncannot be directly applied due to their deterministic generative process. To\\nenable efficient inference-time scaling for flow models, we propose three key\\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\\nInterpolant conversion, broadening the search space and enhancing sample\\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\\ncomputational resources across timesteps to maximize budget utilization. Our\\nexperiments show that SDE-based generation, particularly variance-preserving\\n(VP) interpolant-based generation, improves the performance of particle\\nsampling methods for inference-time scaling in flow models. Additionally, we\\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\\nall previous inference-time scaling approaches.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19385.png', 'numComments': 3, 'submittedBy': {'_id': '6342796a0875f2c99cfd313b', 'avatarUrl': '/avatars/98575092404c4197b20c929a6499a015.svg', 'fullname': 'Yuseung \"Phillip\" Lee', 'name': 'phillipinseoul', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19622', 'authors': [{'_id': '67e3706bc9d8214b5e219149', 'name': 'Hongcheng Gao', 'hidden': False}, {'_id': '67e3706bc9d8214b5e21914a', 'name': 'Jiashu Qu', 'hidden': False}, {'_id': '67e3706bc9d8214b5e21914b', 'name': 'Jingyi Tang', 'hidden': False}, {'_id': '67e3706bc9d8214b5e21914c', 'name': 'Baolong Bi', 'hidden': False}, {'_id': '67e3706bc9d8214b5e21914d', 'name': 'Yue Liu', 'hidden': False}, {'_id': '67e3706bc9d8214b5e21914e', 'name': 'Hongyu Chen', 'hidden': False}, {'_id': '67e3706bc9d8214b5e21914f', 'name': 'Li Liang', 'hidden': False}, {'_id': '67e3706bc9d8214b5e219150', 'name': 'Li Su', 'hidden': False}, {'_id': '67e3706bc9d8214b5e219151', 'name': 'Qingming Huang', 'hidden': False}], 'publishedAt': '2025-03-25T13:12:17.000Z', 'submittedOnDailyAt': '2025-03-26T01:44:03.080Z', 'title': 'Exploring Hallucination of Large Multimodal Models in Video\\n  Understanding: Benchmark, Analysis and Mitigation', 'submittedOnDailyBy': {'_id': '62728f4f6253fe2068da1021', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg', 'isPro': False, 'fullname': 'Hongcheng Gao', 'user': 'HongchengGao', 'type': 'user'}, 'summary': 'The hallucination of large multimodal models (LMMs), providing responses that\\nappear correct but are actually incorrect, limits their reliability and\\napplicability. This paper aims to study the hallucination problem of LMMs in\\nvideo modality, which is dynamic and more challenging compared to static\\nmodalities like images and text. From this motivation, we first present a\\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\\nvideo understanding tasks. It is built upon three dimensions, i.e.,\\nhallucination causes, hallucination aspects, and question formats, resulting in\\n6K questions. Then, we quantitatively study 7 influential factors on\\nhallucinations, e.g., duration time of videos, model sizes, and model\\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\\nenhances reasoning capabilities while TDPO reduces hallucinations in the\\nthinking process. Extensive experiments and analyses demonstrate the\\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\\nare public at https://github.com/Hongcheng-Gao/HAVEN.', 'upvotes': 20, 'discussionId': '67e3706dc9d8214b5e2191e0', 'githubRepo': 'https://github.com/Hongcheng-Gao/HAVEN', 'ai_keywords': ['multimodal models (LMMs)', 'hallucination', 'video modality', 'video understanding', 'HAVEN', 'hallucination causes', 'hallucination aspects', 'question formats', 'duration time', 'model sizes', 'model reasoning', 'supervised reasoning fine-tuning (SRFT)', 'direct preference optimization (TDPO)', 'video-thinking model', 'accuracy', 'bias score']}, 'publishedAt': '2025-03-25T09:12:17.000Z', 'title': 'Exploring Hallucination of Large Multimodal Models in Video\\n  Understanding: Benchmark, Analysis and Mitigation', 'summary': 'The hallucination of large multimodal models (LMMs), providing responses that\\nappear correct but are actually incorrect, limits their reliability and\\napplicability. This paper aims to study the hallucination problem of LMMs in\\nvideo modality, which is dynamic and more challenging compared to static\\nmodalities like images and text. From this motivation, we first present a\\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\\nvideo understanding tasks. It is built upon three dimensions, i.e.,\\nhallucination causes, hallucination aspects, and question formats, resulting in\\n6K questions. Then, we quantitatively study 7 influential factors on\\nhallucinations, e.g., duration time of videos, model sizes, and model\\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\\nenhances reasoning capabilities while TDPO reduces hallucinations in the\\nthinking process. Extensive experiments and analyses demonstrate the\\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\\nare public at https://github.com/Hongcheng-Gao/HAVEN.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19622.png', 'numComments': 3, 'submittedBy': {'_id': '62728f4f6253fe2068da1021', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg', 'fullname': 'Hongcheng Gao', 'name': 'HongchengGao', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19903', 'authors': [{'_id': '67e375d3cc93cc8c42da7699', 'name': 'Baifeng Shi', 'hidden': False}, {'_id': '67e375d3cc93cc8c42da769a', 'name': 'Boyi Li', 'hidden': False}, {'_id': '67e375d3cc93cc8c42da769b', 'name': 'Han Cai', 'hidden': False}, {'_id': '67e375d3cc93cc8c42da769c', 'name': 'Yao Lu', 'hidden': False}, {'_id': '67e375d3cc93cc8c42da769d', 'name': 'Sifei Liu', 'hidden': False}, {'_id': '67e375d3cc93cc8c42da769e', 'name': 'Marco Pavone', 'hidden': False}, {'_id': '67e375d3cc93cc8c42da769f', 'name': 'Jan Kautz', 'hidden': False}, {'_id': '67e375d3cc93cc8c42da76a0', 'name': 'Song Han', 'hidden': False}, {'_id': '67e375d3cc93cc8c42da76a1', 'name': 'Trevor Darrell', 'hidden': False}, {'_id': '67e375d3cc93cc8c42da76a2', 'name': 'Pavlo Molchanov', 'hidden': False}, {'_id': '67e375d3cc93cc8c42da76a3', 'name': 'Hongxu Yin', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/649004218f7cbbc94c782db6/F0o61glyGm9fib9Pl1i-L.mp4'], 'publishedAt': '2025-03-25T17:58:37.000Z', 'submittedOnDailyAt': '2025-03-26T02:13:20.800Z', 'title': 'Scaling Vision Pre-Training to 4K Resolution', 'submittedOnDailyBy': {'_id': '649004218f7cbbc94c782db6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg', 'isPro': False, 'fullname': 'Baifeng Shi', 'user': 'bfshi', 'type': 'user'}, 'summary': 'High-resolution perception of visual details is crucial for daily tasks.\\nCurrent vision pre-training, however, is still limited to low resolutions\\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\\nwith a near-constant cost. Instead of contrastive learning on global image\\nrepresentation, PS3 is pre-trained by selectively processing local regions and\\ncontrasting them with local detailed captions, enabling high-resolution\\nrepresentation learning with greatly reduced computational overhead. The\\npre-trained PS3 is able to both encode the global image at low resolution and\\nselectively process local high-resolution regions based on their saliency or\\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\\nresulting model, named VILA-HD, significantly improves high-resolution visual\\nperception compared to baselines without high-resolution vision pre-training\\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\\nappealing scaling properties of VILA-HD, including scaling up resolution for\\nfree and scaling up test-time compute for better performance. Compared to state\\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\\nacross multiple benchmarks and achieves better efficiency than latest token\\npruning approaches. Finally, we find current benchmarks do not require\\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\\nspeedup over Qwen2-VL.', 'upvotes': 14, 'discussionId': '67e375d9cc93cc8c42da785f', 'projectPage': 'https://nvlabs.github.io/PS3/', 'githubRepo': 'https://github.com/NVlabs/PS3', 'ai_keywords': ['PS3', 'CLIP-style vision pre-training', 'contrastive learning', 'local regions', 'local detailed captions', 'high-resolution representation learning', 'computational overhead', 'saliency', 'text prompt', 'VILA-HD', 'multi-modal LLM', 'high-resolution visual perception', 'AnyRes', 'S^2', 'scaling properties', 'test-time compute', 'NVILA', 'Qwen2-VL', 'benchmarks', 'token pruning approaches', '4KPer', 'image QA', 'GPT-4o']}, 'publishedAt': '2025-03-25T13:58:37.000Z', 'title': 'Scaling Vision Pre-Training to 4K Resolution', 'summary': 'High-resolution perception of visual details is crucial for daily tasks.\\nCurrent vision pre-training, however, is still limited to low resolutions\\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\\nwith a near-constant cost. Instead of contrastive learning on global image\\nrepresentation, PS3 is pre-trained by selectively processing local regions and\\ncontrasting them with local detailed captions, enabling high-resolution\\nrepresentation learning with greatly reduced computational overhead. The\\npre-trained PS3 is able to both encode the global image at low resolution and\\nselectively process local high-resolution regions based on their saliency or\\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\\nresulting model, named VILA-HD, significantly improves high-resolution visual\\nperception compared to baselines without high-resolution vision pre-training\\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\\nappealing scaling properties of VILA-HD, including scaling up resolution for\\nfree and scaling up test-time compute for better performance. Compared to state\\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\\nacross multiple benchmarks and achieves better efficiency than latest token\\npruning approaches. Finally, we find current benchmarks do not require\\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\\nspeedup over Qwen2-VL.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/649004218f7cbbc94c782db6/F0o61glyGm9fib9Pl1i-L.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19903.png', 'numComments': 1, 'submittedBy': {'_id': '649004218f7cbbc94c782db6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg', 'fullname': 'Baifeng Shi', 'name': 'bfshi', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.14905', 'authors': [{'_id': '67e250450487eeecfd9a5880', 'name': 'Siwei Wen', 'hidden': False}, {'_id': '67e250450487eeecfd9a5881', 'name': 'Junyan Ye', 'hidden': False}, {'_id': '67e250450487eeecfd9a5882', 'name': 'Peilin Feng', 'hidden': False}, {'_id': '67e250450487eeecfd9a5883', 'name': 'Hengrui Kang', 'hidden': False}, {'_id': '67e250450487eeecfd9a5884', 'name': 'Zichen Wen', 'hidden': False}, {'_id': '67e250450487eeecfd9a5885', 'name': 'Yize Chen', 'hidden': False}, {'_id': '67e250450487eeecfd9a5886', 'name': 'Jiang Wu', 'hidden': False}, {'_id': '67e250450487eeecfd9a5887', 'name': 'Wenjun Wu', 'hidden': False}, {'_id': '67e250450487eeecfd9a5888', 'name': 'Conghui He', 'hidden': False}, {'_id': '67e250450487eeecfd9a5889', 'name': 'Weijia Li', 'hidden': False}], 'publishedAt': '2025-03-19T05:14:44.000Z', 'submittedOnDailyAt': '2025-03-26T04:00:13.753Z', 'title': 'Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\\n  with Artifact Explanation', 'submittedOnDailyBy': {'_id': '653b8c3e97a4d71d950e2f20', 'avatarUrl': '/avatars/b68880022e14556d0be58c69615db3be.svg', 'isPro': False, 'fullname': 'Zichen Wen', 'user': 'zichenwen', 'type': 'user'}, 'summary': 'With the rapid advancement of Artificial Intelligence Generated Content\\n(AIGC) technologies, synthetic images have become increasingly prevalent in\\neveryday life, posing new challenges for authenticity assessment and detection.\\nDespite the effectiveness of existing methods in evaluating image authenticity\\nand locating forgeries, these approaches often lack human interpretability and\\ndo not fully address the growing complexity of synthetic data. To tackle these\\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\\nexcels in distinguishing real from fake images but also provides clear, natural\\nlanguage explanations for image artifacts, enhancing interpretability.\\nAdditionally, we present FakeClue, a comprehensive dataset containing over\\n100,000 images across seven categories, annotated with fine-grained artifact\\nclues in natural language. FakeVLM demonstrates performance comparable to\\nexpert models while eliminating the need for additional classifiers, making it\\na robust solution for synthetic data detection. Extensive evaluations across\\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\\nclassification and artifact explanation tasks, setting a new benchmark for\\nsynthetic image detection. The dataset and code will be released in:\\nhttps://github.com/opendatalab/FakeVLM.', 'upvotes': 14, 'discussionId': '67e250490487eeecfd9a599e', 'githubRepo': 'https://github.com/opendatalab/FakeVLM', 'ai_keywords': ['large multimodal model', 'FakeVLM', 'DeepFake detection', 'image artifacts', 'natural language explanations', 'FakeClue', 'fine-grained artifact clues']}, 'publishedAt': '2025-03-19T01:14:44.000Z', 'title': 'Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\\n  with Artifact Explanation', 'summary': 'With the rapid advancement of Artificial Intelligence Generated Content\\n(AIGC) technologies, synthetic images have become increasingly prevalent in\\neveryday life, posing new challenges for authenticity assessment and detection.\\nDespite the effectiveness of existing methods in evaluating image authenticity\\nand locating forgeries, these approaches often lack human interpretability and\\ndo not fully address the growing complexity of synthetic data. To tackle these\\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\\nexcels in distinguishing real from fake images but also provides clear, natural\\nlanguage explanations for image artifacts, enhancing interpretability.\\nAdditionally, we present FakeClue, a comprehensive dataset containing over\\n100,000 images across seven categories, annotated with fine-grained artifact\\nclues in natural language. FakeVLM demonstrates performance comparable to\\nexpert models while eliminating the need for additional classifiers, making it\\na robust solution for synthetic data detection. Extensive evaluations across\\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\\nclassification and artifact explanation tasks, setting a new benchmark for\\nsynthetic image detection. The dataset and code will be released in:\\nhttps://github.com/opendatalab/FakeVLM.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14905.png', 'numComments': 2, 'submittedBy': {'_id': '653b8c3e97a4d71d950e2f20', 'avatarUrl': '/avatars/b68880022e14556d0be58c69615db3be.svg', 'fullname': 'Zichen Wen', 'name': 'zichenwen', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19855', 'authors': [{'_id': '67e36792a281c900d76a93c8', 'name': 'Xiaoyu Tian', 'hidden': False}, {'_id': '67e36792a281c900d76a93c9', 'name': 'Sitong Zhao', 'hidden': False}, {'_id': '67e36792a281c900d76a93ca', 'name': 'Haotian Wang', 'hidden': False}, {'_id': '67e36792a281c900d76a93cb', 'name': 'Shuaiting Chen', 'hidden': False}, {'_id': '67e36792a281c900d76a93cc', 'name': 'Yunjie Ji', 'hidden': False}, {'_id': '67e36792a281c900d76a93cd', 'name': 'Yiping Peng', 'hidden': False}, {'_id': '67e36792a281c900d76a93ce', 'name': 'Han Zhao', 'hidden': False}, {'_id': '67e36792a281c900d76a93cf', 'name': 'Xiangang Li', 'hidden': False}], 'publishedAt': '2025-03-25T17:19:38.000Z', 'submittedOnDailyAt': '2025-03-26T01:04:39.479Z', 'title': 'Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\\n  Thinking', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': \"Recent advances in large language models (LLMs), such as OpenAI-o1 and\\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\\nextended reasoning processes substantially enhance model performance. Despite\\nthis, current models are constrained by limitations in handling long texts and\\nreinforcement learning (RL) training efficiency. To address these issues, we\\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\\nThis method iteratively refines model reasoning by leveraging previous answers\\nas prompts for subsequent rounds. Extensive experiments across multiple models,\\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\\nThinking is a broadly applicable, straightforward approach to achieving stable\\nenhancements in model performance, underscoring its potential for future\\ndevelopments in test-time scaling techniques. The key prompt: {Original\\nquestion prompt} The assistant's previous answer is: <answer> {last round\\nanswer} </answer>, and please re-answer.\", 'upvotes': 8, 'discussionId': '67e36793a281c900d76a9459', 'ai_keywords': ['large language models', 'OpenAI-o1', 'DeepSeek-R1', 'test-time scaling', 'extended reasoning processes', 'reinforcement learning', 'Multi-round Thinking', 'iterative refinement', 'AIME 2024', 'MATH-500', 'GPQA-diamond', 'LiveCodeBench', 'accuracy', 'stable enhancements', 'test-time scaling techniques']}, 'publishedAt': '2025-03-25T13:19:38.000Z', 'title': 'Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\\n  Thinking', 'summary': \"Recent advances in large language models (LLMs), such as OpenAI-o1 and\\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\\nextended reasoning processes substantially enhance model performance. Despite\\nthis, current models are constrained by limitations in handling long texts and\\nreinforcement learning (RL) training efficiency. To address these issues, we\\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\\nThis method iteratively refines model reasoning by leveraging previous answers\\nas prompts for subsequent rounds. Extensive experiments across multiple models,\\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\\nThinking is a broadly applicable, straightforward approach to achieving stable\\nenhancements in model performance, underscoring its potential for future\\ndevelopments in test-time scaling techniques. The key prompt: {Original\\nquestion prompt} The assistant's previous answer is: <answer> {last round\\nanswer} </answer>, and please re-answer.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19855.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': True, 'isMod': False, 'followerCount': 6473}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.13964', 'authors': [{'_id': '67e20852c0c932395394dbb0', 'name': 'Siwei Han', 'hidden': False}, {'_id': '67e20852c0c932395394dbb1', 'name': 'Peng Xia', 'hidden': False}, {'_id': '67e20852c0c932395394dbb2', 'name': 'Ruiyi Zhang', 'hidden': False}, {'_id': '67e20852c0c932395394dbb3', 'name': 'Tong Sun', 'hidden': False}, {'_id': '67e20852c0c932395394dbb4', 'name': 'Yun Li', 'hidden': False}, {'_id': '67e20852c0c932395394dbb5', 'name': 'Hongtu Zhu', 'hidden': False}, {'_id': '67e20852c0c932395394dbb6', 'name': 'Huaxiu Yao', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/smmwVmcnReTUxH6xDnyU1.png'], 'publishedAt': '2025-03-18T06:57:21.000Z', 'submittedOnDailyAt': '2025-03-26T03:52:37.520Z', 'title': 'MDocAgent: A Multi-Modal Multi-Agent Framework for Document\\n  Understanding', 'submittedOnDailyBy': {'_id': '643e9ee6f6bb3c31a26e7bc4', 'avatarUrl': '/avatars/acfaa7d6a23dada24c86b954c3be116a.svg', 'isPro': False, 'fullname': 'Peng Xia', 'user': 'richardxp888', 'type': 'user'}, 'summary': \"Document Question Answering (DocQA) is a very common task. Existing methods\\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\\nRetrieval Augmented Generation (RAG) often prioritize information from a single\\nmodal, failing to effectively integrate textual and visual cues. These\\napproaches struggle with complex multi-modal reasoning, limiting their\\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\\nframework that leverages both text and image. Our system employs five\\nspecialized agents: a general agent, a critical agent, a text agent, an image\\nagent and a summarizing agent. These agents engage in multi-modal context\\nretrieval, combining their individual insights to achieve a more comprehensive\\nunderstanding of the document's content. This collaborative approach enables\\nthe system to synthesize information from both textual and visual components,\\nleading to improved accuracy in question answering. Preliminary experiments on\\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\\nour MDocAgent, achieve an average improvement of 12.1% compared to current\\nstate-of-the-art method. This work contributes to the development of more\\nrobust and comprehensive DocQA systems capable of handling the complexities of\\nreal-world documents containing rich textual and visual information. Our data\\nand code are available at https://github.com/aiming-lab/MDocAgent.\", 'upvotes': 8, 'discussionId': '67e20858c0c932395394dde6', 'ai_keywords': ['Large Language Models (LLMs)', 'Large Vision Language Models (LVLMs)', 'Retrieval Augmented Generation (RAG)', 'multi-modal reasoning', 'multi-modal multi-agent framework', 'general agent', 'critical agent', 'text agent', 'image agent', 'summarizing agent', 'multi-modal context retrieval']}, 'publishedAt': '2025-03-18T02:57:21.000Z', 'title': 'MDocAgent: A Multi-Modal Multi-Agent Framework for Document\\n  Understanding', 'summary': \"Document Question Answering (DocQA) is a very common task. Existing methods\\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\\nRetrieval Augmented Generation (RAG) often prioritize information from a single\\nmodal, failing to effectively integrate textual and visual cues. These\\napproaches struggle with complex multi-modal reasoning, limiting their\\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\\nframework that leverages both text and image. Our system employs five\\nspecialized agents: a general agent, a critical agent, a text agent, an image\\nagent and a summarizing agent. These agents engage in multi-modal context\\nretrieval, combining their individual insights to achieve a more comprehensive\\nunderstanding of the document's content. This collaborative approach enables\\nthe system to synthesize information from both textual and visual components,\\nleading to improved accuracy in question answering. Preliminary experiments on\\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\\nour MDocAgent, achieve an average improvement of 12.1% compared to current\\nstate-of-the-art method. This work contributes to the development of more\\nrobust and comprehensive DocQA systems capable of handling the complexities of\\nreal-world documents containing rich textual and visual information. Our data\\nand code are available at https://github.com/aiming-lab/MDocAgent.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/smmwVmcnReTUxH6xDnyU1.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13964.png', 'numComments': 1, 'submittedBy': {'_id': '643e9ee6f6bb3c31a26e7bc4', 'avatarUrl': '/avatars/acfaa7d6a23dada24c86b954c3be116a.svg', 'fullname': 'Peng Xia', 'name': 'richardxp888', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19910', 'authors': [{'_id': '67e35e4cff080b9ee71e3295', 'name': 'Chuong Huynh', 'hidden': False}, {'_id': '67e35e4cff080b9ee71e3296', 'name': 'Jinyu Yang', 'hidden': False}, {'_id': '67e35e4cff080b9ee71e3297', 'name': 'Ashish Tawari', 'hidden': False}, {'_id': '67e35e4cff080b9ee71e3298', 'name': 'Mubarak Shah', 'hidden': False}, {'_id': '67e35e4cff080b9ee71e3299', 'name': 'Son Tran', 'hidden': False}, {'_id': '67e35e4cff080b9ee71e329a', 'name': 'Raffay Hamid', 'hidden': False}, {'_id': '67e35e4cff080b9ee71e329b', 'name': 'Trishul Chilimbi', 'hidden': False}, {'_id': '67e35e4cff080b9ee71e329c', 'name': 'Abhinav Shrivastava', 'hidden': False}], 'publishedAt': '2025-03-25T17:59:50.000Z', 'submittedOnDailyAt': '2025-03-26T00:26:00.764Z', 'title': 'CoLLM: A Large Language Model for Composed Image Retrieval', 'submittedOnDailyBy': {'_id': '63a4d196cde2b28f82a56bd9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png', 'isPro': False, 'fullname': 'Chuong Huynh', 'user': 'chuonghm', 'type': 'user'}, 'summary': 'Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\\nbased on a multimodal query. Typical training data consists of triplets\\ncontaining a reference image, a textual description of desired modifications,\\nand the target image, which are expensive and time-consuming to acquire. The\\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\\nweb-crawled image-caption pairs. However, these methods have significant\\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\\nand unnatural modification text, while image-caption pairs hinder joint\\nembedding learning of the multimodal query due to the absence of triplet data.\\nMoreover, existing approaches struggle with complex and nuanced modification\\ntexts that demand sophisticated fusion and understanding of vision and language\\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\\nthese limitations. Our approach generates triplets on-the-fly from\\nimage-caption pairs, enabling supervised training without manual annotation. We\\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\\nimages and modification texts, facilitating deeper multimodal fusion.\\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\\nand settings. MTCIR yields competitive results, with up to 15% performance\\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\\nfor CIR models, contributing to the advancement of this important field.', 'upvotes': 7, 'discussionId': '67e35e4eff080b9ee71e3353', 'projectPage': 'https://collm-cvpr25.github.io/', 'ai_keywords': ['Composed Image Retrieval (CIR)', 'multimodal query', 'triplets', 'reference image', 'textual description', 'target image', 'zero-shot approaches', 'synthetic triplets', 'vision-language models (VLMs)', 'web-crawled image-caption pairs', 'joint embedding learning', 'complex and nuanced modification texts', 'multimodal fusion', 'CoLLM', 'Large Language Models (LLMs)', 'Multi-Text CIR (MTCIR)', 'CIRR benchmark', 'Fashion-IQ benchmark', 'state-of-the-art performance']}, 'publishedAt': '2025-03-25T13:59:50.000Z', 'title': 'CoLLM: A Large Language Model for Composed Image Retrieval', 'summary': 'Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\\nbased on a multimodal query. Typical training data consists of triplets\\ncontaining a reference image, a textual description of desired modifications,\\nand the target image, which are expensive and time-consuming to acquire. The\\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\\nweb-crawled image-caption pairs. However, these methods have significant\\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\\nand unnatural modification text, while image-caption pairs hinder joint\\nembedding learning of the multimodal query due to the absence of triplet data.\\nMoreover, existing approaches struggle with complex and nuanced modification\\ntexts that demand sophisticated fusion and understanding of vision and language\\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\\nthese limitations. Our approach generates triplets on-the-fly from\\nimage-caption pairs, enabling supervised training without manual annotation. We\\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\\nimages and modification texts, facilitating deeper multimodal fusion.\\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\\nand settings. MTCIR yields competitive results, with up to 15% performance\\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\\nfor CIR models, contributing to the advancement of this important field.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19910.png', 'numComments': 1, 'submittedBy': {'_id': '63a4d196cde2b28f82a56bd9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png', 'fullname': 'Chuong Huynh', 'name': 'chuonghm', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19470', 'authors': [{'_id': '67e365b0dcfc2aeae1bf3da2', 'name': 'Mingyang Chen', 'hidden': False}, {'_id': '67e365b0dcfc2aeae1bf3da3', 'name': 'Tianpeng Li', 'hidden': False}, {'_id': '67e365b0dcfc2aeae1bf3da4', 'name': 'Haoze Sun', 'hidden': False}, {'_id': '67e365b0dcfc2aeae1bf3da5', 'name': 'Yijie Zhou', 'hidden': False}, {'_id': '67e365b0dcfc2aeae1bf3da6', 'name': 'Chenzheng Zhu', 'hidden': False}, {'_id': '67e365b0dcfc2aeae1bf3da7', 'name': 'Fan Yang', 'hidden': False}, {'_id': '67e365b0dcfc2aeae1bf3da8', 'name': 'Zenan Zhou', 'hidden': False}, {'_id': '67e365b0dcfc2aeae1bf3da9', 'name': 'Weipeng Chen', 'hidden': False}, {'_id': '67e365b0dcfc2aeae1bf3daa', 'name': 'Haofen Wang', 'hidden': False}, {'_id': '67e365b0dcfc2aeae1bf3dab', 'name': 'Jeff Z. Pan', 'hidden': False}, {'_id': '67e365b0dcfc2aeae1bf3dac', 'name': 'Wen Zhang', 'hidden': False}, {'_id': '67e365b0dcfc2aeae1bf3dad', 'name': 'Huajun Chen', 'hidden': False}], 'publishedAt': '2025-03-25T09:00:58.000Z', 'submittedOnDailyAt': '2025-03-26T00:56:07.098Z', 'title': 'ReSearch: Learning to Reason with Search for LLMs via Reinforcement\\n  Learning', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': 'Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\\nreasoning with external search processes remains challenging, especially for\\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\\nReSearch, a novel framework that trains LLMs to Reason with Search via\\nreinforcement learning without using any supervised data on reasoning steps.\\nOur approach treats search operations as integral components of the reasoning\\nchain, where when and how to perform searches is guided by text-based thinking,\\nand search results subsequently influence further reasoning. We train ReSearch\\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\\nextensive experiments. Despite being trained on only one dataset, our models\\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\\nthat ReSearch naturally elicits advanced reasoning capabilities such as\\nreflection and self-correction during the reinforcement learning process.', 'upvotes': 5, 'discussionId': '67e365b1dcfc2aeae1bf3df6', 'ai_keywords': ['Large Language Models (LLMs)', 'OpenAI-o1', 'DeepSeek-R1', 'complex multi-hop questions', 'ReSearch', 'reinforcement learning', 'text-based thinking', 'reflection', 'self-correction', 'Qwen2.5-7B(-Instruct)', 'Qwen2.5-32B(-Instruct)']}, 'publishedAt': '2025-03-25T05:00:58.000Z', 'title': 'ReSearch: Learning to Reason with Search for LLMs via Reinforcement\\n  Learning', 'summary': 'Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\\nreasoning with external search processes remains challenging, especially for\\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\\nReSearch, a novel framework that trains LLMs to Reason with Search via\\nreinforcement learning without using any supervised data on reasoning steps.\\nOur approach treats search operations as integral components of the reasoning\\nchain, where when and how to perform searches is guided by text-based thinking,\\nand search results subsequently influence further reasoning. We train ReSearch\\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\\nextensive experiments. Despite being trained on only one dataset, our models\\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\\nthat ReSearch naturally elicits advanced reasoning capabilities such as\\nreflection and self-correction during the reinforcement learning process.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19470.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': True, 'isMod': False, 'followerCount': 6473}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.18446', 'authors': [{'_id': '67e367ee4363e3c4bbbaca3a', 'name': 'Jinho Jeong', 'hidden': False}, {'_id': '67e367ee4363e3c4bbbaca3b', 'name': 'Sangmin Han', 'hidden': False}, {'_id': '67e367ee4363e3c4bbbaca3c', 'name': 'Jinwoo Kim', 'hidden': False}, {'_id': '67e367ee4363e3c4bbbaca3d', 'name': 'Seon Joo Kim', 'hidden': False}], 'publishedAt': '2025-03-24T08:50:15.000Z', 'submittedOnDailyAt': '2025-03-26T01:07:15.007Z', 'title': 'Latent Space Super-Resolution for Higher-Resolution Image Generation\\n  with Diffusion Models', 'submittedOnDailyBy': {'_id': '66b5f733f0c16f37f307f35e', 'avatarUrl': '/avatars/29a97e10b4d65aa23d7eae238f809499.svg', 'isPro': False, 'fullname': 'JinHo Jeong', 'user': '3587jjh', 'type': 'user'}, 'summary': 'In this paper, we propose LSRNA, a novel framework for higher-resolution\\n(exceeding 1K) image generation using diffusion models by leveraging\\nsuper-resolution directly in the latent space. Existing diffusion models\\nstruggle with scaling beyond their training resolutions, often leading to\\nstructural distortions or content repetition. Reference-based methods address\\nthe issues by upsampling a low-resolution reference to guide higher-resolution\\ngeneration. However, they face significant challenges: upsampling in latent\\nspace often causes manifold deviation, which degrades output quality. On the\\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\\nhigh-frequency details. Our extensive experiments demonstrate that integrating\\nLSRNA outperforms state-of-the-art reference-based methods across various\\nresolutions and metrics, while showing the critical role of latent space\\nupsampling in preserving detail and sharpness. The code is available at\\nhttps://github.com/3587jjh/LSRNA.', 'upvotes': 5, 'discussionId': '67e367f14363e3c4bbbacae1', 'ai_keywords': ['LSRNA', 'diffusion models', 'latent space', 'super-resolution', 'structural distortions', 'content repetition', 'reference-based methods', 'manifold deviation', 'RGB space', 'manifold alignment', 'Region-wise Noise Addition (RNA)', 'high-frequency details']}, 'publishedAt': '2025-03-24T04:50:15.000Z', 'title': 'Latent Space Super-Resolution for Higher-Resolution Image Generation\\n  with Diffusion Models', 'summary': 'In this paper, we propose LSRNA, a novel framework for higher-resolution\\n(exceeding 1K) image generation using diffusion models by leveraging\\nsuper-resolution directly in the latent space. Existing diffusion models\\nstruggle with scaling beyond their training resolutions, often leading to\\nstructural distortions or content repetition. Reference-based methods address\\nthe issues by upsampling a low-resolution reference to guide higher-resolution\\ngeneration. However, they face significant challenges: upsampling in latent\\nspace often causes manifold deviation, which degrades output quality. On the\\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\\nhigh-frequency details. Our extensive experiments demonstrate that integrating\\nLSRNA outperforms state-of-the-art reference-based methods across various\\nresolutions and metrics, while showing the critical role of latent space\\nupsampling in preserving detail and sharpness. The code is available at\\nhttps://github.com/3587jjh/LSRNA.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18446.png', 'numComments': 0, 'submittedBy': {'_id': '66b5f733f0c16f37f307f35e', 'avatarUrl': '/avatars/29a97e10b4d65aa23d7eae238f809499.svg', 'fullname': 'JinHo Jeong', 'name': '3587jjh', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19041', 'authors': [{'_id': '67e35da0b1b97cc3392024b1', 'name': 'Kangwei Liu', 'hidden': False}, {'_id': '67e35da0b1b97cc3392024b2', 'name': 'Mengru Wang', 'hidden': False}, {'_id': '67e35da0b1b97cc3392024b3', 'name': 'Yujie Luo', 'hidden': False}, {'_id': '67e35da0b1b97cc3392024b4', 'name': 'Lin Yuan', 'hidden': False}, {'_id': '67e35da0b1b97cc3392024b5', 'name': 'Mengshu Sun', 'hidden': False}, {'_id': '67e35da0b1b97cc3392024b6', 'name': 'Ningyu Zhang', 'hidden': False}, {'_id': '67e35da0b1b97cc3392024b7', 'name': 'Lei Liang', 'hidden': False}, {'_id': '67e35da0b1b97cc3392024b8', 'name': 'Zhiqiang Zhang', 'hidden': False}, {'_id': '67e35da0b1b97cc3392024b9', 'name': 'Jun Zhou', 'hidden': False}, {'_id': '67e35da0b1b97cc3392024ba', 'name': 'Huajun Chen', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/f0fTHNDhXrS7zWpmuxVU-.png'], 'publishedAt': '2025-03-24T18:11:42.000Z', 'submittedOnDailyAt': '2025-03-26T00:22:20.466Z', 'title': 'LookAhead Tuning: Safer Language Models via Partial Answer Previews', 'submittedOnDailyBy': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'isPro': False, 'fullname': 'Ningyu Zhang', 'user': 'Ningyu', 'type': 'user'}, 'summary': \"Fine-tuning enables large language models (LLMs) to adapt to specific\\ndomains, but often undermines their previously established safety alignment. To\\nmitigate the degradation of model safety during fine-tuning, we introduce\\nLookAhead Tuning, which comprises two simple, low-resource, and effective\\ndata-driven methods that modify training data by previewing partial answer\\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\\nby minimizing perturbations to initial token distributions. Comprehensive\\nexperiments demonstrate that LookAhead Tuning effectively maintains model\\nsafety without sacrificing robust performance on downstream tasks. Our findings\\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\\neffective adaptation of LLMs. Code is released at\\nhttps://github.com/zjunlp/LookAheadTuning.\", 'upvotes': 3, 'discussionId': '67e35da1b1b97cc339202525', 'ai_keywords': ['LookAhead Tuning', 'safety alignment', 'data-driven methods', 'partial answer prefixes', 'token distributions', 'robust performance', 'downstream tasks']}, 'publishedAt': '2025-03-24T14:11:42.000Z', 'title': 'LookAhead Tuning: Safer Language Models via Partial Answer Previews', 'summary': \"Fine-tuning enables large language models (LLMs) to adapt to specific\\ndomains, but often undermines their previously established safety alignment. To\\nmitigate the degradation of model safety during fine-tuning, we introduce\\nLookAhead Tuning, which comprises two simple, low-resource, and effective\\ndata-driven methods that modify training data by previewing partial answer\\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\\nby minimizing perturbations to initial token distributions. Comprehensive\\nexperiments demonstrate that LookAhead Tuning effectively maintains model\\nsafety without sacrificing robust performance on downstream tasks. Our findings\\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\\neffective adaptation of LLMs. Code is released at\\nhttps://github.com/zjunlp/LookAheadTuning.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/f0fTHNDhXrS7zWpmuxVU-.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19041.png', 'numComments': 1, 'submittedBy': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'fullname': 'Ningyu Zhang', 'name': 'Ningyu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 20}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19123', 'authors': [{'_id': '67e3fcb76323be71222fce35', 'name': 'Haebin Shin', 'hidden': False}, {'_id': '67e3fcb76323be71222fce36', 'user': {'_id': '664bed934bea570e25a8dc8c', 'avatarUrl': '/avatars/4e5a69db9005759b8812a9b54faf14c1.svg', 'isPro': False, 'fullname': 'Lei Ji', 'user': 'jilei03111', 'type': 'user'}, 'name': 'Lei Ji', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-03-26T13:10:17.004Z', 'hidden': False}, {'_id': '67e3fcb76323be71222fce37', 'user': {'_id': '63fb6e281b4b1bd4e7ffc5be', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg', 'isPro': False, 'fullname': 'Xiao Liu', 'user': 'lx865712528', 'type': 'user'}, 'name': 'Xiao Liu', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-03-26T13:10:33.192Z', 'hidden': False}, {'_id': '67e3fcb76323be71222fce38', 'user': {'_id': '643f615aa16cd6d1f4c581de', 'avatarUrl': '/avatars/47753a3e82b44f81881600c52e1e8495.svg', 'isPro': False, 'fullname': 'Yeyun Gong', 'user': 'yegong', 'type': 'user'}, 'name': 'Yeyun Gong', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-03-26T13:10:17.004Z', 'hidden': False}], 'publishedAt': '2025-03-24T20:19:31.000Z', 'submittedOnDailyAt': '2025-03-26T11:40:56.071Z', 'title': 'Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided\\n  Language Modeling', 'submittedOnDailyBy': {'_id': '63fb6e281b4b1bd4e7ffc5be', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg', 'isPro': False, 'fullname': 'Xiao Liu', 'user': 'lx865712528', 'type': 'user'}, 'summary': 'Using large teacher models to guide the training of smaller student models\\nhas become the prevailing paradigm for efficient and effective learning.\\nHowever, vocabulary mismatches between teacher and student language models pose\\nsignificant challenges in language modeling, resulting in divergent token\\nsequences and output distributions. To overcome these limitations, we propose\\nVocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel\\napproach that bridges the gap caused by vocabulary mismatch through two key\\nmethods: (1) Token-level Lexical Alignment, which aligns token sequences across\\nmismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss\\nof teacher model to guide effective student training. We demonstrate its\\neffectiveness in language modeling with 1B student model using various 7B\\nteacher models with different vocabularies. Notably, with\\nQwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary\\nwith TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to\\nnaive continual pretraining. Furthermore, we demonstrate that VocAgnoLM\\nconsistently benefits from stronger teacher models, providing a robust solution\\nto vocabulary mismatches in language modeling.', 'upvotes': 2, 'discussionId': '67e3fcb96323be71222fce9e', 'ai_keywords': ['Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM)', 'Token-level Lexical Alignment', 'Teacher Guided Loss', 'language modeling', 'vocabulary mismatches', 'token sequences', 'output distributions', 'vocabulary', 'Qwen2.5-Math-Instruct', 'TinyLlama']}, 'publishedAt': '2025-03-24T16:19:31.000Z', 'title': 'Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided\\n  Language Modeling', 'summary': 'Using large teacher models to guide the training of smaller student models\\nhas become the prevailing paradigm for efficient and effective learning.\\nHowever, vocabulary mismatches between teacher and student language models pose\\nsignificant challenges in language modeling, resulting in divergent token\\nsequences and output distributions. To overcome these limitations, we propose\\nVocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel\\napproach that bridges the gap caused by vocabulary mismatch through two key\\nmethods: (1) Token-level Lexical Alignment, which aligns token sequences across\\nmismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss\\nof teacher model to guide effective student training. We demonstrate its\\neffectiveness in language modeling with 1B student model using various 7B\\nteacher models with different vocabularies. Notably, with\\nQwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary\\nwith TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to\\nnaive continual pretraining. Furthermore, we demonstrate that VocAgnoLM\\nconsistently benefits from stronger teacher models, providing a robust solution\\nto vocabulary mismatches in language modeling.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19123.png', 'numComments': 1, 'submittedBy': {'_id': '63fb6e281b4b1bd4e7ffc5be', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg', 'fullname': 'Xiao Liu', 'name': 'lx865712528', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 10}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.18783', 'authors': [{'_id': '67e2a43d5116df47da357eec', 'user': {'_id': '642438eaa3adbc7142c3ca0f', 'avatarUrl': '/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg', 'isPro': False, 'fullname': 'CharlesChen', 'user': 'CharlesChen2023', 'type': 'user'}, 'name': 'Linwei Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-25T14:36:22.430Z', 'hidden': False}, {'_id': '67e2a43d5116df47da357eed', 'name': 'Lin Gu', 'hidden': False}, {'_id': '67e2a43d5116df47da357eee', 'name': 'Liang Li', 'hidden': False}, {'_id': '67e2a43d5116df47da357eef', 'name': 'Chenggang Yan', 'hidden': False}, {'_id': '67e2a43d5116df47da357ef0', 'name': 'Ying Fu', 'hidden': False}], 'publishedAt': '2025-03-24T15:32:06.000Z', 'submittedOnDailyAt': '2025-03-26T01:08:28.390Z', 'title': 'Frequency Dynamic Convolution for Dense Image Prediction', 'submittedOnDailyBy': {'_id': '642438eaa3adbc7142c3ca0f', 'avatarUrl': '/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg', 'isPro': False, 'fullname': 'CharlesChen', 'user': 'CharlesChen2023', 'type': 'user'}, 'summary': 'While Dynamic Convolution (DY-Conv) has shown promising performance by\\nenabling adaptive weight selection through multiple parallel weights combined\\nwith an attention mechanism, the frequency response of these weights tends to\\nexhibit high similarity, resulting in high parameter costs but limited\\nadaptability. In this work, we introduce Frequency Dynamic Convolution\\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\\nparameter budget in the Fourier domain. FDConv divides this budget into\\nfrequency-based groups with disjoint Fourier indices, enabling the construction\\nof frequency-diverse weights without increasing the parameter cost. To further\\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\\nfilter at the spatial level, while FBM decomposes weights into distinct\\nfrequency bands in the frequency domain and modulates them dynamically based on\\nlocal content. Extensive experiments on object detection, segmentation, and\\nclassification validate the effectiveness of FDConv. We demonstrate that when\\napplied to ResNet-50, FDConv achieves superior performance with a modest\\nincrease of +3.6M parameters, outperforming previous methods that require\\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\\nMoreover, FDConv seamlessly integrates into a variety of architectures,\\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\\nsolution for modern vision tasks. The code is made publicly available at\\nhttps://github.com/Linwei-Chen/FDConv.', 'upvotes': 2, 'discussionId': '67e2a4405116df47da357ff7', 'ai_keywords': ['Dynamic Convolution (DY-Conv)', 'Frequency Dynamic Convolution (FDConv)', 'attention mechanism', 'parameter budget', 'Fourier domain', 'frequency-based groups', 'disjoint Fourier indices', 'frequency-diverse weights', 'Kernel Spatial Modulation (KSM)', 'Frequency Band Modulation (FBM)', 'frequency response', 'spatial level', 'frequency bands', 'local content', 'object detection', 'segmentation', 'classification', 'ResNet-50', 'ConvNeXt', 'Swin-Transformer', 'parameter-efficient']}, 'publishedAt': '2025-03-24T11:32:06.000Z', 'title': 'Frequency Dynamic Convolution for Dense Image Prediction', 'summary': 'While Dynamic Convolution (DY-Conv) has shown promising performance by\\nenabling adaptive weight selection through multiple parallel weights combined\\nwith an attention mechanism, the frequency response of these weights tends to\\nexhibit high similarity, resulting in high parameter costs but limited\\nadaptability. In this work, we introduce Frequency Dynamic Convolution\\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\\nparameter budget in the Fourier domain. FDConv divides this budget into\\nfrequency-based groups with disjoint Fourier indices, enabling the construction\\nof frequency-diverse weights without increasing the parameter cost. To further\\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\\nfilter at the spatial level, while FBM decomposes weights into distinct\\nfrequency bands in the frequency domain and modulates them dynamically based on\\nlocal content. Extensive experiments on object detection, segmentation, and\\nclassification validate the effectiveness of FDConv. We demonstrate that when\\napplied to ResNet-50, FDConv achieves superior performance with a modest\\nincrease of +3.6M parameters, outperforming previous methods that require\\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\\nMoreover, FDConv seamlessly integrates into a variety of architectures,\\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\\nsolution for modern vision tasks. The code is made publicly available at\\nhttps://github.com/Linwei-Chen/FDConv.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18783.png', 'numComments': 1, 'submittedBy': {'_id': '642438eaa3adbc7142c3ca0f', 'avatarUrl': '/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg', 'fullname': 'CharlesChen', 'name': 'CharlesChen2023', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.17237', 'authors': [{'_id': '67e2b68e08c6a250edda264a', 'user': {'_id': '67e2063e1ee7f6db889849d6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg', 'isPro': False, 'fullname': 'Yu-Hsi Chen', 'user': 'wish44165', 'type': 'user'}, 'name': 'Yu-Hsi Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-03-25T14:35:46.455Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/67e2063e1ee7f6db889849d6/Dn_qgqu4a6nI8HIAdpDvW.mp4'], 'publishedAt': '2025-03-21T15:40:18.000Z', 'submittedOnDailyAt': '2025-03-26T04:35:14.607Z', 'title': 'Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID', 'submittedOnDailyBy': {'_id': '67e2063e1ee7f6db889849d6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg', 'isPro': False, 'fullname': 'Yu-Hsi Chen', 'user': 'wish44165', 'type': 'user'}, 'summary': 'Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\\ninfrared video is inherently challenging due to low contrast, environmental\\nnoise, and small target sizes. This paper provides a straightforward approach\\nto address multi-UAV tracking in thermal infrared video, leveraging recent\\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\\nour approach following the metrics from the 4th Anti-UAV Challenge and\\ndemonstrate competitive performance. Notably, we achieve strong results without\\nusing contrast enhancement or temporal information fusion to enrich UAV\\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\\ntracking task. We provide implementation details, in-depth experimental\\nanalysis, and a discussion of potential improvements. The code is available at\\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .', 'upvotes': 2, 'discussionId': '67e2b69108c6a250edda279f', 'githubRepo': 'https://github.com/wish44165/YOLOv12-BoT-SORT-ReID', 'ai_keywords': ['YOLOv12', 'BoT-SORT', 'multi-UAV tracking', 'thermal infrared video', 'detection', 'tracking', 'tailored training', 'inference strategies', '4th Anti-UAV Challenge', 'contrast enhancement', 'temporal information fusion', 'UAV features', 'Strong Baseline']}, 'publishedAt': '2025-03-21T11:40:18.000Z', 'title': 'Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID', 'summary': 'Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\\ninfrared video is inherently challenging due to low contrast, environmental\\nnoise, and small target sizes. This paper provides a straightforward approach\\nto address multi-UAV tracking in thermal infrared video, leveraging recent\\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\\nour approach following the metrics from the 4th Anti-UAV Challenge and\\ndemonstrate competitive performance. Notably, we achieve strong results without\\nusing contrast enhancement or temporal information fusion to enrich UAV\\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\\ntracking task. We provide implementation details, in-depth experimental\\nanalysis, and a discussion of potential improvements. The code is available at\\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/67e2063e1ee7f6db889849d6/Dn_qgqu4a6nI8HIAdpDvW.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17237.png', 'numComments': 2, 'submittedBy': {'_id': '67e2063e1ee7f6db889849d6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg', 'fullname': 'Yu-Hsi Chen', 'name': 'wish44165', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.16965', 'authors': [{'_id': '67e35c3bf049c252c672b824', 'name': 'Zhe Hu', 'hidden': False}, {'_id': '67e35c3bf049c252c672b825', 'name': 'Jing Li', 'hidden': False}, {'_id': '67e35c3bf049c252c672b826', 'name': 'Yu Yin', 'hidden': False}], 'publishedAt': '2025-03-21T09:25:23.000Z', 'submittedOnDailyAt': '2025-03-26T00:20:32.465Z', 'title': 'When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\\n  Training For Human-Centered Decision Making', 'submittedOnDailyBy': {'_id': '63999a6fe657365725d0d0a4', 'avatarUrl': '/avatars/99736de1bc0d5decf4a6eda86e3c7937.svg', 'isPro': False, 'fullname': 'Derek Zhe Hu', 'user': 'zhehuderek', 'type': 'user'}, 'summary': \"Embodied decision-making is fundamental for AI agents operating in real-world\\nenvironments. While Visual Language Models (VLMs) have advanced this\\ncapability, they still struggle with complex decisions, particularly in\\nhuman-centered situations that require deep reasoning about human needs and\\nvalues. In this study, we systematically evaluate open-sourced VLMs on\\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\\nonly textual descriptions unexpectedly outperform their VLM counterparts of\\nsimilar scale that process actual images, suggesting that visual alignment may\\nhinder VLM abilities. To address this challenge, we propose a novel text-only\\ntraining approach with synthesized textual data. This method strengthens VLMs'\\nlanguage components and transfers the learned abilities to multimodal\\ninference, eliminating the need for expensive image-text paired data.\\nFurthermore, we show that VLMs can achieve substantial performance gains\\nthrough self-improvement, using training data generated by their LLM\\ncounterparts rather than relying on larger teacher models like GPT-4. Our\\nfindings establish a more efficient and scalable approach to enhancing VLMs'\\nhuman-centered decision-making capabilities, opening new avenues for optimizing\\nVLMs through self-improvement mechanisms.\", 'upvotes': 2, 'discussionId': '67e35c3cf049c252c672b859', 'ai_keywords': ['Visual Language Models (VLMs)', 'multimodal human-centered decision-making tasks', 'Large Language Models (LLMs)', 'textual descriptions', 'visual alignment', 'text-only training approach', 'synthesized textual data', 'self-improvement', 'training data', 'GPT-4', 'human-centered decision-making capabilities']}, 'publishedAt': '2025-03-21T05:25:23.000Z', 'title': 'When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\\n  Training For Human-Centered Decision Making', 'summary': \"Embodied decision-making is fundamental for AI agents operating in real-world\\nenvironments. While Visual Language Models (VLMs) have advanced this\\ncapability, they still struggle with complex decisions, particularly in\\nhuman-centered situations that require deep reasoning about human needs and\\nvalues. In this study, we systematically evaluate open-sourced VLMs on\\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\\nonly textual descriptions unexpectedly outperform their VLM counterparts of\\nsimilar scale that process actual images, suggesting that visual alignment may\\nhinder VLM abilities. To address this challenge, we propose a novel text-only\\ntraining approach with synthesized textual data. This method strengthens VLMs'\\nlanguage components and transfers the learned abilities to multimodal\\ninference, eliminating the need for expensive image-text paired data.\\nFurthermore, we show that VLMs can achieve substantial performance gains\\nthrough self-improvement, using training data generated by their LLM\\ncounterparts rather than relying on larger teacher models like GPT-4. Our\\nfindings establish a more efficient and scalable approach to enhancing VLMs'\\nhuman-centered decision-making capabilities, opening new avenues for optimizing\\nVLMs through self-improvement mechanisms.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16965.png', 'numComments': 1, 'submittedBy': {'_id': '63999a6fe657365725d0d0a4', 'avatarUrl': '/avatars/99736de1bc0d5decf4a6eda86e3c7937.svg', 'fullname': 'Derek Zhe Hu', 'name': 'zhehuderek', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19907', 'authors': [{'_id': '67e419a8e3471e8d9fa0b303', 'name': 'Xuan Ju', 'hidden': False}, {'_id': '67e419a8e3471e8d9fa0b304', 'name': 'Weicai Ye', 'hidden': False}, {'_id': '67e419a8e3471e8d9fa0b305', 'name': 'Quande Liu', 'hidden': False}, {'_id': '67e419a8e3471e8d9fa0b306', 'name': 'Qiulin Wang', 'hidden': False}, {'_id': '67e419a8e3471e8d9fa0b307', 'name': 'Xintao Wang', 'hidden': False}, {'_id': '67e419a8e3471e8d9fa0b308', 'name': 'Pengfei Wan', 'hidden': False}, {'_id': '67e419a8e3471e8d9fa0b309', 'name': 'Di Zhang', 'hidden': False}, {'_id': '67e419a8e3471e8d9fa0b30a', 'name': 'Kun Gai', 'hidden': False}, {'_id': '67e419a8e3471e8d9fa0b30b', 'name': 'Qiang Xu', 'hidden': False}], 'publishedAt': '2025-03-25T17:59:06.000Z', 'submittedOnDailyAt': '2025-03-26T13:43:59.359Z', 'title': 'FullDiT: Multi-Task Video Generative Foundation Model with Full\\n  Attention', 'submittedOnDailyBy': {'_id': '63468720dd6d90d82ccf3450', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg', 'isPro': False, 'fullname': 'YSH', 'user': 'BestWishYsh', 'type': 'user'}, 'summary': 'Current video generative foundation models primarily focus on text-to-video\\ntasks, providing limited control for fine-grained video content creation.\\nAlthough adapter-based approaches (e.g., ControlNet) enable additional controls\\nwith minimal fine-tuning, they encounter challenges when integrating multiple\\nconditions, including: branch conflicts between independently trained adapters,\\nparameter redundancy leading to increased computational cost, and suboptimal\\nperformance compared to full fine-tuning. To address these challenges, we\\nintroduce FullDiT, a unified foundation model for video generation that\\nseamlessly integrates multiple conditions via unified full-attention\\nmechanisms. By fusing multi-task conditions into a unified sequence\\nrepresentation and leveraging the long-context learning ability of full\\nself-attention to capture condition dynamics, FullDiT reduces parameter\\noverhead, avoids conditions conflict, and shows scalability and emergent\\nability. We further introduce FullBench for multi-task video generation\\nevaluation. Experiments demonstrate that FullDiT achieves state-of-the-art\\nresults, highlighting the efficacy of full-attention in complex multi-task\\nvideo generation.', 'upvotes': 1, 'discussionId': '67e419aae3471e8d9fa0b3d4', 'projectPage': 'https://fulldit.github.io'}, 'publishedAt': '2025-03-25T13:59:06.000Z', 'title': 'FullDiT: Multi-Task Video Generative Foundation Model with Full\\n  Attention', 'summary': 'Current video generative foundation models primarily focus on text-to-video\\ntasks, providing limited control for fine-grained video content creation.\\nAlthough adapter-based approaches (e.g., ControlNet) enable additional controls\\nwith minimal fine-tuning, they encounter challenges when integrating multiple\\nconditions, including: branch conflicts between independently trained adapters,\\nparameter redundancy leading to increased computational cost, and suboptimal\\nperformance compared to full fine-tuning. To address these challenges, we\\nintroduce FullDiT, a unified foundation model for video generation that\\nseamlessly integrates multiple conditions via unified full-attention\\nmechanisms. By fusing multi-task conditions into a unified sequence\\nrepresentation and leveraging the long-context learning ability of full\\nself-attention to capture condition dynamics, FullDiT reduces parameter\\noverhead, avoids conditions conflict, and shows scalability and emergent\\nability. We further introduce FullBench for multi-task video generation\\nevaluation. Experiments demonstrate that FullDiT achieves state-of-the-art\\nresults, highlighting the efficacy of full-attention in complex multi-task\\nvideo generation.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19907.png', 'numComments': 1, 'submittedBy': {'_id': '63468720dd6d90d82ccf3450', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg', 'fullname': 'YSH', 'name': 'BestWishYsh', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 35}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19777', 'authors': [{'_id': '67e3d807304f166b6660db63', 'name': 'Vladan Stojni', 'hidden': False}, {'_id': '67e3d807304f166b6660db64', 'name': 'Yannis Kalantidis', 'hidden': False}, {'_id': '67e3d807304f166b6660db65', 'name': 'Ji Matas', 'hidden': False}, {'_id': '67e3d807304f166b6660db66', 'name': 'Giorgos Tolias', 'hidden': False}], 'publishedAt': '2025-03-25T15:47:13.000Z', 'submittedOnDailyAt': '2025-03-26T09:04:59.080Z', 'title': 'LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\\n  Semantic Segmentation', 'submittedOnDailyBy': {'_id': '66a3ae59f33ff23e1c027ccd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66a3ae59f33ff23e1c027ccd/tZzpESNPnmhty62xhHszF.jpeg', 'isPro': True, 'fullname': 'Vladan Stojnic', 'user': 'stojnvla', 'type': 'user'}, 'summary': 'We propose a training-free method for open-vocabulary semantic segmentation\\nusing Vision-and-Language Models (VLMs). Our approach enhances the initial\\nper-patch predictions of VLMs through label propagation, which jointly\\noptimizes predictions by incorporating patch-to-patch relationships. Since VLMs\\nare primarily optimized for cross-modal alignment and not for intra-modal\\nsimilarity, we use a Vision Model (VM) that is observed to better capture these\\nrelationships. We address resolution limitations inherent to patch-based\\nencoders by applying label propagation at the pixel level as a refinement step,\\nsignificantly improving segmentation accuracy near class boundaries. Our\\nmethod, called LPOSS+, performs inference over the entire image, avoiding\\nwindow-based processing and thereby capturing contextual interactions across\\nthe full image. LPOSS+ achieves state-of-the-art performance among\\ntraining-free methods, across a diverse set of datasets. Code:\\nhttps://github.com/vladan-stojnic/LPOSS', 'upvotes': 1, 'discussionId': '67e3d809304f166b6660dc45', 'githubRepo': 'https://github.com/vladan-stojnic/LPOSS', 'ai_keywords': ['Vision-and-Language Models (VLMs)', 'label propagation', 'per-patch predictions', 'patch-to-patch relationships', 'Vision Model (VM)', 'intra-modal similarity', 'patch-based encoders', 'pixel level', 'segmentation accuracy', 'class boundaries', 'contextual interactions', 'LPOSS+', 'inference over the entire image', 'window-based processing', 'state-of-the-art performance', 'datasets']}, 'publishedAt': '2025-03-25T11:47:13.000Z', 'title': 'LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\\n  Semantic Segmentation', 'summary': 'We propose a training-free method for open-vocabulary semantic segmentation\\nusing Vision-and-Language Models (VLMs). Our approach enhances the initial\\nper-patch predictions of VLMs through label propagation, which jointly\\noptimizes predictions by incorporating patch-to-patch relationships. Since VLMs\\nare primarily optimized for cross-modal alignment and not for intra-modal\\nsimilarity, we use a Vision Model (VM) that is observed to better capture these\\nrelationships. We address resolution limitations inherent to patch-based\\nencoders by applying label propagation at the pixel level as a refinement step,\\nsignificantly improving segmentation accuracy near class boundaries. Our\\nmethod, called LPOSS+, performs inference over the entire image, avoiding\\nwindow-based processing and thereby capturing contextual interactions across\\nthe full image. LPOSS+ achieves state-of-the-art performance among\\ntraining-free methods, across a diverse set of datasets. Code:\\nhttps://github.com/vladan-stojnic/LPOSS', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19777.png', 'numComments': 1, 'submittedBy': {'_id': '66a3ae59f33ff23e1c027ccd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66a3ae59f33ff23e1c027ccd/tZzpESNPnmhty62xhHszF.jpeg', 'fullname': 'Vladan Stojnic', 'name': 'stojnvla', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19356', 'authors': [{'_id': '67e376caa26895e9a90af7de', 'name': 'Reza Pourreza', 'hidden': False}, {'_id': '67e376caa26895e9a90af7df', 'name': 'Rishit Dagli', 'hidden': False}, {'_id': '67e376caa26895e9a90af7e0', 'name': 'Apratim Bhattacharyya', 'hidden': False}, {'_id': '67e376caa26895e9a90af7e1', 'name': 'Sunny Panchal', 'hidden': False}, {'_id': '67e376caa26895e9a90af7e2', 'name': 'Guillaume Berger', 'hidden': False}, {'_id': '67e376caa26895e9a90af7e3', 'name': 'Roland Memisevic', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/60796959c59d9e1697fa2324/XDGZe8OhURpl4fPftAwAb.jpeg'], 'publishedAt': '2025-03-25T05:13:12.000Z', 'submittedOnDailyAt': '2025-03-26T12:47:21.215Z', 'title': 'Can Vision-Language Models Answer Face to Face Questions in the\\n  Real-World?', 'submittedOnDailyBy': {'_id': '60796959c59d9e1697fa2324', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png', 'isPro': False, 'fullname': 'Rishit Dagli', 'user': 'rishitdagli', 'type': 'user'}, 'summary': 'AI models have made significant strides in recent years in their ability to\\ndescribe and answer questions about real-world images. They have also made\\nprogress in the ability to converse with users in real-time using audio input.\\nThis raises the question: have we reached the point where AI models, connected\\nto a camera and microphone, can converse with users in real-time about scenes\\nand events that are unfolding live in front of the camera? This has been a\\nlong-standing goal in AI and is a prerequisite for real-world AI assistants and\\nhumanoid robots to interact with humans in everyday situations. In this work,\\nwe introduce a new dataset and benchmark, the Qualcomm Interactive Video\\nDataset (IVD), which allows us to assess the extent to which existing models\\ncan support these abilities, and to what degree these capabilities can be\\ninstilled through fine-tuning. The dataset is based on a simple\\nquestion-answering setup, where users ask questions that the system has to\\nanswer, in real-time, based on the camera and audio input. We show that\\nexisting models fall far behind human performance on this task, and we identify\\nthe main sources for the performance gap. However, we also show that for many\\nof the required perceptual skills, fine-tuning on this form of data can\\nsignificantly reduce this gap.', 'upvotes': 1, 'discussionId': '67e376cea26895e9a90af923', 'ai_keywords': ['Qualcomm Interactive Video Dataset (IVD)', 'question-answering setup', 'real-time', 'camera and audio input', 'perceptual skills', 'fine-tuning']}, 'publishedAt': '2025-03-25T01:13:12.000Z', 'title': 'Can Vision-Language Models Answer Face to Face Questions in the\\n  Real-World?', 'summary': 'AI models have made significant strides in recent years in their ability to\\ndescribe and answer questions about real-world images. They have also made\\nprogress in the ability to converse with users in real-time using audio input.\\nThis raises the question: have we reached the point where AI models, connected\\nto a camera and microphone, can converse with users in real-time about scenes\\nand events that are unfolding live in front of the camera? This has been a\\nlong-standing goal in AI and is a prerequisite for real-world AI assistants and\\nhumanoid robots to interact with humans in everyday situations. In this work,\\nwe introduce a new dataset and benchmark, the Qualcomm Interactive Video\\nDataset (IVD), which allows us to assess the extent to which existing models\\ncan support these abilities, and to what degree these capabilities can be\\ninstilled through fine-tuning. The dataset is based on a simple\\nquestion-answering setup, where users ask questions that the system has to\\nanswer, in real-time, based on the camera and audio input. We show that\\nexisting models fall far behind human performance on this task, and we identify\\nthe main sources for the performance gap. However, we also show that for many\\nof the required perceptual skills, fine-tuning on this form of data can\\nsignificantly reduce this gap.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/60796959c59d9e1697fa2324/XDGZe8OhURpl4fPftAwAb.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19356.png', 'numComments': 1, 'submittedBy': {'_id': '60796959c59d9e1697fa2324', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png', 'fullname': 'Rishit Dagli', 'name': 'rishitdagli', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19207', 'authors': [{'_id': '67e3f27a0d1715115dcd5b28', 'name': 'Rong Wang', 'hidden': False}, {'_id': '67e3f27a0d1715115dcd5b29', 'name': 'Fabian Prada', 'hidden': False}, {'_id': '67e3f27a0d1715115dcd5b2a', 'name': 'Ziyan Wang', 'hidden': False}, {'_id': '67e3f27a0d1715115dcd5b2b', 'name': 'Zhongshi Jiang', 'hidden': False}, {'_id': '67e3f27a0d1715115dcd5b2c', 'name': 'Chengxiang Yin', 'hidden': False}, {'_id': '67e3f27a0d1715115dcd5b2d', 'name': 'Junxuan Li', 'hidden': False}, {'_id': '67e3f27a0d1715115dcd5b2e', 'name': 'Shunsuke Saito', 'hidden': False}, {'_id': '67e3f27a0d1715115dcd5b2f', 'name': 'Igor Santesteban', 'hidden': False}, {'_id': '67e3f27a0d1715115dcd5b30', 'name': 'Javier Romero', 'hidden': False}, {'_id': '67e3f27a0d1715115dcd5b31', 'name': 'Rohan Joshi', 'hidden': False}, {'_id': '67e3f27a0d1715115dcd5b32', 'name': 'Hongdong Li', 'hidden': False}, {'_id': '67e3f27a0d1715115dcd5b33', 'name': 'Jason Saragih', 'hidden': False}, {'_id': '67e3f27a0d1715115dcd5b34', 'name': 'Yaser Sheikh', 'hidden': False}], 'publishedAt': '2025-03-24T23:20:47.000Z', 'submittedOnDailyAt': '2025-03-26T10:57:51.434Z', 'title': 'FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from\\n  Few Images', 'submittedOnDailyBy': {'_id': '6493306970d925ae80523a53', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nRCxbVng_PPBqKd-Z3KVc.jpeg', 'isPro': False, 'fullname': 'Dmitry Ryumin', 'user': 'DmitryRyumin', 'type': 'user'}, 'summary': 'We present a novel method for reconstructing personalized 3D human avatars\\nwith realistic animation from only a few images. Due to the large variations in\\nbody shapes, poses, and cloth types, existing methods mostly require hours of\\nper-subject optimization during inference, which limits their practical\\napplications. In contrast, we learn a universal prior from over a thousand\\nclothed humans to achieve instant feedforward generation and zero-shot\\ngeneralization. Specifically, instead of rigging the avatar with shared\\nskinning weights, we jointly infer personalized avatar shape, skinning weights,\\nand pose-dependent deformations, which effectively improves overall geometric\\nfidelity and reduces deformation artifacts. Moreover, to normalize pose\\nvariations and resolve coupled ambiguity between canonical shapes and skinning\\nweights, we design a 3D canonicalization process to produce pixel-aligned\\ninitial conditions, which helps to reconstruct fine-grained geometric details.\\nWe then propose a multi-frame feature aggregation to robustly reduce artifacts\\nintroduced in canonicalization and fuse a plausible avatar preserving\\nperson-specific identities. Finally, we train the model in an end-to-end\\nframework on a large-scale capture dataset, which contains diverse human\\nsubjects paired with high-quality 3D scans. Extensive experiments show that our\\nmethod generates more authentic reconstruction and animation than\\nstate-of-the-arts, and can be directly generalized to inputs from casually\\ntaken phone photos. Project page and code is available at\\nhttps://github.com/rongakowang/FRESA.', 'upvotes': 1, 'discussionId': '67e3f2810d1715115dcd5d7a', 'projectPage': 'https://rongakowang.github.io/fresa/fresa.html', 'githubRepo': 'https://github.com/rongakowang/FRESA', 'ai_keywords': ['personalized 3D human avatars', 'realistic animation', 'universal prior', 'instant feedforward generation', 'zero-shot generalization', 'personalized avatar shape', 'skinning weights', 'pose-dependent deformations', 'geometric fidelity', 'deformation artifacts', '3D canonicalization', 'pixel-aligned initial conditions', 'fine-grained geometric details', 'multi-frame feature aggregation', 'canonicalization', 'plausible avatar', 'person-specific identities', 'end-to-end framework', 'large-scale capture dataset', 'high-quality 3D scans', 'authentic reconstruction', 'casual phone photos']}, 'publishedAt': '2025-03-24T19:20:47.000Z', 'title': 'FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from\\n  Few Images', 'summary': 'We present a novel method for reconstructing personalized 3D human avatars\\nwith realistic animation from only a few images. Due to the large variations in\\nbody shapes, poses, and cloth types, existing methods mostly require hours of\\nper-subject optimization during inference, which limits their practical\\napplications. In contrast, we learn a universal prior from over a thousand\\nclothed humans to achieve instant feedforward generation and zero-shot\\ngeneralization. Specifically, instead of rigging the avatar with shared\\nskinning weights, we jointly infer personalized avatar shape, skinning weights,\\nand pose-dependent deformations, which effectively improves overall geometric\\nfidelity and reduces deformation artifacts. Moreover, to normalize pose\\nvariations and resolve coupled ambiguity between canonical shapes and skinning\\nweights, we design a 3D canonicalization process to produce pixel-aligned\\ninitial conditions, which helps to reconstruct fine-grained geometric details.\\nWe then propose a multi-frame feature aggregation to robustly reduce artifacts\\nintroduced in canonicalization and fuse a plausible avatar preserving\\nperson-specific identities. Finally, we train the model in an end-to-end\\nframework on a large-scale capture dataset, which contains diverse human\\nsubjects paired with high-quality 3D scans. Extensive experiments show that our\\nmethod generates more authentic reconstruction and animation than\\nstate-of-the-arts, and can be directly generalized to inputs from casually\\ntaken phone photos. Project page and code is available at\\nhttps://github.com/rongakowang/FRESA.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19207.png', 'numComments': 1, 'submittedBy': {'_id': '6493306970d925ae80523a53', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nRCxbVng_PPBqKd-Z3KVc.jpeg', 'fullname': 'Dmitry Ryumin', 'name': 'DmitryRyumin', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 396}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.19065', 'authors': [{'_id': '67e3deabf2cb5de878fbf557', 'name': 'Zhongyu Yang', 'hidden': False}, {'_id': '67e3deabf2cb5de878fbf558', 'name': 'Jun Chen', 'hidden': False}, {'_id': '67e3deabf2cb5de878fbf559', 'name': 'Dannong Xu', 'hidden': False}, {'_id': '67e3deabf2cb5de878fbf55a', 'name': 'Junjie Fei', 'hidden': False}, {'_id': '67e3deabf2cb5de878fbf55b', 'name': 'Xiaoqian Shen', 'hidden': False}, {'_id': '67e3deabf2cb5de878fbf55c', 'name': 'Liangbing Zhao', 'hidden': False}, {'_id': '67e3deabf2cb5de878fbf55d', 'name': 'Chun-Mei Feng', 'hidden': False}, {'_id': '67e3deabf2cb5de878fbf55e', 'name': 'Mohamed Elhoseiny', 'hidden': False}], 'publishedAt': '2025-03-24T18:51:55.000Z', 'submittedOnDailyAt': '2025-03-26T12:36:33.366Z', 'title': 'WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation', 'submittedOnDailyBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'isPro': False, 'fullname': 'AK', 'user': 'akhaliq', 'type': 'user'}, 'summary': 'Knowledge discovery and collection are intelligence-intensive tasks that\\ntraditionally require significant human effort to ensure high-quality outputs.\\nRecent research has explored multi-agent frameworks for automating\\nWikipedia-style article generation by retrieving and synthesizing information\\nfrom the internet. However, these methods primarily focus on text-only\\ngeneration, overlooking the importance of multimodal content in enhancing\\ninformativeness and engagement. In this work, we introduce WikiAutoGen, a novel\\nsystem for automated multimodal Wikipedia-style article generation. Unlike\\nprior approaches, WikiAutoGen retrieves and integrates relevant images\\nalongside text, enriching both the depth and visual appeal of generated\\ncontent. To further improve factual accuracy and comprehensiveness, we propose\\na multi-perspective self-reflection mechanism, which critically assesses\\nretrieved content from diverse viewpoints to enhance reliability, breadth, and\\ncoherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising\\nWikipedia articles with topics paired with both textual and image-based\\nrepresentations, designed to evaluate multimodal knowledge generation on more\\nchallenging topics. Experimental results show that WikiAutoGen outperforms\\nprevious methods by 8%-29% on our WikiSeek benchmark, producing more accurate,\\ncoherent, and visually enriched Wikipedia-style articles. We show some of our\\ngenerated examples in https://wikiautogen.github.io/ .', 'upvotes': 1, 'discussionId': '67e3deacf2cb5de878fbf5f8', 'projectPage': 'https://wikiautogen.github.io/', 'githubRepo': 'https://github.com/01yzzyu/wikiautogen', 'ai_keywords': ['multimodal content', 'WikiAutoGen', 'image retrieval', 'text retrieval', 'multi-perspective self-reflection mechanism', 'WikiSeek', 'multimodal knowledge generation', 'factual accuracy', 'comprehensiveness', 'depth', 'visual appeal', 'reliability', 'coherence', 'visual enrichment']}, 'publishedAt': '2025-03-24T14:51:55.000Z', 'title': 'WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation', 'summary': 'Knowledge discovery and collection are intelligence-intensive tasks that\\ntraditionally require significant human effort to ensure high-quality outputs.\\nRecent research has explored multi-agent frameworks for automating\\nWikipedia-style article generation by retrieving and synthesizing information\\nfrom the internet. However, these methods primarily focus on text-only\\ngeneration, overlooking the importance of multimodal content in enhancing\\ninformativeness and engagement. In this work, we introduce WikiAutoGen, a novel\\nsystem for automated multimodal Wikipedia-style article generation. Unlike\\nprior approaches, WikiAutoGen retrieves and integrates relevant images\\nalongside text, enriching both the depth and visual appeal of generated\\ncontent. To further improve factual accuracy and comprehensiveness, we propose\\na multi-perspective self-reflection mechanism, which critically assesses\\nretrieved content from diverse viewpoints to enhance reliability, breadth, and\\ncoherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising\\nWikipedia articles with topics paired with both textual and image-based\\nrepresentations, designed to evaluate multimodal knowledge generation on more\\nchallenging topics. Experimental results show that WikiAutoGen outperforms\\nprevious methods by 8%-29% on our WikiSeek benchmark, producing more accurate,\\ncoherent, and visually enriched Wikipedia-style articles. We show some of our\\ngenerated examples in https://wikiautogen.github.io/ .', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19065.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isHfAdmin': True, 'isMod': False, 'followerCount': 6473}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.18893', 'authors': [{'_id': '67e4110756b46b70c6d49e2f', 'name': 'Chi-Chih Chang', 'hidden': False}, {'_id': '67e4110756b46b70c6d49e30', 'name': 'Chien-Yu Lin', 'hidden': False}, {'_id': '67e4110756b46b70c6d49e31', 'name': 'Yash Akhauri', 'hidden': False}, {'_id': '67e4110756b46b70c6d49e32', 'name': 'Wei-Cheng Lin', 'hidden': False}, {'_id': '67e4110756b46b70c6d49e33', 'name': 'Kai-Chiang Wu', 'hidden': False}, {'_id': '67e4110756b46b70c6d49e34', 'name': 'Luis Ceze', 'hidden': False}, {'_id': '67e4110756b46b70c6d49e35', 'name': 'Mohamed S. Abdelfattah', 'hidden': False}], 'publishedAt': '2025-03-24T17:06:37.000Z', 'submittedOnDailyAt': '2025-03-26T13:26:48.770Z', 'title': 'xKV: Cross-Layer SVD for KV-Cache Compression', 'submittedOnDailyBy': {'_id': '6589f238d1331d552b1b26f5', 'avatarUrl': '/avatars/4a496d5791a79df9718e4a71845ae8eb.svg', 'isPro': False, 'fullname': 'Chi-Chih Chang', 'user': 'shadowpa0327', 'type': 'user'}, 'summary': \"Large Language Models (LLMs) with long context windows enable powerful\\napplications but come at the cost of high memory consumption to store the Key\\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\\nmultiple layers into shared representations, yet these approaches either\\nrequire expensive pretraining or rely on assumptions of high per-token cosine\\nsimilarity across layers which generally does not hold in practice. We find\\nthat the dominant singular vectors are remarkably well-aligned across multiple\\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\\npost-training method that applies Singular Value Decomposition (SVD) on the\\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\\ntasks without performance degradation. These results highlight xKV's strong\\ncapability and versatility in addressing memory bottlenecks for long-context\\nLLM inference. Our code is publicly available at:\\nhttps://github.com/abdelfattah-lab/xKV.\", 'upvotes': 1, 'discussionId': '67e4110856b46b70c6d49e7d', 'projectPage': 'https://abdelfattah-lab.github.io/xKV/', 'githubRepo': 'https://github.com/abdelfattah-lab/xKV'}, 'publishedAt': '2025-03-24T13:06:37.000Z', 'title': 'xKV: Cross-Layer SVD for KV-Cache Compression', 'summary': \"Large Language Models (LLMs) with long context windows enable powerful\\napplications but come at the cost of high memory consumption to store the Key\\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\\nmultiple layers into shared representations, yet these approaches either\\nrequire expensive pretraining or rely on assumptions of high per-token cosine\\nsimilarity across layers which generally does not hold in practice. We find\\nthat the dominant singular vectors are remarkably well-aligned across multiple\\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\\npost-training method that applies Singular Value Decomposition (SVD) on the\\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\\ntasks without performance degradation. These results highlight xKV's strong\\ncapability and versatility in addressing memory bottlenecks for long-context\\nLLM inference. Our code is publicly available at:\\nhttps://github.com/abdelfattah-lab/xKV.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18893.png', 'numComments': 1, 'submittedBy': {'_id': '6589f238d1331d552b1b26f5', 'avatarUrl': '/avatars/4a496d5791a79df9718e4a71845ae8eb.svg', 'fullname': 'Chi-Chih Chang', 'name': 'shadowpa0327', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2503.17361', 'authors': [{'_id': '67e35ca7363374850440d91d', 'name': 'Sophia Tang', 'hidden': False}, {'_id': '67e35ca7363374850440d91e', 'name': 'Yinuo Zhang', 'hidden': False}, {'_id': '67e35ca7363374850440d91f', 'name': 'Alexander Tong', 'hidden': False}, {'_id': '67e35ca7363374850440d920', 'user': {'_id': '64cd5b3f0494187a9e8b7c69', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg', 'isPro': False, 'fullname': 'Pranam Chatterjee', 'user': 'pranamanam', 'type': 'user'}, 'name': 'Pranam Chatterjee', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-03-26T01:57:51.167Z', 'hidden': False}], 'publishedAt': '2025-03-21T17:59:43.000Z', 'submittedOnDailyAt': '2025-03-26T00:18:51.908Z', 'title': 'Gumbel-Softmax Flow Matching with Straight-Through Guidance for\\n  Controllable Biological Sequence Generation', 'submittedOnDailyBy': {'_id': '64cd5b3f0494187a9e8b7c69', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg', 'isPro': False, 'fullname': 'Pranam Chatterjee', 'user': 'pranamanam', 'type': 'user'}, 'summary': 'Flow matching in the continuous simplex has emerged as a promising strategy\\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\\nand Score Matching, a generative framework on the simplex based on a novel\\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\\nparameterized velocity field that transports from smooth categorical\\ndistributions to distributions concentrated at a single vertex of the simplex.\\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\\nthe gradient of the probability density. Our framework enables high-quality,\\ndiverse generation and scales efficiently to higher-dimensional simplices. To\\nenable training-free guidance, we propose Straight-Through Guided Flows\\n(STGFlow), a classifier-based guidance method that leverages straight-through\\nestimators to steer the unconditional velocity field toward optimal vertices of\\nthe simplex. STGFlow enables efficient inference-time guidance using\\nclassifiers pre-trained on clean sequences, and can be used with any discrete\\nflow method. Together, these components form a robust framework for\\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\\nperformance in conditional DNA promoter design, sequence-only protein\\ngeneration, and target-binding peptide design for rare disease treatment.', 'upvotes': 1, 'discussionId': '67e35caa363374850440d9df', 'ai_keywords': ['Gumbel-Softmax Flow', 'Score Matching', 'simplex', 'Gumbel-Softmax interpolant', 'time-dependent temperature', 'parameterized velocity field', 'smooth categorical distributions', 'Gumbel-Softmax Flow Matching', 'Straight-Through Guided Flows', 'STGFlow', 'straight-through estimators', 'classifiers', 'de novo sequence generation', 'conditional DNA promoter design', 'sequence-only protein generation', 'target-binding peptide design']}, 'publishedAt': '2025-03-21T13:59:43.000Z', 'title': 'Gumbel-Softmax Flow Matching with Straight-Through Guidance for\\n  Controllable Biological Sequence Generation', 'summary': 'Flow matching in the continuous simplex has emerged as a promising strategy\\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\\nand Score Matching, a generative framework on the simplex based on a novel\\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\\nparameterized velocity field that transports from smooth categorical\\ndistributions to distributions concentrated at a single vertex of the simplex.\\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\\nthe gradient of the probability density. Our framework enables high-quality,\\ndiverse generation and scales efficiently to higher-dimensional simplices. To\\nenable training-free guidance, we propose Straight-Through Guided Flows\\n(STGFlow), a classifier-based guidance method that leverages straight-through\\nestimators to steer the unconditional velocity field toward optimal vertices of\\nthe simplex. STGFlow enables efficient inference-time guidance using\\nclassifiers pre-trained on clean sequences, and can be used with any discrete\\nflow method. Together, these components form a robust framework for\\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\\nperformance in conditional DNA promoter design, sequence-only protein\\ngeneration, and target-binding peptide design for rare disease treatment.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17361.png', 'numComments': 1, 'submittedBy': {'_id': '64cd5b3f0494187a9e8b7c69', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg', 'fullname': 'Pranam Chatterjee', 'name': 'pranamanam', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2503.11849', 'authors': [{'_id': '67e3d0ac304f166b665e4a67', 'name': 'Yi Wang', 'hidden': False}, {'_id': '67e3d0ac304f166b665e4a68', 'name': 'Zhitong Xiong', 'hidden': False}, {'_id': '67e3d0ac304f166b665e4a69', 'name': 'Chenying Liu', 'hidden': False}, {'_id': '67e3d0ac304f166b665e4a6a', 'name': 'Adam J. Stewart', 'hidden': False}, {'_id': '67e3d0ac304f166b665e4a6b', 'name': 'Thomas Dujardin', 'hidden': False}, {'_id': '67e3d0ac304f166b665e4a6c', 'name': 'Nikolaos Ioannis Bountos', 'hidden': False}, {'_id': '67e3d0ac304f166b665e4a6d', 'name': 'Angelos Zavras', 'hidden': False}, {'_id': '67e3d0ac304f166b665e4a6e', 'name': 'Franziska Gerken', 'hidden': False}, {'_id': '67e3d0ac304f166b665e4a6f', 'name': 'Ioannis Papoutsis', 'hidden': False}, {'_id': '67e3d0ac304f166b665e4a70', 'name': 'Laura Leal-Taix', 'hidden': False}, {'_id': '67e3d0ac304f166b665e4a71', 'name': 'Xiao Xiang Zhu', 'hidden': False}], 'publishedAt': '2025-03-14T20:16:48.000Z', 'submittedOnDailyAt': '2025-03-26T08:34:37.209Z', 'title': 'Towards a Unified Copernicus Foundation Model for Earth Vision', 'submittedOnDailyBy': {'_id': '64cba974a81988d0734c9925', 'avatarUrl': '/avatars/645c326ca38eb751144f356076cef60f.svg', 'isPro': False, 'fullname': 'Yi Wang', 'user': 'wangyi111', 'type': 'user'}, 'summary': \"Advances in Earth observation (EO) foundation models have unlocked the\\npotential of big satellite data to learn generic representations from space,\\nbenefiting a wide range of downstream applications crucial to our planet.\\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\\nIn this work, we take a step towards next-generation EO foundation models with\\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\\nmissions, spanning from the Earth's surface to its atmosphere; 2)\\nCopernicus-FM, a unified foundation model capable of processing any spectral or\\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\\napplications for each Sentinel mission. Our dataset, model, and benchmark\\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\\nfoundation models, while also creating new opportunities to connect EO,\\nweather, and climate research. Codes, datasets and models are available at\\nhttps://github.com/zhu-xlab/Copernicus-FM.\", 'upvotes': 1, 'discussionId': '67e3d0af304f166b665e4b68', 'githubRepo': 'https://github.com/zhu-xlab/Copernicus-FM', 'ai_keywords': ['extended dynamic hypernetworks', 'flexible metadata encoding']}, 'publishedAt': '2025-03-14T16:16:48.000Z', 'title': 'Towards a Unified Copernicus Foundation Model for Earth Vision', 'summary': \"Advances in Earth observation (EO) foundation models have unlocked the\\npotential of big satellite data to learn generic representations from space,\\nbenefiting a wide range of downstream applications crucial to our planet.\\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\\nIn this work, we take a step towards next-generation EO foundation models with\\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\\nmissions, spanning from the Earth's surface to its atmosphere; 2)\\nCopernicus-FM, a unified foundation model capable of processing any spectral or\\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\\napplications for each Sentinel mission. Our dataset, model, and benchmark\\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\\nfoundation models, while also creating new opportunities to connect EO,\\nweather, and climate research. Codes, datasets and models are available at\\nhttps://github.com/zhu-xlab/Copernicus-FM.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11849.png', 'numComments': 1, 'submittedBy': {'_id': '64cba974a81988d0734c9925', 'avatarUrl': '/avatars/645c326ca38eb751144f356076cef60f.svg', 'fullname': 'Yi Wang', 'name': 'wangyi111', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': False}"
]