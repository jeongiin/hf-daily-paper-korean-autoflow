[
    "{'paper': {'id': '2507.09477', 'authors': [{'_id': '68787030001546c83aa4f9ae', 'name': 'Yangning Li', 'hidden': False}, {'_id': '68787030001546c83aa4f9af', 'user': {'_id': '6667e801fd95ddf66cac84ff', 'avatarUrl': '/avatars/b75f6253160c81f558e6b40ed2ca1755.svg', 'isPro': False, 'fullname': 'Weizhi Zhang', 'user': 'WZDavid', 'type': 'user'}, 'name': 'Weizhi Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:22:48.379Z', 'hidden': False}, {'_id': '68787030001546c83aa4f9b0', 'name': 'Yuyao Yang', 'hidden': False}, {'_id': '68787030001546c83aa4f9b1', 'name': 'Wei-Chieh Huang', 'hidden': False}, {'_id': '68787030001546c83aa4f9b2', 'name': 'Yaozu Wu', 'hidden': False}, {'_id': '68787030001546c83aa4f9b3', 'name': 'Junyu Luo', 'hidden': False}, {'_id': '68787030001546c83aa4f9b4', 'name': 'Yuanchen Bei', 'hidden': False}, {'_id': '68787030001546c83aa4f9b5', 'user': {'_id': '633f112013e836a0fc4fa567', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1665077519962-noauth.jpeg', 'isPro': False, 'fullname': 'Henry Peng Zou', 'user': 'TreeForest', 'type': 'user'}, 'name': 'Henry Peng Zou', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:22:46.466Z', 'hidden': False}, {'_id': '68787030001546c83aa4f9b6', 'name': 'Xiao Luo', 'hidden': False}, {'_id': '68787030001546c83aa4f9b7', 'name': 'Yusheng Zhao', 'hidden': False}, {'_id': '68787030001546c83aa4f9b8', 'name': 'Chunkit Chan', 'hidden': False}, {'_id': '68787030001546c83aa4f9b9', 'name': 'Yankai Chen', 'hidden': False}, {'_id': '68787030001546c83aa4f9ba', 'name': 'Zhongfen Deng', 'hidden': False}, {'_id': '68787030001546c83aa4f9bb', 'name': 'Yinghui Li', 'hidden': False}, {'_id': '68787030001546c83aa4f9bc', 'name': 'Hai-Tao Zheng', 'hidden': False}, {'_id': '68787030001546c83aa4f9bd', 'name': 'Dongyuan Li', 'hidden': False}, {'_id': '68787030001546c83aa4f9be', 'name': 'Renhe Jiang', 'hidden': False}, {'_id': '68787030001546c83aa4f9bf', 'name': 'Ming Zhang', 'hidden': False}, {'_id': '68787030001546c83aa4f9c0', 'name': 'Yangqiu Song', 'hidden': False}, {'_id': '68787030001546c83aa4f9c1', 'name': 'Philip S. Yu', 'hidden': False}], 'publishedAt': '2025-07-13T03:29:41.000Z', 'submittedOnDailyAt': '2025-07-17T02:27:52.541Z', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\\n  Systems in LLMs', 'submittedOnDailyBy': {'_id': '6667e801fd95ddf66cac84ff', 'avatarUrl': '/avatars/b75f6253160c81f558e6b40ed2ca1755.svg', 'isPro': False, 'fullname': 'Weizhi Zhang', 'user': 'WZDavid', 'type': 'user'}, 'summary': 'Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\\nthat demand multi-step inference; conversely, purely reasoning-oriented\\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\\nstrands under a unified reasoning-retrieval perspective. We first map how\\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\\nwe show how retrieved knowledge of different type supply missing premises and\\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\\niteratively interleave search and reasoning to achieve state-of-the-art\\nperformance across knowledge-intensive benchmarks. We categorize methods,\\ndatasets, and open challenges, and outline research avenues toward deeper\\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\\ntrustworthy, and human-centric. The collection is available at\\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.', 'upvotes': 42, 'discussionId': '68787031001546c83aa4f9c2', 'githubRepo': 'https://github.com/DavidZWZ/Awesome-RAG-Reasoning', 'ai_summary': 'This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.', 'ai_keywords': ['Retrieval-Augmented Generation', 'Large Language Models', 'Reasoning-Enhanced RAG', 'RAG-Enhanced Reasoning', 'Synergized RAG-Reasoning', 'knowledge-intensive benchmarks', 'multimodally-adaptive', 'trustworthy', 'human-centric'], 'githubStars': 59}, 'publishedAt': '2025-07-12T23:29:41.000Z', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\\n  Systems in LLMs', 'summary': 'Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\\nthat demand multi-step inference; conversely, purely reasoning-oriented\\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\\nstrands under a unified reasoning-retrieval perspective. We first map how\\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\\nwe show how retrieved knowledge of different type supply missing premises and\\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\\niteratively interleave search and reasoning to achieve state-of-the-art\\nperformance across knowledge-intensive benchmarks. We categorize methods,\\ndatasets, and open challenges, and outline research avenues toward deeper\\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\\ntrustworthy, and human-centric. The collection is available at\\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09477.png', 'numComments': 2, 'submittedBy': {'_id': '6667e801fd95ddf66cac84ff', 'avatarUrl': '/avatars/b75f6253160c81f558e6b40ed2ca1755.svg', 'fullname': 'Weizhi Zhang', 'name': 'WZDavid', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.12465', 'authors': [{'_id': '6878635e001546c83aa4f979', 'user': {'_id': '65af6f6b52e1b2aae437af2e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg', 'isPro': False, 'fullname': 'Ziang Cao', 'user': 'Caoza', 'type': 'user'}, 'name': 'Ziang Cao', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:22:44.605Z', 'hidden': False}, {'_id': '6878635e001546c83aa4f97a', 'user': {'_id': '62fc8cf7ee999004b5a8b982', 'avatarUrl': '/avatars/6c5dda9e58747054a989f077a078f3dc.svg', 'isPro': False, 'fullname': 'Zhaoxi Chen', 'user': 'FrozenBurning', 'type': 'user'}, 'name': 'Zhaoxi Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:22:11.615Z', 'hidden': False}, {'_id': '6878635e001546c83aa4f97b', 'name': 'Linag Pan', 'hidden': False}, {'_id': '6878635e001546c83aa4f97c', 'user': {'_id': '62ab1ac1d48b4d8b048a3473', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png', 'isPro': False, 'fullname': 'Ziwei Liu', 'user': 'liuziwei7', 'type': 'user'}, 'name': 'Ziwei Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:21:56.595Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65af6f6b52e1b2aae437af2e/nrL7wGaZ1Z5FZWuu0GTbg.mp4'], 'publishedAt': '2025-07-16T17:59:35.000Z', 'submittedOnDailyAt': '2025-07-17T01:26:11.001Z', 'title': 'PhysX: Physical-Grounded 3D Asset Generation', 'submittedOnDailyBy': {'_id': '65af6f6b52e1b2aae437af2e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg', 'isPro': False, 'fullname': 'Ziang Cao', 'user': 'Caoza', 'type': 'user'}, 'summary': '3D modeling is moving from virtual to physical. Existing 3D generation\\nprimarily emphasizes geometries and textures while neglecting physical-grounded\\nmodeling. Consequently, despite the rapid development of 3D generative models,\\nthe synthesized 3D assets often overlook rich and important physical\\nproperties, hampering their real-world application in physical domains like\\nsimulation and embodied AI. As an initial attempt to address this challenge, we\\npropose PhysX, an end-to-end paradigm for physical-grounded 3D asset\\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\\npresent PhysXNet - the first physics-grounded 3D dataset systematically\\nannotated across five foundational dimensions: absolute scale, material,\\naffordance, kinematics, and function description. In particular, we devise a\\nscalable human-in-the-loop annotation pipeline based on vision-language models,\\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\\nFurthermore, we propose PhysXGen, a feed-forward framework for\\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\\ndual-branch architecture to explicitly model the latent correlations between 3D\\nstructures and physical properties, thereby producing 3D assets with plausible\\nphysical predictions while preserving the native geometry quality. Extensive\\nexperiments validate the superior performance and promising generalization\\ncapability of our framework. All the code, data, and models will be released to\\nfacilitate future research in generative physical AI.', 'upvotes': 20, 'discussionId': '6878635e001546c83aa4f97d', 'projectPage': 'https://physx-3d.github.io/', 'githubRepo': 'https://github.com/ziangcao0312/PhysX', 'ai_summary': 'PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.', 'ai_keywords': ['physics-grounded 3D asset generation', 'PhysXNet', 'physics-annotated 3D datasets', 'vision-language models', 'human-in-the-loop annotation pipeline', 'PhysXGen', 'feed-forward framework', 'dual-branch architecture', 'latent correlations', 'physical predictions', 'geometry quality'], 'githubStars': 54}, 'publishedAt': '2025-07-16T13:59:35.000Z', 'title': 'PhysX: Physical-Grounded 3D Asset Generation', 'summary': '3D modeling is moving from virtual to physical. Existing 3D generation\\nprimarily emphasizes geometries and textures while neglecting physical-grounded\\nmodeling. Consequently, despite the rapid development of 3D generative models,\\nthe synthesized 3D assets often overlook rich and important physical\\nproperties, hampering their real-world application in physical domains like\\nsimulation and embodied AI. As an initial attempt to address this challenge, we\\npropose PhysX, an end-to-end paradigm for physical-grounded 3D asset\\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\\npresent PhysXNet - the first physics-grounded 3D dataset systematically\\nannotated across five foundational dimensions: absolute scale, material,\\naffordance, kinematics, and function description. In particular, we devise a\\nscalable human-in-the-loop annotation pipeline based on vision-language models,\\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\\nFurthermore, we propose PhysXGen, a feed-forward framework for\\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\\ndual-branch architecture to explicitly model the latent correlations between 3D\\nstructures and physical properties, thereby producing 3D assets with plausible\\nphysical predictions while preserving the native geometry quality. Extensive\\nexperiments validate the superior performance and promising generalization\\ncapability of our framework. All the code, data, and models will be released to\\nfacilitate future research in generative physical AI.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65af6f6b52e1b2aae437af2e/nrL7wGaZ1Z5FZWuu0GTbg.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12465.png', 'numComments': 1, 'submittedBy': {'_id': '65af6f6b52e1b2aae437af2e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg', 'fullname': 'Ziang Cao', 'name': 'Caoza', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.12463', 'authors': [{'_id': '68788789001546c83aa4f9e4', 'user': {'_id': '686fe2365998cd8af12fc8ba', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GQbxskSw-98mdjbM3szaC.png', 'isPro': False, 'fullname': 'Renjie Li', 'user': 'renjie-li', 'type': 'user'}, 'name': 'Renjie Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T14:59:16.875Z', 'hidden': False}, {'_id': '68788789001546c83aa4f9e5', 'user': {'_id': '6825fc0e58cf56d164cb339d', 'avatarUrl': '/avatars/4ade4a5bbb0a805a92a83bfb233f805f.svg', 'isPro': False, 'fullname': 'Ruijie Ye', 'user': 'jerryye0110', 'type': 'user'}, 'name': 'Ruijie Ye', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:17:53.937Z', 'hidden': False}, {'_id': '68788789001546c83aa4f9e6', 'name': 'Mingyang Wu', 'hidden': False}, {'_id': '68788789001546c83aa4f9e7', 'name': 'Hao Frank Yang', 'hidden': False}, {'_id': '68788789001546c83aa4f9e8', 'user': {'_id': '63b354bb7091e602f1a0e2e8', 'avatarUrl': '/avatars/a388d93c0af2f57eadb6fa60d6789041.svg', 'isPro': False, 'fullname': 'wayne', 'user': 'waynefan', 'type': 'user'}, 'name': 'Zhiwen Fan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:23:18.102Z', 'hidden': False}, {'_id': '68788789001546c83aa4f9e9', 'name': 'Hezhen Hu', 'hidden': False}, {'_id': '68788789001546c83aa4f9ea', 'user': {'_id': '62548d5fef3debb2ddf91217', 'avatarUrl': '/avatars/14975b45568f9c399c92c3986b6ce83e.svg', 'isPro': False, 'fullname': 'Zhengzhong Tu', 'user': 'vztu', 'type': 'user'}, 'name': 'Zhengzhong Tu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:23:16.047Z', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ARblfCBCEmNaS2zzbTJLm.jpeg'], 'publishedAt': '2025-07-16T17:59:30.000Z', 'submittedOnDailyAt': '2025-07-17T03:48:41.381Z', 'title': 'MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\\n  Understanding', 'submittedOnDailyBy': {'_id': '62548d5fef3debb2ddf91217', 'avatarUrl': '/avatars/14975b45568f9c399c92c3986b6ce83e.svg', 'isPro': False, 'fullname': 'Zhengzhong Tu', 'user': 'vztu', 'type': 'user'}, 'summary': 'Humans are integral components of the transportation ecosystem, and\\nunderstanding their behaviors is crucial to facilitating the development of\\nsafe driving systems. Although recent progress has explored various aspects of\\nhuman behaviorx2014such as motion, trajectories, and\\nintentionx2014a comprehensive benchmark for evaluating human\\nbehavior understanding in autonomous driving remains unavailable. In this work,\\nwe propose MMHU, a large-scale benchmark for human behavior analysis\\nfeaturing rich annotations, such as human motion and trajectories, text\\ndescription for human motions, human intention, and critical behavior labels\\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\\n1.73M frames gathered from diverse sources, including established driving\\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\\nbehavior captions. We provide a thorough dataset analysis and benchmark\\nmultiple tasksx2014ranging from motion prediction to motion\\ngeneration and human behavior question answeringx2014thereby\\noffering a broad evaluation suite. Project page :\\nhttps://MMHU-Benchmark.github.io.', 'upvotes': 12, 'discussionId': '6878878a001546c83aa4f9eb', 'projectPage': 'https://mmhu-benchmark.github.io/', 'ai_summary': 'A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.', 'ai_keywords': ['human behavior analysis', 'motion prediction', 'motion generation', 'human behavior question answering', 'human-in-the-loop annotation', 'Waymo', 'YouTube', 'self-collected data']}, 'publishedAt': '2025-07-16T13:59:30.000Z', 'title': 'MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\\n  Understanding', 'summary': 'Humans are integral components of the transportation ecosystem, and\\nunderstanding their behaviors is crucial to facilitating the development of\\nsafe driving systems. Although recent progress has explored various aspects of\\nhuman behaviorx2014such as motion, trajectories, and\\nintentionx2014a comprehensive benchmark for evaluating human\\nbehavior understanding in autonomous driving remains unavailable. In this work,\\nwe propose MMHU, a large-scale benchmark for human behavior analysis\\nfeaturing rich annotations, such as human motion and trajectories, text\\ndescription for human motions, human intention, and critical behavior labels\\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\\n1.73M frames gathered from diverse sources, including established driving\\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\\nbehavior captions. We provide a thorough dataset analysis and benchmark\\nmultiple tasksx2014ranging from motion prediction to motion\\ngeneration and human behavior question answeringx2014thereby\\noffering a broad evaluation suite. Project page :\\nhttps://MMHU-Benchmark.github.io.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ARblfCBCEmNaS2zzbTJLm.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12463.png', 'numComments': 1, 'submittedBy': {'_id': '62548d5fef3debb2ddf91217', 'avatarUrl': '/avatars/14975b45568f9c399c92c3986b6ce83e.svg', 'fullname': 'Zhengzhong Tu', 'name': 'vztu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.11949', 'authors': [{'_id': '687872c3001546c83aa4f9cf', 'user': {'_id': '683d94e5ba11bab2cc848aab', 'avatarUrl': '/avatars/4a1915ad48c78b3a71733b3282f2f93c.svg', 'isPro': False, 'fullname': 'Shuyang Xu', 'user': 'JimSYXu', 'type': 'user'}, 'name': 'Shuyang Xu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:22:22.253Z', 'hidden': False}, {'_id': '687872c3001546c83aa4f9d0', 'user': {'_id': '645223fb01d7bd9555ea399a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png', 'isPro': False, 'fullname': 'Zhiyang Dou', 'user': 'frankzydou', 'type': 'user'}, 'name': 'Zhiyang Dou', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:22:59.825Z', 'hidden': False}, {'_id': '687872c3001546c83aa4f9d1', 'name': 'Mingyi Shi', 'hidden': False}, {'_id': '687872c3001546c83aa4f9d2', 'name': 'Liang Pan', 'hidden': False}, {'_id': '687872c3001546c83aa4f9d3', 'name': 'Leo Ho', 'hidden': False}, {'_id': '687872c3001546c83aa4f9d4', 'name': 'Jingbo Wang', 'hidden': False}, {'_id': '687872c3001546c83aa4f9d5', 'name': 'Yuan Liu', 'hidden': False}, {'_id': '687872c3001546c83aa4f9d6', 'name': 'Cheng Lin', 'hidden': False}, {'_id': '687872c3001546c83aa4f9d7', 'name': 'Yuexin Ma', 'hidden': False}, {'_id': '687872c3001546c83aa4f9d8', 'name': 'Wenping Wang', 'hidden': False}, {'_id': '687872c3001546c83aa4f9d9', 'name': 'Taku Komura', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/645223fb01d7bd9555ea399a/KoPBJkGwyft7hfVmM8ikZ.mp4'], 'publishedAt': '2025-07-16T06:33:11.000Z', 'submittedOnDailyAt': '2025-07-17T02:48:15.112Z', 'title': 'MOSPA: Human Motion Generation Driven by Spatial Audio', 'submittedOnDailyBy': {'_id': '645223fb01d7bd9555ea399a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png', 'isPro': False, 'fullname': 'Zhiyang Dou', 'user': 'frankzydou', 'type': 'user'}, 'summary': 'Enabling virtual humans to dynamically and realistically respond to diverse\\nauditory stimuli remains a key challenge in character animation, demanding the\\nintegration of perceptual modeling and motion synthesis. Despite its\\nsignificance, this task remains largely unexplored. Most previous works have\\nprimarily focused on mapping modalities like speech, audio, and music to\\ngenerate human motion. As of yet, these models typically overlook the impact of\\nspatial features encoded in spatial audio signals on human motion. To bridge\\nthis gap and enable high-quality modeling of human movements in response to\\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\\nmotion data. For benchmarking, we develop a simple yet effective\\ndiffusion-based generative framework for human MOtion generation driven by\\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\\nbody motion and spatial audio through an effective fusion mechanism. Once\\ntrained, MOSPA could generate diverse realistic human motions conditioned on\\nvarying spatial audio inputs. We perform a thorough investigation of the\\nproposed dataset and conduct extensive experiments for benchmarking, where our\\nmethod achieves state-of-the-art performance on this task. Our model and\\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\\nvideo for more details.', 'upvotes': 12, 'discussionId': '687872c9001546c83aa4f9da', 'ai_summary': 'A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.', 'ai_keywords': ['diffusion-based generative framework', 'spatial audio', 'human motion', 'SAM dataset', 'MOSPA', 'spatial features', 'perceptual modeling', 'motion synthesis']}, 'publishedAt': '2025-07-16T02:33:11.000Z', 'title': 'MOSPA: Human Motion Generation Driven by Spatial Audio', 'summary': 'Enabling virtual humans to dynamically and realistically respond to diverse\\nauditory stimuli remains a key challenge in character animation, demanding the\\nintegration of perceptual modeling and motion synthesis. Despite its\\nsignificance, this task remains largely unexplored. Most previous works have\\nprimarily focused on mapping modalities like speech, audio, and music to\\ngenerate human motion. As of yet, these models typically overlook the impact of\\nspatial features encoded in spatial audio signals on human motion. To bridge\\nthis gap and enable high-quality modeling of human movements in response to\\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\\nmotion data. For benchmarking, we develop a simple yet effective\\ndiffusion-based generative framework for human MOtion generation driven by\\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\\nbody motion and spatial audio through an effective fusion mechanism. Once\\ntrained, MOSPA could generate diverse realistic human motions conditioned on\\nvarying spatial audio inputs. We perform a thorough investigation of the\\nproposed dataset and conduct extensive experiments for benchmarking, where our\\nmethod achieves state-of-the-art performance on this task. Our model and\\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\\nvideo for more details.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/645223fb01d7bd9555ea399a/KoPBJkGwyft7hfVmM8ikZ.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11949.png', 'numComments': 1, 'submittedBy': {'_id': '645223fb01d7bd9555ea399a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png', 'fullname': 'Zhiyang Dou', 'name': 'frankzydou', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.12415', 'authors': [{'_id': '68788b9b001546c83aa4f9ed', 'name': 'Xinyi He', 'hidden': False}, {'_id': '68788b9b001546c83aa4f9ee', 'user': {'_id': '612ee6a7b960e78c6d2319d4', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg', 'isPro': False, 'fullname': 'Qian Liu', 'user': 'SivilTaram', 'type': 'user'}, 'name': 'Qian Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:17:46.375Z', 'hidden': False}, {'_id': '68788b9b001546c83aa4f9ef', 'user': {'_id': '61711f02e0b1ddb56eb9b526', 'avatarUrl': '/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg', 'isPro': True, 'fullname': 'Mingzhe Du', 'user': 'Elfsong', 'type': 'user'}, 'name': 'Mingzhe Du', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T09:08:30.259Z', 'hidden': False}, {'_id': '68788b9b001546c83aa4f9f0', 'name': 'Lin Yan', 'hidden': False}, {'_id': '68788b9b001546c83aa4f9f1', 'name': 'Zhijie Fan', 'hidden': False}, {'_id': '68788b9b001546c83aa4f9f2', 'name': 'Yiming Huang', 'hidden': False}, {'_id': '68788b9b001546c83aa4f9f3', 'name': 'Zejian Yuan', 'hidden': False}, {'_id': '68788b9b001546c83aa4f9f4', 'name': 'Zejun Ma', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/_XkK-c8Xm-G1Ui5AwtGdu.png'], 'publishedAt': '2025-07-16T17:05:17.000Z', 'submittedOnDailyAt': '2025-07-17T04:10:30.408Z', 'title': 'SWE-Perf: Can Language Models Optimize Code Performance on Real-World\\n  Repositories?', 'submittedOnDailyBy': {'_id': '612ee6a7b960e78c6d2319d4', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg', 'isPro': False, 'fullname': 'Qian Liu', 'user': 'SivilTaram', 'type': 'user'}, 'summary': 'Code performance optimization is paramount in real-world software engineering\\nand critical for production-level systems. While Large Language Models (LLMs)\\nhave demonstrated impressive capabilities in code generation and bug fixing,\\ntheir proficiency in enhancing code performance at the repository level remains\\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\\nbenchmark specifically designed to systematically evaluate LLMs on code\\nperformance optimization tasks within authentic repository contexts. SWE-Perf\\ncomprises 140 carefully curated instances, each derived from\\nperformance-improving pull requests from popular GitHub repositories. Each\\nbenchmark instance includes the relevant codebase, target functions,\\nperformance-related tests, expert-authored patches, and executable\\nenvironments. Through a comprehensive evaluation of representative methods that\\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\\nreveal a substantial capability gap between existing LLMs and expert-level\\noptimization performance, highlighting critical research opportunities in this\\nemerging field.', 'upvotes': 11, 'discussionId': '68788b9b001546c83aa4f9f5', 'projectPage': 'https://swe-perf.github.io/', 'githubRepo': 'https://github.com/SWE-Perf/SWE-Perf', 'ai_summary': 'SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.', 'ai_keywords': ['Large Language Models', 'code performance optimization', 'benchmark', 'performance-improving pull requests', 'codebase', 'target functions', 'performance-related tests', 'expert-authored patches', 'executable environments', 'Agentless', 'OpenHands'], 'githubStars': 5}, 'publishedAt': '2025-07-16T13:05:17.000Z', 'title': 'SWE-Perf: Can Language Models Optimize Code Performance on Real-World\\n  Repositories?', 'summary': 'Code performance optimization is paramount in real-world software engineering\\nand critical for production-level systems. While Large Language Models (LLMs)\\nhave demonstrated impressive capabilities in code generation and bug fixing,\\ntheir proficiency in enhancing code performance at the repository level remains\\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\\nbenchmark specifically designed to systematically evaluate LLMs on code\\nperformance optimization tasks within authentic repository contexts. SWE-Perf\\ncomprises 140 carefully curated instances, each derived from\\nperformance-improving pull requests from popular GitHub repositories. Each\\nbenchmark instance includes the relevant codebase, target functions,\\nperformance-related tests, expert-authored patches, and executable\\nenvironments. Through a comprehensive evaluation of representative methods that\\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\\nreveal a substantial capability gap between existing LLMs and expert-level\\noptimization performance, highlighting critical research opportunities in this\\nemerging field.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/_XkK-c8Xm-G1Ui5AwtGdu.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12415.png', 'numComments': 1, 'submittedBy': {'_id': '612ee6a7b960e78c6d2319d4', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg', 'fullname': 'Qian Liu', 'name': 'SivilTaram', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 85}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.11527', 'authors': [{'_id': '68785ee1001546c83aa4f967', 'user': {'_id': '65c950ebd908bd52a4477116', 'avatarUrl': '/avatars/bc6ba0dd2903c7bea37f7b9c40857718.svg', 'isPro': False, 'fullname': 'Yinsheng Li', 'user': 'Eason666', 'type': 'user'}, 'name': 'Yinsheng Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:22:42.470Z', 'hidden': False}, {'_id': '68785ee1001546c83aa4f968', 'user': {'_id': '643ba2f725681c3afaa8f05e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg', 'isPro': False, 'fullname': 'Zhen Dong', 'user': 'zhendongucb', 'type': 'user'}, 'name': 'Zhen Dong', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:22:37.102Z', 'hidden': False}, {'_id': '68785ee1001546c83aa4f969', 'name': 'Yi Shao', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/643ba2f725681c3afaa8f05e/2T1jxRrhqMGVLESbEDwJT.png'], 'publishedAt': '2025-07-15T17:56:04.000Z', 'submittedOnDailyAt': '2025-07-17T01:30:58.515Z', 'title': 'DrafterBench: Benchmarking Large Language Models for Tasks Automation in\\n  Civil Engineering', 'submittedOnDailyBy': {'_id': '643ba2f725681c3afaa8f05e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg', 'isPro': False, 'fullname': 'Zhen Dong', 'user': 'zhendongucb', 'type': 'user'}, 'summary': \"Large Language Model (LLM) agents have shown great potential for solving\\nreal-world problems and promise to be a solution for tasks automation in\\nindustry. However, more benchmarks are needed to systematically evaluate\\nautomation agents from an industrial perspective, for example, in Civil\\nEngineering. Therefore, we propose DrafterBench for the comprehensive\\nevaluation of LLM agents in the context of technical drawing revision, a\\nrepresentation task in civil engineering. DrafterBench contains twelve types of\\ntasks summarized from real-world drawing files, with 46 customized\\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\\nand long-context instructions, leveraging prior knowledge, and adapting to\\ndynamic instruction quality via implicit policy awareness. The toolkit\\ncomprehensively assesses distinct capabilities in structured data\\ncomprehension, function execution, instruction following, and critical\\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\\nstatistics, aiming to provide deeper insight into agent capabilities and\\nidentify improvement targets for integrating LLMs in engineering applications.\\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\\nwith the test set hosted at\\nhttps://huggingface.co/datasets/Eason666/DrafterBench.\", 'upvotes': 8, 'discussionId': '68785ee1001546c83aa4f96a', 'githubRepo': 'https://github.com/Eason-Li-AIS/DrafterBench', 'ai_summary': 'DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.', 'ai_keywords': ['Large Language Model (LLM)', 'technical drawing revision', 'structured data comprehension', 'function execution', 'instruction following', 'critical reasoning', 'benchmark', 'open-source', 'implicit policy awareness'], 'githubStars': 29}, 'publishedAt': '2025-07-15T13:56:04.000Z', 'title': 'DrafterBench: Benchmarking Large Language Models for Tasks Automation in\\n  Civil Engineering', 'summary': \"Large Language Model (LLM) agents have shown great potential for solving\\nreal-world problems and promise to be a solution for tasks automation in\\nindustry. However, more benchmarks are needed to systematically evaluate\\nautomation agents from an industrial perspective, for example, in Civil\\nEngineering. Therefore, we propose DrafterBench for the comprehensive\\nevaluation of LLM agents in the context of technical drawing revision, a\\nrepresentation task in civil engineering. DrafterBench contains twelve types of\\ntasks summarized from real-world drawing files, with 46 customized\\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\\nand long-context instructions, leveraging prior knowledge, and adapting to\\ndynamic instruction quality via implicit policy awareness. The toolkit\\ncomprehensively assesses distinct capabilities in structured data\\ncomprehension, function execution, instruction following, and critical\\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\\nstatistics, aiming to provide deeper insight into agent capabilities and\\nidentify improvement targets for integrating LLMs in engineering applications.\\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\\nwith the test set hosted at\\nhttps://huggingface.co/datasets/Eason666/DrafterBench.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/643ba2f725681c3afaa8f05e/2T1jxRrhqMGVLESbEDwJT.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11527.png', 'numComments': 1, 'submittedBy': {'_id': '643ba2f725681c3afaa8f05e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg', 'fullname': 'Zhen Dong', 'name': 'zhendongucb', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.11412', 'authors': [{'_id': '6877521f257d4f043537084f', 'name': 'Orion Weller', 'hidden': False}, {'_id': '6877521f257d4f0435370850', 'name': 'Kathryn Ricci', 'hidden': False}, {'_id': '6877521f257d4f0435370851', 'name': 'Marc Marone', 'hidden': False}, {'_id': '6877521f257d4f0435370852', 'user': {'_id': '609bbe2f4932693ca2009d6a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1620819560688-609bbe2f4932693ca2009d6a.jpeg', 'isPro': False, 'fullname': 'Antoine Chaffin', 'user': 'NohTow', 'type': 'user'}, 'name': 'Antoine Chaffin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-16T15:16:06.045Z', 'hidden': False}, {'_id': '6877521f257d4f0435370853', 'name': 'Dawn Lawrie', 'hidden': False}, {'_id': '6877521f257d4f0435370854', 'name': 'Benjamin Van Durme', 'hidden': False}], 'publishedAt': '2025-07-15T15:31:51.000Z', 'submittedOnDailyAt': '2025-07-17T11:43:56.880Z', 'title': 'Seq vs Seq: An Open Suite of Paired Encoders and Decoders', 'submittedOnDailyBy': {'_id': '609bbe2f4932693ca2009d6a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1620819560688-609bbe2f4932693ca2009d6a.jpeg', 'isPro': False, 'fullname': 'Antoine Chaffin', 'user': 'NohTow', 'type': 'user'}, 'summary': 'The large language model (LLM) community focuses almost exclusively on\\ndecoder-only language models, since they are easier to use for text generation.\\nHowever, a large subset of the community still uses encoder-only models for\\ntasks such as classification or retrieval. Previous work has attempted to\\ncompare these architectures, but is forced to make comparisons with models that\\nhave different numbers of parameters, training techniques, and datasets. We\\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\\ndecoder-only models produces SOTA recipes in both categories for their\\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\\ndecoders. Like previous work, we find that encoder-only models excel at\\nclassification and retrieval tasks while decoders excel at generative tasks.\\nHowever, we show that adapting a decoder model to encoder tasks (and vice\\nversa) through continued training is subpar compared to using only the reverse\\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\\nfor generative tasks). We open-source all artifacts of this study including\\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\\nallow future work to analyze or extend all aspects of training.', 'upvotes': 6, 'discussionId': '68775220257d4f0435370855', 'githubRepo': 'https://github.com/jhu-clsp/ettin-encoder-vs-decoder', 'githubStars': 15}, 'publishedAt': '2025-07-15T11:31:51.000Z', 'title': 'Seq vs Seq: An Open Suite of Paired Encoders and Decoders', 'summary': 'The large language model (LLM) community focuses almost exclusively on\\ndecoder-only language models, since they are easier to use for text generation.\\nHowever, a large subset of the community still uses encoder-only models for\\ntasks such as classification or retrieval. Previous work has attempted to\\ncompare these architectures, but is forced to make comparisons with models that\\nhave different numbers of parameters, training techniques, and datasets. We\\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\\ndecoder-only models produces SOTA recipes in both categories for their\\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\\ndecoders. Like previous work, we find that encoder-only models excel at\\nclassification and retrieval tasks while decoders excel at generative tasks.\\nHowever, we show that adapting a decoder model to encoder tasks (and vice\\nversa) through continued training is subpar compared to using only the reverse\\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\\nfor generative tasks). We open-source all artifacts of this study including\\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\\nallow future work to analyze or extend all aspects of training.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11412.png', 'numComments': 7, 'submittedBy': {'_id': '609bbe2f4932693ca2009d6a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1620819560688-609bbe2f4932693ca2009d6a.jpeg', 'fullname': 'Antoine Chaffin', 'name': 'NohTow', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 35}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.02857', 'authors': [{'_id': '68788cd9001546c83aa4f9f7', 'user': {'_id': '66aef8691dd7d0a8c6584724', 'avatarUrl': '/avatars/df9c2a56f3d0746cf64a330137a105b4.svg', 'isPro': False, 'fullname': 'Ziye Li', 'user': 'TribeRinb', 'type': 'user'}, 'name': 'Ziye Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:48:49.437Z', 'hidden': False}, {'_id': '68788cd9001546c83aa4f9f8', 'name': 'Hao Luo', 'hidden': False}, {'_id': '68788cd9001546c83aa4f9f9', 'user': {'_id': '6335c7fa2db86a181cca723f', 'avatarUrl': '/avatars/13f04af01914f8473f9939f49b4eecd4.svg', 'isPro': False, 'fullname': 'XC', 'user': 'XinchengShuai', 'type': 'user'}, 'name': 'Xincheng Shuai', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:48:34.400Z', 'hidden': False}, {'_id': '68788cd9001546c83aa4f9fa', 'user': {'_id': '67ff29ecbf6889a333c69c7a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png', 'isPro': False, 'fullname': 'Henghui Ding', 'user': 'HenghuiDing', 'type': 'user'}, 'name': 'Henghui Ding', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:48:26.454Z', 'hidden': False}], 'publishedAt': '2025-07-03T17:59:02.000Z', 'submittedOnDailyAt': '2025-07-17T04:11:55.501Z', 'title': 'AnyI2V: Animating Any Conditional Image with Motion Control', 'submittedOnDailyBy': {'_id': '67ff29ecbf6889a333c69c7a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png', 'isPro': False, 'fullname': 'Henghui Ding', 'user': 'HenghuiDing', 'type': 'user'}, 'summary': 'Recent advancements in video generation, particularly in diffusion models,\\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\\nsynthesis. However, challenges remain in effectively integrating dynamic motion\\nsignals and flexible spatial constraints. Existing T2V methods typically rely\\non text prompts, which inherently lack precise control over the spatial layout\\nof generated content. In contrast, I2V methods are limited by their dependence\\non real images, which restricts the editability of the synthesized content.\\nAlthough some methods incorporate ControlNet to introduce image-based\\nconditioning, they often lack explicit motion control and require\\ncomputationally expensive training. To address these limitations, we propose\\nAnyI2V, a training-free framework that animates any conditional images with\\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\\nas the conditional image, including data types such as meshes and point clouds\\nthat are not supported by ControlNet, enabling more flexible and versatile\\nvideo generation. Additionally, it supports mixed conditional inputs and\\nenables style transfer and editing via LoRA and text prompts. Extensive\\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\\nand provides a new perspective in spatial- and motion-controlled video\\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.', 'upvotes': 5, 'discussionId': '68788cda001546c83aa4f9fb', 'projectPage': 'https://henghuiding.com/AnyI2V/', 'githubRepo': 'https://github.com/FudanCVL/AnyI2V', 'ai_summary': 'AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.', 'ai_keywords': ['diffusion models', 'text-to-video', 'image-to-video', 'ControlNet', 'motion trajectories', 'conditional images', 'meshes', 'point clouds', 'mixed conditional inputs', 'style transfer', 'LoRA', 'text prompts'], 'githubStars': 84}, 'publishedAt': '2025-07-03T13:59:02.000Z', 'title': 'AnyI2V: Animating Any Conditional Image with Motion Control', 'summary': 'Recent advancements in video generation, particularly in diffusion models,\\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\\nsynthesis. However, challenges remain in effectively integrating dynamic motion\\nsignals and flexible spatial constraints. Existing T2V methods typically rely\\non text prompts, which inherently lack precise control over the spatial layout\\nof generated content. In contrast, I2V methods are limited by their dependence\\non real images, which restricts the editability of the synthesized content.\\nAlthough some methods incorporate ControlNet to introduce image-based\\nconditioning, they often lack explicit motion control and require\\ncomputationally expensive training. To address these limitations, we propose\\nAnyI2V, a training-free framework that animates any conditional images with\\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\\nas the conditional image, including data types such as meshes and point clouds\\nthat are not supported by ControlNet, enabling more flexible and versatile\\nvideo generation. Additionally, it supports mixed conditional inputs and\\nenables style transfer and editing via LoRA and text prompts. Extensive\\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\\nand provides a new perspective in spatial- and motion-controlled video\\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02857.png', 'numComments': 1, 'submittedBy': {'_id': '67ff29ecbf6889a333c69c7a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png', 'fullname': 'Henghui Ding', 'name': 'HenghuiDing', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.12462', 'authors': [{'_id': '68785eb6001546c83aa4f95b', 'name': 'Yuxi Xiao', 'hidden': False}, {'_id': '68785eb6001546c83aa4f95c', 'user': {'_id': '649bf403fd9cea8366d669ad', 'avatarUrl': '/avatars/27bd8ca9a948ec38fee950b64f669ce3.svg', 'isPro': False, 'fullname': 'Jianyuan Wang', 'user': 'JianyuanWang', 'type': 'user'}, 'name': 'Jianyuan Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:37:32.434Z', 'hidden': False}, {'_id': '68785eb6001546c83aa4f95d', 'user': {'_id': '6485ce5ec7f19728a49df17a', 'avatarUrl': '/avatars/e83966e6906c1d0f151300981e30f85a.svg', 'isPro': True, 'fullname': 'Nan', 'user': 'cherubicxn', 'type': 'user'}, 'name': 'Nan Xue', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:15:41.525Z', 'hidden': False}, {'_id': '68785eb6001546c83aa4f95e', 'user': {'_id': '6393a73584c565d2c3416cb9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6393a73584c565d2c3416cb9/OGtX-i-_MLg5--qA054ti.jpeg', 'isPro': True, 'fullname': 'Nikita Karaev', 'user': 'nikkar', 'type': 'user'}, 'name': 'Nikita Karaev', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:40:30.784Z', 'hidden': False}, {'_id': '68785eb6001546c83aa4f95f', 'name': 'Yuri Makarov', 'hidden': False}, {'_id': '68785eb6001546c83aa4f960', 'user': {'_id': '647b5fef6a79fbf5e996c47c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/647b5fef6a79fbf5e996c47c/IkSMnDsCY_CyEFCiMDuxe.jpeg', 'isPro': False, 'fullname': 'Bingyi Kang', 'user': 'bykang', 'type': 'user'}, 'name': 'Bingyi Kang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:40:39.614Z', 'hidden': False}, {'_id': '68785eb6001546c83aa4f961', 'name': 'Xing Zhu', 'hidden': False}, {'_id': '68785eb6001546c83aa4f962', 'name': 'Hujun Bao', 'hidden': False}, {'_id': '68785eb6001546c83aa4f963', 'name': 'Yujun Shen', 'hidden': False}, {'_id': '68785eb6001546c83aa4f964', 'name': 'Xiaowei Zhou', 'hidden': False}], 'publishedAt': '2025-07-16T17:59:03.000Z', 'submittedOnDailyAt': '2025-07-17T06:55:04.439Z', 'title': 'SpatialTrackerV2: 3D Point Tracking Made Easy', 'submittedOnDailyBy': {'_id': '6688a8f30bf195d6e53ac28d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6688a8f30bf195d6e53ac28d/5izbVmdCWWwA1wBjcxZPB.jpeg', 'isPro': True, 'fullname': 'Yuxi Xiao', 'user': 'Yuxihenry', 'type': 'user'}, 'summary': 'We present SpatialTrackerV2, a feed-forward 3D point tracking method for\\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\\ncomponents for 3D tracking, our approach unifies the intrinsic connections\\nbetween point tracking, monocular depth, and camera pose estimation into a\\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\\nwith a fully differentiable and end-to-end architecture, allowing scalable\\ntraining across a wide range of datasets, including synthetic sequences, posed\\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\\ndynamic 3D reconstruction approaches while running 50times faster.', 'upvotes': 4, 'discussionId': '68785eb7001546c83aa4f965', 'projectPage': 'https://spatialtracker.github.io', 'githubRepo': 'https://github.com/henry123-boy/SpaTrackerV2', 'ai_summary': 'SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos that integrates point tracking, monocular depth, and camera pose estimation into a unified, end-to-end architecture, achieving high performance and speed.', 'ai_keywords': ['feed-forward', '3D point tracking', 'monocular videos', 'intrinsic connections', 'monocular depth', 'camera pose estimation', 'fully differentiable', 'end-to-end architecture', 'scene geometry', 'camera ego-motion', 'pixel-wise object motion', 'synthetic sequences', 'posed RGB-D videos', 'unlabeled in-the-wild footage', 'dynamic 3D reconstruction'], 'githubStars': 482}, 'publishedAt': '2025-07-16T13:59:03.000Z', 'title': 'SpatialTrackerV2: 3D Point Tracking Made Easy', 'summary': 'We present SpatialTrackerV2, a feed-forward 3D point tracking method for\\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\\ncomponents for 3D tracking, our approach unifies the intrinsic connections\\nbetween point tracking, monocular depth, and camera pose estimation into a\\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\\nwith a fully differentiable and end-to-end architecture, allowing scalable\\ntraining across a wide range of datasets, including synthetic sequences, posed\\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\\ndynamic 3D reconstruction approaches while running 50times faster.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12462.png', 'numComments': 2, 'submittedBy': {'_id': '6688a8f30bf195d6e53ac28d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6688a8f30bf195d6e53ac28d/5izbVmdCWWwA1wBjcxZPB.jpeg', 'fullname': 'Yuxi Xiao', 'name': 'Yuxihenry', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 8}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.11764', 'authors': [{'_id': '6878a0ee001546c83aa4fa25', 'user': {'_id': '65c4aa3f08a42345687aaa3a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65c4aa3f08a42345687aaa3a/8HZT84z58rp6YMRyX_zha.jpeg', 'isPro': False, 'fullname': 'MatteoFasulo', 'user': 'MatteoFasulo', 'type': 'user'}, 'name': 'Matteo Fasulo', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:17:28.898Z', 'hidden': False}, {'_id': '6878a0ee001546c83aa4fa26', 'user': {'_id': '64cf700c749587dbe0172d39', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64cf700c749587dbe0172d39/VoZYUTI9SwfSCZJD64UtH.jpeg', 'isPro': False, 'fullname': 'Luca', 'user': 'ElectroDuck', 'type': 'user'}, 'name': 'Luca Babboni', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:17:26.160Z', 'hidden': False}, {'_id': '6878a0ee001546c83aa4fa27', 'user': {'_id': '647e3bab11084fb5831c4c51', 'avatarUrl': '/avatars/8126f7406f1516cbedfaff16b2617c6e.svg', 'isPro': False, 'fullname': 'Luca Tedeschini ', 'user': 'Luca289', 'type': 'user'}, 'name': 'Luca Tedeschini', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:17:31.749Z', 'hidden': False}], 'publishedAt': '2025-07-15T22:10:20.000Z', 'submittedOnDailyAt': '2025-07-17T11:10:51.050Z', 'title': 'AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings\\n  with Sentiment for Subjectivity Detection in News Articles', 'submittedOnDailyBy': {'_id': '65c4aa3f08a42345687aaa3a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65c4aa3f08a42345687aaa3a/8HZT84z58rp6YMRyX_zha.jpeg', 'isPro': False, 'fullname': 'MatteoFasulo', 'user': 'MatteoFasulo', 'type': 'user'}, 'summary': \"This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\\nTraining/development datasets were provided for Arabic, German, English,\\nItalian, and Bulgarian; final evaluation included additional unseen languages\\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\\nprimary strategy enhanced transformer-based classifiers by integrating\\nsentiment scores, derived from an auxiliary model, with sentence\\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\\nlanguages, we employed decision threshold calibration optimized on the\\ndevelopment set. Our experiments show sentiment feature integration\\nsignificantly boosts performance, especially subjective F1 score. This\\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).\", 'upvotes': 2, 'discussionId': '6878a0f0001546c83aa4fa28', 'githubRepo': 'https://github.com/MatteoFasulo/clef2025-checkthat', 'ai_summary': 'Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.', 'ai_keywords': ['transformer-based classifiers', 'sentiment scores', 'mDeBERTaV3-base', 'ModernBERT-base', 'Llama3.2-1B', 'decision threshold calibration', 'class imbalance', 'subjective F1 score', 'Macro F1'], 'githubStars': 2}, 'publishedAt': '2025-07-15T18:10:20.000Z', 'title': 'AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings\\n  with Sentiment for Subjectivity Detection in News Articles', 'summary': \"This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\\nTraining/development datasets were provided for Arabic, German, English,\\nItalian, and Bulgarian; final evaluation included additional unseen languages\\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\\nprimary strategy enhanced transformer-based classifiers by integrating\\nsentiment scores, derived from an auxiliary model, with sentence\\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\\nlanguages, we employed decision threshold calibration optimized on the\\ndevelopment set. Our experiments show sentiment feature integration\\nsignificantly boosts performance, especially subjective F1 score. This\\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11764.png', 'numComments': 1, 'submittedBy': {'_id': '65c4aa3f08a42345687aaa3a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65c4aa3f08a42345687aaa3a/8HZT84z58rp6YMRyX_zha.jpeg', 'fullname': 'MatteoFasulo', 'name': 'MatteoFasulo', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.09025', 'authors': [{'_id': '68786e45001546c83aa4f9a0', 'name': 'Chien Van Nguyen', 'hidden': False}, {'_id': '68786e45001546c83aa4f9a1', 'name': 'Ruiyi Zhang', 'hidden': False}, {'_id': '68786e45001546c83aa4f9a2', 'user': {'_id': '652767bfbdcf00b9b9ac9a74', 'avatarUrl': '/avatars/2cc8e9167562f364f0c25410f13a9d62.svg', 'isPro': False, 'fullname': 'Hanieh Deilamsalehy', 'user': 'haniehds', 'type': 'user'}, 'name': 'Hanieh Deilamsalehy', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:36:52.637Z', 'hidden': False}, {'_id': '68786e45001546c83aa4f9a3', 'name': 'Puneet Mathur', 'hidden': False}, {'_id': '68786e45001546c83aa4f9a4', 'name': 'Viet Dac Lai', 'hidden': False}, {'_id': '68786e45001546c83aa4f9a5', 'name': 'Haoliang Wang', 'hidden': False}, {'_id': '68786e45001546c83aa4f9a6', 'user': {'_id': '66d32a678819c81cce2052f4', 'avatarUrl': '/avatars/3a5b40ef9e9e73512743756d1c24ab6c.svg', 'isPro': False, 'fullname': 'Jayakumar Subramanian', 'user': 'jasubram', 'type': 'user'}, 'name': 'Jayakumar Subramanian', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:37:03.196Z', 'hidden': False}, {'_id': '68786e45001546c83aa4f9a7', 'name': 'Ryan A. Rossi', 'hidden': False}, {'_id': '68786e45001546c83aa4f9a8', 'user': {'_id': '67f1035155fe17a33dc71f23', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ohlRrzUI8VwvyYYxFXJuY.png', 'isPro': False, 'fullname': 'Trung Bui', 'user': 'TrungBui1111', 'type': 'user'}, 'name': 'Trung Bui', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:37:13.349Z', 'hidden': False}, {'_id': '68786e45001546c83aa4f9a9', 'user': {'_id': '675346f0ab1d47a36ca60b89', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mqWUUxAuFn3DsgIqoF_ah.png', 'isPro': False, 'fullname': 'Nikos Vlassis', 'user': 'Nikosapa', 'type': 'user'}, 'name': 'Nikos Vlassis', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:36:17.107Z', 'hidden': False}, {'_id': '68786e45001546c83aa4f9aa', 'user': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'isPro': False, 'fullname': 'Franck Dernoncourt', 'user': 'Franck-Dernoncourt', 'type': 'user'}, 'name': 'Franck Dernoncourt', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:14:23.916Z', 'hidden': False}, {'_id': '68786e45001546c83aa4f9ab', 'user': {'_id': '64804fad8c6a3b8f11f73912', 'avatarUrl': '/avatars/61e37a91d4bba35fda9bf52aadd87745.svg', 'isPro': False, 'fullname': 'Thien Huu Nguyen', 'user': 'anoperson', 'type': 'user'}, 'name': 'Thien Huu Nguyen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:36:11.546Z', 'hidden': False}], 'publishedAt': '2025-07-11T21:19:18.000Z', 'submittedOnDailyAt': '2025-07-17T02:00:25.332Z', 'title': 'Lizard: An Efficient Linearization Framework for Large Language Models', 'submittedOnDailyBy': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'isPro': False, 'fullname': 'Franck Dernoncourt', 'user': 'Franck-Dernoncourt', 'type': 'user'}, 'summary': \"We propose Lizard, a linearization framework that transforms pretrained\\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\\narchitectures for infinite-context generation. Transformer-based LLMs face\\nsignificant memory and computational bottlenecks as context lengths increase,\\ndue to the quadratic complexity of softmax attention and the growing key-value\\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\\nattention mechanism that closely approximates softmax attention while\\npreserving the output quality. Unlike previous linearization methods, which are\\noften limited by fixed model structures and therefore exclude gating\\nmechanisms, Lizard incorporates a gating module inspired by recent\\nstate-of-the-art linear models. This enables adaptive memory control, supports\\nconstant-memory inference, offers strong length generalization, and allows more\\nflexible model design. Lizard combines gated linear attention for global\\ncontext compression with sliding window attention enhanced by meta memory,\\nforming a hybrid mechanism that captures both long-range dependencies and\\nfine-grained local interactions. Moreover, we introduce a hardware-aware\\nalgorithm that accelerates the training speed of our models. Extensive\\nexperiments show that Lizard achieves near-lossless recovery of the teacher\\nmodel's performance across standard language modeling tasks, while\\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\\nbenchmark, Lizard improves over prior models by 18 points and shows significant\\nimprovements on associative recall tasks.\", 'upvotes': 2, 'discussionId': '68786e45001546c83aa4f9ac', 'ai_summary': 'Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.', 'ai_keywords': ['Transformer-based LLMs', 'subquadratic architectures', 'softmax attention', 'key-value (KV) cache', 'subquadratic attention mechanism', 'gating module', 'adaptive memory control', 'constant-memory inference', 'length generalization', 'gated linear attention', 'sliding window attention', 'meta memory', 'hardware-aware algorithm', 'MMLU benchmark', 'associative recall tasks']}, 'publishedAt': '2025-07-11T17:19:18.000Z', 'title': 'Lizard: An Efficient Linearization Framework for Large Language Models', 'summary': \"We propose Lizard, a linearization framework that transforms pretrained\\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\\narchitectures for infinite-context generation. Transformer-based LLMs face\\nsignificant memory and computational bottlenecks as context lengths increase,\\ndue to the quadratic complexity of softmax attention and the growing key-value\\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\\nattention mechanism that closely approximates softmax attention while\\npreserving the output quality. Unlike previous linearization methods, which are\\noften limited by fixed model structures and therefore exclude gating\\nmechanisms, Lizard incorporates a gating module inspired by recent\\nstate-of-the-art linear models. This enables adaptive memory control, supports\\nconstant-memory inference, offers strong length generalization, and allows more\\nflexible model design. Lizard combines gated linear attention for global\\ncontext compression with sliding window attention enhanced by meta memory,\\nforming a hybrid mechanism that captures both long-range dependencies and\\nfine-grained local interactions. Moreover, we introduce a hardware-aware\\nalgorithm that accelerates the training speed of our models. Extensive\\nexperiments show that Lizard achieves near-lossless recovery of the teacher\\nmodel's performance across standard language modeling tasks, while\\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\\nbenchmark, Lizard improves over prior models by 18 points and shows significant\\nimprovements on associative recall tasks.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09025.png', 'numComments': 1, 'submittedBy': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'fullname': 'Franck Dernoncourt', 'name': 'Franck-Dernoncourt', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 10}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2507.07451', 'authors': [{'_id': '68747204257d4f04353702de', 'user': {'_id': '6474b290d815855e4ef59b05', 'avatarUrl': '/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg', 'isPro': False, 'fullname': 'Hongzhi Zhang', 'user': 'hongzhizhang', 'type': 'user'}, 'name': 'Hongzhi Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-17T08:21:31.114Z', 'hidden': False}, {'_id': '68747204257d4f04353702df', 'name': 'Jia Fu', 'hidden': False}, {'_id': '68747204257d4f04353702e0', 'name': 'Jingyuan Zhang', 'hidden': False}, {'_id': '68747204257d4f04353702e1', 'name': 'Kai Fu', 'hidden': False}, {'_id': '68747204257d4f04353702e2', 'name': 'Qi Wang', 'hidden': False}, {'_id': '68747204257d4f04353702e3', 'user': {'_id': '67c5945da1661d5fa6f29adb', 'avatarUrl': '/avatars/62561f3875c0c251cae949acc38d72dc.svg', 'isPro': False, 'fullname': 'Fuzheng Zhang', 'user': 'Edrex', 'type': 'user'}, 'name': 'Fuzheng Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:35:13.823Z', 'hidden': False}, {'_id': '68747204257d4f04353702e4', 'user': {'_id': '67c6c570cf87e2d2ebfc81aa', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/67c6c570cf87e2d2ebfc81aa/7qAstZtIT86Uwrz3u_anv.jpeg', 'isPro': False, 'fullname': 'Guorui Zhou', 'user': 'GuoruiZhou', 'type': 'user'}, 'name': 'Guorui Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-07-17T09:34:55.007Z', 'hidden': False}], 'publishedAt': '2025-07-10T05:58:55.000Z', 'submittedOnDailyAt': '2025-07-17T07:36:19.087Z', 'title': 'RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning', 'submittedOnDailyBy': {'_id': '6474b290d815855e4ef59b05', 'avatarUrl': '/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg', 'isPro': False, 'fullname': 'Hongzhi Zhang', 'user': 'hongzhizhang', 'type': 'user'}, 'summary': 'Reinforcement learning (RL) for large language models is an energy-intensive\\nendeavor: training can be unstable, and the policy may gradually drift away\\nfrom its pretrained weights. We present RLEP\\\\, -- \\\\,Reinforcement\\nLearning with Experience rePlay\\\\, -- \\\\,a two-phase framework that first\\ncollects verified trajectories and then replays them during subsequent\\ntraining. At every update step, the policy is optimized on mini-batches that\\nblend newly generated rollouts with these replayed successes. By replaying\\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\\nfocuses learning on promising reasoning paths, and delivers both faster\\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\\nRLEP reaches baseline peak accuracy with substantially fewer updates and\\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\\ncode, datasets, and checkpoints are publicly available at\\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\\nresearch.', 'upvotes': 2, 'discussionId': '68747204257d4f04353702e5', 'githubRepo': 'https://github.com/Kwai-Klear/RLEP', 'ai_summary': 'RLEP, a reinforcement learning framework with experience replay, enhances large language model training by focusing on high-quality examples, leading to faster convergence and improved performance on math-related benchmarks.', 'ai_keywords': ['reinforcement learning', 'experience replay', 'trajectories', 'rollouts', 'policy optimization', 'convergence', 'performance', 'Qwen2.5-Math-7B', 'AIME-2024', 'AIME-2025', 'AMC-2023'], 'githubStars': 10}, 'publishedAt': '2025-07-10T01:58:55.000Z', 'title': 'RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning', 'summary': 'Reinforcement learning (RL) for large language models is an energy-intensive\\nendeavor: training can be unstable, and the policy may gradually drift away\\nfrom its pretrained weights. We present RLEP\\\\, -- \\\\,Reinforcement\\nLearning with Experience rePlay\\\\, -- \\\\,a two-phase framework that first\\ncollects verified trajectories and then replays them during subsequent\\ntraining. At every update step, the policy is optimized on mini-batches that\\nblend newly generated rollouts with these replayed successes. By replaying\\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\\nfocuses learning on promising reasoning paths, and delivers both faster\\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\\nRLEP reaches baseline peak accuracy with substantially fewer updates and\\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\\ncode, datasets, and checkpoints are publicly available at\\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\\nresearch.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07451.png', 'numComments': 1, 'submittedBy': {'_id': '6474b290d815855e4ef59b05', 'avatarUrl': '/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg', 'fullname': 'Hongzhi Zhang', 'name': 'hongzhizhang', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.05065', 'authors': [{'_id': '68776e57ff8f47a7f86442bd', 'user': {'_id': '669e707ec517d804cfce91c5', 'avatarUrl': '/avatars/fc18e64e100cbf28763db04b44242747.svg', 'isPro': False, 'fullname': 'Corrado Rainone', 'user': 'crainone', 'type': 'user'}, 'name': 'Corrado Rainone', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-16T15:05:55.722Z', 'hidden': False}, {'_id': '68776e57ff8f47a7f86442be', 'name': 'Tim Bakker', 'hidden': False}, {'_id': '68776e57ff8f47a7f86442bf', 'name': 'Roland Memisevic', 'hidden': False}], 'publishedAt': '2025-07-07T14:49:18.000Z', 'submittedOnDailyAt': '2025-07-17T06:25:52.334Z', 'title': 'Replacing thinking with tool usage enables reasoning in small language\\n  models', 'submittedOnDailyBy': {'_id': '669e707ec517d804cfce91c5', 'avatarUrl': '/avatars/fc18e64e100cbf28763db04b44242747.svg', 'isPro': False, 'fullname': 'Corrado Rainone', 'user': 'crainone', 'type': 'user'}, 'summary': 'Recent advances have established a new machine learning paradigm based on\\nscaling up compute at inference time as well as at training time. In that line\\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\\nused for training Large Language Models to expend extra compute during\\ninference in the form of \"thoughts\" expressed in natural language. In this\\npaper, we propose to instead format these tokens as a multi-turn interaction\\ntrace with a stateful tool. At each turn, the new state of the tool is appended\\nto the context of the model, whose job is to generate the tokens necessary to\\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\\nrepairing malfunctioning Python code, and show that this constrained setup\\nallows for faster sampling of experience and a denser reward signal, allowing\\neven models of size up to 3B parameters to learn how to proficiently expend\\nadditional compute on the task.', 'upvotes': 2, 'discussionId': '68776e58ff8f47a7f86442c0', 'ai_summary': 'A new approach formats tokens as a multi-turn interaction trace with a stateful tool for training Large Language Models, enabling faster sampling and denser reward signals for tasks like repairing Python code.', 'ai_keywords': ['Supervised Fine-Tuning', 'Reinforcement Learning with Verifiable Rewards', 'Large Language Models', 'multi-turn interaction trace', 'stateful tool', 'custom DSL']}, 'publishedAt': '2025-07-07T10:49:18.000Z', 'title': 'Replacing thinking with tool usage enables reasoning in small language\\n  models', 'summary': 'Recent advances have established a new machine learning paradigm based on\\nscaling up compute at inference time as well as at training time. In that line\\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\\nused for training Large Language Models to expend extra compute during\\ninference in the form of \"thoughts\" expressed in natural language. In this\\npaper, we propose to instead format these tokens as a multi-turn interaction\\ntrace with a stateful tool. At each turn, the new state of the tool is appended\\nto the context of the model, whose job is to generate the tokens necessary to\\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\\nrepairing malfunctioning Python code, and show that this constrained setup\\nallows for faster sampling of experience and a denser reward signal, allowing\\neven models of size up to 3B parameters to learn how to proficiently expend\\nadditional compute on the task.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05065.png', 'numComments': 1, 'submittedBy': {'_id': '669e707ec517d804cfce91c5', 'avatarUrl': '/avatars/fc18e64e100cbf28763db04b44242747.svg', 'fullname': 'Corrado Rainone', 'name': 'crainone', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}"
]