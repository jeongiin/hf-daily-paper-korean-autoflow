[
    "{'paper': {'id': '2507.22827', 'authors': [{'_id': '688abfb98b724c8c7187dd3c', 'name': 'Yilei Jiang', 'hidden': False}, {'_id': '688abfb98b724c8c7187dd3d', 'name': 'Yaozhi Zheng', 'hidden': False}, {'_id': '688abfb98b724c8c7187dd3e', 'name': 'Yuxuan Wan', 'hidden': False}, {'_id': '688abfb98b724c8c7187dd3f', 'user': {'_id': '62318c0386753f5f41d0e261', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg', 'isPro': False, 'fullname': 'Jiaming Han', 'user': 'csuhan', 'type': 'user'}, 'name': 'Jiaming Han', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-31T08:15:11.876Z', 'hidden': False}, {'_id': '688abfb98b724c8c7187dd40', 'name': 'Qunzhong Wang', 'hidden': False}, {'_id': '688abfb98b724c8c7187dd41', 'name': 'Michael R. Lyu', 'hidden': False}, {'_id': '688abfb98b724c8c7187dd42', 'name': 'Xiangyu Yue', 'hidden': False}], 'publishedAt': '2025-07-30T16:41:21.000Z', 'submittedOnDailyAt': '2025-07-31T01:37:19.365Z', 'title': 'ScreenCoder: Advancing Visual-to-Code Generation for Front-End\\n  Automation via Modular Multimodal Agents', 'submittedOnDailyBy': {'_id': '62318c0386753f5f41d0e261', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg', 'isPro': False, 'fullname': 'Jiaming Han', 'user': 'csuhan', 'type': 'user'}, 'summary': 'Automating the transformation of user interface (UI) designs into front-end\\ncode holds significant promise for accelerating software development and\\ndemocratizing design workflows. While recent large language models (LLMs) have\\ndemonstrated progress in text-to-code generation, many existing approaches rely\\nsolely on natural language prompts, limiting their effectiveness in capturing\\nspatial layout and visual design intent. In contrast, UI development in\\npractice is inherently multimodal, often starting from visual sketches or\\nmockups. To address this gap, we introduce a modular multi-agent framework that\\nperforms UI-to-code generation in three interpretable stages: grounding,\\nplanning, and generation. The grounding agent uses a vision-language model to\\ndetect and label UI components, the planning agent constructs a hierarchical\\nlayout using front-end engineering priors, and the generation agent produces\\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\\nFurthermore, we extend the framework into a scalable data engine that\\nautomatically produces large-scale image-code pairs. Using these synthetic\\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\\nin UI understanding and code quality. Extensive experiments demonstrate that\\nour approach achieves state-of-the-art performance in layout accuracy,\\nstructural coherence, and code correctness. Our code is made publicly available\\nat https://github.com/leigest519/ScreenCoder.', 'upvotes': 55, 'discussionId': '688abfb98b724c8c7187dd43', 'projectPage': 'https://huggingface.co/spaces/Jimmyzheng-10/ScreenCoder', 'githubRepo': 'https://github.com/leigest519/ScreenCoder', 'ai_summary': 'A modular multi-agent framework improves UI-to-code generation by integrating vision-language models, hierarchical layout planning, and adaptive prompt-based synthesis, achieving state-of-the-art performance.', 'ai_keywords': ['vision-language model', 'hierarchical layout', 'adaptive prompt-based synthesis', 'UI-to-code generation', 'multimodal', 'grounding agent', 'planning agent', 'generation agent', 'data engine', 'fine-tuning', 'reinforcement'], 'githubStars': 84}, 'publishedAt': '2025-07-30T12:41:21.000Z', 'title': 'ScreenCoder: Advancing Visual-to-Code Generation for Front-End\\n  Automation via Modular Multimodal Agents', 'summary': 'Automating the transformation of user interface (UI) designs into front-end\\ncode holds significant promise for accelerating software development and\\ndemocratizing design workflows. While recent large language models (LLMs) have\\ndemonstrated progress in text-to-code generation, many existing approaches rely\\nsolely on natural language prompts, limiting their effectiveness in capturing\\nspatial layout and visual design intent. In contrast, UI development in\\npractice is inherently multimodal, often starting from visual sketches or\\nmockups. To address this gap, we introduce a modular multi-agent framework that\\nperforms UI-to-code generation in three interpretable stages: grounding,\\nplanning, and generation. The grounding agent uses a vision-language model to\\ndetect and label UI components, the planning agent constructs a hierarchical\\nlayout using front-end engineering priors, and the generation agent produces\\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\\nFurthermore, we extend the framework into a scalable data engine that\\nautomatically produces large-scale image-code pairs. Using these synthetic\\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\\nin UI understanding and code quality. Extensive experiments demonstrate that\\nour approach achieves state-of-the-art performance in layout accuracy,\\nstructural coherence, and code correctness. Our code is made publicly available\\nat https://github.com/leigest519/ScreenCoder.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22827.png', 'numComments': 2, 'submittedBy': {'_id': '62318c0386753f5f41d0e261', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg', 'fullname': 'Jiaming Han', 'name': 'csuhan', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 21}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.21493', 'authors': [{'_id': '688ad3c18b724c8c7187dd67', 'user': {'_id': '636d12455aaed143cd665607', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1679399015950-636d12455aaed143cd665607.png', 'isPro': False, 'fullname': 'ZLW', 'user': 'ZarkLngeW', 'type': 'user'}, 'name': 'Longwen Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-31T08:15:02.207Z', 'hidden': False}, {'_id': '688ad3c18b724c8c7187dd68', 'name': 'Qixuan Zhang', 'hidden': False}, {'_id': '688ad3c18b724c8c7187dd69', 'name': 'Haoran Jiang', 'hidden': False}, {'_id': '688ad3c18b724c8c7187dd6a', 'name': 'Yinuo Bai', 'hidden': False}, {'_id': '688ad3c18b724c8c7187dd6b', 'name': 'Wei Yang', 'hidden': False}, {'_id': '688ad3c18b724c8c7187dd6c', 'name': 'Lan Xu', 'hidden': False}, {'_id': '688ad3c18b724c8c7187dd6d', 'name': 'Jingyi Yu', 'hidden': False}], 'publishedAt': '2025-07-29T04:21:21.000Z', 'submittedOnDailyAt': '2025-07-31T00:56:30.360Z', 'title': 'BANG: Dividing 3D Assets via Generative Exploded Dynamics', 'submittedOnDailyBy': {'_id': '636d12455aaed143cd665607', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1679399015950-636d12455aaed143cd665607.png', 'isPro': False, 'fullname': 'ZLW', 'user': 'ZarkLngeW', 'type': 'user'}, 'summary': '3D creation has always been a unique human strength, driven by our ability to\\ndeconstruct and reassemble objects using our eyes, mind and hand. However,\\ncurrent 3D design tools struggle to replicate this natural process, requiring\\nconsiderable artistic expertise and manual labor. This paper introduces BANG, a\\nnovel generative approach that bridges 3D generation and reasoning, allowing\\nfor intuitive and flexible part-level decomposition of 3D objects. At the heart\\nof BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of\\nexploded states for an input geometry, progressively separating parts while\\npreserving their geometric and semantic coherence.\\n  BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned\\nfor exploded dynamics with a lightweight exploded view adapter, allowing\\nprecise control over the decomposition process. It also incorporates a temporal\\nattention module to ensure smooth transitions and consistency across time. BANG\\nenhances control with spatial prompts, such as bounding boxes and surface\\nregions, enabling users to specify which parts to decompose and how. This\\ninteraction can be extended with multimodal models like GPT-4, enabling\\n2D-to-3D manipulations for more intuitive and creative workflows.\\n  The capabilities of BANG extend to generating detailed part-level geometry,\\nassociating parts with functional descriptions, and facilitating\\ncomponent-aware 3D creation and manufacturing workflows. Additionally, BANG\\noffers applications in 3D printing, where separable parts are generated for\\neasy printing and reassembly. In essence, BANG enables seamless transformation\\nfrom imaginative concepts to detailed 3D assets, offering a new perspective on\\ncreation that resonates with human intuition.', 'upvotes': 39, 'discussionId': '688ad3c18b724c8c7187dd6e', 'projectPage': 'https://sites.google.com/view/bang7355608', 'ai_summary': 'BANG is a generative approach that uses latent diffusion models and temporal attention to enable intuitive part-level decomposition and manipulation of 3D objects, enhancing 3D creation workflows.', 'ai_keywords': ['Generative Exploded Dynamics', 'latent diffusion model', 'exploded view adapter', 'temporal attention module', 'spatial prompts', 'multimodal models', 'GPT-4', '2D-to-3D manipulations', 'component-aware 3D creation', '3D printing']}, 'publishedAt': '2025-07-29T00:21:21.000Z', 'title': 'BANG: Dividing 3D Assets via Generative Exploded Dynamics', 'summary': '3D creation has always been a unique human strength, driven by our ability to\\ndeconstruct and reassemble objects using our eyes, mind and hand. However,\\ncurrent 3D design tools struggle to replicate this natural process, requiring\\nconsiderable artistic expertise and manual labor. This paper introduces BANG, a\\nnovel generative approach that bridges 3D generation and reasoning, allowing\\nfor intuitive and flexible part-level decomposition of 3D objects. At the heart\\nof BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of\\nexploded states for an input geometry, progressively separating parts while\\npreserving their geometric and semantic coherence.\\n  BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned\\nfor exploded dynamics with a lightweight exploded view adapter, allowing\\nprecise control over the decomposition process. It also incorporates a temporal\\nattention module to ensure smooth transitions and consistency across time. BANG\\nenhances control with spatial prompts, such as bounding boxes and surface\\nregions, enabling users to specify which parts to decompose and how. This\\ninteraction can be extended with multimodal models like GPT-4, enabling\\n2D-to-3D manipulations for more intuitive and creative workflows.\\n  The capabilities of BANG extend to generating detailed part-level geometry,\\nassociating parts with functional descriptions, and facilitating\\ncomponent-aware 3D creation and manufacturing workflows. Additionally, BANG\\noffers applications in 3D printing, where separable parts are generated for\\neasy printing and reassembly. In essence, BANG enables seamless transformation\\nfrom imaginative concepts to detailed 3D assets, offering a new perspective on\\ncreation that resonates with human intuition.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21493.png', 'numComments': 2, 'submittedBy': {'_id': '636d12455aaed143cd665607', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1679399015950-636d12455aaed143cd665607.png', 'fullname': 'ZLW', 'name': 'ZarkLngeW', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.22448', 'authors': [{'_id': '688adbb28b724c8c7187dd70', 'user': {'_id': '6460c3811db65f878513bcaf', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg', 'isPro': False, 'fullname': 'Jingwei Zuo', 'user': 'JingweiZuo', 'type': 'user'}, 'name': 'Jingwei Zuo', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-31T08:14:55.282Z', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd71', 'name': 'Maksim Velikanov', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd72', 'name': 'Ilyas Chahed', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd73', 'user': {'_id': '62441d1d9fdefb55a0b7d12c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png', 'isPro': False, 'fullname': 'Younes B', 'user': 'ybelkada', 'type': 'user'}, 'name': 'Younes Belkada', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-31T08:14:53.389Z', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd74', 'name': 'Dhia Eddine Rhayem', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd75', 'name': 'Guillaume Kunsch', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd76', 'name': 'Hakim Hacid', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd77', 'name': 'Hamza Yous', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd78', 'user': {'_id': '664c3101d1ba9237d34ae972', 'avatarUrl': '/avatars/a830c0ee4c95571419770f1ffb41ef11.svg', 'isPro': False, 'fullname': 'BrahimFarhat', 'user': 'ifarhat1993', 'type': 'user'}, 'name': 'Brahim Farhat', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-31T08:14:59.117Z', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd79', 'name': 'Ibrahim Khadraoui', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd7a', 'name': 'Mugariya Farooq', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd7b', 'name': 'Giulia Campesan', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd7c', 'name': 'Ruxandra Cojocaru', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd7d', 'name': 'Yasser Djilali', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd7e', 'name': 'Shi Hu', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd7f', 'user': {'_id': '660bd11884eca4537c4aeedd', 'avatarUrl': '/avatars/10fab6ec552f3d34ceac2bf01df32975.svg', 'isPro': False, 'fullname': 'Iheb Chaabane', 'user': 'Iheb-Chaabane', 'type': 'user'}, 'name': 'Iheb Chaabane', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-31T08:14:57.046Z', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd80', 'name': 'Puneesh Khanna', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd81', 'name': 'Mohamed El Amine Seddik', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd82', 'name': 'Ngoc Dung Huynh', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd83', 'name': 'Phuc Le Khac', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd84', 'name': 'Leen AlQadi', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd85', 'name': 'Billel Mokeddem', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd86', 'name': 'Mohamed Chami', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd87', 'name': 'Abdalgader Abubaker', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd88', 'name': 'Mikhail Lubinets', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd89', 'name': 'Kacper Piskorski', 'hidden': False}, {'_id': '688adbb28b724c8c7187dd8a', 'name': 'Slim Frikha', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6460c3811db65f878513bcaf/qdWzn2RRJ-qgCk6E3H4-u.png'], 'publishedAt': '2025-07-30T07:55:33.000Z', 'submittedOnDailyAt': '2025-07-31T01:30:54.022Z', 'title': 'Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\\n  and Performance', 'submittedOnDailyBy': {'_id': '6460c3811db65f878513bcaf', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg', 'isPro': False, 'fullname': 'Jingwei Zuo', 'user': 'JingweiZuo', 'type': 'user'}, 'summary': 'In this report, we introduce Falcon-H1, a new series of large language models\\n(LLMs) featuring hybrid architecture designs optimized for both high\\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\\nparallel hybrid approach that combines Transformer-based attention with State\\nSpace Models (SSMs), known for superior long-context memory and computational\\nefficiency. We systematically revisited model design, data strategy, and\\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\\nis released in multiple configurations, including base and instruction-tuned\\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\\ninstruction-tuned models are also available, totaling over 30 checkpoints on\\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\\nThese models excel across reasoning, mathematics, multilingual tasks,\\ninstruction following, and scientific knowledge. With support for up to 256K\\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\\napplications. All models are released under a permissive open-source license,\\nunderscoring our commitment to accessible and impactful AI research.', 'upvotes': 36, 'discussionId': '688adbb28b724c8c7187dd8b', 'projectPage': 'https://tiiuae.github.io/Falcon-H1/', 'githubRepo': 'https://github.com/tiiuae/Falcon-H1/', 'ai_summary': 'Falcon-H1, a new series of large language models with a hybrid architecture combining Transformer-based attention and State Space Models, achieves state-of-the-art performance and efficiency across various tasks and sizes.', 'ai_keywords': ['large language models', 'hybrid architecture', 'Transformer-based attention', 'State Space Models', 'long-context memory', 'computational efficiency', 'model design', 'data strategy', 'training dynamics', 'instruction-tuned', 'quantized models', 'parameter efficiency', 'training efficiency', 'context tokens', 'multilingual tasks', 'instruction following', 'scientific knowledge', 'open-source license'], 'githubStars': 48}, 'publishedAt': '2025-07-30T03:55:33.000Z', 'title': 'Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\\n  and Performance', 'summary': 'In this report, we introduce Falcon-H1, a new series of large language models\\n(LLMs) featuring hybrid architecture designs optimized for both high\\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\\nparallel hybrid approach that combines Transformer-based attention with State\\nSpace Models (SSMs), known for superior long-context memory and computational\\nefficiency. We systematically revisited model design, data strategy, and\\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\\nis released in multiple configurations, including base and instruction-tuned\\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\\ninstruction-tuned models are also available, totaling over 30 checkpoints on\\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\\nThese models excel across reasoning, mathematics, multilingual tasks,\\ninstruction following, and scientific knowledge. With support for up to 256K\\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\\napplications. All models are released under a permissive open-source license,\\nunderscoring our commitment to accessible and impactful AI research.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6460c3811db65f878513bcaf/qdWzn2RRJ-qgCk6E3H4-u.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22448.png', 'numComments': 2, 'submittedBy': {'_id': '6460c3811db65f878513bcaf', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg', 'fullname': 'Jingwei Zuo', 'name': 'JingweiZuo', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 32}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.22607', 'authors': [{'_id': '688b17b78b724c8c7187de81', 'name': 'Ruifeng Yuan', 'hidden': False}, {'_id': '688b17b78b724c8c7187de82', 'name': 'Chenghao Xiao', 'hidden': False}, {'_id': '688b17b78b724c8c7187de83', 'name': 'Sicong Leng', 'hidden': False}, {'_id': '688b17b78b724c8c7187de84', 'name': 'Jianyu Wang', 'hidden': False}, {'_id': '688b17b78b724c8c7187de85', 'name': 'Long Li', 'hidden': False}, {'_id': '688b17b78b724c8c7187de86', 'name': 'Weiwen Xu', 'hidden': False}, {'_id': '688b17b78b724c8c7187de87', 'user': {'_id': '604f67ef0fe8ff3ec13d71ef', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png', 'isPro': False, 'fullname': 'Hou Pong (Ken) Chan', 'user': 'kenchan0226', 'type': 'user'}, 'name': 'Hou Pong Chan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-31T08:14:42.264Z', 'hidden': False}, {'_id': '688b17b78b724c8c7187de88', 'name': 'Deli Zhao', 'hidden': False}, {'_id': '688b17b78b724c8c7187de89', 'name': 'Tingyang Xu', 'hidden': False}, {'_id': '688b17b78b724c8c7187de8a', 'name': 'Zhongyu Wei', 'hidden': False}, {'_id': '688b17b78b724c8c7187de8b', 'user': {'_id': '64b7cd74ff6d81ae297feded', 'avatarUrl': '/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg', 'isPro': False, 'fullname': 'ZHANG HAO', 'user': '26hzhang', 'type': 'user'}, 'name': 'Hao Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-31T08:14:40.297Z', 'hidden': False}, {'_id': '688b17b78b724c8c7187de8c', 'name': 'Yu Rong', 'hidden': False}], 'publishedAt': '2025-07-30T12:23:21.000Z', 'submittedOnDailyAt': '2025-07-31T05:46:56.595Z', 'title': 'VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced\\n  Multimodal Reasoning', 'submittedOnDailyBy': {'_id': '604f67ef0fe8ff3ec13d71ef', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png', 'isPro': False, 'fullname': 'Hou Pong (Ken) Chan', 'user': 'kenchan0226', 'type': 'user'}, 'summary': 'Reinforcement learning has proven its effectiveness in enhancing the\\nreasoning capabilities of large language models. Recent research efforts have\\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\\ninherent complexity and diversity of multimodal tasks, especially in semantic\\ncontent and problem formulations, existing models often exhibit unstable\\nperformance across various domains and difficulty levels. To address these\\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\\ngradually increasing difficulty, substantially improving its reasoning\\nabilities across diverse multimodal contexts. The framework introduces two key\\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\\nadjusting training difficulty across successive RL training stages; and (2) a\\ndynamic length reward mechanism, which encourages the model to adaptively\\nregulate its reasoning path length according to task complexity, thus balancing\\nreasoning efficiency with correctness. Experimental evaluations demonstrate\\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\\nlogic, and general understanding, validating the effectiveness of our approach.', 'upvotes': 25, 'discussionId': '688b17b88b724c8c7187de8d', 'ai_summary': 'VL-Cogito, a multimodal reasoning model, uses a Progressive Curriculum Reinforcement Learning framework to improve performance across diverse tasks by dynamically adjusting difficulty and reasoning path length.', 'ai_keywords': ['reinforcement learning', 'multimodal reasoning', 'Progressive Curriculum Reinforcement Learning', 'PCuRL', 'online difficulty soft weighting', 'dynamic length reward mechanism']}, 'publishedAt': '2025-07-30T08:23:21.000Z', 'title': 'VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced\\n  Multimodal Reasoning', 'summary': 'Reinforcement learning has proven its effectiveness in enhancing the\\nreasoning capabilities of large language models. Recent research efforts have\\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\\ninherent complexity and diversity of multimodal tasks, especially in semantic\\ncontent and problem formulations, existing models often exhibit unstable\\nperformance across various domains and difficulty levels. To address these\\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\\ngradually increasing difficulty, substantially improving its reasoning\\nabilities across diverse multimodal contexts. The framework introduces two key\\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\\nadjusting training difficulty across successive RL training stages; and (2) a\\ndynamic length reward mechanism, which encourages the model to adaptively\\nregulate its reasoning path length according to task complexity, thus balancing\\nreasoning efficiency with correctness. Experimental evaluations demonstrate\\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\\nlogic, and general understanding, validating the effectiveness of our approach.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22607.png', 'numComments': 1, 'submittedBy': {'_id': '604f67ef0fe8ff3ec13d71ef', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png', 'fullname': 'Hou Pong (Ken) Chan', 'name': 'kenchan0226', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.20976', 'authors': [{'_id': '6888dca266e37b2a6b291f63', 'user': {'_id': '6697d30fbfec78c1bf6a4962', 'avatarUrl': '/avatars/17cfd7d7eb2b89b5051bd912e41e5a62.svg', 'isPro': False, 'fullname': 'Xiao Fang', 'user': 'xiaofanghf', 'type': 'user'}, 'name': 'Xiao Fang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-30T09:05:02.766Z', 'hidden': False}, {'_id': '6888dca266e37b2a6b291f64', 'user': {'_id': '65b53aa41e45f6ee8bc63e28', 'avatarUrl': '/avatars/09ee1059cbb3e48f4fb30d84e8c4e809.svg', 'isPro': False, 'fullname': 'Minhyek Jeon', 'user': 'Min0326', 'type': 'user'}, 'name': 'Minhyek Jeon', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-31T08:16:05.128Z', 'hidden': False}, {'_id': '6888dca266e37b2a6b291f65', 'name': 'Zheyang Qin', 'hidden': False}, {'_id': '6888dca266e37b2a6b291f66', 'name': 'Stanislav Panev', 'hidden': False}, {'_id': '6888dca266e37b2a6b291f67', 'name': 'Celso de Melo', 'hidden': False}, {'_id': '6888dca266e37b2a6b291f68', 'name': 'Shuowen Hu', 'hidden': False}, {'_id': '6888dca266e37b2a6b291f69', 'name': 'Shayok Chakraborty', 'hidden': False}, {'_id': '6888dca266e37b2a6b291f6a', 'name': 'Fernando De la Torre', 'hidden': False}], 'publishedAt': '2025-07-28T16:38:06.000Z', 'submittedOnDailyAt': '2025-07-31T02:46:45.983Z', 'title': 'Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with\\n  Weak Supervision', 'submittedOnDailyBy': {'_id': '6697d30fbfec78c1bf6a4962', 'avatarUrl': '/avatars/17cfd7d7eb2b89b5051bd912e41e5a62.svg', 'isPro': False, 'fullname': 'Xiao Fang', 'user': 'xiaofanghf', 'type': 'user'}, 'summary': 'Detecting vehicles in aerial imagery is a critical task with applications in\\ntraffic monitoring, urban planning, and defense intelligence. Deep learning\\nmethods have provided state-of-the-art (SOTA) results for this application.\\nHowever, a significant challenge arises when models trained on data from one\\ngeographic region fail to generalize effectively to other areas. Variability in\\nfactors such as environmental conditions, urban layouts, road networks, vehicle\\ntypes, and image acquisition parameters (e.g., resolution, lighting, and angle)\\nleads to domain shifts that degrade model performance. This paper proposes a\\nnovel method that uses generative AI to synthesize high-quality aerial images\\nand their labels, improving detector training through data augmentation. Our\\nkey contribution is the development of a multi-stage, multi-modal knowledge\\ntransfer framework utilizing fine-tuned latent diffusion models (LDMs) to\\nmitigate the distribution gap between the source and target environments.\\nExtensive experiments across diverse aerial imagery domains show consistent\\nperformance improvements in AP50 over supervised learning on source domain\\ndata, weakly supervised adaptation methods, unsupervised domain adaptation\\nmethods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than\\n50%, respectively. Furthermore, we introduce two newly annotated aerial\\ndatasets from New Zealand and Utah to support further research in this field.\\nProject page is available at: https://humansensinglab.github.io/AGenDA', 'upvotes': 7, 'discussionId': '6888dca366e37b2a6b291f6b', 'projectPage': 'https://humansensinglab.github.io/AGenDA', 'githubRepo': 'https://github.com/humansensinglab/AGenDA', 'ai_summary': 'A multi-stage, multi-modal knowledge transfer framework using fine-tuned latent diffusion models improves vehicle detection in aerial imagery across different domains.', 'ai_keywords': ['latent diffusion models', 'data augmentation', 'domain adaptation', 'aerial imagery', 'vehicle detection', 'distribution gap', 'knowledge transfer', 'AP50', 'open-set object detectors'], 'githubStars': 3}, 'publishedAt': '2025-07-28T12:38:06.000Z', 'title': 'Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with\\n  Weak Supervision', 'summary': 'Detecting vehicles in aerial imagery is a critical task with applications in\\ntraffic monitoring, urban planning, and defense intelligence. Deep learning\\nmethods have provided state-of-the-art (SOTA) results for this application.\\nHowever, a significant challenge arises when models trained on data from one\\ngeographic region fail to generalize effectively to other areas. Variability in\\nfactors such as environmental conditions, urban layouts, road networks, vehicle\\ntypes, and image acquisition parameters (e.g., resolution, lighting, and angle)\\nleads to domain shifts that degrade model performance. This paper proposes a\\nnovel method that uses generative AI to synthesize high-quality aerial images\\nand their labels, improving detector training through data augmentation. Our\\nkey contribution is the development of a multi-stage, multi-modal knowledge\\ntransfer framework utilizing fine-tuned latent diffusion models (LDMs) to\\nmitigate the distribution gap between the source and target environments.\\nExtensive experiments across diverse aerial imagery domains show consistent\\nperformance improvements in AP50 over supervised learning on source domain\\ndata, weakly supervised adaptation methods, unsupervised domain adaptation\\nmethods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than\\n50%, respectively. Furthermore, we introduce two newly annotated aerial\\ndatasets from New Zealand and Utah to support further research in this field.\\nProject page is available at: https://humansensinglab.github.io/AGenDA', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20976.png', 'numComments': 2, 'submittedBy': {'_id': '6697d30fbfec78c1bf6a4962', 'avatarUrl': '/avatars/17cfd7d7eb2b89b5051bd912e41e5a62.svg', 'fullname': 'Xiao Fang', 'name': 'xiaofanghf', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.22886', 'authors': [{'_id': '688ac9788b724c8c7187dd54', 'name': 'Kaining Ying', 'hidden': False}, {'_id': '688ac9788b724c8c7187dd55', 'name': 'Henghui Ding', 'hidden': False}, {'_id': '688ac9788b724c8c7187dd56', 'name': 'Guanquan Jie', 'hidden': False}, {'_id': '688ac9788b724c8c7187dd57', 'name': 'Yu-Gang Jiang', 'hidden': False}], 'publishedAt': '2025-07-30T17:59:31.000Z', 'submittedOnDailyAt': '2025-07-31T00:11:21.540Z', 'title': 'Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual\\n  Segmentation', 'submittedOnDailyBy': {'_id': '67ff29ecbf6889a333c69c7a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png', 'isPro': False, 'fullname': 'Henghui Ding', 'user': 'HenghuiDing', 'type': 'user'}, 'summary': 'Referring audio-visual segmentation (RAVS) has recently seen significant\\nadvancements, yet challenges remain in integrating multimodal information and\\ndeeply understanding and reasoning about audiovisual content. To extend the\\nboundaries of RAVS and facilitate future research in this field, we propose\\nOmnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset\\ncontaining 2,098 videos and 59,458 multimodal referring expressions. OmniAVS\\nstands out with three key innovations: (1) 8 types of multimodal expressions\\nthat flexibly combine text, speech, sound, and visual cues; (2) an emphasis on\\nunderstanding audio content beyond just detecting their presence; and (3) the\\ninclusion of complex reasoning and world knowledge in expressions. Furthermore,\\nwe introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the\\nchallenges of multimodal reasoning and fine-grained understanding of\\naudiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and\\nperform reasoning-based segmentation. Extensive experiments show that OISA\\noutperforms existing methods on OmniAVS and achieves competitive results on\\nother related tasks.', 'upvotes': 6, 'discussionId': '688ac9798b724c8c7187dd58', 'ai_summary': 'Omnimodal Referring Audio-Visual Segmentation (OmniAVS) and Omnimodal Instructed Segmentation Assistant (OISA) advance audio-visual segmentation by integrating complex multimodal expressions and leveraging MLLM for reasoning.', 'ai_keywords': ['multimodal referring expressions', 'audio-visual segmentation', 'multimodal reasoning', 'fine-grained understanding', 'MLLM']}, 'publishedAt': '2025-07-30T13:59:31.000Z', 'title': 'Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual\\n  Segmentation', 'summary': 'Referring audio-visual segmentation (RAVS) has recently seen significant\\nadvancements, yet challenges remain in integrating multimodal information and\\ndeeply understanding and reasoning about audiovisual content. To extend the\\nboundaries of RAVS and facilitate future research in this field, we propose\\nOmnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset\\ncontaining 2,098 videos and 59,458 multimodal referring expressions. OmniAVS\\nstands out with three key innovations: (1) 8 types of multimodal expressions\\nthat flexibly combine text, speech, sound, and visual cues; (2) an emphasis on\\nunderstanding audio content beyond just detecting their presence; and (3) the\\ninclusion of complex reasoning and world knowledge in expressions. Furthermore,\\nwe introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the\\nchallenges of multimodal reasoning and fine-grained understanding of\\naudiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and\\nperform reasoning-based segmentation. Extensive experiments show that OISA\\noutperforms existing methods on OmniAVS and achieves competitive results on\\nother related tasks.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22886.png', 'numComments': 1, 'submittedBy': {'_id': '67ff29ecbf6889a333c69c7a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png', 'fullname': 'Henghui Ding', 'name': 'HenghuiDing', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2507.22565', 'authors': [{'_id': '688b0f7c8b724c8c7187de5f', 'user': {'_id': '632292053007fcbf2932da22', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/632292053007fcbf2932da22/Cf4w-OunyQzHAJhyZsHTf.jpeg', 'isPro': True, 'fullname': 'Afshin Khadangi', 'user': 'akhadangi', 'type': 'user'}, 'name': 'Afshin Khadangi', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-31T08:14:44.109Z', 'hidden': False}, {'_id': '688b0f7c8b724c8c7187de60', 'name': 'Amir Sartipi', 'hidden': False}, {'_id': '688b0f7c8b724c8c7187de61', 'name': 'Igor Tchappi', 'hidden': False}, {'_id': '688b0f7c8b724c8c7187de62', 'name': 'Ramin Bahmani', 'hidden': False}, {'_id': '688b0f7c8b724c8c7187de63', 'name': 'Gilbert Fridgen', 'hidden': False}], 'publishedAt': '2025-07-30T10:46:53.000Z', 'submittedOnDailyAt': '2025-07-31T05:12:37.542Z', 'title': 'Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement\\n  Learning', 'submittedOnDailyBy': {'_id': '632292053007fcbf2932da22', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/632292053007fcbf2932da22/Cf4w-OunyQzHAJhyZsHTf.jpeg', 'isPro': True, 'fullname': 'Afshin Khadangi', 'user': 'akhadangi', 'type': 'user'}, 'summary': \"The tension between data privacy and model utility has become the defining\\nbottleneck for the practical deployment of large language models (LLMs) trained\\non sensitive corpora including healthcare. Differentially private stochastic\\ngradient descent (DP-SGD) guarantees formal privacy, yet it does so at a\\npronounced cost: gradients are forcibly clipped and perturbed with noise,\\ndegrading sample efficiency and final accuracy. Numerous variants have been\\nproposed to soften this trade-off, but they all share a handicap: their control\\nknobs are hard-coded, global, and oblivious to the evolving optimization\\nlandscape. Consequently, practitioners are forced either to over-spend privacy\\nbudget in pursuit of utility, or to accept mediocre models in order to stay\\nwithin privacy constraints. We present RLDP, the first framework to cast DP\\noptimization itself as a closed-loop control problem amenable to modern deep\\nreinforcement learning (RL). RLDP continuously senses rich statistics of the\\nlearning dynamics and acts by selecting fine-grained per parameter\\ngradient-clipping thresholds as well as the magnitude of injected Gaussian\\nnoise. A soft actor-critic (SAC) hyper-policy is trained online during language\\nmodel fine-tuning; it learns, from scratch, how to allocate the privacy budget\\nwhere it matters and when it matters. Across more than 1,600 ablation\\nexperiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers\\nperplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream\\nutility gain. RLDP reaches each baseline's final utility after only 13-43% of\\nthe gradient-update budget (mean speed-up 71%), all while honoring the same\\n(epsilon, delta)-DP contract and exhibiting equal or lower susceptibility\\nto membership-inference and canary-extraction attacks.\", 'upvotes': 6, 'discussionId': '688b0f7d8b724c8c7187de64', 'projectPage': 'https://wandb.ai/afshin-khadangi-university-of-luxembourg/RLDP/reports/Efficient-Differentially-Private-Fine-Tuning-of-LLMs-via-Reinforcement-Learning--VmlldzoxMzc4NTEwMA?accessToken=qhs4n7sh3o93yql2wprb2vylpmer07r2bjvzft7gty5mhhplt3numljxppfd8z66', 'githubRepo': 'https://github.com/akhadangi/RLDP', 'ai_summary': 'RLDP, a deep reinforcement learning framework, optimizes differentially private training by dynamically adjusting gradient clipping and noise, enhancing model utility and speed while maintaining privacy.', 'ai_keywords': ['differentially private stochastic gradient descent', 'DP-SGD', 'deep reinforcement learning', 'RL', 'soft actor-critic', 'SAC', 'gradient-clipping thresholds', 'Gaussian noise', '($\\\\epsilon$', '$\\\\delta$)-DP', 'membership-inference', 'canary-extraction attacks'], 'githubStars': 0}, 'publishedAt': '2025-07-30T06:46:53.000Z', 'title': 'Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement\\n  Learning', 'summary': \"The tension between data privacy and model utility has become the defining\\nbottleneck for the practical deployment of large language models (LLMs) trained\\non sensitive corpora including healthcare. Differentially private stochastic\\ngradient descent (DP-SGD) guarantees formal privacy, yet it does so at a\\npronounced cost: gradients are forcibly clipped and perturbed with noise,\\ndegrading sample efficiency and final accuracy. Numerous variants have been\\nproposed to soften this trade-off, but they all share a handicap: their control\\nknobs are hard-coded, global, and oblivious to the evolving optimization\\nlandscape. Consequently, practitioners are forced either to over-spend privacy\\nbudget in pursuit of utility, or to accept mediocre models in order to stay\\nwithin privacy constraints. We present RLDP, the first framework to cast DP\\noptimization itself as a closed-loop control problem amenable to modern deep\\nreinforcement learning (RL). RLDP continuously senses rich statistics of the\\nlearning dynamics and acts by selecting fine-grained per parameter\\ngradient-clipping thresholds as well as the magnitude of injected Gaussian\\nnoise. A soft actor-critic (SAC) hyper-policy is trained online during language\\nmodel fine-tuning; it learns, from scratch, how to allocate the privacy budget\\nwhere it matters and when it matters. Across more than 1,600 ablation\\nexperiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers\\nperplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream\\nutility gain. RLDP reaches each baseline's final utility after only 13-43% of\\nthe gradient-update budget (mean speed-up 71%), all while honoring the same\\n(epsilon, delta)-DP contract and exhibiting equal or lower susceptibility\\nto membership-inference and canary-extraction attacks.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22565.png', 'numComments': 1, 'submittedBy': {'_id': '632292053007fcbf2932da22', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/632292053007fcbf2932da22/Cf4w-OunyQzHAJhyZsHTf.jpeg', 'fullname': 'Afshin Khadangi', 'name': 'akhadangi', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.22853', 'authors': [{'_id': '688ae0f78b724c8c7187dd8d', 'name': 'Haichuan Hu', 'hidden': False}, {'_id': '688ae0f78b724c8c7187dd8e', 'name': 'Xiaochen Xie', 'hidden': False}, {'_id': '688ae0f78b724c8c7187dd8f', 'name': 'Quanjun Zhang', 'hidden': False}], 'publishedAt': '2025-07-30T17:24:05.000Z', 'submittedOnDailyAt': '2025-07-31T01:51:54.126Z', 'title': 'Repair-R1: Better Test Before Repair', 'submittedOnDailyBy': {'_id': '630341a4ef6f432d84e97db9', 'avatarUrl': '/avatars/dc9910b5b9053123bad5ff848f677e12.svg', 'isPro': False, 'fullname': 'tomsawyer', 'user': 'tomhu', 'type': 'user'}, 'summary': \"APR (Automated Program Repair) aims to automatically locate program defects,\\ngenerate patches and validate the repairs. Existing techniques for APR are\\noften combined with LLMs (Large Language Models), which leverages the\\ncode-related knowledge of LLMs to improve repair effectiveness. Current\\nLLM-based APR methods typically utilize test cases only during the inference\\nstage, adopting an iterative approach that performs repair first and validates\\nit through test execution afterward. This conventional paradigm neglects two\\nimportant aspects: the potential contribution of test cases in the training\\nphase, and the possibility of leveraging testing prior to repair. To address\\nthis, we propose Repair-R1, which introduces test cases into the model's\\ntraining phase and shifts test generation to precede repair. The model is\\nrequired to first generate discriminative test cases that can distinguish\\ndefective behaviors, and then perform repair based on these tests. This enables\\nthe model to better locate defects and understand the underlying causes of\\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\\nthree different backbone models, using RL (reinforcement learning) to\\nco-optimize test generation and bug repair. Experimental results on four widely\\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\\\% to\\n48.29\\\\%, test generation success rate by 16.38\\\\% to 53.28\\\\%, and test coverage\\nby 0.78\\\\% to 53.96\\\\%. We publish the code and weights at\\nhttps://github.com/Tomsawyerhu/APR-RL and\\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.\", 'upvotes': 4, 'discussionId': '688ae0f88b724c8c7187dd90', 'githubRepo': 'https://github.com/Tomsawyerhu/APR-RL', 'ai_summary': 'Repair-R1 enhances automated program repair by integrating test cases into the training phase and prioritizing test generation before repair, improving repair success, test generation success, and test coverage.', 'ai_keywords': ['Automated Program Repair', 'Large Language Models', 'reinforcement learning', 'test cases', 'discriminative test cases', 'bug repair', 'repair success rate', 'test generation success rate', 'test coverage'], 'githubStars': 3}, 'publishedAt': '2025-07-30T13:24:05.000Z', 'title': 'Repair-R1: Better Test Before Repair', 'summary': \"APR (Automated Program Repair) aims to automatically locate program defects,\\ngenerate patches and validate the repairs. Existing techniques for APR are\\noften combined with LLMs (Large Language Models), which leverages the\\ncode-related knowledge of LLMs to improve repair effectiveness. Current\\nLLM-based APR methods typically utilize test cases only during the inference\\nstage, adopting an iterative approach that performs repair first and validates\\nit through test execution afterward. This conventional paradigm neglects two\\nimportant aspects: the potential contribution of test cases in the training\\nphase, and the possibility of leveraging testing prior to repair. To address\\nthis, we propose Repair-R1, which introduces test cases into the model's\\ntraining phase and shifts test generation to precede repair. The model is\\nrequired to first generate discriminative test cases that can distinguish\\ndefective behaviors, and then perform repair based on these tests. This enables\\nthe model to better locate defects and understand the underlying causes of\\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\\nthree different backbone models, using RL (reinforcement learning) to\\nco-optimize test generation and bug repair. Experimental results on four widely\\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\\\% to\\n48.29\\\\%, test generation success rate by 16.38\\\\% to 53.28\\\\%, and test coverage\\nby 0.78\\\\% to 53.96\\\\%. We publish the code and weights at\\nhttps://github.com/Tomsawyerhu/APR-RL and\\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22853.png', 'numComments': 1, 'submittedBy': {'_id': '630341a4ef6f432d84e97db9', 'avatarUrl': '/avatars/dc9910b5b9053123bad5ff848f677e12.svg', 'fullname': 'tomsawyer', 'name': 'tomhu', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2507.21802', 'authors': [{'_id': '6889ce8231e1218a089282c1', 'name': 'Junzhe Li', 'hidden': False}, {'_id': '6889ce8231e1218a089282c2', 'name': 'Yutao Cui', 'hidden': False}, {'_id': '6889ce8231e1218a089282c3', 'name': 'Tao Huang', 'hidden': False}, {'_id': '6889ce8231e1218a089282c4', 'name': 'Yinping Ma', 'hidden': False}, {'_id': '6889ce8231e1218a089282c5', 'name': 'Chun Fan', 'hidden': False}, {'_id': '6889ce8231e1218a089282c6', 'name': 'Miles Yang', 'hidden': False}, {'_id': '6889ce8231e1218a089282c7', 'name': 'Zhao Zhong', 'hidden': False}], 'publishedAt': '2025-07-29T13:40:09.000Z', 'submittedOnDailyAt': '2025-07-31T11:26:47.399Z', 'title': 'MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE', 'submittedOnDailyBy': {'_id': '64c860d23a3f428da65ea499', 'avatarUrl': '/avatars/f0bcc6ae7e558babe691b6bbf1059c9d.svg', 'isPro': False, 'fullname': 'lijunzhe', 'user': 'tulvgengenr', 'type': 'user'}, 'summary': 'Although GRPO substantially enhances flow matching models in human preference\\nalignment of image generation, methods such as FlowGRPO still exhibit\\ninefficiency due to the necessity of sampling and optimizing over all denoising\\nsteps specified by the Markov Decision Process (MDP). In this paper, we propose\\nMixGRPO, a novel framework that leverages the flexibility of mixed\\nsampling strategies through the integration of stochastic differential\\nequations (SDE) and ordinary differential equations (ODE). This streamlines the\\noptimization process within the MDP to improve efficiency and boost\\nperformance. Specifically, MixGRPO introduces a sliding window mechanism, using\\nSDE sampling and GRPO-guided optimization only within the window, while\\napplying ODE sampling outside. This design confines sampling randomness to the\\ntime-steps within the window, thereby reducing the optimization overhead, and\\nallowing for more focused gradient updates to accelerate convergence.\\nAdditionally, as time-steps beyond the sliding window are not involved in\\noptimization, higher-order solvers are supported for sampling. So we present a\\nfaster variant, termed MixGRPO-Flash, which further improves\\ntraining efficiency while achieving comparable performance. MixGRPO exhibits\\nsubstantial gains across multiple dimensions of human preference alignment,\\noutperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%\\nlower training time. Notably, MixGRPO-Flash further reduces training time by\\n71%. Codes and models are available at\\nhttps://github.com/Tencent-Hunyuan/MixGRPO{MixGRPO}.', 'upvotes': 2, 'discussionId': '6889ce8331e1218a089282c8', 'projectPage': 'https://tulvgengenr.github.io/MixGRPO-Project-Page/', 'githubRepo': 'https://github.com/Tencent-Hunyuan/MixGRPO', 'ai_summary': 'MixGRPO, a novel framework integrating SDE and ODE, enhances flow matching models for image generation by optimizing only within a sliding window, improving efficiency and performance.', 'ai_keywords': ['flow matching models', 'human preference alignment', 'image generation', 'Markov Decision Process', 'stochastic differential equations', 'ordinary differential equations', 'sliding window mechanism', 'gradient updates', 'MixGRPO-Flash'], 'githubStars': 60}, 'publishedAt': '2025-07-29T09:40:09.000Z', 'title': 'MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE', 'summary': 'Although GRPO substantially enhances flow matching models in human preference\\nalignment of image generation, methods such as FlowGRPO still exhibit\\ninefficiency due to the necessity of sampling and optimizing over all denoising\\nsteps specified by the Markov Decision Process (MDP). In this paper, we propose\\nMixGRPO, a novel framework that leverages the flexibility of mixed\\nsampling strategies through the integration of stochastic differential\\nequations (SDE) and ordinary differential equations (ODE). This streamlines the\\noptimization process within the MDP to improve efficiency and boost\\nperformance. Specifically, MixGRPO introduces a sliding window mechanism, using\\nSDE sampling and GRPO-guided optimization only within the window, while\\napplying ODE sampling outside. This design confines sampling randomness to the\\ntime-steps within the window, thereby reducing the optimization overhead, and\\nallowing for more focused gradient updates to accelerate convergence.\\nAdditionally, as time-steps beyond the sliding window are not involved in\\noptimization, higher-order solvers are supported for sampling. So we present a\\nfaster variant, termed MixGRPO-Flash, which further improves\\ntraining efficiency while achieving comparable performance. MixGRPO exhibits\\nsubstantial gains across multiple dimensions of human preference alignment,\\noutperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%\\nlower training time. Notably, MixGRPO-Flash further reduces training time by\\n71%. Codes and models are available at\\nhttps://github.com/Tencent-Hunyuan/MixGRPO{MixGRPO}.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21802.png', 'numComments': 1, 'submittedBy': {'_id': '64c860d23a3f428da65ea499', 'avatarUrl': '/avatars/f0bcc6ae7e558babe691b6bbf1059c9d.svg', 'fullname': 'lijunzhe', 'name': 'tulvgengenr', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2507.22062', 'authors': [{'_id': '688b6429292830d55fa68a2c', 'name': 'Yung-Sung Chuang', 'hidden': False}, {'_id': '688b6429292830d55fa68a2d', 'name': 'Yang Li', 'hidden': False}, {'_id': '688b6429292830d55fa68a2e', 'name': 'Dong Wang', 'hidden': False}, {'_id': '688b6429292830d55fa68a2f', 'name': 'Ching-Feng Yeh', 'hidden': False}, {'_id': '688b6429292830d55fa68a30', 'name': 'Kehan Lyu', 'hidden': False}, {'_id': '688b6429292830d55fa68a31', 'name': 'Ramya Raghavendra', 'hidden': False}, {'_id': '688b6429292830d55fa68a32', 'name': 'James Glass', 'hidden': False}, {'_id': '688b6429292830d55fa68a33', 'name': 'Lifei Huang', 'hidden': False}, {'_id': '688b6429292830d55fa68a34', 'name': 'Jason Weston', 'hidden': False}, {'_id': '688b6429292830d55fa68a35', 'name': 'Luke Zettlemoyer', 'hidden': False}, {'_id': '688b6429292830d55fa68a36', 'name': 'Xinlei Chen', 'hidden': False}, {'_id': '688b6429292830d55fa68a37', 'name': 'Zhuang Liu', 'hidden': False}, {'_id': '688b6429292830d55fa68a38', 'name': 'Saining Xie', 'hidden': False}, {'_id': '688b6429292830d55fa68a39', 'name': 'Wen-tau Yih', 'hidden': False}, {'_id': '688b6429292830d55fa68a3a', 'name': 'Shang-Wen Li', 'hidden': False}, {'_id': '688b6429292830d55fa68a3b', 'name': 'Hu Xu', 'hidden': False}], 'publishedAt': '2025-07-29T17:59:58.000Z', 'submittedOnDailyAt': '2025-07-31T11:10:21.332Z', 'title': 'MetaCLIP 2: A Worldwide Scaling Recipe', 'submittedOnDailyBy': {'_id': '5f1158120c833276f61f1a84', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg', 'isPro': True, 'fullname': 'Niels Rogge', 'user': 'nielsr', 'type': 'user'}, 'summary': 'Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\\nsupporting from zero-shot classification, retrieval to encoders for multimodal\\nlarge language models (MLLMs). Although CLIP is successfully trained on\\nbillion-scale image-text pairs from the English world, scaling CLIP\\'s training\\nfurther to learning from the worldwide web data is still challenging: (1) no\\ncuration method is available to handle data points from non-English world; (2)\\nthe English performance from existing multilingual CLIP is worse than its\\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\\nLLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch\\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\\nrigorous ablations with minimal changes that are necessary to address the above\\nchallenges and present a recipe enabling mutual benefits from English and\\nnon-English world data. In zero-shot ImageNet classification, MetaCLIP 2\\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\\nand surprisingly sets new state-of-the-art without system-level confounding\\nfactors (e.g., translation, bespoke architecture changes) on multilingual\\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\\n64.3% on image-to-text retrieval.', 'upvotes': 1, 'discussionId': '688b6429292830d55fa68a3c', 'ai_summary': 'MetaCLIP 2, trained on worldwide web-scale image-text pairs, improves zero-shot classification and multilingual benchmarks without system-level confounding factors.', 'ai_keywords': ['Contrastive Language-Image Pretraining', 'CLIP', 'multilingual CLIP', 'MetaCLIP 2', 'zero-shot classification', 'image-text pairs', 'multilingual benchmarks', 'CVQA', 'Babel-ImageNet', 'XM3600', 'image-to-text retrieval']}, 'publishedAt': '2025-07-29T13:59:58.000Z', 'title': 'MetaCLIP 2: A Worldwide Scaling Recipe', 'summary': 'Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\\nsupporting from zero-shot classification, retrieval to encoders for multimodal\\nlarge language models (MLLMs). Although CLIP is successfully trained on\\nbillion-scale image-text pairs from the English world, scaling CLIP\\'s training\\nfurther to learning from the worldwide web data is still challenging: (1) no\\ncuration method is available to handle data points from non-English world; (2)\\nthe English performance from existing multilingual CLIP is worse than its\\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\\nLLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch\\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\\nrigorous ablations with minimal changes that are necessary to address the above\\nchallenges and present a recipe enabling mutual benefits from English and\\nnon-English world data. In zero-shot ImageNet classification, MetaCLIP 2\\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\\nand surprisingly sets new state-of-the-art without system-level confounding\\nfactors (e.g., translation, bespoke architecture changes) on multilingual\\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\\n64.3% on image-to-text retrieval.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22062.png', 'numComments': 1, 'submittedBy': {'_id': '5f1158120c833276f61f1a84', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg', 'fullname': 'Niels Rogge', 'name': 'nielsr', 'type': 'user', 'isPro': True, 'isHf': True, 'isHfAdmin': False, 'isMod': False, 'followerCount': 929}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2507.13985', 'authors': [{'_id': '688b7d308c434640078cc2a5', 'name': 'Haoran Li', 'hidden': False}, {'_id': '688b7d308c434640078cc2a6', 'name': 'Yuli Tian', 'hidden': False}, {'_id': '688b7d308c434640078cc2a7', 'name': 'Kun Lan', 'hidden': False}, {'_id': '688b7d308c434640078cc2a8', 'name': 'Yong Liao', 'hidden': False}, {'_id': '688b7d308c434640078cc2a9', 'name': 'Lin Wang', 'hidden': False}, {'_id': '688b7d308c434640078cc2aa', 'name': 'Pan Hui', 'hidden': False}, {'_id': '688b7d308c434640078cc2ab', 'name': 'Peng Yuan Zhou', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/683d173214785f6d5902f9c0/AzBTbq8t1-UFAUdZoDdJ0.mp4'], 'publishedAt': '2025-07-18T14:45:54.000Z', 'submittedOnDailyAt': '2025-07-31T13:03:34.453Z', 'title': 'DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation', 'submittedOnDailyBy': {'_id': '683d173214785f6d5902f9c0', 'avatarUrl': '/avatars/2357d97bbcecd3a51279442bd99a0c50.svg', 'isPro': False, 'fullname': 'Haoran Li', 'user': 'jahnsonblack', 'type': 'user'}, 'summary': 'Generating 3D scenes from natural language holds great promise for\\napplications in gaming, film, and design. However, existing methods struggle\\nwith automation, 3D consistency, and fine-grained control. We present\\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\\ngeneration from text or dialogue. DreamScene begins with a scene planning\\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\\nstructured, collision-free layout. Based on this layout, Formation Pattern\\nSampling (FPS) generates object geometry using multi-timestep sampling and\\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\\nglobal consistent, DreamScene employs a progressive camera sampling strategy\\ntailored to both indoor and outdoor settings. Finally, the system supports\\nfine-grained scene editing, including object movement, appearance changes, and\\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\\nmethods in quality, consistency, and flexibility, offering a practical solution\\nfor open-domain 3D content creation. Code and demos are available at\\nhttps://jahnsonblack.github.io/DreamScene-Full/.', 'upvotes': 0, 'discussionId': '688b7d318c434640078cc2ac', 'projectPage': 'https://jahnsonblack.github.io/DreamScene-Full/', 'githubRepo': 'https://github.com/DreamScene-Project/DreamScene', 'ai_summary': 'DreamScene is an end-to-end framework that generates high-quality, editable 3D scenes from text or dialogue, ensuring automation, 3D consistency, and fine-grained control through a combination of scene planning, graph-based placement, formation pattern sampling, and progressive camera sampling.', 'ai_keywords': ['GPT-4', 'hybrid graph', 'graph-based placement algorithm', 'Formation Pattern Sampling', 'multi-timestep sampling', 'reconstructive optimization', 'progressive camera sampling', '3D scene generation', 'scene editing', '4D dynamic motion']}, 'publishedAt': '2025-07-18T10:45:54.000Z', 'title': 'DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation', 'summary': 'Generating 3D scenes from natural language holds great promise for\\napplications in gaming, film, and design. However, existing methods struggle\\nwith automation, 3D consistency, and fine-grained control. We present\\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\\ngeneration from text or dialogue. DreamScene begins with a scene planning\\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\\nstructured, collision-free layout. Based on this layout, Formation Pattern\\nSampling (FPS) generates object geometry using multi-timestep sampling and\\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\\nglobal consistent, DreamScene employs a progressive camera sampling strategy\\ntailored to both indoor and outdoor settings. Finally, the system supports\\nfine-grained scene editing, including object movement, appearance changes, and\\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\\nmethods in quality, consistency, and flexibility, offering a practical solution\\nfor open-domain 3D content creation. Code and demos are available at\\nhttps://jahnsonblack.github.io/DreamScene-Full/.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/683d173214785f6d5902f9c0/AzBTbq8t1-UFAUdZoDdJ0.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13985.png', 'numComments': 1, 'submittedBy': {'_id': '683d173214785f6d5902f9c0', 'avatarUrl': '/avatars/2357d97bbcecd3a51279442bd99a0c50.svg', 'fullname': 'Haoran Li', 'name': 'jahnsonblack', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': False}"
]