[
    "{'paper': {'id': '2601.06943', 'authors': [{'_id': '6965babdfc8c4ecc02c7f8f5', 'name': 'Chengwen Liu', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f8f6', 'name': 'Xiaomin Yu', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f8f7', 'name': 'Zhuoyue Chang', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f8f8', 'name': 'Zhe Huang', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f8f9', 'name': 'Shuo Zhang', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f8fa', 'name': 'Heng Lian', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f8fb', 'name': 'Kunyi Wang', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f8fc', 'name': 'Rui Xu', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f8fd', 'name': 'Sen Hu', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f8fe', 'name': 'Jianheng Hou', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f8ff', 'name': 'Hao Peng', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f900', 'name': 'Chengwei Qin', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f901', 'name': 'Xiaobin Hu', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f902', 'name': 'Hong Peng', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f903', 'name': 'Ronghao Chen', 'hidden': False}, {'_id': '6965babdfc8c4ecc02c7f904', 'name': 'Huacan Wang', 'hidden': False}], 'publishedAt': '2026-01-11T15:07:37.000Z', 'submittedOnDailyAt': '2026-01-13T01:12:08.706Z', 'title': 'Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning', 'submittedOnDailyBy': {'_id': '64084fa192033c150738e4f2', 'avatarUrl': '/avatars/dfff2216eb235c635e5abe6fda3084f0.svg', 'isPro': False, 'fullname': 'Yu_xm', 'user': 'Yu2020', 'type': 'user'}, 'summary': \"In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.\", 'upvotes': 160, 'discussionId': '6965babdfc8c4ecc02c7f905', 'githubRepo': 'https://github.com/QuantaAlpha/VideoDR-Benchmark', 'githubRepoAddedBy': 'user', 'ai_summary': 'VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.', 'ai_keywords': ['video question answering', 'cross-frame visual anchor extraction', 'interactive web retrieval', 'multi-hop reasoning', 'multimodal large language models', 'Workflow paradigm', 'Agentic paradigm', 'goal drift', 'long-horizon consistency'], 'githubStars': 45}, 'publishedAt': '2026-01-11T10:07:37.000Z', 'title': 'Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning', 'summary': \"In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png', 'numComments': 4, 'submittedBy': {'_id': '64084fa192033c150738e4f2', 'avatarUrl': '/avatars/dfff2216eb235c635e5abe6fda3084f0.svg', 'fullname': 'Yu_xm', 'name': 'Yu2020', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5, 'isUserFollowing': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.06521', 'authors': [{'_id': '6965c124fc8c4ecc02c7f930', 'name': 'Liang Chen', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f931', 'name': 'Weichu Xie', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f932', 'name': 'Yiyan Liang', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f933', 'name': 'Hongfeng He', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f934', 'name': 'Hans Zhao', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f935', 'name': 'Zhibo Yang', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f936', 'name': 'Zhiqi Huang', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f937', 'name': 'Haoning Wu', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f938', 'name': 'Haoyu Lu', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f939', 'name': 'Y. charles', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f93a', 'name': 'Yiping Bao', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f93b', 'name': 'Yuantao Fan', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f93c', 'name': 'Guopeng Li', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f93d', 'name': 'Haiyang Shen', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f93e', 'user': {'_id': '65e6970d135c27ea806526fe', 'avatarUrl': '/avatars/4aced113d9cab055ae06f3945869a280.svg', 'isPro': False, 'fullname': 'Xuanzhong Chen', 'user': 'chenxz', 'type': 'user'}, 'name': 'Xuanzhong Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:23:52.086Z', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f93f', 'name': 'Wendong Xu', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f940', 'name': 'Shuzheng Si', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f941', 'name': 'Zefan Cai', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f942', 'name': 'Wenhao Chai', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f943', 'user': {'_id': '60efe7fa0d920bc7805cada5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png', 'isPro': False, 'fullname': 'Ziqi Huang', 'user': 'Ziqi', 'type': 'user'}, 'name': 'Ziqi Huang', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:23:50.242Z', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f944', 'name': 'Fangfu Liu', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f945', 'name': 'Tianyu Liu', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f946', 'name': 'Baobao Chang', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f947', 'name': 'Xiaobo Hu', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f948', 'name': 'Kaiyuan Chen', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f949', 'name': 'Yixin Ren', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f94a', 'name': 'Yang Liu', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f94b', 'name': 'Yuan Gong', 'hidden': False}, {'_id': '6965c124fc8c4ecc02c7f94c', 'name': 'Kuan Li', 'hidden': False}], 'publishedAt': '2026-01-10T10:42:44.000Z', 'submittedOnDailyAt': '2026-01-13T01:21:01.708Z', 'title': 'BabyVision: Visual Reasoning Beyond Language', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.', 'upvotes': 135, 'discussionId': '6965c124fc8c4ecc02c7f94d', 'projectPage': 'https://unipat.ai/blog/BabyVision', 'githubRepo': 'https://github.com/UniPat-AI/BabyVision', 'githubRepoAddedBy': 'user', 'ai_summary': 'Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.', 'ai_keywords': ['Multimodal LLMs', 'visual reasoning', 'core visual skills', 'BabyVision benchmark', 'visual perception', 'visual primitives'], 'githubStars': 73}, 'publishedAt': '2026-01-10T05:42:44.000Z', 'title': 'BabyVision: Visual Reasoning Beyond Language', 'summary': 'While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png', 'numComments': 3, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 207, 'isUserFollowing': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.05593', 'authors': [{'_id': '6965b990fc8c4ecc02c7f8df', 'name': 'Jingcheng Hu', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8e0', 'name': 'Yinmin Zhang', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8e1', 'name': 'Shijie Shang', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8e2', 'name': 'Xiaobo Yang', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8e3', 'name': 'Yue Peng', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8e4', 'name': 'Zhewei Huang', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8e5', 'name': 'Hebin Zhou', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8e6', 'name': 'Xin Wu', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8e7', 'name': 'Jie Cheng', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8e8', 'name': 'Fanqi Wan', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8e9', 'name': 'Xiangwen Kong', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8ea', 'name': 'Chengyuan Yao', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8eb', 'name': 'Kaiwen Yan', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8ec', 'name': 'Ailin Huang', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8ed', 'name': 'Hongyu Zhou', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8ee', 'name': 'Qi Han', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8ef', 'name': 'Zheng Ge', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8f0', 'name': 'Daxin Jiang', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8f1', 'name': 'Xiangyu Zhang', 'hidden': False}, {'_id': '6965b990fc8c4ecc02c7f8f2', 'name': 'Heung-Yeung Shum', 'hidden': False}], 'publishedAt': '2026-01-09T07:24:43.000Z', 'submittedOnDailyAt': '2026-01-13T00:51:45.124Z', 'title': 'PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning', 'submittedOnDailyBy': {'_id': '625026b7d2d191ac43320c5e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg', 'isPro': False, 'fullname': 'Jingcheng Hu', 'user': 'reign12', 'type': 'user'}, 'summary': \"We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.\", 'upvotes': 60, 'discussionId': '6965b990fc8c4ecc02c7f8f3', 'githubRepo': 'https://github.com/stepfun-ai/PaCoRe', 'githubRepoAddedBy': 'user', 'ai_summary': 'Parallel Coordinated Reasoning enables large-scale test-time compute scaling beyond sequential reasoning limitations through parallel exploration and message-passing architecture.', 'ai_keywords': ['test-time compute', 'sequential reasoning', 'parallel exploration', 'message-passing architecture', 'reinforcement learning', 'multi-million-token', 'HMMT 2025', 'GPT-5'], 'githubStars': 258, 'organization': {'_id': '66e43eae9d477f566f937935', 'name': 'stepfun-ai', 'fullname': 'StepFun', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png'}}, 'publishedAt': '2026-01-09T02:24:43.000Z', 'title': 'PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning', 'summary': \"We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05593.png', 'numComments': 1, 'submittedBy': {'_id': '625026b7d2d191ac43320c5e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg', 'fullname': 'Jingcheng Hu', 'name': 'reign12', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 17, 'isUserFollowing': False}, 'organization': {'_id': '66e43eae9d477f566f937935', 'name': 'stepfun-ai', 'fullname': 'StepFun', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.06953', 'authors': [{'_id': '6965af14fc8c4ecc02c7f873', 'name': 'Jie Wu', 'hidden': False}, {'_id': '6965af14fc8c4ecc02c7f874', 'user': {'_id': '650be23ec4e52db6a4db63ef', 'avatarUrl': '/avatars/03af548029b38bee49ec295fefe74f9a.svg', 'isPro': False, 'fullname': 'Haoling Li', 'user': 'Ringo1110', 'type': 'user'}, 'name': 'Haoling Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:24:20.281Z', 'hidden': False}, {'_id': '6965af14fc8c4ecc02c7f875', 'name': 'Xin Zhang', 'hidden': False}, {'_id': '6965af14fc8c4ecc02c7f876', 'name': 'Jiani Guo', 'hidden': False}, {'_id': '6965af14fc8c4ecc02c7f877', 'name': 'Jane Luo', 'hidden': False}, {'_id': '6965af14fc8c4ecc02c7f878', 'name': 'Steven Liu', 'hidden': False}, {'_id': '6965af14fc8c4ecc02c7f879', 'name': 'Yangyu Huang', 'hidden': False}, {'_id': '6965af14fc8c4ecc02c7f87a', 'name': 'Ruihang Chu', 'hidden': False}, {'_id': '6965af14fc8c4ecc02c7f87b', 'name': 'Scarlett Li', 'hidden': False}, {'_id': '6965af14fc8c4ecc02c7f87c', 'name': 'Yujiu Yang', 'hidden': False}], 'publishedAt': '2026-01-11T15:22:33.000Z', 'submittedOnDailyAt': '2026-01-13T01:12:35.419Z', 'title': 'X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.', 'upvotes': 28, 'discussionId': '6965af14fc8c4ecc02c7f87d', 'githubRepo': 'https://github.com/JieWu02/X-Coder', 'githubRepoAddedBy': 'user', 'ai_summary': 'Code LLMs trained on fully synthetic data using a feature-based synthesis pipeline achieve superior performance on competitive programming benchmarks while reducing dependence on real-world coding datasets.', 'ai_keywords': ['Code LLMs', 'synthetic data', 'feature-based synthesis', 'data synthesis pipeline', 'SynthSmith', 'supervised fine-tuning', 'reinforcement learning', 'X-Coder model series', 'LiveCodeBench', 'scaling laws', 'staged training', 'code reasoning'], 'githubStars': 35}, 'publishedAt': '2026-01-11T10:22:33.000Z', 'title': 'X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests', 'summary': 'Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06953.png', 'numComments': 0, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 207, 'isUserFollowing': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.07832', 'authors': [{'_id': '6965c791fc8c4ecc02c7f9d3', 'user': {'_id': '659698e6f67e8fb2a5985445', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6jxohd_DsLVfSnWgKMGrn.jpeg', 'isPro': False, 'fullname': 'Kewei Zhang', 'user': 'xiwenyoumu', 'type': 'user'}, 'name': 'Kewei Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:23:45.745Z', 'hidden': False}, {'_id': '6965c791fc8c4ecc02c7f9d4', 'name': 'Ye Huang', 'hidden': False}, {'_id': '6965c791fc8c4ecc02c7f9d5', 'user': {'_id': '68fce03ed1d0efce7ca87075', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png', 'isPro': False, 'fullname': 'yfdeng', 'user': 'yfdeng10', 'type': 'user'}, 'name': 'Yufan Deng', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:23:39.576Z', 'hidden': False}, {'_id': '6965c791fc8c4ecc02c7f9d6', 'name': 'Jincheng Yu', 'hidden': False}, {'_id': '6965c791fc8c4ecc02c7f9d7', 'name': 'Junsong Chen', 'hidden': False}, {'_id': '6965c791fc8c4ecc02c7f9d8', 'name': 'Huan Ling', 'hidden': False}, {'_id': '6965c791fc8c4ecc02c7f9d9', 'name': 'Enze Xie', 'hidden': False}, {'_id': '6965c791fc8c4ecc02c7f9da', 'name': 'Daquan Zhou', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/68fce03ed1d0efce7ca87075/nan3M8p_MfDdxZgb1KI4j.mp4'], 'publishedAt': '2026-01-12T18:59:18.000Z', 'submittedOnDailyAt': '2026-01-13T06:02:44.452Z', 'title': 'MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head', 'submittedOnDailyBy': {'_id': '68fce03ed1d0efce7ca87075', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png', 'isPro': False, 'fullname': 'yfdeng', 'user': 'yfdeng10', 'type': 'user'}, 'summary': 'While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\\\% improvement on ImageNet classification, a 6.3\\\\% gain on NLP, a 12.6\\\\% improvement on image generation, and a 41\\\\% enhancement on video generation under the same time complexity.', 'upvotes': 25, 'discussionId': '6965c791fc8c4ecc02c7f9db', 'projectPage': 'https://dagroup-pku.github.io/MHLA/', 'githubRepo': 'https://github.com/DAGroup-PKU/MHLA', 'githubRepoAddedBy': 'user', 'ai_summary': \"Multi-Head Linear Attention addresses the performance degradation in linear attention by preserving representational diversity through head-wise token dimension computation, maintaining linear complexity while recovering softmax attention's expressive power across multiple domains.\", 'ai_keywords': ['Transformer architecture', 'self-attention', 'linear attention', 'global context collapse', 'Multi-Head Linear Attention', 'token dimension', 'softmax attention', 'ImageNet classification', 'NLP', 'image generation', 'video generation'], 'githubStars': 34, 'organization': {'_id': '6953c657d2ff7b60c8527d3c', 'name': 'DAGroup-PKU', 'fullname': 'DAGroup-PKU', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/665c91e15b11dca02f0c5891/ek9KGIc02tiFfaYeDfLaU.png'}}, 'publishedAt': '2026-01-12T13:59:18.000Z', 'title': 'MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head', 'summary': 'While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\\\% improvement on ImageNet classification, a 6.3\\\\% gain on NLP, a 12.6\\\\% improvement on image generation, and a 41\\\\% enhancement on video generation under the same time complexity.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/68fce03ed1d0efce7ca87075/nan3M8p_MfDdxZgb1KI4j.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07832.png', 'numComments': 1, 'submittedBy': {'_id': '68fce03ed1d0efce7ca87075', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png', 'fullname': 'yfdeng', 'name': 'yfdeng10', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isUserFollowing': False}, 'organization': {'_id': '6953c657d2ff7b60c8527d3c', 'name': 'DAGroup-PKU', 'fullname': 'DAGroup-PKU', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/665c91e15b11dca02f0c5891/ek9KGIc02tiFfaYeDfLaU.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.05110', 'authors': [{'_id': '696493ff138cc47cbd7653ab', 'name': 'Wenhao Zeng', 'hidden': False}, {'_id': '696493ff138cc47cbd7653ac', 'name': 'Xuteng Zhang', 'hidden': False}, {'_id': '696493ff138cc47cbd7653ad', 'user': {'_id': '645b0c3ec35da9c7afd95421', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg', 'isPro': False, 'fullname': 'Yuling', 'user': 'YerbaPage', 'type': 'user'}, 'name': 'Yuling Shi', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-12T10:33:29.307Z', 'hidden': False}, {'_id': '696493ff138cc47cbd7653ae', 'name': 'Chao Hu', 'hidden': False}, {'_id': '696493ff138cc47cbd7653af', 'name': 'Yuting Chen', 'hidden': False}, {'_id': '696493ff138cc47cbd7653b0', 'name': 'Beijun Shen', 'hidden': False}, {'_id': '696493ff138cc47cbd7653b1', 'name': 'Xiaodong Gu', 'hidden': False}], 'publishedAt': '2026-01-08T16:58:07.000Z', 'submittedOnDailyAt': '2026-01-13T03:10:55.068Z', 'title': 'GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts', 'submittedOnDailyBy': {'_id': '645b0c3ec35da9c7afd95421', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg', 'isPro': False, 'fullname': 'Yuling', 'user': 'YerbaPage', 'type': 'user'}, 'summary': 'Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the \"Aha Moment\" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.', 'upvotes': 24, 'discussionId': '696493ff138cc47cbd7653b2', 'githubRepo': 'https://github.com/Zengwh02/GlimpRouter', 'githubRepoAddedBy': 'user', 'ai_summary': \"Large reasoning models' inference latency can be reduced by routing reasoning steps to larger models based on the entropy of their first token, enabling efficient collaborative inference without additional training.\", 'ai_keywords': ['large reasoning models', 'multi-step chains of thought', 'inference latency', 'computational cost', 'collaborative inference', 'routing strategies', 'token probabilities', 'post-hoc verification', 'step-wise collaboration', 'reasoning step difficulty', 'initial token entropy', 'Aha Moment phenomenon', 'training-free framework', 'lightweight model', 'large model', 'threshold routing', 'entropy-based prediction', 'inference overhead', 'GlimpRouter'], 'githubStars': 5, 'organization': {'_id': '63e5ef7bf2e9a8f22c515654', 'name': 'SJTU', 'fullname': 'Shanghai Jiao Tong University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png'}}, 'publishedAt': '2026-01-08T11:58:07.000Z', 'title': 'GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts', 'summary': 'Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the \"Aha Moment\" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05110.png', 'numComments': 3, 'submittedBy': {'_id': '645b0c3ec35da9c7afd95421', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg', 'fullname': 'Yuling', 'name': 'YerbaPage', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 186, 'isUserFollowing': False}, 'organization': {'_id': '63e5ef7bf2e9a8f22c515654', 'name': 'SJTU', 'fullname': 'Shanghai Jiao Tong University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.07779', 'authors': [{'_id': '6965f217fc8c4ecc02c7fa6b', 'name': 'Bowen Yang', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa6c', 'name': 'Kaiming Jin', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa6d', 'name': 'Zhenyu Wu', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa6e', 'name': 'Zhaoyang Liu', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa6f', 'user': {'_id': '6064a0eeb1703ddba0d458b9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png', 'isPro': False, 'fullname': 'Qiushi', 'user': 'QiushiSun', 'type': 'user'}, 'name': 'Qiushi Sun', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:23:13.865Z', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa70', 'name': 'Zehao Li', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa71', 'name': 'JingJing Xie', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa72', 'name': 'Zhoumianze Liu', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa73', 'name': 'Fangzhi Xu', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa74', 'name': 'Kanzhi Cheng', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa75', 'name': 'Qingyun Li', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa76', 'name': 'Yian Wang', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa77', 'name': 'Yu Qiao', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa78', 'name': 'Zun Wang', 'hidden': False}, {'_id': '6965f217fc8c4ecc02c7fa79', 'user': {'_id': '642b9861bb77f8456634b048', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg', 'isPro': False, 'fullname': 'Zichen Ding', 'user': 'heroding77', 'type': 'user'}, 'name': 'Zichen Ding', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:23:16.476Z', 'hidden': False}], 'publishedAt': '2026-01-12T17:55:51.000Z', 'submittedOnDailyAt': '2026-01-13T05:38:29.779Z', 'title': 'OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent', 'submittedOnDailyBy': {'_id': '642b9861bb77f8456634b048', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg', 'isPro': False, 'fullname': 'Zichen Ding', 'user': 'heroding77', 'type': 'user'}, 'summary': 'While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.', 'upvotes': 22, 'discussionId': '6965f217fc8c4ecc02c7fa7a', 'projectPage': 'https://os-copilot.github.io/OS-Symphony', 'githubRepo': 'https://github.com/OS-Copilot/OS-Symphony', 'githubRepoAddedBy': 'user', 'ai_summary': 'OS-Symphony presents a comprehensive framework for computer-using agents that enhances robustness in long-horizon tasks through reflection-memory and multimodal search capabilities.', 'ai_keywords': ['Vision-Language Models', 'Computer-Using Agents', 'long-horizon workflows', 'visual context curation', 'visual-aware tutorial retrieval', 'Orchestrator', 'Reflection-Memory Agent', 'milestone-driven long-term memory', 'trajectory-level self-correction', 'Versatile Tool Agents', 'Multimodal Searcher', 'SeeAct paradigm', 'browser-based sandbox', 'live visually aligned tutorials'], 'githubStars': 14, 'organization': {'_id': '61d8000084231b832e5bbd99', 'name': 'ustc', 'fullname': 'university of science and technology  of china', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png'}}, 'publishedAt': '2026-01-12T12:55:51.000Z', 'title': 'OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent', 'summary': 'While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07779.png', 'numComments': 1, 'submittedBy': {'_id': '642b9861bb77f8456634b048', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg', 'fullname': 'Zichen Ding', 'name': 'heroding77', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 12, 'isUserFollowing': False}, 'organization': {'_id': '61d8000084231b832e5bbd99', 'name': 'ustc', 'fullname': 'university of science and technology  of china', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.07226', 'authors': [{'_id': '6965bbbcfc8c4ecc02c7f907', 'user': {'_id': '6550c4f27bbfce1878f5f280', 'avatarUrl': '/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg', 'isPro': False, 'fullname': 'seongyun_lee', 'user': 'Seongyun', 'type': 'user'}, 'name': 'Seongyun Lee', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:24:04.198Z', 'hidden': False}, {'_id': '6965bbbcfc8c4ecc02c7f908', 'name': 'Yongrae Jo', 'hidden': False}, {'_id': '6965bbbcfc8c4ecc02c7f909', 'name': 'Minju Seo', 'hidden': False}, {'_id': '6965bbbcfc8c4ecc02c7f90a', 'name': 'Moontae Lee', 'hidden': False}, {'_id': '6965bbbcfc8c4ecc02c7f90b', 'name': 'Minjoon Seo', 'hidden': False}], 'publishedAt': '2026-01-12T05:43:51.000Z', 'submittedOnDailyAt': '2026-01-13T01:11:30.100Z', 'title': 'Lost in the Noise: How Reasoning Models Fail with Contextual Distractors', 'submittedOnDailyBy': {'_id': '6550c4f27bbfce1878f5f280', 'avatarUrl': '/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg', 'isPro': False, 'fullname': 'seongyun_lee', 'user': 'Seongyun', 'type': 'user'}, 'summary': 'Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.', 'upvotes': 22, 'discussionId': '6965bbbcfc8c4ecc02c7f90c', 'ai_summary': 'NoisyBench benchmark reveals significant performance degradation in state-of-the-art models when exposed to noisy contextual information, with agentic workflows amplifying errors and attention mechanisms disproportionately focusing on distractor tokens.', 'ai_keywords': ['RAG', 'reasoning models', 'agentic AI systems', 'contextual distractors', 'attention visualization', 'reward modeling', 'SFT', 'outcome-reward RL', 'Rationale-Aware Reward', 'inverse scaling trend'], 'organization': {'_id': '6475760c33192631bad2bb38', 'name': 'kaist-ai', 'fullname': 'KAIST AI', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png'}}, 'publishedAt': '2026-01-12T00:43:51.000Z', 'title': 'Lost in the Noise: How Reasoning Models Fail with Contextual Distractors', 'summary': 'Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07226.png', 'numComments': 1, 'submittedBy': {'_id': '6550c4f27bbfce1878f5f280', 'avatarUrl': '/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg', 'fullname': 'seongyun_lee', 'name': 'Seongyun', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9, 'isUserFollowing': False}, 'organization': {'_id': '6475760c33192631bad2bb38', 'name': 'kaist-ai', 'fullname': 'KAIST AI', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.07351', 'authors': [{'_id': '6965e693fc8c4ecc02c7fa43', 'user': {'_id': '644238f6fcbe90d73b319ea6', 'avatarUrl': '/avatars/06dcede0ea77344de7dded917706bcb2.svg', 'isPro': False, 'fullname': 'zhongzero', 'user': 'zhongzero', 'type': 'user'}, 'name': 'Linhao Zhong', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:23:27.270Z', 'hidden': False}, {'_id': '6965e693fc8c4ecc02c7fa44', 'name': 'Linyu Wu', 'hidden': False}, {'_id': '6965e693fc8c4ecc02c7fa45', 'name': 'Bozhen Fang', 'hidden': False}, {'_id': '6965e693fc8c4ecc02c7fa46', 'name': 'Tianjian Feng', 'hidden': False}, {'_id': '6965e693fc8c4ecc02c7fa47', 'name': 'Chenchen Jing', 'hidden': False}, {'_id': '6965e693fc8c4ecc02c7fa48', 'name': 'Wen Wang', 'hidden': False}, {'_id': '6965e693fc8c4ecc02c7fa49', 'name': 'Jiaheng Zhang', 'hidden': False}, {'_id': '6965e693fc8c4ecc02c7fa4a', 'name': 'Hao Chen', 'hidden': False}, {'_id': '6965e693fc8c4ecc02c7fa4b', 'name': 'Chunhua Shen', 'hidden': False}], 'publishedAt': '2026-01-12T09:25:14.000Z', 'submittedOnDailyAt': '2026-01-13T06:04:17.414Z', 'title': 'Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models', 'submittedOnDailyBy': {'_id': '644238f6fcbe90d73b319ea6', 'avatarUrl': '/avatars/06dcede0ea77344de7dded917706bcb2.svg', 'isPro': False, 'fullname': 'zhongzero', 'user': 'zhongzero', 'type': 'user'}, 'summary': 'Diffusion Language Models (DLMs) offer a promising alternative for language modeling by enabling parallel decoding through iterative refinement. However, most DLMs rely on hard binary masking and discrete token assignments, which hinder the revision of early decisions and underutilize intermediate probabilistic representations. In this paper, we propose EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. EvoToken-DLM enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. To effectively support this evolution, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently achieves superior performance, outperforming strong diffusion-based and masked DLM baselines. Project webpage: https://aim-uofa.github.io/EvoTokenDLM.', 'upvotes': 19, 'discussionId': '6965e693fc8c4ecc02c7fa4c', 'projectPage': 'https://aim-uofa.github.io/EvoTokenDLM/', 'githubRepo': 'https://github.com/aim-uofa/EvoTokenDLM', 'githubRepoAddedBy': 'user', 'ai_summary': 'EvoToken-DLM introduces a diffusion-based language modeling approach that uses soft token distributions and continuous trajectory supervision to enable revisable decoding and outperforms existing baselines.', 'ai_keywords': ['diffusion language models', 'hard binary masking', 'discrete token assignments', 'soft token distributions', 'iterative refinement', 'progressive transition', 'continuous trajectory supervision', 'revisable decoding'], 'githubStars': 12}, 'publishedAt': '2026-01-12T04:25:14.000Z', 'title': 'Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models', 'summary': 'Diffusion Language Models (DLMs) offer a promising alternative for language modeling by enabling parallel decoding through iterative refinement. However, most DLMs rely on hard binary masking and discrete token assignments, which hinder the revision of early decisions and underutilize intermediate probabilistic representations. In this paper, we propose EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. EvoToken-DLM enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. To effectively support this evolution, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently achieves superior performance, outperforming strong diffusion-based and masked DLM baselines. Project webpage: https://aim-uofa.github.io/EvoTokenDLM.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07351.png', 'numComments': 1, 'submittedBy': {'_id': '644238f6fcbe90d73b319ea6', 'avatarUrl': '/avatars/06dcede0ea77344de7dded917706bcb2.svg', 'fullname': 'zhongzero', 'name': 'zhongzero', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isUserFollowing': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.05107', 'authors': [{'_id': '69645331138cc47cbd76520a', 'name': 'Muzhao Tian', 'hidden': False}, {'_id': '69645331138cc47cbd76520b', 'user': {'_id': '662b6a8f0b7f23f3c000559e', 'avatarUrl': '/avatars/0a5b4e09ac9a8e40342131319ff32b29.svg', 'isPro': False, 'fullname': 'Zisu Huang', 'user': 'zisuh', 'type': 'user'}, 'name': 'Zisu Huang', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-12T10:34:47.680Z', 'hidden': False}, {'_id': '69645331138cc47cbd76520c', 'name': 'Xiaohua Wang', 'hidden': False}, {'_id': '69645331138cc47cbd76520d', 'name': 'Jingwen Xu', 'hidden': False}, {'_id': '69645331138cc47cbd76520e', 'name': 'Zhengkang Guo', 'hidden': False}, {'_id': '69645331138cc47cbd76520f', 'name': 'Qi Qian', 'hidden': False}, {'_id': '69645331138cc47cbd765210', 'name': 'Yuanzhe Shen', 'hidden': False}, {'_id': '69645331138cc47cbd765211', 'name': 'Kaitao Song', 'hidden': False}, {'_id': '69645331138cc47cbd765212', 'name': 'Jiakang Yuan', 'hidden': False}, {'_id': '69645331138cc47cbd765213', 'name': 'Changze Lv', 'hidden': False}, {'_id': '69645331138cc47cbd765214', 'name': 'Xiaoqing Zheng', 'hidden': False}], 'publishedAt': '2026-01-08T16:54:30.000Z', 'submittedOnDailyAt': '2026-01-13T04:35:37.744Z', 'title': 'Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction', 'submittedOnDailyBy': {'_id': '662b6a8f0b7f23f3c000559e', 'avatarUrl': '/avatars/0a5b4e09ac9a8e40342131319ff32b29.svg', 'isPro': False, 'fullname': 'Zisu Huang', 'user': 'zisuh', 'type': 'user'}, 'summary': \"As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to Memory Anchoring, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose Steerable Memory Agent, SteeM, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.\", 'upvotes': 16, 'discussionId': '69645331138cc47cbd765215', 'ai_summary': 'A framework is presented that enables dynamic regulation of memory reliance in LLM-based agents, allowing users to control the balance between innovation and historical fidelity in long-term interactions.', 'ai_keywords': ['LLM-based agents', 'cumulative memory', 'memory anchoring', 'memory dependence', 'SteeM framework', 'personalized human-agent collaboration']}, 'publishedAt': '2026-01-08T11:54:30.000Z', 'title': 'Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction', 'summary': \"As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to Memory Anchoring, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose Steerable Memory Agent, SteeM, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05107.png', 'numComments': 2, 'submittedBy': {'_id': '662b6a8f0b7f23f3c000559e', 'avatarUrl': '/avatars/0a5b4e09ac9a8e40342131319ff32b29.svg', 'fullname': 'Zisu Huang', 'name': 'zisuh', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1, 'isUserFollowing': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.01528', 'authors': [{'_id': '69607a4e5b7998385e639539', 'user': {'_id': '673da13113236319ef2228ad', 'avatarUrl': '/avatars/682da7a5896e1cabfb1e9420ec9bbff4.svg', 'isPro': False, 'fullname': 'yang', 'user': 'yangzhou99', 'type': 'user'}, 'name': 'Yang Zhou', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-09T08:34:59.297Z', 'hidden': False}, {'_id': '69607a4e5b7998385e63953a', 'name': 'Hao Shao', 'hidden': False}, {'_id': '69607a4e5b7998385e63953b', 'name': 'Letian Wang', 'hidden': False}, {'_id': '69607a4e5b7998385e63953c', 'name': 'Zhuofan Zong', 'hidden': False}, {'_id': '69607a4e5b7998385e63953d', 'name': 'Hongsheng Li', 'hidden': False}, {'_id': '69607a4e5b7998385e63953e', 'name': 'Steven L. Waslander', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/673da13113236319ef2228ad/uTE4IhjonlJOEdqFcSSPa.mp4'], 'publishedAt': '2026-01-04T13:36:21.000Z', 'submittedOnDailyAt': '2026-01-13T05:11:19.227Z', 'title': 'DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving', 'submittedOnDailyBy': {'_id': '673da13113236319ef2228ad', 'avatarUrl': '/avatars/682da7a5896e1cabfb1e9420ec9bbff4.svg', 'isPro': False, 'fullname': 'yang', 'user': 'yangzhou99', 'type': 'user'}, 'summary': 'Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.', 'upvotes': 16, 'discussionId': '69607a4f5b7998385e63953f', 'projectPage': 'https://drivinggen-bench.github.io/', 'githubRepo': 'https://github.com/youngzhou1999/DrivingGen', 'githubRepoAddedBy': 'user', 'ai_summary': 'DrivingGen presents the first comprehensive benchmark for generative driving world models, addressing limitations in existing evaluations through diverse datasets and metrics that assess visual realism, trajectory plausibility, temporal coherence, and controllability.', 'ai_keywords': ['video generation models', 'world models', 'autonomous driving', 'generative simulators', 'driving world models', 'DrivingGen', 'evaluation dataset', 'visual realism', 'trajectory plausibility', 'temporal coherence', 'controllability'], 'githubStars': 7, 'organization': {'_id': '62c5000b4d3cf26ce7c62822', 'name': 'uoft', 'fullname': 'University of Toronto', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1657077766523-62c4ff85cb7033fd49b7a559.png'}}, 'publishedAt': '2026-01-04T08:36:21.000Z', 'title': 'DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving', 'summary': 'Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/673da13113236319ef2228ad/uTE4IhjonlJOEdqFcSSPa.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01528.png', 'numComments': 1, 'submittedBy': {'_id': '673da13113236319ef2228ad', 'avatarUrl': '/avatars/682da7a5896e1cabfb1e9420ec9bbff4.svg', 'fullname': 'yang', 'name': 'yangzhou99', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3, 'isUserFollowing': False}, 'organization': {'_id': '62c5000b4d3cf26ce7c62822', 'name': 'uoft', 'fullname': 'University of Toronto', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1657077766523-62c4ff85cb7033fd49b7a559.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.07526', 'authors': [{'_id': '6965c626fc8c4ecc02c7f9b2', 'name': 'Lei Zhang', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9b3', 'name': 'Mouxiang Chen', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9b4', 'name': 'Ruisheng Cao', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9b5', 'name': 'Jiawei Chen', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9b6', 'name': 'Fan Zhou', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9b7', 'name': 'Yiheng Xu', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9b8', 'name': 'Jiaxi Yang', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9b9', 'name': 'Liang Chen', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9ba', 'name': 'Changwei Luo', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9bb', 'name': 'Kai Zhang', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9bc', 'name': 'Fan Yan', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9bd', 'name': 'KaShun Shum', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9be', 'name': 'Jiajun Zhang', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9bf', 'name': 'Zeyu Cui', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9c0', 'name': 'Hu Feng', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9c1', 'name': 'Junyang Lin', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9c2', 'name': 'Binyuan Hui', 'hidden': False}, {'_id': '6965c626fc8c4ecc02c7f9c3', 'name': 'Min Yang', 'hidden': False}], 'publishedAt': '2026-01-12T13:25:33.000Z', 'submittedOnDailyAt': '2026-01-13T01:47:40.945Z', 'title': 'MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era', 'submittedOnDailyBy': {'_id': '64c38871f9cd765462fa1a17', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg', 'isPro': False, 'fullname': 'Lei Zhang', 'user': 'Lemoncoke', 'type': 'user'}, 'summary': 'The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.', 'upvotes': 14, 'discussionId': '6965c626fc8c4ecc02c7f9c4', 'ai_summary': 'MegaFlow is a distributed orchestration system that enables large-scale training and evaluation of agents on complex tasks by providing efficient scheduling, resource allocation, and task management through modular services.', 'ai_keywords': ['distributed orchestration system', 'agent-environment interactions', 'large-scale training', 'resource allocation', 'task management', 'modular services', 'concurrent agent tasks', 'system stability', 'resource utilization']}, 'publishedAt': '2026-01-12T08:25:33.000Z', 'title': 'MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era', 'summary': 'The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07526.png', 'numComments': 1, 'submittedBy': {'_id': '64c38871f9cd765462fa1a17', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg', 'fullname': 'Lei Zhang', 'name': 'Lemoncoke', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4, 'isUserFollowing': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.06165', 'authors': [{'_id': '6965ae99fc8c4ecc02c7f867', 'user': {'_id': '66120647cac232c1507e13da', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66120647cac232c1507e13da/iOBrsyr2mVdVLAFF6pZt6.png', 'isPro': False, 'fullname': 'DasolChoi', 'user': 'Dasool', 'type': 'user'}, 'name': 'Dasol Choi', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:24:22.667Z', 'hidden': False}, {'_id': '6965ae99fc8c4ecc02c7f868', 'name': 'Guijin Son', 'hidden': False}, {'_id': '6965ae99fc8c4ecc02c7f869', 'name': 'Hanwool Lee', 'hidden': False}, {'_id': '6965ae99fc8c4ecc02c7f86a', 'name': 'Minhyuk Kim', 'hidden': False}, {'_id': '6965ae99fc8c4ecc02c7f86b', 'name': 'Hyunwoo Ko', 'hidden': False}, {'_id': '6965ae99fc8c4ecc02c7f86c', 'name': 'Teabin Lim', 'hidden': False}, {'_id': '6965ae99fc8c4ecc02c7f86d', 'name': 'Ahn Eungyeol', 'hidden': False}, {'_id': '6965ae99fc8c4ecc02c7f86e', 'name': 'Jungwhan Kim', 'hidden': False}, {'_id': '6965ae99fc8c4ecc02c7f86f', 'name': 'Seunghyeok Hong', 'hidden': False}, {'_id': '6965ae99fc8c4ecc02c7f870', 'name': 'Youngsook Song', 'hidden': False}], 'publishedAt': '2026-01-07T02:33:03.000Z', 'submittedOnDailyAt': '2026-01-13T00:16:35.620Z', 'title': 'What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models', 'submittedOnDailyBy': {'_id': '66120647cac232c1507e13da', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66120647cac232c1507e13da/iOBrsyr2mVdVLAFF6pZt6.png', 'isPro': False, 'fullname': 'DasolChoi', 'user': 'Dasool', 'type': 'user'}, 'summary': 'Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment.', 'upvotes': 14, 'discussionId': '6965ae99fc8c4ecc02c7f871', 'githubRepo': 'https://github.com/HAE-RAE/HAERAE-VISION', 'githubRepoAddedBy': 'user', 'ai_summary': 'Real-world vision-language benchmarks reveal that under-specified user queries pose significant challenges for current models, with explicit query rewriting leading to substantial performance improvements.', 'ai_keywords': ['vision-language models', 'query explicitation', 'real-world benchmarks', 'visual questions', 'user queries', 'retrieval systems'], 'githubStars': 10, 'organization': {'_id': '645ae5e85e6871b4b2d6bd80', 'name': 'HAERAE-HUB', 'fullname': 'HAE-RAE', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/60d3e619b8448e1785bbda2a/zasfyk1U_yRBgrluB6ggc.png'}}, 'publishedAt': '2026-01-06T21:33:03.000Z', 'title': 'What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models', 'summary': 'Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06165.png', 'numComments': 1, 'submittedBy': {'_id': '66120647cac232c1507e13da', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66120647cac232c1507e13da/iOBrsyr2mVdVLAFF6pZt6.png', 'fullname': 'DasolChoi', 'name': 'Dasool', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5, 'isUserFollowing': False}, 'organization': {'_id': '645ae5e85e6871b4b2d6bd80', 'name': 'HAERAE-HUB', 'fullname': 'HAE-RAE', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/60d3e619b8448e1785bbda2a/zasfyk1U_yRBgrluB6ggc.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.06860', 'authors': [{'_id': '6965b7a8fc8c4ecc02c7f8b1', 'name': 'Yifei Chen', 'hidden': False}, {'_id': '6965b7a8fc8c4ecc02c7f8b2', 'user': {'_id': '61cd4b833dd34ba1985e0753', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png', 'isPro': False, 'fullname': 'KABI', 'user': 'dongguanting', 'type': 'user'}, 'name': 'Guanting Dong', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:24:17.728Z', 'hidden': False}, {'_id': '6965b7a8fc8c4ecc02c7f8b3', 'name': 'Zhicheng Dou', 'hidden': False}], 'publishedAt': '2026-01-11T11:05:26.000Z', 'submittedOnDailyAt': '2026-01-13T00:45:45.425Z', 'title': 'ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration', 'submittedOnDailyBy': {'_id': '6621ec2524eb2673fe0790fc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg', 'isPro': False, 'fullname': 'Ania Forge', 'user': 'zhangboguodong', 'type': 'user'}, 'summary': \"Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of  across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent\", 'upvotes': 13, 'discussionId': '6965b7a9fc8c4ecc02c7f8b4', 'ai_summary': 'ET-Agent is a training framework that calibrates tool-use behavior in large language models through self-evolving data flywheels and behavior calibration training to improve task execution effectiveness.', 'ai_keywords': ['Tool-Integrated Reasoning', 'large language models', 'agent training framework', 'behavior calibration training', 'self-evolving data flywheel', 'tool-use behavior', 'exploration ability', 'two-phases training'], 'organization': {'_id': '622177ac43826d6f261f8208', 'name': 'RUC', 'fullname': 'Renmin University of China', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg'}}, 'publishedAt': '2026-01-11T06:05:26.000Z', 'title': 'ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration', 'summary': \"Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of  across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06860.png', 'numComments': 1, 'submittedBy': {'_id': '6621ec2524eb2673fe0790fc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg', 'fullname': 'Ania Forge', 'name': 'zhangboguodong', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7, 'isUserFollowing': False}, 'organization': {'_id': '622177ac43826d6f261f8208', 'name': 'RUC', 'fullname': 'Renmin University of China', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.05823', 'authors': [{'_id': '6965b93ffc8c4ecc02c7f8d3', 'name': 'John Page', 'hidden': False}, {'_id': '6965b93ffc8c4ecc02c7f8d4', 'name': 'Xuesong Niu', 'hidden': False}, {'_id': '6965b93ffc8c4ecc02c7f8d5', 'name': 'Kai Wu', 'hidden': False}, {'_id': '6965b93ffc8c4ecc02c7f8d6', 'name': 'Kun Gai', 'hidden': False}], 'publishedAt': '2026-01-09T14:54:30.000Z', 'submittedOnDailyAt': '2026-01-13T00:51:24.367Z', 'title': 'Boosting Latent Diffusion Models via Disentangled Representation Alignment', 'submittedOnDailyBy': {'_id': '68e741ea3edb0ff47e20084e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg', 'isPro': False, 'fullname': 'Wu Kai', 'user': 'KaiiWuu1993', 'type': 'user'}, 'summary': 'Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.', 'upvotes': 13, 'discussionId': '6965b940fc8c4ecc02c7f8d7', 'ai_summary': 'Latent Diffusion Models generate high-quality images by operating in compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.', 'ai_keywords': ['Latent Diffusion Models', 'Variational Autoencoders', 'Vision Foundation Models', 'semantic disentanglement', 'non-linear mapper network', 'flow-based transformers', 'classifier-free guidance', 'FID', 'ImageNet'], 'organization': {'_id': '665f02ce9f9e5b38d0a256a8', 'name': 'Kwai-Kolors', 'fullname': 'Kolors Team, Kuaishou Technology', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png'}}, 'publishedAt': '2026-01-09T09:54:30.000Z', 'title': 'Boosting Latent Diffusion Models via Disentangled Representation Alignment', 'summary': 'Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05823.png', 'numComments': 1, 'submittedBy': {'_id': '68e741ea3edb0ff47e20084e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg', 'fullname': 'Wu Kai', 'name': 'KaiiWuu1993', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4, 'isUserFollowing': False}, 'organization': {'_id': '665f02ce9f9e5b38d0a256a8', 'name': 'Kwai-Kolors', 'fullname': 'Kolors Team, Kuaishou Technology', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.04698', 'authors': [{'_id': '6965b35dfc8c4ecc02c7f897', 'name': 'Yinuo Wang', 'hidden': False}, {'_id': '6965b35dfc8c4ecc02c7f898', 'name': 'Mining Tan', 'hidden': False}, {'_id': '6965b35dfc8c4ecc02c7f899', 'name': 'Wenxiang Jiao', 'hidden': False}, {'_id': '6965b35dfc8c4ecc02c7f89a', 'name': 'Xiaoxi Li', 'hidden': False}, {'_id': '6965b35dfc8c4ecc02c7f89b', 'name': 'Hao Wang', 'hidden': False}, {'_id': '6965b35dfc8c4ecc02c7f89c', 'name': 'Xuanyu Zhang', 'hidden': False}, {'_id': '6965b35dfc8c4ecc02c7f89d', 'name': 'Yuan Lu', 'hidden': False}, {'_id': '6965b35dfc8c4ecc02c7f89e', 'name': 'Weiming Dong', 'hidden': False}], 'publishedAt': '2026-01-08T08:08:35.000Z', 'submittedOnDailyAt': '2026-01-13T00:24:40.447Z', 'title': 'TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning', 'submittedOnDailyBy': {'_id': '66e03eace17fb5ff054b7686', 'avatarUrl': '/avatars/2b739ff11e43dd9e701c647a92617f20.svg', 'isPro': False, 'fullname': 'Xiaoxi Li', 'user': 'lixiaoxi45', 'type': 'user'}, 'summary': \"Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.\", 'upvotes': 8, 'discussionId': '6965b35dfc8c4ecc02c7f89f', 'ai_summary': 'TourPlanner addresses travel planning challenges through multi-path reasoning and constraint-gated reinforcement learning to optimize both hard and soft constraints effectively.', 'ai_keywords': ['multi-path reasoning', 'constraint-gated reinforcement learning', 'Personalized Recall and Spatial Optimization', 'CCoT', 'spatially-aware candidate POIs', 'feasible solution space', 'hard constraints', 'soft constraints']}, 'publishedAt': '2026-01-08T03:08:35.000Z', 'title': 'TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning', 'summary': \"Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04698.png', 'numComments': 1, 'submittedBy': {'_id': '66e03eace17fb5ff054b7686', 'avatarUrl': '/avatars/2b739ff11e43dd9e701c647a92617f20.svg', 'fullname': 'Xiaoxi Li', 'name': 'lixiaoxi45', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 19, 'isUserFollowing': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.07055', 'authors': [{'_id': '6965c753fc8c4ecc02c7f9c6', 'name': 'Zhenrui Yue', 'hidden': False}, {'_id': '6965c753fc8c4ecc02c7f9c7', 'name': 'Kartikeya Upasani', 'hidden': False}, {'_id': '6965c753fc8c4ecc02c7f9c8', 'name': 'Xianjun Yang', 'hidden': False}, {'_id': '6965c753fc8c4ecc02c7f9c9', 'name': 'Suyu Ge', 'hidden': False}, {'_id': '6965c753fc8c4ecc02c7f9ca', 'name': 'Shaoliang Nie', 'hidden': False}, {'_id': '6965c753fc8c4ecc02c7f9cb', 'name': 'Yuning Mao', 'hidden': False}, {'_id': '6965c753fc8c4ecc02c7f9cc', 'name': 'Zhe Liu', 'hidden': False}, {'_id': '6965c753fc8c4ecc02c7f9cd', 'name': 'Dong Wang', 'hidden': False}], 'publishedAt': '2026-01-11T20:27:55.000Z', 'submittedOnDailyAt': '2026-01-13T01:47:35.465Z', 'title': 'Dr. Zero: Self-Evolving Search Agents without Training Data', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': \"As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.\", 'upvotes': 6, 'discussionId': '6965c753fc8c4ecc02c7f9ce', 'githubRepo': 'https://github.com/facebookresearch/drzero', 'githubRepoAddedBy': 'user', 'ai_summary': 'A data-free self-evolution framework enables large language models to autonomously improve reasoning capabilities through iterative question generation and solving, achieving performance comparable to supervised methods.', 'ai_keywords': ['large language models', 'self-evolution', 'search agents', 'proposer', 'solver', 'automated curriculum', 'hop-grouped relative policy optimization', 'HRPO', 'multi-turn reasoning', 'tool using', 'question diversity', 'compute requirements', 'training efficiency', 'performance', 'stability'], 'githubStars': 31}, 'publishedAt': '2026-01-11T15:27:55.000Z', 'title': 'Dr. Zero: Self-Evolving Search Agents without Training Data', 'summary': \"As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07055.png', 'numComments': 1, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 207, 'isUserFollowing': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.06803', 'authors': [{'_id': '69663b7f87c71000b5a910a3', 'name': 'Yubo Wang', 'hidden': False}, {'_id': '69663b7f87c71000b5a910a4', 'name': 'Juntian Zhang', 'hidden': False}, {'_id': '69663b7f87c71000b5a910a5', 'name': 'Yichen Wu', 'hidden': False}, {'_id': '69663b7f87c71000b5a910a6', 'name': 'Yankai Lin', 'hidden': False}, {'_id': '69663b7f87c71000b5a910a7', 'name': 'Nils Lukas', 'hidden': False}, {'_id': '69663b7f87c71000b5a910a8', 'name': 'Yuhan Liu', 'hidden': False}], 'publishedAt': '2026-01-11T08:30:49.000Z', 'submittedOnDailyAt': '2026-01-13T10:09:35.584Z', 'title': 'Forest Before Trees: Latent Superposition for Efficient Visual Reasoning', 'submittedOnDailyBy': {'_id': '627a124ffe55fa0f8ce0eaf7', 'avatarUrl': '/avatars/41e0dc029faed6dc45d620c5fe2652a5.svg', 'isPro': False, 'fullname': 'Serendipity', 'user': 'Yuhan', 'type': 'user'}, 'summary': 'While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, a novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). Instead of forcing a point-wise prediction, Laser aligns the latent state with a dynamic validity window of future semantics. This mechanism enforces a \"Forest-before-Trees\" cognitive hierarchy, enabling the model to maintain a probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains.', 'upvotes': 5, 'discussionId': '69663b8087c71000b5a910a9', 'ai_summary': 'Laser introduces a new visual reasoning paradigm using dynamic windowed alignment learning to maintain global features during latent deduction while achieving superior performance with reduced computational overhead.', 'ai_keywords': ['Chain-of-Thought', 'Large Vision-Language Models', 'latent reasoning', 'semantic collapse', 'Dynamic Windowed Alignment Learning', 'DWAL', 'latent state', 'validity window', 'cognitive hierarchy', 'probabilistic superposition', 'decodable trajectories', 'Self-Refined Superposition', 'Monet', 'inference tokens']}, 'publishedAt': '2026-01-11T03:30:49.000Z', 'title': 'Forest Before Trees: Latent Superposition for Efficient Visual Reasoning', 'summary': 'While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, a novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). Instead of forcing a point-wise prediction, Laser aligns the latent state with a dynamic validity window of future semantics. This mechanism enforces a \"Forest-before-Trees\" cognitive hierarchy, enabling the model to maintain a probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06803.png', 'numComments': 1, 'submittedBy': {'_id': '627a124ffe55fa0f8ce0eaf7', 'avatarUrl': '/avatars/41e0dc029faed6dc45d620c5fe2652a5.svg', 'fullname': 'Serendipity', 'name': 'Yuhan', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isUserFollowing': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.07376', 'authors': [{'_id': '6965c42cfc8c4ecc02c7f999', 'user': {'_id': '65621fd68631d43d2baf33b2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1JemNCnPkS1mE3SNygsE2.png', 'isPro': False, 'fullname': 'siqi zhu', 'user': 'zsqzz', 'type': 'user'}, 'name': 'Siqi Zhu', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:23:48.238Z', 'hidden': False}, {'_id': '6965c42cfc8c4ecc02c7f99a', 'name': 'Jiaxuan You', 'hidden': False}], 'publishedAt': '2026-01-12T09:57:46.000Z', 'submittedOnDailyAt': '2026-01-13T02:38:15.070Z', 'title': 'OpenTinker: Separating Concerns in Agentic Reinforcement Learning', 'submittedOnDailyBy': {'_id': '65621fd68631d43d2baf33b2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1JemNCnPkS1mE3SNygsE2.png', 'isPro': False, 'fullname': 'siqi zhu', 'user': 'zsqzz', 'type': 'user'}, 'summary': 'We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.', 'upvotes': 4, 'discussionId': '6965c42dfc8c4ecc02c7f99b', 'projectPage': 'https://open-tinker.github.io/opentinker-page/', 'githubRepo': 'https://github.com/open-tinker/OpenTinker?tab=readme-ov-file', 'githubRepoAddedBy': 'user', 'ai_summary': 'OpenTinker provides a modular infrastructure for reinforcement learning of large language model agents with separated components and managed execution runtime.', 'ai_keywords': ['reinforcement learning', 'large language models', 'agent-environment interaction', 'LoRA-based RL', 'full-parameter RL', 'supervised fine-tuning'], 'githubStars': 566}, 'publishedAt': '2026-01-12T04:57:46.000Z', 'title': 'OpenTinker: Separating Concerns in Agentic Reinforcement Learning', 'summary': 'We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07376.png', 'numComments': 1, 'submittedBy': {'_id': '65621fd68631d43d2baf33b2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1JemNCnPkS1mE3SNygsE2.png', 'fullname': 'siqi zhu', 'name': 'zsqzz', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9, 'isUserFollowing': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.07767', 'authors': [{'_id': '6965e479fc8c4ecc02c7fa3d', 'name': 'Jiawei Wang', 'hidden': False}, {'_id': '6965e479fc8c4ecc02c7fa3e', 'name': 'Yanfei Zhou', 'hidden': False}, {'_id': '6965e479fc8c4ecc02c7fa3f', 'name': 'Siddartha Devic', 'hidden': False}, {'_id': '6965e479fc8c4ecc02c7fa40', 'user': {'_id': '63c8454e46421a2efe82709d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png', 'isPro': True, 'fullname': 'Deqing Fu', 'user': 'deqing', 'type': 'user'}, 'name': 'Deqing Fu', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:23:29.136Z', 'hidden': False}], 'publishedAt': '2026-01-12T17:49:51.000Z', 'submittedOnDailyAt': '2026-01-13T03:55:16.806Z', 'title': 'Are LLM Decisions Faithful to Verbal Confidence?', 'submittedOnDailyBy': {'_id': '63c8454e46421a2efe82709d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png', 'isPro': True, 'fullname': 'Deqing Fu', 'user': 'deqing', 'type': 'user'}, 'summary': 'Large Language Models (LLMs) can produce surprisingly sophisticated estimates of their own uncertainty. However, it remains unclear to what extent this expressed confidence is tied to the reasoning, knowledge, or decision making of the model. To test this, we introduce RiskEval: a framework designed to evaluate whether models adjust their abstention policies in response to varying error penalties. Our evaluation of several frontier models reveals a critical dissociation: models are neither cost-aware when articulating their verbal confidence, nor strategically responsive when deciding whether to engage or abstain under high-penalty conditions. Even when extreme penalties render frequent abstention the mathematically optimal strategy, models almost never abstain, resulting in utility collapse. This indicates that calibrated verbal confidence scores may not be sufficient to create trustworthy and interpretable AI systems, as current models lack the strategic agency to convert uncertainty signals into optimal and risk-sensitive decisions.', 'upvotes': 3, 'discussionId': '6965e479fc8c4ecc02c7fa41', 'ai_summary': 'Large language models exhibit a disconnect between their expressed uncertainty and strategic decision-making under varying penalty conditions, failing to adjust abstention policies even when optimal.', 'ai_keywords': ['large language models', 'uncertainty estimation', 'abstention policies', 'risk-sensitive decisions', 'calibration', 'strategic agency'], 'organization': {'_id': '63cc6a2ff488db9bb3ca61f3', 'name': 'USC', 'fullname': 'University of Southern California'}}, 'publishedAt': '2026-01-12T12:49:51.000Z', 'title': 'Are LLM Decisions Faithful to Verbal Confidence?', 'summary': 'Large Language Models (LLMs) can produce surprisingly sophisticated estimates of their own uncertainty. However, it remains unclear to what extent this expressed confidence is tied to the reasoning, knowledge, or decision making of the model. To test this, we introduce RiskEval: a framework designed to evaluate whether models adjust their abstention policies in response to varying error penalties. Our evaluation of several frontier models reveals a critical dissociation: models are neither cost-aware when articulating their verbal confidence, nor strategically responsive when deciding whether to engage or abstain under high-penalty conditions. Even when extreme penalties render frequent abstention the mathematically optimal strategy, models almost never abstain, resulting in utility collapse. This indicates that calibrated verbal confidence scores may not be sufficient to create trustworthy and interpretable AI systems, as current models lack the strategic agency to convert uncertainty signals into optimal and risk-sensitive decisions.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07767.png', 'numComments': 1, 'submittedBy': {'_id': '63c8454e46421a2efe82709d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png', 'fullname': 'Deqing Fu', 'name': 'deqing', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 12, 'isUserFollowing': False}, 'organization': {'_id': '63cc6a2ff488db9bb3ca61f3', 'name': 'USC', 'fullname': 'University of Southern California'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.06411', 'authors': [{'_id': '6965c168fc8c4ecc02c7f94f', 'name': 'Zhengxuan Lu', 'hidden': False}, {'_id': '6965c168fc8c4ecc02c7f950', 'name': 'Dongfang Li', 'hidden': False}, {'_id': '6965c168fc8c4ecc02c7f951', 'name': 'Yukun Shi', 'hidden': False}, {'_id': '6965c168fc8c4ecc02c7f952', 'name': 'Beilun Wang', 'hidden': False}, {'_id': '6965c168fc8c4ecc02c7f953', 'name': 'Longyue Wang', 'hidden': False}, {'_id': '6965c168fc8c4ecc02c7f954', 'name': 'Baotian Hu', 'hidden': False}], 'publishedAt': '2026-01-10T03:17:25.000Z', 'submittedOnDailyAt': '2026-01-13T01:23:34.080Z', 'title': 'Structured Episodic Event Memory', 'submittedOnDailyBy': {'_id': '62e38a3ef3b208e2aecf2c84', 'avatarUrl': '/avatars/eee9d7d53f3f9b7a18a21d289abd3c64.svg', 'isPro': False, 'fullname': 'Dongfang Li', 'user': 'crazyofapple', 'type': 'user'}, 'summary': 'Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.', 'upvotes': 3, 'discussionId': '6965c169fc8c4ecc02c7f955', 'ai_summary': 'Structured Episodic Event Memory (SEEM) enhances LLMs with hierarchical memory architecture combining graph and episodic layers for improved narrative coherence and reasoning.', 'ai_keywords': ['Retrieval-Augmented Generation', 'graph memory layer', 'episodic memory layer', 'cognitive frame theory', 'Episodic Event Frames', 'agentic associative fusion', 'Reverse Provenance Expansion'], 'organization': {'_id': '670819b38c9c6f598f37d86f', 'name': 'HarbinInstituteofTechnologyHIT', 'fullname': 'Harbin Institute of Technology'}}, 'publishedAt': '2026-01-09T22:17:25.000Z', 'title': 'Structured Episodic Event Memory', 'summary': 'Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06411.png', 'numComments': 1, 'submittedBy': {'_id': '62e38a3ef3b208e2aecf2c84', 'avatarUrl': '/avatars/eee9d7d53f3f9b7a18a21d289abd3c64.svg', 'fullname': 'Dongfang Li', 'name': 'crazyofapple', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3, 'isUserFollowing': False}, 'organization': {'_id': '670819b38c9c6f598f37d86f', 'name': 'HarbinInstituteofTechnologyHIT', 'fullname': 'Harbin Institute of Technology'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.07786', 'authors': [{'_id': '6965f09bfc8c4ecc02c7fa59', 'name': 'Abdullah Al Mujahid', 'hidden': False}, {'_id': '6965f09bfc8c4ecc02c7fa5a', 'name': 'Mia Mohammad Imran', 'hidden': False}], 'publishedAt': '2026-01-12T17:59:34.000Z', 'submittedOnDailyAt': '2026-01-13T04:44:10.677Z', 'title': '\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt', 'submittedOnDailyBy': {'_id': '6331c3f618711776b468e9ec', 'avatarUrl': '/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg', 'isPro': False, 'fullname': 'Mia Mohammad Imran', 'user': 'imranraad', 'type': 'user'}, 'summary': 'As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.', 'upvotes': 2, 'discussionId': '6965f09bfc8c4ecc02c7fa5b', 'ai_summary': 'Analysis of AI-referencing code comments reveals that developers explicitly acknowledge technical debt in AI-assisted code, identifying patterns of postponed testing, incomplete adaptation, and limited understanding as key factors in AI-induced technical debt emergence.', 'ai_keywords': ['large language models', 'generative AI', 'technical debt', 'self-admitted technical debt', 'AI-assisted code', 'code comments', 'software development workflows', 'artificial intelligence']}, 'publishedAt': '2026-01-12T12:59:34.000Z', 'title': '\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt', 'summary': 'As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07786.png', 'numComments': 1, 'submittedBy': {'_id': '6331c3f618711776b468e9ec', 'avatarUrl': '/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg', 'fullname': 'Mia Mohammad Imran', 'name': 'imranraad', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isUserFollowing': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.07181', 'authors': [{'_id': '6965bfabfc8c4ecc02c7f91d', 'name': 'Yichun Zhang', 'hidden': False}, {'_id': '6965bfabfc8c4ecc02c7f91e', 'name': 'Xiangwu Guo', 'hidden': False}, {'_id': '6965bfabfc8c4ecc02c7f91f', 'name': 'Yauhong Goh', 'hidden': False}, {'_id': '6965bfabfc8c4ecc02c7f920', 'name': 'Jessica Hu', 'hidden': False}, {'_id': '6965bfabfc8c4ecc02c7f921', 'name': 'Zhiheng Chen', 'hidden': False}, {'_id': '6965bfabfc8c4ecc02c7f922', 'name': 'Xin Wang', 'hidden': False}, {'_id': '6965bfabfc8c4ecc02c7f923', 'name': 'Difei Gao', 'hidden': False}, {'_id': '6965bfabfc8c4ecc02c7f924', 'name': 'Mike Zheng Shou', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/XXoUp3GCP7OY4DpF_NQgk.mp4'], 'publishedAt': '2026-01-12T04:04:20.000Z', 'submittedOnDailyAt': '2026-01-13T01:14:44.534Z', 'title': 'ShowUI-Aloha: Human-Taught GUI Agent', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': 'Graphical User Interfaces (GUIs) are central to human-computer interaction, yet automating complex GUI tasks remains a major challenge for autonomous agents, largely due to a lack of scalable, high-quality training data. While recordings of human demonstrations offer a rich data source, they are typically long, unstructured, and lack annotations, making them difficult for agents to learn from.To address this, we introduce ShowUI-Aloha, a comprehensive pipeline that transforms unstructured, in-the-wild human screen recordings from desktop environments into structured, actionable tasks. Our framework includes four key components: A recorder that captures screen video along with precise user interactions like mouse clicks, keystrokes, and scrolls. A learner that semantically interprets these raw interactions and the surrounding visual context, translating them into descriptive natural language captions. A planner that reads the parsed demonstrations, maintains task states, and dynamically formulates the next high-level action plan based on contextual reasoning. An executor that faithfully carries out these action plans at the OS level, performing precise clicks, drags, text inputs, and window operations with safety checks and real-time feedback. Together, these components provide a scalable solution for collecting and parsing real-world human data, demonstrating a viable path toward building general-purpose GUI agents that can learn effectively from simply observing humans.', 'upvotes': 2, 'discussionId': '6965bfabfc8c4ecc02c7f925', 'projectPage': 'https://showlab.github.io/Aloha_Page/', 'ai_summary': 'ShowUI-Aloha presents a pipeline that converts unstructured human screen recordings into structured GUI tasks through recording, semantic interpretation, planning, and execution components.', 'ai_keywords': ['GUI agents', 'screen recordings', 'semantic interpretation', 'task planning', 'action execution', 'human-computer interaction', 'autonomous agents', 'natural language captions', 'contextual reasoning']}, 'publishedAt': '2026-01-11T23:04:20.000Z', 'title': 'ShowUI-Aloha: Human-Taught GUI Agent', 'summary': 'Graphical User Interfaces (GUIs) are central to human-computer interaction, yet automating complex GUI tasks remains a major challenge for autonomous agents, largely due to a lack of scalable, high-quality training data. While recordings of human demonstrations offer a rich data source, they are typically long, unstructured, and lack annotations, making them difficult for agents to learn from.To address this, we introduce ShowUI-Aloha, a comprehensive pipeline that transforms unstructured, in-the-wild human screen recordings from desktop environments into structured, actionable tasks. Our framework includes four key components: A recorder that captures screen video along with precise user interactions like mouse clicks, keystrokes, and scrolls. A learner that semantically interprets these raw interactions and the surrounding visual context, translating them into descriptive natural language captions. A planner that reads the parsed demonstrations, maintains task states, and dynamically formulates the next high-level action plan based on contextual reasoning. An executor that faithfully carries out these action plans at the OS level, performing precise clicks, drags, text inputs, and window operations with safety checks and real-time feedback. Together, these components provide a scalable solution for collecting and parsing real-world human data, demonstrating a viable path toward building general-purpose GUI agents that can learn effectively from simply observing humans.', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/XXoUp3GCP7OY4DpF_NQgk.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07181.png', 'numComments': 0, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 207, 'isUserFollowing': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.07033', 'authors': [{'_id': '6965e456fc8c4ecc02c7fa36', 'name': 'Longfei Yun', 'hidden': False}, {'_id': '6965e456fc8c4ecc02c7fa37', 'name': 'Kun Zhou', 'hidden': False}, {'_id': '6965e456fc8c4ecc02c7fa38', 'user': {'_id': '64a62c2f500beb50968e5c9c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wfL3ojJmXqyzGUmCblPf4.jpeg', 'isPro': False, 'fullname': 'Yupeng Hou', 'user': 'hyp1231', 'type': 'user'}, 'name': 'Yupeng Hou', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:23:31.019Z', 'hidden': False}, {'_id': '6965e456fc8c4ecc02c7fa39', 'name': 'Letian Peng', 'hidden': False}, {'_id': '6965e456fc8c4ecc02c7fa3a', 'name': 'Jingbo Shang', 'hidden': False}], 'publishedAt': '2026-01-11T19:05:37.000Z', 'submittedOnDailyAt': '2026-01-13T03:51:47.479Z', 'title': 'Codified Foreshadowing-Payoff Text Generation', 'submittedOnDailyBy': {'_id': '64323dd503d81fa4d26deaf9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png', 'isPro': False, 'fullname': 'Letian Peng', 'user': 'KomeijiForce', 'type': 'user'}, 'summary': 'Foreshadowing and payoff are ubiquitous narrative devices through which authors introduce commitments early in a story and resolve them through concrete, observable outcomes. However, despite advances in story generation, large language models (LLMs) frequently fail to bridge these long-range narrative dependencies, often leaving \"Chekhov\\'s guns\" unfired even when the necessary context is present. Existing evaluations largely overlook this structural failure, focusing on surface-level coherence rather than the logical fulfillment of narrative setups. In this paper, we introduce Codified Foreshadowing-Payoff Generation (CFPG), a novel framework that reframes narrative quality through the lens of payoff realization. Recognizing that LLMs struggle to intuitively grasp the \"triggering mechanism\" of a foreshadowed event, CFPG transforms narrative continuity into a set of executable causal predicates. By mining and encoding Foreshadow-Trigger-Payoff triples from the BookSum corpus, we provide structured supervision that ensures foreshadowed commitments are not only mentioned but also temporally and logically fulfilled. Experiments demonstrate that CFPG significantly outperforms standard prompting baselines in payoff accuracy and narrative alignment. Our findings suggest that explicitly codifying narrative mechanics is essential for moving LLMs from surface-level fluency to genuine narrative competence.', 'upvotes': 2, 'discussionId': '6965e456fc8c4ecc02c7fa3b', 'ai_summary': 'Large language models struggle with maintaining long-range narrative dependencies, but a new framework called CFPG addresses this by structuring narrative continuity through executable causal predicates to ensure proper fulfillment of foreshadowed events.', 'ai_keywords': ['Codified Foreshadowing-Payoff Generation', 'narrative quality', 'payoff realization', 'causal predicates', 'BookSum corpus', 'foreshadow-Trigger-Payoff triples', 'narrative continuity', 'surface-level coherence', 'narrative competence']}, 'publishedAt': '2026-01-11T14:05:37.000Z', 'title': 'Codified Foreshadowing-Payoff Text Generation', 'summary': 'Foreshadowing and payoff are ubiquitous narrative devices through which authors introduce commitments early in a story and resolve them through concrete, observable outcomes. However, despite advances in story generation, large language models (LLMs) frequently fail to bridge these long-range narrative dependencies, often leaving \"Chekhov\\'s guns\" unfired even when the necessary context is present. Existing evaluations largely overlook this structural failure, focusing on surface-level coherence rather than the logical fulfillment of narrative setups. In this paper, we introduce Codified Foreshadowing-Payoff Generation (CFPG), a novel framework that reframes narrative quality through the lens of payoff realization. Recognizing that LLMs struggle to intuitively grasp the \"triggering mechanism\" of a foreshadowed event, CFPG transforms narrative continuity into a set of executable causal predicates. By mining and encoding Foreshadow-Trigger-Payoff triples from the BookSum corpus, we provide structured supervision that ensures foreshadowed commitments are not only mentioned but also temporally and logically fulfilled. Experiments demonstrate that CFPG significantly outperforms standard prompting baselines in payoff accuracy and narrative alignment. Our findings suggest that explicitly codifying narrative mechanics is essential for moving LLMs from surface-level fluency to genuine narrative competence.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07033.png', 'numComments': 1, 'submittedBy': {'_id': '64323dd503d81fa4d26deaf9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png', 'fullname': 'Letian Peng', 'name': 'KomeijiForce', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 10, 'isUserFollowing': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.03666', 'authors': [{'_id': '695fa6f62450f142afb3c58f', 'user': {'_id': '66add675c7a575aa0e03d5f3', 'avatarUrl': '/avatars/b72b18130664c1de197c1f8df371aa70.svg', 'isPro': True, 'fullname': 'Haonan Chen', 'user': 'Haon-Chen', 'type': 'user'}, 'name': 'Haonan Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-09T08:35:49.824Z', 'hidden': False}, {'_id': '695fa6f62450f142afb3c590', 'name': 'Sicheng Gao', 'hidden': False}, {'_id': '695fa6f62450f142afb3c591', 'name': 'Radu Timofte', 'hidden': False}, {'_id': '695fa6f62450f142afb3c592', 'name': 'Tetsuya Sakai', 'hidden': False}, {'_id': '695fa6f62450f142afb3c593', 'name': 'Zhicheng Dou', 'hidden': False}], 'publishedAt': '2026-01-07T07:39:40.000Z', 'submittedOnDailyAt': '2026-01-13T01:03:03.967Z', 'title': 'e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings', 'submittedOnDailyBy': {'_id': '66add675c7a575aa0e03d5f3', 'avatarUrl': '/avatars/b72b18130664c1de197c1f8df371aa70.svg', 'isPro': True, 'fullname': 'Haonan Chen', 'user': 'Haon-Chen', 'type': 'user'}, 'summary': 'Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.', 'upvotes': 2, 'discussionId': '695fa6f62450f142afb3c594', 'ai_summary': 'Omni-modal embedding models face challenges with modality-dependent similarity scaling, ineffective in-batch negatives, and mismatched statistics across modalities, which are addressed through explicit alignment techniques including temperature calibration, controlled negative curriculum, and batch whitening with covariance regularization.', 'ai_keywords': ['omni-modal embedding models', 'vision-language models', 'similarity logits', 'in-batch negatives', 'modality-aware temperature calibration', 'negative curriculum', 'debiasing', 'batch whitening', 'covariance regularization', 'cross-modal geometry']}, 'publishedAt': '2026-01-07T02:39:40.000Z', 'title': 'e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings', 'summary': 'Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03666.png', 'numComments': 1, 'submittedBy': {'_id': '66add675c7a575aa0e03d5f3', 'avatarUrl': '/avatars/b72b18130664c1de197c1f8df371aa70.svg', 'fullname': 'Haonan Chen', 'name': 'Haon-Chen', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 5, 'isUserFollowing': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.06944', 'authors': [{'_id': '6965c064fc8c4ecc02c7f927', 'name': 'Yuhang Su', 'hidden': False}, {'_id': '6965c064fc8c4ecc02c7f928', 'name': 'Mei Wang', 'hidden': False}, {'_id': '6965c064fc8c4ecc02c7f929', 'name': 'Yaoyao Zhong', 'hidden': False}, {'_id': '6965c064fc8c4ecc02c7f92a', 'name': 'Guozhang Li', 'hidden': False}, {'_id': '6965c064fc8c4ecc02c7f92b', 'name': 'Shixing Li', 'hidden': False}, {'_id': '6965c064fc8c4ecc02c7f92c', 'name': 'Yihan Feng', 'hidden': False}, {'_id': '6965c064fc8c4ecc02c7f92d', 'name': 'Hua Huang', 'hidden': False}], 'publishedAt': '2026-01-11T15:08:05.000Z', 'submittedOnDailyAt': '2026-01-13T01:17:50.475Z', 'title': 'SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models', 'submittedOnDailyBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'isPro': True, 'fullname': 'taesiri', 'user': 'taesiri', 'type': 'user'}, 'summary': \"While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark's effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/yuhangsu82/SketchJudge.\", 'upvotes': 1, 'discussionId': '6965c064fc8c4ecc02c7f92e', 'ai_summary': \"SketchJudge benchmark evaluates multimodal large language models' ability to grade hand-drawn STEM diagrams, revealing significant limitations in visual understanding compared to human performance.\", 'ai_keywords': ['Multimodal Large Language Models', 'visual understanding', 'hand-drawn diagrams', 'visual grading', 'STEM diagrams', 'symbolic context', 'noisy context']}, 'publishedAt': '2026-01-11T10:08:05.000Z', 'title': 'SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models', 'summary': \"While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark's effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/yuhangsu82/SketchJudge.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06944.png', 'numComments': 0, 'submittedBy': {'_id': '6039478ab3ecf716b1a5fd4d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg', 'fullname': 'taesiri', 'name': 'taesiri', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 207, 'isUserFollowing': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.06747', 'authors': [{'_id': '6965de3ffc8c4ecc02c7fa1d', 'name': 'Glenn Matlin', 'hidden': False}, {'_id': '6965de3ffc8c4ecc02c7fa1e', 'user': {'_id': '676c67da7bad1587f2d046e2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/676c67da7bad1587f2d046e2/Y-aJmRj9FB49n36X1qKo9.jpeg', 'isPro': True, 'fullname': 'Akhil Theerthala', 'user': 'Akhil-Theerthala', 'type': 'user'}, 'name': 'Akhil Theerthala', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:23:32.745Z', 'hidden': False}, {'_id': '6965de3ffc8c4ecc02c7fa1f', 'name': 'Anant Gupta', 'hidden': False}, {'_id': '6965de3ffc8c4ecc02c7fa20', 'name': 'Anirudh JM', 'hidden': False}, {'_id': '6965de3ffc8c4ecc02c7fa21', 'name': 'Rayan Castilla', 'hidden': False}, {'_id': '6965de3ffc8c4ecc02c7fa22', 'name': 'Yi Mei Ng', 'hidden': False}, {'_id': '6965de3ffc8c4ecc02c7fa23', 'name': 'Sudheer Chava', 'hidden': False}], 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/676c67da7bad1587f2d046e2/K6vyIIZ3Sw-g9CKXM_MbG.png'], 'publishedAt': '2026-01-11T01:38:33.000Z', 'submittedOnDailyAt': '2026-01-13T06:19:37.937Z', 'title': 'FinForge: Semi-Synthetic Financial Benchmark Generation', 'submittedOnDailyBy': {'_id': '676c67da7bad1587f2d046e2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/676c67da7bad1587f2d046e2/Y-aJmRj9FB49n36X1qKo9.jpeg', 'isPro': True, 'fullname': 'Akhil Theerthala', 'user': 'Akhil-Theerthala', 'type': 'user'}, 'summary': \"Evaluating Language Models (LMs) in specialized, high-stakes domains such as finance remains a significant challenge due to the scarcity of open, high-quality, and domain-specific datasets. Existing general-purpose benchmarks provide broad coverage but lack the depth and domain fidelity needed to assess LMs' capabilities for real-world financial reasoning, which requires both conceptual understanding and quantitative rigor. To address this gap, we introduce FinForge, a scalable, semi-synthetic pipeline for constructing finance-specific evaluation benchmarks through a hybrid of expert-guided data curation and controlled LM-based synthesis. FinForge combines manual and programmatic corpus construction from authoritative financial sources with structured question generation and validation using Gemini 2.5 Flash. To demonstrate the pipeline's efficacy, we produce FinForge-5k, a snapshot benchmark comprising over 5,000 human-validated question-answer pairs across 11 finance subdomains, derived from a curated corpus of 100,000 verified documents totaling 143M tokens. Evaluation of state-of-the-art open-source and closed-source models on FinForge-5k reveals significant differences in financial reasoning, with leading models achieving accuracy levels near 80%. These findings underscore the framework's utility for diagnosing current model limitations and guiding future improvements in financial domain competence. All code and data are available at https://github.com/gtfintechlab/FinForge.\", 'upvotes': 1, 'discussionId': '6965de3ffc8c4ecc02c7fa24', 'ai_summary': 'FinForge presents a scalable semi-synthetic pipeline for creating domain-specific financial evaluation benchmarks using expert curation and language model synthesis, demonstrating significant variations in financial reasoning capabilities among state-of-the-art models.', 'ai_keywords': ['language models', 'finance-specific evaluation benchmarks', 'semi-synthetic pipeline', 'expert-guided data curation', 'controlled LM-based synthesis', 'question-generation', 'validation', 'FinForge-5k', 'financial reasoning', 'domain competence'], 'organization': {'_id': '63aa1957769a10efc401988c', 'name': 'gtfintechlab', 'fullname': 'Financial Services Innovation Lab, Georgia Tech', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1672091972335-63aa18e53453852ef5414e77.jpeg'}}, 'publishedAt': '2026-01-10T20:38:33.000Z', 'title': 'FinForge: Semi-Synthetic Financial Benchmark Generation', 'summary': \"Evaluating Language Models (LMs) in specialized, high-stakes domains such as finance remains a significant challenge due to the scarcity of open, high-quality, and domain-specific datasets. Existing general-purpose benchmarks provide broad coverage but lack the depth and domain fidelity needed to assess LMs' capabilities for real-world financial reasoning, which requires both conceptual understanding and quantitative rigor. To address this gap, we introduce FinForge, a scalable, semi-synthetic pipeline for constructing finance-specific evaluation benchmarks through a hybrid of expert-guided data curation and controlled LM-based synthesis. FinForge combines manual and programmatic corpus construction from authoritative financial sources with structured question generation and validation using Gemini 2.5 Flash. To demonstrate the pipeline's efficacy, we produce FinForge-5k, a snapshot benchmark comprising over 5,000 human-validated question-answer pairs across 11 finance subdomains, derived from a curated corpus of 100,000 verified documents totaling 143M tokens. Evaluation of state-of-the-art open-source and closed-source models on FinForge-5k reveals significant differences in financial reasoning, with leading models achieving accuracy levels near 80%. These findings underscore the framework's utility for diagnosing current model limitations and guiding future improvements in financial domain competence. All code and data are available at https://github.com/gtfintechlab/FinForge.\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/676c67da7bad1587f2d046e2/K6vyIIZ3Sw-g9CKXM_MbG.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06747.png', 'numComments': 1, 'submittedBy': {'_id': '676c67da7bad1587f2d046e2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/676c67da7bad1587f2d046e2/Y-aJmRj9FB49n36X1qKo9.jpeg', 'fullname': 'Akhil Theerthala', 'name': 'Akhil-Theerthala', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 26, 'isUserFollowing': False}, 'organization': {'_id': '63aa1957769a10efc401988c', 'name': 'gtfintechlab', 'fullname': 'Financial Services Innovation Lab, Georgia Tech', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/1672091972335-63aa18e53453852ef5414e77.jpeg'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.06463', 'authors': [{'_id': '69662fa087c71000b5a9106f', 'name': 'Xuezhe Ma', 'hidden': False}, {'_id': '69662fa087c71000b5a91070', 'name': 'Shicheng Wen', 'hidden': False}, {'_id': '69662fa087c71000b5a91071', 'name': 'Linghao Jin', 'hidden': False}, {'_id': '69662fa087c71000b5a91072', 'name': 'Bilge Acun', 'hidden': False}, {'_id': '69662fa087c71000b5a91073', 'name': 'Ruihang Lai', 'hidden': False}, {'_id': '69662fa087c71000b5a91074', 'name': 'Bohan Hou', 'hidden': False}, {'_id': '69662fa087c71000b5a91075', 'name': 'Will Lin', 'hidden': False}, {'_id': '69662fa087c71000b5a91076', 'name': 'Hao Zhang', 'hidden': False}, {'_id': '69662fa087c71000b5a91077', 'name': 'Songlin Yang', 'hidden': False}, {'_id': '69662fa087c71000b5a91078', 'name': 'Ryan Lee', 'hidden': False}, {'_id': '69662fa087c71000b5a91079', 'name': 'Mengxi Wu', 'hidden': False}, {'_id': '69662fa087c71000b5a9107a', 'name': 'Jonathan May', 'hidden': False}, {'_id': '69662fa087c71000b5a9107b', 'name': 'Luke Zettlemoyer', 'hidden': False}, {'_id': '69662fa087c71000b5a9107c', 'name': 'Carole-Jean Wu', 'hidden': False}], 'publishedAt': '2026-01-10T07:12:41.000Z', 'submittedOnDailyAt': '2026-01-13T09:19:17.514Z', 'title': 'Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths', 'submittedOnDailyBy': {'_id': '6624517c204cf7d22a90306e', 'avatarUrl': '/avatars/82da7b7194f1d9ad49e8e1a470cf02ce.svg', 'isPro': False, 'fullname': 'Xuezhe Ma', 'user': 'maxma1987', 'type': 'user'}, 'summary': 'Designing a unified neural network to efficiently and inherently process sequential data with arbitrary lengths is a central and challenging problem in sequence modeling. The design choices in Transformer, including quadratic complexity and weak length extrapolation, have limited their ability to scale to long sequences. In this work, we propose Gecko, a neural architecture that inherits the design of Mega and Megalodon (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability to capture long range dependencies, including timestep decay normalization, sliding chunk attention mechanism, and adaptive working memory. In a controlled pretraining comparison with Llama2 and Megalodon in the scale of 7 billion parameters and 2 trillion training tokens, Gecko achieves better efficiency and long-context scalability. Gecko reaches a training loss of 1.68, significantly outperforming Llama2-7B (1.75) and Megalodon-7B (1.70), and landing close to Llama2-13B (1.67). Notably, without relying on any context-extension techniques, Gecko exhibits inherent long-context processing and retrieval capabilities, stably handling sequences of up to 4 million tokens and retrieving information from contexts up to 4times longer than its attention window. Code: https://github.com/XuezheMax/gecko-llm', 'upvotes': 1, 'discussionId': '69662fa087c71000b5a9107d', 'githubRepo': 'https://github.com/XuezheMax/gecko-llm', 'githubRepoAddedBy': 'user', 'ai_summary': 'Gecko is a neural architecture that improves long-range dependency capture through exponential moving average with gated attention and additional components like timestep decay normalization and sliding chunk attention, achieving efficient processing of arbitrary-length sequential data with superior long-context scalability compared to existing models.', 'ai_keywords': ['Transformer', 'exponential moving average', 'gated attention', 'timestep decay normalization', 'sliding chunk attention mechanism', 'adaptive working memory', 'long-range dependencies', 'sequence modeling', 'attention window', 'context-extension techniques'], 'githubStars': 1, 'organization': {'_id': '5fc6a2ad2d79acbef39dcb19', 'name': 'usc-isi', 'fullname': 'USC Information Sciences Institute', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/noauth/2MzG9bQdTfWFN22cAy7Xu.png'}}, 'publishedAt': '2026-01-10T02:12:41.000Z', 'title': 'Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths', 'summary': 'Designing a unified neural network to efficiently and inherently process sequential data with arbitrary lengths is a central and challenging problem in sequence modeling. The design choices in Transformer, including quadratic complexity and weak length extrapolation, have limited their ability to scale to long sequences. In this work, we propose Gecko, a neural architecture that inherits the design of Mega and Megalodon (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability to capture long range dependencies, including timestep decay normalization, sliding chunk attention mechanism, and adaptive working memory. In a controlled pretraining comparison with Llama2 and Megalodon in the scale of 7 billion parameters and 2 trillion training tokens, Gecko achieves better efficiency and long-context scalability. Gecko reaches a training loss of 1.68, significantly outperforming Llama2-7B (1.75) and Megalodon-7B (1.70), and landing close to Llama2-13B (1.67). Notably, without relying on any context-extension techniques, Gecko exhibits inherent long-context processing and retrieval capabilities, stably handling sequences of up to 4 million tokens and retrieving information from contexts up to 4times longer than its attention window. Code: https://github.com/XuezheMax/gecko-llm', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06463.png', 'numComments': 1, 'submittedBy': {'_id': '6624517c204cf7d22a90306e', 'avatarUrl': '/avatars/82da7b7194f1d9ad49e8e1a470cf02ce.svg', 'fullname': 'Xuezhe Ma', 'name': 'maxma1987', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 3, 'isUserFollowing': False}, 'organization': {'_id': '5fc6a2ad2d79acbef39dcb19', 'name': 'usc-isi', 'fullname': 'USC Information Sciences Institute', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/noauth/2MzG9bQdTfWFN22cAy7Xu.png'}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2601.05747', 'authors': [{'_id': '6964e1e7ba573026e23e3455', 'user': {'_id': '6638c426fdbfbc17db25e4a7', 'avatarUrl': '/avatars/4406050f3ba5b37bbb103964c02ec716.svg', 'isPro': False, 'fullname': 'Hassaan Farooq', 'user': 'farooqhassaan', 'type': 'user'}, 'name': 'Hassaan Farooq', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-12T14:16:29.435Z', 'hidden': False}, {'_id': '6964e1e7ba573026e23e3456', 'name': 'Marvin Brenner', 'hidden': False}, {'_id': '6964e1e7ba573026e23e3457', 'name': 'Peter St\\\\tz', 'hidden': False}], 'publishedAt': '2026-01-09T12:01:36.000Z', 'submittedOnDailyAt': '2026-01-13T11:17:00.807Z', 'title': 'FlyPose: Towards Robust Human Pose Estimation From Aerial Views', 'submittedOnDailyBy': {'_id': '6638c426fdbfbc17db25e4a7', 'avatarUrl': '/avatars/4406050f3ba5b37bbb103964c02ec716.svg', 'isPro': False, 'fullname': 'Hassaan Farooq', 'user': 'farooqhassaan', 'type': 'user'}, 'summary': 'Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.', 'upvotes': 1, 'discussionId': '6964e1e8ba573026e23e3458', 'ai_summary': 'A lightweight aerial human pose estimation system achieves improved accuracy and real-time performance through multi-dataset training and deployment on UAV platforms.', 'ai_keywords': ['human pose estimation', 'aerial imagery', 'multi-dataset training', 'inference latency', 'Jetson Orin AGX Developer Kit', 'UAV deployment']}, 'publishedAt': '2026-01-09T07:01:36.000Z', 'title': 'FlyPose: Towards Robust Human Pose Estimation From Aerial Views', 'summary': 'Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05747.png', 'numComments': 1, 'submittedBy': {'_id': '6638c426fdbfbc17db25e4a7', 'avatarUrl': '/avatars/4406050f3ba5b37bbb103964c02ec716.svg', 'fullname': 'Hassaan Farooq', 'name': 'farooqhassaan', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isUserFollowing': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.06496', 'authors': [{'_id': '6965bcdffc8c4ecc02c7f90e', 'name': 'Hao Tang', 'hidden': False}, {'_id': '6965bcdffc8c4ecc02c7f90f', 'name': 'Ting Huang', 'hidden': False}, {'_id': '6965bcdffc8c4ecc02c7f910', 'user': {'_id': '64ec877bb93654d4ca5c92e9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg', 'isPro': False, 'fullname': 'Zeyu Zhang', 'user': 'SteveZeyuZhang', 'type': 'user'}, 'name': 'Zeyu Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2026-01-13T08:24:01.374Z', 'hidden': False}], 'publishedAt': '2026-01-10T09:13:10.000Z', 'submittedOnDailyAt': '2026-01-13T01:03:28.160Z', 'title': '3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence', 'submittedOnDailyBy': {'_id': '64ec877bb93654d4ca5c92e9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg', 'isPro': False, 'fullname': 'Zeyu Zhang', 'user': 'SteveZeyuZhang', 'type': 'user'}, 'summary': 'Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.', 'upvotes': 0, 'discussionId': '6965bcdffc8c4ecc02c7f911', 'githubRepo': 'https://github.com/AIGeeksGroup/3DCoCav2', 'githubRepoAddedBy': 'user', 'ai_summary': '3D CoCa v2 enhances 3D captioning by combining contrastive vision-language learning with spatially-aware 3D scene encoding and test-time search for improved generalization across diverse environments.', 'ai_keywords': ['3D captioning', 'contrastive vision-language learning', 'spatially-aware 3D scene encoder', 'multimodal decoder', 'test-time search', 'zero-shot OOD evaluation', 'CLIP-based semantic prior'], 'githubStars': 0, 'organization': {'_id': '61dcd8e344f59573371b5cb6', 'name': 'PekingUniversity', 'fullname': 'Peking University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png'}}, 'publishedAt': '2026-01-10T04:13:10.000Z', 'title': '3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence', 'summary': 'Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06496.png', 'numComments': 1, 'submittedBy': {'_id': '64ec877bb93654d4ca5c92e9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg', 'fullname': 'Zeyu Zhang', 'name': 'SteveZeyuZhang', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4, 'isUserFollowing': False}, 'organization': {'_id': '61dcd8e344f59573371b5cb6', 'name': 'PekingUniversity', 'fullname': 'Peking University', 'avatar': 'https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png'}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2601.06329', 'authors': [{'_id': '6965d559fc8c4ecc02c7f9fa', 'name': 'Jeff Chan-Jan Sju', 'hidden': False}, {'_id': '6965d559fc8c4ecc02c7f9fb', 'name': 'Liang-Hsuan Tseng', 'hidden': False}, {'_id': '6965d559fc8c4ecc02c7f9fc', 'name': 'Yi-Cheng Lin', 'hidden': False}, {'_id': '6965d559fc8c4ecc02c7f9fd', 'name': 'Yen-Chun Kuo', 'hidden': False}, {'_id': '6965d559fc8c4ecc02c7f9fe', 'name': 'Ju-Chieh Chou', 'hidden': False}, {'_id': '6965d559fc8c4ecc02c7f9ff', 'name': 'Kai-Wei Chang', 'hidden': False}, {'_id': '6965d559fc8c4ecc02c7fa00', 'name': 'Hung-yi Lee', 'hidden': False}, {'_id': '6965d559fc8c4ecc02c7fa01', 'name': 'Carlos Busso', 'hidden': False}], 'publishedAt': '2026-01-09T22:01:56.000Z', 'submittedOnDailyAt': '2026-01-13T02:48:35.637Z', 'title': 'On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation', 'submittedOnDailyBy': {'_id': '650b0d66664f7b7d088ca281', 'avatarUrl': '/avatars/fce475c301f53e166fc3c8f5c5112c4a.svg', 'isPro': False, 'fullname': 'Yi-Cheng Lin', 'user': 'dlion168', 'type': 'user'}, 'summary': \"Generative spoken language models pretrained on large-scale raw audio can continue a speech prompt with appropriate content while preserving attributes like speaker and emotion, serving as foundation models for spoken dialogue. In prior literature, these models are often evaluated using ``global token perplexity'', which directly applies the text perplexity formulation to speech tokens. However, this practice overlooks fundamental differences between speech and text modalities, possibly leading to an underestimation of the speech characteristics. In this work, we propose a variety of likelihood- and generative-based evaluation methods that serve in place of naive global token perplexity. We demonstrate that the proposed evaluations more faithfully reflect perceived generation quality, as evidenced by stronger correlations with human-rated mean opinion scores (MOS). When assessed under the new metrics, the relative performance landscape of spoken language models is reshaped, revealing a significantly reduced gap between the best-performing model and the human topline. Together, these results suggest that appropriate evaluation is critical for accurately assessing progress in spoken language modeling.\", 'upvotes': 0, 'discussionId': '6965d55afc8c4ecc02c7fa02', 'ai_summary': 'Speech models trained on raw audio can generate appropriate content while maintaining speaker and emotion attributes, but traditional text-based evaluation methods underestimate speech characteristics; new evaluation approaches better correlate with human perception.', 'ai_keywords': ['generative spoken language models', 'raw audio', 'speech tokens', 'global token perplexity', 'likelihood-based evaluation', 'generative-based evaluation', 'mean opinion scores', 'human topline']}, 'publishedAt': '2026-01-09T17:01:56.000Z', 'title': 'On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation', 'summary': \"Generative spoken language models pretrained on large-scale raw audio can continue a speech prompt with appropriate content while preserving attributes like speaker and emotion, serving as foundation models for spoken dialogue. In prior literature, these models are often evaluated using ``global token perplexity'', which directly applies the text perplexity formulation to speech tokens. However, this practice overlooks fundamental differences between speech and text modalities, possibly leading to an underestimation of the speech characteristics. In this work, we propose a variety of likelihood- and generative-based evaluation methods that serve in place of naive global token perplexity. We demonstrate that the proposed evaluations more faithfully reflect perceived generation quality, as evidenced by stronger correlations with human-rated mean opinion scores (MOS). When assessed under the new metrics, the relative performance landscape of spoken language models is reshaped, revealing a significantly reduced gap between the best-performing model and the human topline. Together, these results suggest that appropriate evaluation is critical for accurately assessing progress in spoken language modeling.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06329.png', 'numComments': 1, 'submittedBy': {'_id': '650b0d66664f7b7d088ca281', 'avatarUrl': '/avatars/fce475c301f53e166fc3c8f5c5112c4a.svg', 'fullname': 'Yi-Cheng Lin', 'name': 'dlion168', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6, 'isUserFollowing': False}, 'isAuthorParticipating': False}"
]