[
    "{'paper': {'id': '2507.18553', 'authors': [{'_id': '688389bb5d77c97374117c30', 'user': {'_id': '6298d8dab58e71e2ac9e2967', 'avatarUrl': '/avatars/99fec4ba78ab61c5952f51e6ebf03ffa.svg', 'isPro': False, 'fullname': 'Jiale Chen', 'user': 'softmax', 'type': 'user'}, 'name': 'Jiale Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-28T08:45:38.736Z', 'hidden': False}, {'_id': '688389bb5d77c97374117c31', 'name': 'Torsten Hoefler', 'hidden': False}, {'_id': '688389bb5d77c97374117c32', 'name': 'Dan Alistarh', 'hidden': False}], 'publishedAt': '2025-07-24T16:22:18.000Z', 'submittedOnDailyAt': '2025-07-28T08:41:14.809Z', 'title': \"The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\\n  Algorithm\", 'submittedOnDailyBy': {'_id': '6298d8dab58e71e2ac9e2967', 'avatarUrl': '/avatars/99fec4ba78ab61c5952f51e6ebf03ffa.svg', 'isPro': False, 'fullname': 'Jiale Chen', 'user': 'softmax', 'type': 'user'}, 'summary': \"Quantizing the weights of large language models (LLMs) from 16-bit to lower\\nbitwidth is the de facto approach to deploy massive transformers onto more\\naffordable accelerators. GPTQ emerged as one of the standard methods for\\none-shot post-training quantization at LLM scale. Yet, its inner workings are\\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\\nmeaning or worst-case guarantees. In this work, we show that, when executed\\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\\nmathematically identical to Babai's nearest plane algorithm for the classical\\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\\nlayer's inputs. This equivalence is based on a sophisticated mathematical\\nargument, and has two analytical consequences: (i) the GPTQ error propagation\\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\\nupper bound of Babai's algorithm under the no-clipping condition. Taken\\ntogether, these results place GPTQ on firm theoretical footing and open the\\ndoor to importing decades of progress in lattice algorithms towards the design\\nof future quantization algorithms for billion-parameter models.\", 'upvotes': 16, 'discussionId': '688389bb5d77c97374117c33'}, 'publishedAt': '2025-07-24T12:22:18.000Z', 'title': \"The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\\n  Algorithm\", 'summary': \"Quantizing the weights of large language models (LLMs) from 16-bit to lower\\nbitwidth is the de facto approach to deploy massive transformers onto more\\naffordable accelerators. GPTQ emerged as one of the standard methods for\\none-shot post-training quantization at LLM scale. Yet, its inner workings are\\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\\nmeaning or worst-case guarantees. In this work, we show that, when executed\\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\\nmathematically identical to Babai's nearest plane algorithm for the classical\\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\\nlayer's inputs. This equivalence is based on a sophisticated mathematical\\nargument, and has two analytical consequences: (i) the GPTQ error propagation\\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\\nupper bound of Babai's algorithm under the no-clipping condition. Taken\\ntogether, these results place GPTQ on firm theoretical footing and open the\\ndoor to importing decades of progress in lattice algorithms towards the design\\nof future quantization algorithms for billion-parameter models.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18553.png', 'numComments': 1, 'submittedBy': {'_id': '6298d8dab58e71e2ac9e2967', 'avatarUrl': '/avatars/99fec4ba78ab61c5952f51e6ebf03ffa.svg', 'fullname': 'Jiale Chen', 'name': 'softmax', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.16075', 'authors': [{'_id': '688501d57d7a19a208cdf03a', 'name': 'Rujun Han', 'hidden': False}, {'_id': '688501d57d7a19a208cdf03b', 'name': 'Yanfei Chen', 'hidden': False}, {'_id': '688501d57d7a19a208cdf03c', 'name': 'Zoey CuiZhu', 'hidden': False}, {'_id': '688501d57d7a19a208cdf03d', 'name': 'Lesly Miculicich', 'hidden': False}, {'_id': '688501d57d7a19a208cdf03e', 'name': 'Guan Sun', 'hidden': False}, {'_id': '688501d57d7a19a208cdf03f', 'name': 'Yuanjun Bi', 'hidden': False}, {'_id': '688501d57d7a19a208cdf040', 'name': 'Weiming Wen', 'hidden': False}, {'_id': '688501d57d7a19a208cdf041', 'name': 'Hui Wan', 'hidden': False}, {'_id': '688501d57d7a19a208cdf042', 'name': 'Chunfeng Wen', 'hidden': False}, {'_id': '688501d57d7a19a208cdf043', 'name': 'Solène Maître', 'hidden': False}, {'_id': '688501d57d7a19a208cdf044', 'name': 'George Lee', 'hidden': False}, {'_id': '688501d57d7a19a208cdf045', 'name': 'Vishy Tirumalashetty', 'hidden': False}, {'_id': '688501d57d7a19a208cdf046', 'name': 'Emily Xue', 'hidden': False}, {'_id': '688501d57d7a19a208cdf047', 'name': 'Zizhao Zhang', 'hidden': False}, {'_id': '688501d57d7a19a208cdf048', 'name': 'Salem Haykal', 'hidden': False}, {'_id': '688501d57d7a19a208cdf049', 'name': 'Burak Gokturk', 'hidden': False}, {'_id': '688501d57d7a19a208cdf04a', 'name': 'Tomas Pfister', 'hidden': False}, {'_id': '688501d57d7a19a208cdf04b', 'name': 'Chen-Yu Lee', 'hidden': False}], 'publishedAt': '2025-07-21T21:23:21.000Z', 'submittedOnDailyAt': '2025-07-28T05:49:28.861Z', 'title': 'Deep Researcher with Test-Time Diffusion', 'submittedOnDailyBy': {'_id': '62f32eab52ad88c930bb3f3b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png', 'isPro': True, 'fullname': 'Asankhaya Sharma', 'user': 'codelion', 'type': 'user'}, 'summary': 'Deep research agents, powered by Large Language Models (LLMs), are rapidly\\nadvancing; yet, their performance often plateaus when generating complex,\\nlong-form research reports using generic test-time scaling algorithms. Drawing\\ninspiration from the iterative nature of human research, which involves cycles\\nof searching, reasoning, and revision, we propose the Test-Time Diffusion Deep\\nResearcher (TTD-DR). This novel framework conceptualizes research report\\ngeneration as a diffusion process. TTD-DR initiates this process with a\\npreliminary draft, an updatable skeleton that serves as an evolving foundation\\nto guide the research direction. The draft is then iteratively refined through\\na \"denoising\" process, which is dynamically informed by a retrieval mechanism\\nthat incorporates external information at each step. The core process is\\nfurther enhanced by a self-evolutionary algorithm applied to each component of\\nthe agentic workflow, ensuring the generation of high-quality context for the\\ndiffusion process. This draft-centric design makes the report writing process\\nmore timely and coherent while reducing information loss during the iterative\\nsearch process. We demonstrate that our TTD-DR achieves state-of-the-art\\nresults on a wide array of benchmarks that require intensive search and\\nmulti-hop reasoning, significantly outperforming existing deep research agents.', 'upvotes': 13, 'discussionId': '688501d67d7a19a208cdf04c', 'githubRepo': 'https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research', 'ai_summary': 'The Test-Time Diffusion Deep Researcher (TTD-DR) framework uses a diffusion process with iterative refinement and external information retrieval to generate high-quality research reports, outperforming existing methods.', 'ai_keywords': ['Test-Time Diffusion Deep Researcher', 'TTD-DR', 'diffusion process', 'preliminary draft', 'denoising process', 'retrieval mechanism', 'self-evolutionary algorithm', 'agentic workflow', 'iterative search', 'multi-hop reasoning'], 'githubStars': 2663}, 'publishedAt': '2025-07-21T17:23:21.000Z', 'title': 'Deep Researcher with Test-Time Diffusion', 'summary': 'Deep research agents, powered by Large Language Models (LLMs), are rapidly\\nadvancing; yet, their performance often plateaus when generating complex,\\nlong-form research reports using generic test-time scaling algorithms. Drawing\\ninspiration from the iterative nature of human research, which involves cycles\\nof searching, reasoning, and revision, we propose the Test-Time Diffusion Deep\\nResearcher (TTD-DR). This novel framework conceptualizes research report\\ngeneration as a diffusion process. TTD-DR initiates this process with a\\npreliminary draft, an updatable skeleton that serves as an evolving foundation\\nto guide the research direction. The draft is then iteratively refined through\\na \"denoising\" process, which is dynamically informed by a retrieval mechanism\\nthat incorporates external information at each step. The core process is\\nfurther enhanced by a self-evolutionary algorithm applied to each component of\\nthe agentic workflow, ensuring the generation of high-quality context for the\\ndiffusion process. This draft-centric design makes the report writing process\\nmore timely and coherent while reducing information loss during the iterative\\nsearch process. We demonstrate that our TTD-DR achieves state-of-the-art\\nresults on a wide array of benchmarks that require intensive search and\\nmulti-hop reasoning, significantly outperforming existing deep research agents.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16075.png', 'numComments': 2, 'submittedBy': {'_id': '62f32eab52ad88c930bb3f3b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png', 'fullname': 'Asankhaya Sharma', 'name': 'codelion', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 153}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2507.19478', 'authors': [{'_id': '68877a6bc8db48c925156eec', 'name': 'Xuehui Wang', 'hidden': False}, {'_id': '68877a6bc8db48c925156eed', 'name': 'Zhenyu Wu', 'hidden': False}, {'_id': '68877a6bc8db48c925156eee', 'name': 'JingJing Xie', 'hidden': False}, {'_id': '68877a6bc8db48c925156eef', 'name': 'Zichen Ding', 'hidden': False}, {'_id': '68877a6bc8db48c925156ef0', 'name': 'Bowen Yang', 'hidden': False}, {'_id': '68877a6bc8db48c925156ef1', 'name': 'Zehao Li', 'hidden': False}, {'_id': '68877a6bc8db48c925156ef2', 'name': 'Zhaoyang Liu', 'hidden': False}, {'_id': '68877a6bc8db48c925156ef3', 'name': 'Qingyun Li', 'hidden': False}, {'_id': '68877a6bc8db48c925156ef4', 'name': 'Xuan Dong', 'hidden': False}, {'_id': '68877a6bc8db48c925156ef5', 'name': 'Zhe Chen', 'hidden': False}, {'_id': '68877a6bc8db48c925156ef6', 'name': 'Weiyun Wang', 'hidden': False}, {'_id': '68877a6bc8db48c925156ef7', 'name': 'Xiangyu Zhao', 'hidden': False}, {'_id': '68877a6bc8db48c925156ef8', 'name': 'Jixuan Chen', 'hidden': False}, {'_id': '68877a6bc8db48c925156ef9', 'name': 'Haodong Duan', 'hidden': False}, {'_id': '68877a6bc8db48c925156efa', 'name': 'Tianbao Xie', 'hidden': False}, {'_id': '68877a6bc8db48c925156efb', 'name': 'Chenyu Yang', 'hidden': False}, {'_id': '68877a6bc8db48c925156efc', 'name': 'Shiqian Su', 'hidden': False}, {'_id': '68877a6bc8db48c925156efd', 'name': 'Yue Yu', 'hidden': False}, {'_id': '68877a6bc8db48c925156efe', 'name': 'Yuan Huang', 'hidden': False}, {'_id': '68877a6bc8db48c925156eff', 'name': 'Yiqian Liu', 'hidden': False}, {'_id': '68877a6bc8db48c925156f00', 'name': 'Xiao Zhang', 'hidden': False}, {'_id': '68877a6bc8db48c925156f01', 'name': 'Yanting Zhang', 'hidden': False}, {'_id': '68877a6bc8db48c925156f02', 'name': 'Xiangyu Yue', 'hidden': False}, {'_id': '68877a6bc8db48c925156f03', 'name': 'Weijie Su', 'hidden': False}, {'_id': '68877a6bc8db48c925156f04', 'name': 'Xizhou Zhu', 'hidden': False}, {'_id': '68877a6bc8db48c925156f05', 'name': 'Wei Shen', 'hidden': False}, {'_id': '68877a6bc8db48c925156f06', 'name': 'Jifeng Dai', 'hidden': False}, {'_id': '68877a6bc8db48c925156f07', 'name': 'Wenhai Wang', 'hidden': False}], 'publishedAt': '2025-07-25T17:59:26.000Z', 'submittedOnDailyAt': '2025-07-28T12:29:10.972Z', 'title': 'MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI\\n  Agents', 'submittedOnDailyBy': {'_id': '649cf4ecdd87dd9ef76fe020', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/M7RpD_AcNewA2xADhhyCB.jpeg', 'isPro': False, 'fullname': 'Xuehui Wang', 'user': 'huiserwang', 'type': 'user'}, 'summary': 'We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI\\nautomation agents across Windows, macOS, Linux, iOS, Android, and Web\\nplatforms. It comprises four levels: GUI Content Understanding, Element\\nGrounding, Task Automation, and Task Collaboration, covering essential skills\\nfor GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)\\nmetric to assess GUI agent execution efficiency in online automation scenarios.\\nThrough MMBench-GUI, we identify accurate visual grounding as a critical\\ndeterminant of overall task success, emphasizing the substantial benefits of\\nmodular frameworks that integrate specialized grounding modules. Furthermore,\\nto achieve reliable GUI automation, an agent requires strong task planning and\\ncross-platform generalization abilities, with long-context memory, a broad\\naction space, and long-term reasoning playing a critical role. More important,\\ntask efficiency remains a critically underexplored dimension, and all models\\nsuffer from substantial inefficiencies, with excessive redundant steps even\\nwhen tasks are ultimately completed. The integration of precise localization,\\neffective planning, and early stopping strategies is indispensable to enable\\ntruly efficient and scalable GUI automation. Our benchmark code, evaluation\\ndata, and running environment will be publicly available at\\nhttps://github.com/open-compass/MMBench-GUI.', 'upvotes': 12, 'discussionId': '68877a6bc8db48c925156f08', 'ai_summary': 'A hierarchical benchmark evaluates GUI automation agents across multiple platforms, emphasizing key skills such as visual grounding, task planning, and efficiency.', 'ai_keywords': ['GUI Content Understanding', 'Element Grounding', 'Task Automation', 'Task Collaboration', 'Efficiency-Quality Area', 'EQA', 'modular frameworks', 'specialized grounding modules', 'long-context memory', 'broad action space', 'long-term reasoning', 'precise localization', 'effective planning', 'early stopping strategies']}, 'publishedAt': '2025-07-25T13:59:26.000Z', 'title': 'MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI\\n  Agents', 'summary': 'We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI\\nautomation agents across Windows, macOS, Linux, iOS, Android, and Web\\nplatforms. It comprises four levels: GUI Content Understanding, Element\\nGrounding, Task Automation, and Task Collaboration, covering essential skills\\nfor GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)\\nmetric to assess GUI agent execution efficiency in online automation scenarios.\\nThrough MMBench-GUI, we identify accurate visual grounding as a critical\\ndeterminant of overall task success, emphasizing the substantial benefits of\\nmodular frameworks that integrate specialized grounding modules. Furthermore,\\nto achieve reliable GUI automation, an agent requires strong task planning and\\ncross-platform generalization abilities, with long-context memory, a broad\\naction space, and long-term reasoning playing a critical role. More important,\\ntask efficiency remains a critically underexplored dimension, and all models\\nsuffer from substantial inefficiencies, with excessive redundant steps even\\nwhen tasks are ultimately completed. The integration of precise localization,\\neffective planning, and early stopping strategies is indispensable to enable\\ntruly efficient and scalable GUI automation. Our benchmark code, evaluation\\ndata, and running environment will be publicly available at\\nhttps://github.com/open-compass/MMBench-GUI.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.19478.png', 'numComments': 1, 'submittedBy': {'_id': '649cf4ecdd87dd9ef76fe020', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/M7RpD_AcNewA2xADhhyCB.jpeg', 'fullname': 'Xuehui Wang', 'name': 'huiserwang', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2507.18392', 'authors': [{'_id': '68878f837e66e00ef8c1cbd3', 'name': 'Asaf Yehudai', 'hidden': False}, {'_id': '68878f837e66e00ef8c1cbd4', 'name': 'Lilach Eden', 'hidden': False}, {'_id': '68878f837e66e00ef8c1cbd5', 'name': 'Yotam Perlitz', 'hidden': False}, {'_id': '68878f837e66e00ef8c1cbd6', 'name': 'Roy Bar-Haim', 'hidden': False}, {'_id': '68878f837e66e00ef8c1cbd7', 'name': 'Michal Shmueli-Scheuer', 'hidden': False}], 'publishedAt': '2025-07-24T13:15:21.000Z', 'submittedOnDailyAt': '2025-07-28T13:26:30.022Z', 'title': 'CLEAR: Error Analysis via LLM-as-a-Judge Made Easy', 'submittedOnDailyBy': {'_id': '638324f862badff43269e588', 'avatarUrl': '/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg', 'isPro': False, 'fullname': 'Asaf Yehudai', 'user': 'Asaf-Yehudai', 'type': 'user'}, 'summary': \"The evaluation of Large Language Models (LLMs) increasingly relies on other\\nLLMs acting as judges. However, current evaluation paradigms typically yield a\\nsingle score or ranking, answering which model is better but not why. While\\nessential for benchmarking, these top-level scores obscure the specific,\\nactionable reasons behind a model's performance. To bridge this gap, we\\nintroduce CLEAR, an interactive, open-source package for LLM-based error\\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\\na set of system-level error issues, and quantifies the prevalence of each\\nidentified issue. Our package also provides users with an interactive dashboard\\nthat allows for a comprehensive error analysis through aggregate\\nvisualizations, applies interactive filters to isolate specific issues or score\\nranges, and drills down to the individual instances that exemplify a particular\\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\\nand showcase its utility through a user case study.\", 'upvotes': 3, 'discussionId': '68878f837e66e00ef8c1cbd8', 'githubRepo': 'https://github.com/IBM/CLEAR', 'ai_summary': 'CLEAR is an interactive open-source package that provides detailed error analysis by generating per-instance feedback and system-level error issues, aiding in understanding and improving the performance of Large Language Models.', 'ai_keywords': ['Large Language Models (LLMs)', 'CLEAR', 'error analysis', 'per-instance feedback', 'system-level error issues', 'interactive dashboard', 'aggregate visualizations', 'interactive filters', 'RAG benchmarks', 'Math benchmarks'], 'githubStars': 5}, 'publishedAt': '2025-07-24T09:15:21.000Z', 'title': 'CLEAR: Error Analysis via LLM-as-a-Judge Made Easy', 'summary': \"The evaluation of Large Language Models (LLMs) increasingly relies on other\\nLLMs acting as judges. However, current evaluation paradigms typically yield a\\nsingle score or ranking, answering which model is better but not why. While\\nessential for benchmarking, these top-level scores obscure the specific,\\nactionable reasons behind a model's performance. To bridge this gap, we\\nintroduce CLEAR, an interactive, open-source package for LLM-based error\\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\\na set of system-level error issues, and quantifies the prevalence of each\\nidentified issue. Our package also provides users with an interactive dashboard\\nthat allows for a comprehensive error analysis through aggregate\\nvisualizations, applies interactive filters to isolate specific issues or score\\nranges, and drills down to the individual instances that exemplify a particular\\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\\nand showcase its utility through a user case study.\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18392.png', 'numComments': 1, 'submittedBy': {'_id': '638324f862badff43269e588', 'avatarUrl': '/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg', 'fullname': 'Asaf Yehudai', 'name': 'Asaf-Yehudai', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2507.18742', 'authors': [{'_id': '688728d6a5a841b139945452', 'user': {'_id': '5fad8602b8423e1d80b8a965', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg', 'isPro': False, 'fullname': 'Victor Gallego', 'user': 'vicgalle', 'type': 'user'}, 'name': 'Víctor Gallego', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-28T08:44:19.745Z', 'hidden': False}], 'publishedAt': '2025-07-24T18:44:28.000Z', 'submittedOnDailyAt': '2025-07-28T06:09:19.276Z', 'title': 'Specification Self-Correction: Mitigating In-Context Reward Hacking\\n  Through Test-Time Refinement', 'submittedOnDailyBy': {'_id': '5fad8602b8423e1d80b8a965', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg', 'isPro': False, 'fullname': 'Victor Gallego', 'user': 'vicgalle', 'type': 'user'}, 'summary': \"Language models (LMs) are susceptible to in-context reward hacking, where\\nthey exploit flaws in tainted or faulty written specifications or rubrics to\\nachieve high scores without fulfilling the user's true intent. We introduce\\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\\nan LM to identify and correct flaws within its own guiding specification. SSC\\nemploys a multi-step inference process where the model first generates a\\nresponse based on a potentially tainted specification, critiques its output,\\nand then revises the specification itself to remove the exploitable loophole. A\\nfinal, more robust response is then generated using this self-corrected\\nspecification. Across experiments spanning creative writing and agentic coding\\ntasks with several LMs, we demonstrate that while models initially game tainted\\nspecifications in 50-70\\\\% of cases, the SSC process reduces this vulnerability\\nby over 90\\\\%. This dynamic repair occurs at inference time, requires no weight\\nmodification, and leads to more robustly aligned model behavior. Code at\\nhttps://github.com/vicgalle/specification-self-correction .\", 'upvotes': 2, 'discussionId': '688728d7a5a841b139945453', 'githubRepo': 'https://github.com/vicgalle/specification-self-correction', 'ai_summary': 'A new framework called Specification Self-Correction allows language models to dynamically correct flawed instructions during inference, reducing reward hacking vulnerabilities.', 'ai_keywords': ['language models', 'reward hacking', 'Specification Self-Correction', 'multi-step inference', 'self-corrected specification'], 'githubStars': 1}, 'publishedAt': '2025-07-24T14:44:28.000Z', 'title': 'Specification Self-Correction: Mitigating In-Context Reward Hacking\\n  Through Test-Time Refinement', 'summary': \"Language models (LMs) are susceptible to in-context reward hacking, where\\nthey exploit flaws in tainted or faulty written specifications or rubrics to\\nachieve high scores without fulfilling the user's true intent. We introduce\\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\\nan LM to identify and correct flaws within its own guiding specification. SSC\\nemploys a multi-step inference process where the model first generates a\\nresponse based on a potentially tainted specification, critiques its output,\\nand then revises the specification itself to remove the exploitable loophole. A\\nfinal, more robust response is then generated using this self-corrected\\nspecification. Across experiments spanning creative writing and agentic coding\\ntasks with several LMs, we demonstrate that while models initially game tainted\\nspecifications in 50-70\\\\% of cases, the SSC process reduces this vulnerability\\nby over 90\\\\%. This dynamic repair occurs at inference time, requires no weight\\nmodification, and leads to more robustly aligned model behavior. Code at\\nhttps://github.com/vicgalle/specification-self-correction .\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18742.png', 'numComments': 1, 'submittedBy': {'_id': '5fad8602b8423e1d80b8a965', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg', 'fullname': 'Victor Gallego', 'name': 'vicgalle', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 134}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.17596', 'authors': [{'_id': '688223086a54dd1e77daa947', 'user': {'_id': '641862440956be7233a1788f', 'avatarUrl': '/avatars/8710a44806a98f44d36b60814144186a.svg', 'isPro': False, 'fullname': 'Maciej Wozniak', 'user': 'maciejw94', 'type': 'user'}, 'name': 'Maciej K. Wozniak', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-24T14:11:19.043Z', 'hidden': False}, {'_id': '688223086a54dd1e77daa948', 'name': 'Lianhang Liu', 'hidden': False}, {'_id': '688223086a54dd1e77daa949', 'name': 'Yixi Cai', 'hidden': False}, {'_id': '688223086a54dd1e77daa94a', 'name': 'Patric Jensfelt', 'hidden': False}], 'publishedAt': '2025-07-23T15:28:23.000Z', 'submittedOnDailyAt': '2025-07-28T05:33:52.120Z', 'title': 'PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving', 'submittedOnDailyBy': {'_id': '641862440956be7233a1788f', 'avatarUrl': '/avatars/8710a44806a98f44d36b60814144186a.svg', 'isPro': False, 'fullname': 'Maciej Wozniak', 'user': 'maciejw94', 'type': 'user'}, 'summary': 'While end-to-end autonomous driving models show promising results, their\\npractical deployment is often hindered by large model sizes, a reliance on\\nexpensive LiDAR sensors and computationally intensive BEV feature\\nrepresentations. This limits their scalability, especially for mass-market\\nvehicles equipped only with cameras. To address these challenges, we propose\\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\\narchitecture operates using only camera data, without explicit BEV\\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\\nextractor coupled with a generative planning head to predict safe trajectories\\nfrom raw pixel inputs directly. A core component of our architecture is the\\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\\neffectively enhance multi-level visual features for more robust planning. We\\ndemonstrate through comprehensive experiments that PRIX achieves\\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\\nthe capabilities of larger, multimodal diffusion planners while being\\nsignificantly more efficient in terms of inference speed and model size, making\\nit a practical solution for real-world deployment. Our work is open-source and\\nthe code will be at https://maxiuw.github.io/prix.', 'upvotes': 2, 'discussionId': '688223086a54dd1e77daa94b', 'ai_summary': 'PRIX, an end-to-end driving architecture using only camera data, achieves state-of-the-art performance with a Context-aware Recalibration Transformer, outperforming larger multimodal planners in efficiency and scalability.', 'ai_keywords': ['end-to-end driving architecture', 'Context-aware Recalibration Transformer', 'CaRT', 'NavSim', 'nuScenes', 'BEV feature representations', 'LiDAR', 'visual feature extractor', 'generative planning head', 'safe trajectories', 'raw pixel inputs']}, 'publishedAt': '2025-07-23T11:28:23.000Z', 'title': 'PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving', 'summary': 'While end-to-end autonomous driving models show promising results, their\\npractical deployment is often hindered by large model sizes, a reliance on\\nexpensive LiDAR sensors and computationally intensive BEV feature\\nrepresentations. This limits their scalability, especially for mass-market\\nvehicles equipped only with cameras. To address these challenges, we propose\\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\\narchitecture operates using only camera data, without explicit BEV\\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\\nextractor coupled with a generative planning head to predict safe trajectories\\nfrom raw pixel inputs directly. A core component of our architecture is the\\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\\neffectively enhance multi-level visual features for more robust planning. We\\ndemonstrate through comprehensive experiments that PRIX achieves\\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\\nthe capabilities of larger, multimodal diffusion planners while being\\nsignificantly more efficient in terms of inference speed and model size, making\\nit a practical solution for real-world deployment. Our work is open-source and\\nthe code will be at https://maxiuw.github.io/prix.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17596.png', 'numComments': 1, 'submittedBy': {'_id': '641862440956be7233a1788f', 'avatarUrl': '/avatars/8710a44806a98f44d36b60814144186a.svg', 'fullname': 'Maciej Wozniak', 'name': 'maciejw94', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2507.10510', 'authors': [{'_id': '6886fab77d7a19a208cdf212', 'user': {'_id': '67e43f9b29d49781111e4013', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/67e43f9b29d49781111e4013/UXMNGH3-PsVEE3oazmGx2.jpeg', 'isPro': False, 'fullname': 'Jiangkai', 'user': 'keyonN', 'type': 'user'}, 'name': 'Jiangkai Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-07-28T08:44:35.831Z', 'hidden': False}, {'_id': '6886fab77d7a19a208cdf213', 'name': 'Zhiyuan Ren', 'hidden': False}, {'_id': '6886fab77d7a19a208cdf214', 'name': 'Liming Liu', 'hidden': False}, {'_id': '6886fab77d7a19a208cdf215', 'name': 'Xinggong Zhang', 'hidden': False}], 'publishedAt': '2025-07-14T17:34:49.000Z', 'submittedOnDailyAt': '2025-07-28T02:51:56.268Z', 'title': 'Chat with AI: The Surprising Turn of Real-time Video Communication from\\n  Human to AI', 'submittedOnDailyBy': {'_id': '67e43f9b29d49781111e4013', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/67e43f9b29d49781111e4013/UXMNGH3-PsVEE3oazmGx2.jpeg', 'isPro': False, 'fullname': 'Jiangkai', 'user': 'keyonN', 'type': 'user'}, 'summary': 'AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\\nThis makes interaction between humans and AI more intuitive, as if chatting\\nface-to-face with a real person. However, this poses significant challenges to\\nlatency, because the MLLM inference takes up most of the response time, leaving\\nvery little time for video streaming. Due to network uncertainty and\\ninstability, transmission latency becomes a critical bottleneck preventing AI\\nfrom being like a real person. To address this, we propose Artic, an\\nAI-oriented Real-time Communication framework, exploring the network\\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\\nContext-Aware Video Streaming that recognizes the importance of each video\\nregion for chat and allocates bitrate almost exclusively to chat-important\\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\\non MLLM accuracy, we build the first benchmark, named Degraded Video\\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\\nand ongoing solutions for AI Video Chat.', 'upvotes': 2, 'discussionId': '6886fab77d7a19a208cdf216', 'ai_summary': 'Artic addresses latency issues in AI Video Chat by optimizing video streaming and frame rate adaptation to enhance MLLM accuracy and reduce bitrate.', 'ai_keywords': ['Multimodal Large Language Model', 'MLLM', 'Context-Aware Video Streaming', 'Loss-Resilient Adaptive Frame Rate', 'Degraded Video Understanding Benchmark', 'DeViBench']}, 'publishedAt': '2025-07-14T13:34:49.000Z', 'title': 'Chat with AI: The Surprising Turn of Real-time Video Communication from\\n  Human to AI', 'summary': 'AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\\nThis makes interaction between humans and AI more intuitive, as if chatting\\nface-to-face with a real person. However, this poses significant challenges to\\nlatency, because the MLLM inference takes up most of the response time, leaving\\nvery little time for video streaming. Due to network uncertainty and\\ninstability, transmission latency becomes a critical bottleneck preventing AI\\nfrom being like a real person. To address this, we propose Artic, an\\nAI-oriented Real-time Communication framework, exploring the network\\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\\nContext-Aware Video Streaming that recognizes the importance of each video\\nregion for chat and allocates bitrate almost exclusively to chat-important\\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\\non MLLM accuracy, we build the first benchmark, named Degraded Video\\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\\nand ongoing solutions for AI Video Chat.', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10510.png', 'numComments': 1, 'submittedBy': {'_id': '67e43f9b29d49781111e4013', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/67e43f9b29d49781111e4013/UXMNGH3-PsVEE3oazmGx2.jpeg', 'fullname': 'Jiangkai', 'name': 'keyonN', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}"
]