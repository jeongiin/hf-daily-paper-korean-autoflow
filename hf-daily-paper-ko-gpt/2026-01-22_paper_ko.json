[
    {
        "paper": {
            "id": "2601.12538",
            "authors": [
                {
                    "_id": "6971913fc1c7409747bf9564",
                    "name": "Tianxin Wei",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9565",
                    "name": "Ting-Wei Li",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9566",
                    "name": "Zhining Liu",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9567",
                    "name": "Xuying Ning",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9568",
                    "name": "Ze Yang",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9569",
                    "name": "Jiaru Zou",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf956a",
                    "name": "Zhichen Zeng",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf956b",
                    "name": "Ruizhong Qiu",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf956c",
                    "name": "Xiao Lin",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf956d",
                    "name": "Dongqi Fu",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf956e",
                    "name": "Zihao Li",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf956f",
                    "user": {
                        "_id": "653962e75c8e4863e1a2068f",
                        "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg",
                        "isPro": false,
                        "fullname": "Mengting Ai",
                        "user": "famous-blue-raincoat",
                        "type": "user"
                    },
                    "name": "Mengting Ai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-22T08:45:10.378Z",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9570",
                    "user": {
                        "_id": "677830bd3f2e3ec475576303",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png",
                        "isPro": false,
                        "fullname": "Duo Zhou",
                        "user": "Claudius7",
                        "type": "user"
                    },
                    "name": "Duo Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-22T08:45:12.476Z",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9571",
                    "name": "Wenxuan Bao",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9572",
                    "user": {
                        "_id": "646323556c27a7e33b23f198",
                        "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg",
                        "isPro": false,
                        "fullname": "Yunzhe Li",
                        "user": "yunzhel2",
                        "type": "user"
                    },
                    "name": "Yunzhe Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-22T08:45:14.383Z",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9573",
                    "name": "Gaotang Li",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9574",
                    "name": "Cheng Qian",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9575",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9576",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9577",
                    "name": "Yin Xiao",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9578",
                    "name": "Liri Fang",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9579",
                    "name": "Hui Liu",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf957a",
                    "name": "Xianfeng Tang",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf957b",
                    "name": "Yuji Zhang",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf957c",
                    "name": "Chi Wang",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf957d",
                    "name": "Jiaxuan You",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf957e",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf957f",
                    "name": "Hanghang Tong",
                    "hidden": false
                },
                {
                    "_id": "6971913fc1c7409747bf9580",
                    "name": "Jingrui He",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-18T18:58:23.000Z",
            "submittedOnDailyAt": "2026-01-22T00:27:25.162Z",
            "title": "Agentic Reasoning for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "65c288280aa2d53135734a42",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg",
                "isPro": false,
                "fullname": "Jiaru Zou",
                "user": "jiaruz2",
                "type": "user"
            },
            "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.",
            "upvotes": 107,
            "discussionId": "69719140c1c7409747bf9581",
            "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning",
            "githubRepoAddedBy": "user",
            "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.",
            "ai_keywords": [
                "large language models",
                "agentic reasoning",
                "autonomous agents",
                "planning",
                "tool use",
                "search",
                "feedback",
                "memory",
                "adaptation",
                "collaborative settings",
                "coordination",
                "knowledge sharing",
                "reinforcement learning",
                "supervised fine-tuning",
                "in-context reasoning",
                "post-training reasoning",
                "real-world applications",
                "benchmarks",
                "thought and action",
                "world modeling",
                "scalable multi-agent training",
                "governance"
            ],
            "githubStars": 48,
            "organization": {
                "_id": "65448bef5b5d9185ba3202b9",
                "name": "UIUC-CS",
                "fullname": "University of Illinois at Urbana-Champaign",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
            }
        },
        "translation_title": "대형 언어 모델을 위한 에이전틱 추론",
        "purpose": "에이전틱 추론의 기본 개념과 방법을 정리하여 LLM의 의사결정 및 행동 능력을 향상시키기 위한 연구",
        "method": [
            "에이전틱 추론을 기초적인 에이전트 능력(계획, 도구 사용, 안정적인 환경에서의 탐색)으로 구성함(First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments.)",
            "자기 진화하는 에이전틱 추론(피드백, 기억 및 적응을 통한 능력 개선)과 집단 다중 에이전트 추론(협력, 지식 공유 및 공동 목표 관련)을 묘사함(self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals.)",
            "각 층을 통해 구조화된 협력을 통한 in-context reasoning과 강화학습 및 감독된 미세 조정을 통한 post-training reasoning의 차이를 구분함(Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning.)"
        ],
        "conclusion": "에이전틱 추론 방법은 LLM에 대한 생각과 행동을 통합하여 개인화 및 실세계 배포를 위한 향후 방향을 제시하며, 여러 실제 적용 분야에서 도전 과제를 정리함.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2601.12346",
            "authors": [
                {
                    "_id": "697145b5c1c7409747bf94c7",
                    "name": "Peizhou Huang",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94c8",
                    "name": "Zixuan Zhong",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94c9",
                    "name": "Zhongwei Wan",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94ca",
                    "user": {
                        "_id": "67136093d2e50f1e8c9fad52",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png",
                        "isPro": false,
                        "fullname": "Donghao Zhou",
                        "user": "donghao-zhou",
                        "type": "user"
                    },
                    "name": "Donghao Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-22T08:45:36.605Z",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94cb",
                    "name": "Samiul Alam",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94cc",
                    "name": "Xin Wang",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94cd",
                    "name": "Zexin Li",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94ce",
                    "name": "Zhihao Dou",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94cf",
                    "name": "Li Zhu",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94d0",
                    "name": "Jing Xiong",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94d1",
                    "name": "Chaofan Tao",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94d2",
                    "name": "Yan Xu",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94d3",
                    "name": "Dimitrios Dimitriadis",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94d4",
                    "name": "Tuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "697145b5c1c7409747bf94d5",
                    "name": "Mi Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-18T10:41:33.000Z",
            "submittedOnDailyAt": "2026-01-22T02:19:13.211Z",
            "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents",
            "submittedOnDailyBy": {
                "_id": "67136093d2e50f1e8c9fad52",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png",
                "isPro": false,
                "fullname": "Donghao Zhou",
                "user": "donghao-zhou",
                "type": "user"
            },
            "summary": "Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.",
            "upvotes": 32,
            "discussionId": "697145b5c1c7409747bf94d6",
            "projectPage": "https://mmdeepresearch-bench.github.io/",
            "githubRepo": "https://github.com/AIoT-MLSys-Lab/MMDeepResearch-Bench",
            "githubRepoAddedBy": "user",
            "ai_summary": "MMDeepResearch-Bench evaluates multimodal research agents on report generation with visual evidence, revealing trade-offs between prose quality, citation accuracy, and visual grounding.",
            "ai_keywords": [
                "multimodal evidence use",
                "citation-grounded report generation",
                "multimodal understanding",
                "deep research agents",
                "Formula-LLM Adaptive Evaluation",
                "Trustworthy Retrieval-Aligned Citation Evaluation",
                "Multimodal Support-Aligned Integrity Check"
            ],
            "githubStars": 10
        },
        "translation_title": "MMDeepResearch-Bench: 멀티모달 딥 리서치 에이전트를 위한 벤치마크",
        "purpose": "멀티모달 이해 및 인용 기반 보고서 생성을 평가하기 위한 벤치마크 구축",
        "method": [
            "140개의 전문가 작성 작업으로 구성된 MMDeepResearch-Bench 벤치마크를 도입함(We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains.)",
            "각 작업에서 이미지-텍스트 번들을 제공하여 멀티모달 이해를 평가함(where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation.)",
            "보고서 품질 평가를 위한 통합된 평가 파이프라인을 제안함(We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality.)"
        ],
        "conclusion": "강력한 서술만으로는 신뢰할 수 있는 증거 사용을 보장할 수 없으며, 멀티모달 무결성이 딥 리서치 에이전트의 주요 병목 현상으로 남아 있음",
        "keywords": [
            "Multimodal Learning",
            "Image Understanding",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2601.15282",
            "authors": [
                {
                    "_id": "69719a70c1c7409747bf9601",
                    "user": {
                        "_id": "68fce03ed1d0efce7ca87075",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png",
                        "isPro": false,
                        "fullname": "yfdeng",
                        "user": "yfdeng10",
                        "type": "user"
                    },
                    "name": "Yufan Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-22T08:44:51.909Z",
                    "hidden": false
                },
                {
                    "_id": "69719a70c1c7409747bf9602",
                    "name": "Zilin Pan",
                    "hidden": false
                },
                {
                    "_id": "69719a70c1c7409747bf9603",
                    "name": "Hongyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "69719a70c1c7409747bf9604",
                    "name": "Xiaojie Li",
                    "hidden": false
                },
                {
                    "_id": "69719a70c1c7409747bf9605",
                    "name": "Ruoqing Hu",
                    "hidden": false
                },
                {
                    "_id": "69719a70c1c7409747bf9606",
                    "user": {
                        "_id": "6661917459720067b2a15bd6",
                        "avatarUrl": "/avatars/f1afe7dd1c538d209016eb5740772d8b.svg",
                        "isPro": false,
                        "fullname": "dyflional10",
                        "user": "dyflional10",
                        "type": "user"
                    },
                    "name": "Yufei Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-22T08:44:59.616Z",
                    "hidden": true
                },
                {
                    "_id": "69719a70c1c7409747bf9607",
                    "name": "Yiming Zou",
                    "hidden": false
                },
                {
                    "_id": "69719a70c1c7409747bf9608",
                    "name": "Yan Zeng",
                    "hidden": false
                },
                {
                    "_id": "69719a70c1c7409747bf9609",
                    "name": "Daquan Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/l2EDevN63IoqsgFC9Xn13.mp4"
            ],
            "publishedAt": "2026-01-21T18:59:18.000Z",
            "submittedOnDailyAt": "2026-01-22T01:05:21.353Z",
            "title": "Rethinking Video Generation Model for the Embodied World",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
            "upvotes": 30,
            "discussionId": "69719a70c1c7409747bf960a",
            "projectPage": "https://dagroup-pku.github.io/ReVidgen.github.io/",
            "githubRepo": "https://github.com/DAGroup-PKU/ReVidgen/",
            "githubRepoAddedBy": "user",
            "ai_summary": "A comprehensive robotics benchmark evaluates video generation models across multiple task domains and robot embodiments, revealing significant gaps in physical realism and introducing a large-scale dataset to address training data limitations.",
            "ai_keywords": [
                "video generation models",
                "embodied intelligence",
                "robotics benchmark",
                "robot-oriented video generation",
                "task domains",
                "physical plausibility",
                "action completeness",
                "Spearman correlation coefficient",
                "RoVid-X",
                "data pipeline",
                "robotic dataset",
                "video models",
                "embodied AI",
                "general intelligence"
            ],
            "githubStars": 32,
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "translation_title": "구체적 세계를 위한 비디오 생성 모델 재고",
        "purpose": "로봇 중심 비디오 생성의 표준화된 벤치마크를 도입하여 물리적으로 현실적인 비디오 생성을 위한 평가와 훈련 데이터 부족 문제를 해결하기 위함",
        "method": [
            "로봇 지향 비디오 생성을 평가하기 위한 종합적인 로봇 벤치마크 RBench를 소개함(To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation.)",
            "RBench는 구조적 일관성, 물리적 신뢰성, 행동 완전성 등 복제 가능한 하위 지표를 통해 작업 수준 타당성과 시각적 충실도를 평가함(It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness.)",
            "4단계 데이터 파이프라인을 도입하여 RoVid-X라는 대규모 공개 로봇 비디오 데이터셋을 생성함(Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation.)"
        ],
        "conclusion": "RBench와 RoVid-X는 비디오 모델 평가와 교육의 강력한 기초를 마련하여 구체적 AI의 발전을 가속화함.",
        "keywords": [
            "Video Generation",
            "Robotics",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2601.14171",
            "authors": [
                {
                    "_id": "69710b60c1c7409747bf9431",
                    "user": {
                        "_id": "6448b2f53e7b3c11be684348",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg",
                        "isPro": true,
                        "fullname": "Qianli Ma",
                        "user": "Mqleet",
                        "type": "user"
                    },
                    "name": "Qianli Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-22T08:46:28.336Z",
                    "hidden": false
                },
                {
                    "_id": "69710b60c1c7409747bf9432",
                    "name": "Chang Guo",
                    "hidden": false
                },
                {
                    "_id": "69710b60c1c7409747bf9433",
                    "name": "Zhiheng Tian",
                    "hidden": false
                },
                {
                    "_id": "69710b60c1c7409747bf9434",
                    "name": "Siyu Wang",
                    "hidden": false
                },
                {
                    "_id": "69710b60c1c7409747bf9435",
                    "name": "Jipeng Xiao",
                    "hidden": false
                },
                {
                    "_id": "69710b60c1c7409747bf9436",
                    "name": "Yuanhao Yue",
                    "hidden": false
                },
                {
                    "_id": "69710b60c1c7409747bf9437",
                    "name": "Zhipeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-20T17:23:51.000Z",
            "submittedOnDailyAt": "2026-01-22T01:11:23.304Z",
            "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
            "submittedOnDailyBy": {
                "_id": "6448b2f53e7b3c11be684348",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg",
                "isPro": true,
                "fullname": "Qianli Ma",
                "user": "Mqleet",
                "type": "user"
            },
            "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.",
            "upvotes": 30,
            "discussionId": "69710b60c1c7409747bf9438",
            "projectPage": "https://mqleet.github.io/Paper2Rebuttal_ProjectPage/",
            "githubRepo": "https://github.com/AutoLab-SAI-SJTU/Paper2Rebuttal",
            "githubRepoAddedBy": "user",
            "ai_summary": "RebuttalAgent is a multi-agent framework that reframes rebuttal generation as an evidence-centric planning task, improving coverage, faithfulness, and strategic coherence in academic peer review.",
            "ai_keywords": [
                "multi-agents framework",
                "evidence-centric planning",
                "rebuttal generation",
                "peer review",
                "strategic coherence",
                "faithful generation",
                "external search module"
            ],
            "githubStars": 116,
            "organization": {
                "_id": "68ee0edd23dc954f7744ac27",
                "name": "AutoLab-SJTU",
                "fullname": "AutoLab"
            }
        },
        "translation_title": "Paper2Rebuttal: 투명한 저자 응답 지원을 위한 다중 에이전트 프레임워크",
        "purpose": "효과적인 반박 작성 과정에서 리뷰어의 의도와 원고 내용을 정확히 일치시키기 위한 지원 시스템 개발",
        "method": [
            "RebuttalAgent라는 다중 에이전트 프레임워크를 도입하여 반박 생성을 증거 중심의 계획 작업으로 재구성함(To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task.)",
            "복잡한 피드백을 세분화하고, 높은 품질의 텍스트와 압축된 요약을 합성하여 동적으로 하이브리드 상황을 구성함(Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text.)",
            "외부 검색 모듈을 통합하여 추가 문헌이 필요한 문제를 해결함(By integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature.)"
        ],
        "conclusion": "RebuttalAgent는 반박 계획을 생성한 후 초안을 작성하여 각 주장이 명확한 증거에 기반하도록 보장하며, 기존의 기준보다 높은 성능을 보여줌.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Document Parsing"
        ]
    },
    {
        "paper": {
            "id": "2601.14750",
            "authors": [
                {
                    "_id": "697191c6c1c7409747bf9583",
                    "name": "Yifan Wang",
                    "hidden": false
                },
                {
                    "_id": "697191c6c1c7409747bf9584",
                    "name": "Shiyu Li",
                    "hidden": false
                },
                {
                    "_id": "697191c6c1c7409747bf9585",
                    "name": "Peiming Li",
                    "hidden": false
                },
                {
                    "_id": "697191c6c1c7409747bf9586",
                    "name": "Xiaochen Yang",
                    "hidden": false
                },
                {
                    "_id": "697191c6c1c7409747bf9587",
                    "name": "Yang Tang",
                    "hidden": false
                },
                {
                    "_id": "697191c6c1c7409747bf9588",
                    "name": "Zheng Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-21T08:09:25.000Z",
            "submittedOnDailyAt": "2026-01-22T00:26:21.515Z",
            "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT",
            "upvotes": 12,
            "discussionId": "697191c7c1c7409747bf9589",
            "ai_summary": "Render-of-Thought framework converts textual reasoning steps into images using vision-language models to improve reasoning traceability and efficiency while maintaining competitive performance.",
            "ai_keywords": [
                "Chain-of-Thought prompting",
                "Large Language Models",
                "vision encoders",
                "Vision Language Models",
                "token compression",
                "inference acceleration",
                "reasoning chain",
                "semantic anchors",
                "latent reasoning",
                "traceability"
            ],
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "translation_title": "Render-of-Thought: 시각적 잠재 추론을 위한 텍스트 기반 사고 과정 시각화",
        "purpose": "Large Language Models의 잠재 추론 과정을 명확히 하기 위해 텍스트 단계들을 이미지로 변환하여 분석할 수 있도록 하는 방법을 제안",
        "method": [
            "Render-of-Thought(RoT) 프레임워크를 도입하여 reasoning chain을 렌더링함(we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images)",
            "기존 Vision Language Models의 vision 인코더를 활용하여 비전 임베딩을 텍스트 공간에 정렬함(This design ensures plug-and-play implementation without incurring additional pre-training overhead)",
            "수학 및 논리적 추론 벤치마크에서 방법을 실험하여 3-4배의 토큰 압축과 추론 속도 향상을 입증함(Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT)"
        ],
        "conclusion": "RoT는 추론 과정의 가시성을 높이며 기존 방법과 유사한 성능을 유지하는 동시에 효율성을 극대화함.",
        "keywords": [
            "Natural Language Processing",
            "Vision-Language Models",
            "Large Language Models"
        ]
    }
]