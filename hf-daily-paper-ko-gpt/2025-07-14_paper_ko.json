[
    {
        "paper": {
            "id": "2507.01951",
            "authors": [
                {
                    "_id": "6868daac213f123a1f88b9c8",
                    "name": "Zixiao Wang",
                    "hidden": false
                },
                {
                    "_id": "6868daac213f123a1f88b9c9",
                    "name": "Yuxin Wang",
                    "hidden": false
                },
                {
                    "_id": "6868daac213f123a1f88b9ca",
                    "name": "Xiaorui Wang",
                    "hidden": false
                },
                {
                    "_id": "6868daac213f123a1f88b9cb",
                    "name": "Mengting Xing",
                    "hidden": false
                },
                {
                    "_id": "6868daac213f123a1f88b9cc",
                    "name": "Jie Gao",
                    "hidden": false
                },
                {
                    "_id": "6868daac213f123a1f88b9cd",
                    "name": "Jianjun Xu",
                    "hidden": false
                },
                {
                    "_id": "6868daac213f123a1f88b9ce",
                    "name": "Guangcan Liu",
                    "hidden": false
                },
                {
                    "_id": "6868daac213f123a1f88b9cf",
                    "name": "Chenhui Jin",
                    "hidden": false
                },
                {
                    "_id": "6868daac213f123a1f88b9d0",
                    "name": "Zhuo Wang",
                    "hidden": false
                },
                {
                    "_id": "6868daac213f123a1f88b9d1",
                    "name": "Shengzhuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "6868daac213f123a1f88b9d2",
                    "name": "Hongtao Xie",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/638700c723da90491eb72722/mRIN68aSkxejrT4UqCaKz.jpeg"
            ],
            "publishedAt": "2025-07-02T17:58:01.000Z",
            "submittedOnDailyAt": "2025-07-14T00:42:27.924Z",
            "title": "Test-Time Scaling with Reflective Generative Model",
            "submittedOnDailyBy": {
                "_id": "638700c723da90491eb72722",
                "avatarUrl": "/avatars/6dcddca5c31121b60e45aab0816e11be.svg",
                "isPro": false,
                "fullname": "Yuxin Wang",
                "user": "wangyuxin87",
                "type": "user"
            },
            "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",
            "upvotes": 67,
            "discussionId": "6868daac213f123a1f88b9d3",
            "githubRepo": "https://github.com/MetaStone-AI/MetaStone-S1",
            "ai_summary": "MetaStone-S1, a reflective generative model using a self-supervised process reward model, achieves efficient reasoning and scalable performance with fewer parameters compared to existing models.",
            "ai_keywords": [
                "reflective generative model",
                "self-supervised process reward model",
                "backbone network",
                "task-specific heads",
                "policy model",
                "process reward model",
                "test time scaling",
                "controllable thinking length",
                "scaling law",
                "total thinking computation"
            ],
            "githubStars": 44
        },
        "translation_title": "반사적 생성 모델을 이용한 테스트 시간 스케일링",
        "purpose": "self-supervised 과정 보상 모델(SPRM)을 통해 OpenAI o3의 성능을 달성하기 위한 반사적 생성 모델 개발",
        "method": [
            "MetaStone-S1이라는 반사적 생성 모델을 소개함(We introduce our first reflective generative model MetaStone-S1)",
            "공유된 백본 네트워크와 특정 작업에 맞는 헤드를 사용하여 정책 모델과 과정 보상 모델(PRM)을 통합함(Through sharing the backbone network and using task-specific heads for next token prediction and process scoring respectively, SPRM successfully integrates the policy model and process reward model(PRM) into a unified interface)",
            "SPRM을 통해 테스트 시간 스케일링(TTS)을 위해 적합한 시스템을 제공하며, 세 가지 추론 모드를 설정함(Equipped with SPRM, MetaStone-S1 is naturally suitable for test time scaling (TTS), and we provide three reasoning effort modes (low, medium, and high))."
        ],
        "conclusion": "MetaStone-S1은 32B 매개변수 크기로 OpenAI-o3-mini의 성능에 필적하는 결과를 달성함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2507.08776",
            "authors": [
                {
                    "_id": "68745e0f257d4f0435370288",
                    "name": "Zhengqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68745e0f257d4f0435370289",
                    "name": "Yuefan Wu",
                    "hidden": false
                },
                {
                    "_id": "68745e0f257d4f043537028a",
                    "name": "Jiacheng Chen",
                    "hidden": false
                },
                {
                    "_id": "68745e0f257d4f043537028b",
                    "name": "Fuyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68745e0f257d4f043537028c",
                    "name": "Yasutaka Furukawa",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64d97c5bfd0b55d501ba00cf/xQVrjtR_Sb4SOz-GghYdp.mp4"
            ],
            "publishedAt": "2025-07-11T17:38:52.000Z",
            "submittedOnDailyAt": "2025-07-14T00:07:38.676Z",
            "title": "CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive\n  Neural Rendering",
            "submittedOnDailyBy": {
                "_id": "64d97c5bfd0b55d501ba00cf",
                "avatarUrl": "/avatars/47505f2a573acea7176a96f538226ecb.svg",
                "isPro": false,
                "fullname": "Zhengqing Wang",
                "user": "EricW123456",
                "type": "user"
            },
            "summary": "This paper proposes a neural rendering approach that represents a scene as\n\"compressed light-field tokens (CLiFTs)\", retaining rich appearance and\ngeometric information of a scene. CLiFT enables compute-efficient rendering by\ncompressed tokens, while being capable of changing the number of tokens to\nrepresent a scene or render a novel view with one trained network. Concretely,\ngiven a set of images, multi-view encoder tokenizes the images with the camera\nposes. Latent-space K-means selects a reduced set of rays as cluster centroids\nusing the tokens. The multi-view ``condenser'' compresses the information of\nall the tokens into the centroid tokens to construct CLiFTs. At test time,\ngiven a target view and a compute budget (i.e., the number of CLiFTs), the\nsystem collects the specified number of nearby tokens and synthesizes a novel\nview using a compute-adaptive renderer. Extensive experiments on RealEstate10K\nand DL3DV datasets quantitatively and qualitatively validate our approach,\nachieving significant data reduction with comparable rendering quality and the\nhighest overall rendering score, while providing trade-offs of data size,\nrendering quality, and rendering speed.",
            "upvotes": 39,
            "discussionId": "68745e10257d4f043537028d",
            "projectPage": "https://clift-nvs.github.io/",
            "githubRepo": "https://github.com/eric-zqwang/CLiFT",
            "ai_summary": "A neural rendering method uses compressed light-field tokens to efficiently represent scenes and render novel views with varying compute budgets.",
            "ai_keywords": [
                "neural rendering",
                "compressed light-field tokens",
                "CLiFTs",
                "multi-view encoder",
                "latent-space K-means",
                "condenser",
                "compute-adaptive renderer",
                "RealEstate10K",
                "DL3DV datasets"
            ],
            "githubStars": 8
        },
        "translation_title": "CLiFT: 계산 효율적이고 적응 가능한 신경 렌더링을 위한 압축된 라이트 필드 토큰",
        "purpose": "장면을 압축된 라이트 필드 토큰으로 표현하여 계산 효율성을 개선하고 적응적인 렌더링을 달성",
        "method": [
            "다양한 이미지를 주어지면, 멀티뷰 인코더가 이미지를 카메라 자세와 함께 토큰화함(This paper proposes a neural rendering approach that represents a scene as 'compressed light-field tokens (CLiFTs)', retaining rich appearance and geometric information of a scene.)",
            "잠재 공간 K-평균을 이용해 토큰들 중에서 클러스터 중심을 선택함(Latent-space K-means selects a reduced set of rays as cluster centroids using the tokens.)",
            "멀티뷰 '컨덴서'가 모든 토큰의 정보를 중심 토큰으로 압축하여 CLiFT 생성(Multi-view 'condenser' compresses the information of all the tokens into the centroid tokens to construct CLiFTs.)",
            "테스트 시, 정해진 뷰와 계산 예산에 따라 근처 토큰을 수집하고 새로운 뷰를 합성함(At test time, given a target view and a compute budget, the system collects the specified number of nearby tokens and synthesizes a novel view using a compute-adaptive renderer.)"
        ],
        "conclusion": "데이터 크기, 렌더링 품질 및 렌더링 속도의 균형을 유지하면서도 유의미한 데이터 감소와 유사한 렌더링 품질을 달성함.",
        "keywords": [
            "Computer Vision",
            "Neural Rendering",
            "3D Vision"
        ]
    },
    {
        "paper": {
            "id": "2507.08800",
            "authors": [
                {
                    "_id": "6874615e257d4f043537028f",
                    "name": "Luke Rivard",
                    "hidden": false
                },
                {
                    "_id": "6874615e257d4f0435370290",
                    "name": "Sun Sun",
                    "hidden": false
                },
                {
                    "_id": "6874615e257d4f0435370291",
                    "name": "Hongyu Guo",
                    "hidden": false
                },
                {
                    "_id": "6874615e257d4f0435370292",
                    "name": "Wenhu Chen",
                    "hidden": false
                },
                {
                    "_id": "6874615e257d4f0435370293",
                    "name": "Yuntian Deng",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/8bwvqBSzlNYlfcdn-pVWu.mp4"
            ],
            "publishedAt": "2025-07-11T17:59:40.000Z",
            "submittedOnDailyAt": "2025-07-14T01:10:17.755Z",
            "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models",
            "submittedOnDailyBy": {
                "_id": "63081e15a670ed10f9d44229",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
                "isPro": true,
                "fullname": "Yuntian Deng",
                "user": "yuntian-deng",
                "type": "user"
            },
            "summary": "We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.",
            "upvotes": 31,
            "discussionId": "6874615e257d4f0435370294",
            "projectPage": "https://neural-os.com/",
            "githubRepo": "https://github.com/yuntian-group/neural-os",
            "ai_summary": "NeuralOS uses a combination of RNNs and diffusion-based rendering to simulate OS GUIs by predicting screen frames from user inputs, demonstrating realistic GUI rendering and state transitions.",
            "ai_keywords": [
                "recurrent neural network",
                "RNN",
                "diffusion-based neural renderer",
                "GUI",
                "user inputs",
                "mouse interactions",
                "keyboard events",
                "state transitions",
                "application launches"
            ],
            "githubStars": 2
        },
        "translation_title": "NeuralOS: 신경 생성 모델을 통한 운영 체제 시뮬레이션",
        "purpose": "사용자의 입력에 따라 그래픽 사용자 인터페이스(GUI)를 시뮬레이션하는 프레임워크 연구",
        "method": [
            "신경망 구조인 NeuralOS를 통해 사용자 입력에 응답하여 화면을 직접 예측함으로써 GUI를 시뮬레이션함(We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs.)",
            "RNN과 diffusion 기반 신경 렌더러를 조합하여 컴퓨터 상태를 추적하고 화면 이미지를 생성함(NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images.)",
            "우분투 XFCE의 대규모 데이터세트에서 훈련하여 랜덤 및 실제 상호작용을 모델링함(The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents.)"
        ],
        "conclusion": "NeuralOS는 현실적인 GUI 시퀀스를 성공적으로 렌더링하고 사용자 상호작용을 정확히 캡처하며, 향후 인간-컴퓨터 상호작용 시스템을 위한 생성적인 신경 인터페이스 개발에 한 걸음 더 나아갔음을 보여줌.",
        "keywords": [
            "Robotics",
            "Image Understanding",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2507.08799",
            "authors": [
                {
                    "_id": "6874ac1e257d4f0435370389",
                    "name": "Max Belitsky",
                    "hidden": false
                },
                {
                    "_id": "6874ac1e257d4f043537038a",
                    "name": "Dawid J. Kopiczko",
                    "hidden": false
                },
                {
                    "_id": "6874ac1e257d4f043537038b",
                    "name": "Michael Dorkenwald",
                    "hidden": false
                },
                {
                    "_id": "6874ac1e257d4f043537038c",
                    "name": "M. Jehanzeb Mirza",
                    "hidden": false
                },
                {
                    "_id": "6874ac1e257d4f043537038d",
                    "name": "Cees G. M. Snoek",
                    "hidden": false
                },
                {
                    "_id": "6874ac1e257d4f043537038e",
                    "name": "Yuki M. Asano",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/637d21239a5217b88b7549c3/w5UPBRgGM_-9_ELxgOBlL.png"
            ],
            "publishedAt": "2025-07-11T17:59:36.000Z",
            "submittedOnDailyAt": "2025-07-14T05:40:13.268Z",
            "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
            "submittedOnDailyBy": {
                "_id": "637d21239a5217b88b7549c3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
                "isPro": false,
                "fullname": "Yuki Asano",
                "user": "yukimasano",
                "type": "user"
            },
            "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
            "upvotes": 20,
            "discussionId": "6874ac1e257d4f043537038f",
            "ai_summary": "Cache steering improves reasoning in language models through a single intervention in the key-value cache, enhancing both reasoning structure and task performance.",
            "ai_keywords": [
                "cache steering",
                "key-value cache",
                "chain-of-thought reasoning",
                "GPT-4o",
                "steering vectors",
                "multi-step reasoning",
                "activation steering",
                "hyperparameter stability",
                "inference-time efficiency",
                "ease of integration",
                "controlled generation"
            ]
        },
        "translation_title": "소형 언어 모델에서 추론 유도를 위한 KV Cache 조정",
        "purpose": "소형 언어 모델에서 체인-오브-생각(Chain-of-Thought) 추론을 유도하기 위한 경량 방법 개발",
        "method": [
            "KV Cache에 직접 적용하는 일회성 개입을 통해 언어 모델의 암묵적 조정을 수행함(We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache.)",
            "GPT-4o로 생성된 추론 추적을 활용해 모델 행동을 더 명시적이고 다단계 추론으로 전환하는 조정 벡터를 구성함(Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning.)",
            "다양한 추론 벤치마크에서 실험적 평가를 수행하여 cache steering의 효과를 입증함(Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance.)"
        ],
        "conclusion": "KV Cache 조정은 하이퍼파라미터 안정성, 추론 시간 효율성, 통합 용이성 측면에서 큰 장점을 제공하여 보다 강력하고 실용적인 제어 생성 솔루션을 제시함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2507.05397",
            "authors": [
                {
                    "_id": "6874a1ff257d4f0435370344",
                    "name": "Pengfei Zhou",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f0435370345",
                    "name": "Jie Xia",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f0435370346",
                    "name": "Xiaopeng Peng",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f0435370347",
                    "name": "Wangbo Zhao",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f0435370348",
                    "name": "Zilong Ye",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f0435370349",
                    "name": "Zekai Li",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f043537034a",
                    "name": "Suorong Yang",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f043537034b",
                    "name": "Jiadong Pan",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f043537034c",
                    "name": "Yuanxiang Chen",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f043537034d",
                    "name": "Ziqiao Wang",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f043537034e",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f043537034f",
                    "name": "Qian Zheng",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f0435370350",
                    "name": "Xiaojun Chang",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f0435370351",
                    "name": "Gang Pan",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f0435370352",
                    "name": "Shurong Dong",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f0435370353",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6874a1ff257d4f0435370354",
                    "name": "Yang You",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/KhRuhMNBeK8P8MfGydldo.png"
            ],
            "publishedAt": "2025-07-07T18:31:50.000Z",
            "submittedOnDailyAt": "2025-07-14T04:53:28.591Z",
            "title": "Neural-Driven Image Editing",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Traditional image editing typically relies on manual prompting, making it\nlabor-intensive and inaccessible to individuals with limited motor control or\nlanguage abilities. Leveraging recent advances in brain-computer interfaces\n(BCIs) and generative models, we propose LoongX, a hands-free image editing\napproach driven by multimodal neurophysiological signals. LoongX utilizes\nstate-of-the-art diffusion models trained on a comprehensive dataset of 23,928\nimage editing pairs, each paired with synchronized electroencephalography\n(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography\n(PPG), and head motion signals that capture user intent. To effectively address\nthe heterogeneity of these signals, LoongX integrates two key modules. The\ncross-scale state space (CS3) module encodes informative modality-specific\nfeatures. The dynamic gated fusion (DGF) module further aggregates these\nfeatures into a unified latent space, which is then aligned with edit semantics\nvia fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train\nthe encoders using contrastive learning to align cognitive states with semantic\nintentions from embedded natural language. Extensive experiments demonstrate\nthat LoongX achieves performance comparable to text-driven methods (CLIP-I:\n0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural\nsignals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results\nhighlight the promise of neural-driven generative models in enabling\naccessible, intuitive image editing and open new directions for\ncognitive-driven creative technologies. Datasets and code will be released to\nsupport future work and foster progress in this emerging area.",
            "upvotes": 20,
            "discussionId": "6874a200257d4f0435370355",
            "projectPage": "https://loongx1.github.io/",
            "githubRepo": "https://github.com/LanceZPF/loongx",
            "ai_summary": "LoongX uses multimodal neurophysiological signals and diffusion models for hands-free image editing, achieving performance comparable to text-driven methods and outperforming them when combined with speech.",
            "ai_keywords": [
                "diffusion models",
                "electroencephalography (EEG)",
                "functional near-infrared spectroscopy (fNIRS)",
                "photoplethysmography (PPG)",
                "head motion signals",
                "cross-scale state space (CS3) module",
                "dynamic gated fusion (DGF) module",
                "diffusion transformer (DiT)",
                "contrastive learning"
            ],
            "githubStars": 5
        },
        "translation_title": "신경 기반 이미지 편집",
        "purpose": "손으로 조작하기 어려운 사람들도 사용할 수 있는 이미지 편집 방법 개발",
        "method": [
            "최신의 뇌-컴퓨터 인터페이스(BCI)와 생성 모델을 활용하여 LoongX라는 무손 조작 이미지 편집 방법 제안(Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, a hands-free image editing approach driven by multimodal neurophysiological signals.)",
            "23,928개의 이미지 편집 쌍으로 구성된 데이터셋을 활용하여 최첨단 diffusion 모델 훈련(LoongX utilizes state-of-the-art diffusion models trained on a comprehensive dataset of 23,928 image editing pairs.)",
            "사용자 의도를 포착할 수 있는 전기생리학 신호를 통합하여 신호의 이질성을 효과적으로 처리(Cross-scale state space (CS3) module encodes informative modality-specific features. The dynamic gated fusion (DGF) module further aggregates these features into a unified latent space.)",
            "대조 학습을 통해 인코더를 사전 훈련하여 인지 상태와 의미적 의도를 정렬(Additionally, we pre-train the encoders using contrastive learning to align cognitive states with semantic intentions from embedded natural language.)"
        ],
        "conclusion": "LoongX는 텍스트 기반 방법과 유사한 성능을 보여주며, 신경 신호와 음성을 결합했을 때 성능이 향상된다는 결과를 얻음으로써, 접근 가능한 직관적인 이미지 편집 가능성을 증명하였습니다.",
        "keywords": [
            "Image Generation",
            "Multimodal Learning",
            "Natural Language Processing"
        ]
    }
]