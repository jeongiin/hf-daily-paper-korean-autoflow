[
    {
        "paper": {
            "id": "2512.15431",
            "authors": [
                {
                    "_id": "69437417542d62d58a7bf6c4",
                    "name": "Haolong Yan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6c5",
                    "name": "Jia Wang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6c6",
                    "name": "Xin Huang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6c7",
                    "name": "Yeqing Shen",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6c8",
                    "user": {
                        "_id": "653614073f4248157d60ccdc",
                        "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg",
                        "isPro": false,
                        "fullname": "mengziyang",
                        "user": "zylate",
                        "type": "user"
                    },
                    "name": "Ziyang Meng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:53.033Z",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6c9",
                    "name": "Zhimin Fan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ca",
                    "name": "Kaijun Tan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6cb",
                    "name": "Jin Gao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6cc",
                    "name": "Lieyu Shi",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6cd",
                    "name": "Mi Yang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ce",
                    "name": "Shiliang Yang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6cf",
                    "name": "Zhirui Wang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d0",
                    "name": "Brian Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d1",
                    "name": "Kang An",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d2",
                    "name": "Chenyang Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d3",
                    "name": "Lei Lei",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d4",
                    "name": "Mengmeng Duan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d5",
                    "name": "Danxun Liang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d6",
                    "name": "Guodong Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d7",
                    "name": "Hang Cheng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d8",
                    "name": "Hao Wu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d9",
                    "name": "Jie Dong",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6da",
                    "name": "Junhao Huang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6db",
                    "name": "Mei Chen",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6dc",
                    "name": "Renjie Yu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6dd",
                    "name": "Shunshan Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6de",
                    "name": "Xu Zhou",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6df",
                    "name": "Yiting Dai",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e0",
                    "name": "Yineng Deng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e1",
                    "name": "Yingdan Liang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e2",
                    "name": "Zelin Chen",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e3",
                    "name": "Wen Sun",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e4",
                    "name": "Chengxu Yan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e5",
                    "name": "Chunqin Xu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e6",
                    "name": "Dong Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e7",
                    "name": "Fengqiong Xiao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e8",
                    "name": "Guanghao Fan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e9",
                    "name": "Guopeng Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ea",
                    "name": "Guozhen Peng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6eb",
                    "name": "Hongbing Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ec",
                    "name": "Hang Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ed",
                    "name": "Hongming Chen",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ee",
                    "name": "Jingjing Xie",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ef",
                    "name": "Jianyong Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f0",
                    "name": "Jingyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f1",
                    "name": "Jiaju Ren",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f2",
                    "name": "Jiayu Yuan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f3",
                    "name": "Jianpeng Yin",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f4",
                    "name": "Kai Cao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f5",
                    "name": "Liang Zhao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f6",
                    "name": "Liguo Tan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f7",
                    "name": "Liying Shi",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f8",
                    "name": "Mengqiang Ren",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f9",
                    "name": "Min Xu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6fa",
                    "name": "Manjiao Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6fb",
                    "name": "Mao Luo",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6fc",
                    "name": "Mingxin Wan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6fd",
                    "name": "Na Wang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6fe",
                    "name": "Nan Wu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ff",
                    "name": "Ning Wang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf700",
                    "name": "Peiyao Ma",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf701",
                    "name": "Qingzhou Zhang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf702",
                    "name": "Qiao Wang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf703",
                    "name": "Qinlin Zeng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf704",
                    "name": "Qiong Gao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf705",
                    "name": "Qiongyao Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf706",
                    "name": "Shangwu Zhong",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf707",
                    "name": "Shuli Gao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf708",
                    "name": "Shaofan Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf709",
                    "name": "Shisi Gao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf70a",
                    "name": "Shuang Luo",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf70b",
                    "name": "Xingbin Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf70c",
                    "name": "Xiaojia Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf70d",
                    "name": "Xiaojie Hou",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf70e",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf70f",
                    "name": "Xuanti Feng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf710",
                    "name": "Xuedan Cai",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf711",
                    "name": "Xuan Wen",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf712",
                    "name": "Xianwei Zhu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf713",
                    "name": "Xin Liang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf714",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf715",
                    "name": "Xin Zhou",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf716",
                    "name": "Yingxiu Zhao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf717",
                    "name": "Yukang Shi",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf718",
                    "name": "Yunfang Xu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf719",
                    "name": "Yuqing Zeng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf71a",
                    "name": "Yixun Zhang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf71b",
                    "name": "Zejia Weng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf71c",
                    "name": "Zhonghao Yan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf71d",
                    "name": "Zhiguo Huang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf71e",
                    "name": "Zhuoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf71f",
                    "name": "Zheng Ge",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf720",
                    "name": "Jing Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf721",
                    "name": "Yibo Zhu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf722",
                    "name": "Binxing Jiao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf723",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf724",
                    "name": "Daxin Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T13:26:30.000Z",
            "submittedOnDailyAt": "2025-12-18T00:55:26.804Z",
            "title": "Step-GUI Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.",
            "upvotes": 72,
            "discussionId": "69437418542d62d58a7bf725",
            "projectPage": "https://opengelab.github.io/",
            "githubRepo": "https://github.com/stepfun-ai/gelab-zero",
            "githubRepoAddedBy": "user",
            "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.",
            "ai_keywords": [
                "multimodal large language models",
                "GUI automation",
                "self-evolving training pipeline",
                "Calibrated Step Reward System",
                "trajectory-level calibration",
                "Step-GUI",
                "GUI performance",
                "GUI-MCP",
                "Model Context Protocol",
                "AndroidWorld",
                "OSWorld",
                "ScreenShot-Pro",
                "AndroidDaily",
                "real-world mobile usage patterns",
                "hierarchical architecture",
                "low-level atomic operations",
                "high-level task delegation",
                "local specialist models",
                "high-privacy execution"
            ],
            "organization": {
                "_id": "66e43eae9d477f566f937935",
                "name": "stepfun-ai",
                "fullname": "StepFun",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
            }
        },
        "translation_title": "Step-GUI 기술 보고서",
        "purpose": "GUI 자동화를 위한 고품질 훈련 데이터를 효율적으로 획득하고 주석 신뢰성을 유지하기 위한 방법 연구",
        "method": [
            "Calibrated Step Reward System을 활용해 모델 생성 경로를 신뢰할 수 있는 훈련 신호로 변환하는 자기 발전 훈련 파이프라인을 소개함 (We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals.)",
            "이 훈련 파이프라인을 통해 고성능 GUI 모델인 Step-GUI(4B/8B)를 개발함 (Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance.)",
            "GUI 자동화를 위한 최초의 Model Context Protocol인 GUI-MCP를 제안하며, 민감한 데이터가 장치 내에 유지될 수 있도록 설계함 (To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture.)"
        ],
        "conclusion": "이 연구는 실용적인 GUI 에이전트 개발을 진전시키고, 일상적인 디지털 상호작용에서 실제 배포의 강력한 가능성을 보여줌.",
        "keywords": [
            "Multimodal Learning",
            "Large Language Models",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2512.15176",
            "authors": [
                {
                    "_id": "694370d7542d62d58a7bf698",
                    "name": "Zicong Cheng",
                    "hidden": false
                },
                {
                    "_id": "694370d7542d62d58a7bf699",
                    "name": "Guo-Wei Yang",
                    "hidden": false
                },
                {
                    "_id": "694370d7542d62d58a7bf69a",
                    "name": "Jia Li",
                    "hidden": false
                },
                {
                    "_id": "694370d7542d62d58a7bf69b",
                    "name": "Zhijie Deng",
                    "hidden": false
                },
                {
                    "_id": "694370d7542d62d58a7bf69c",
                    "user": {
                        "_id": "6571b51fd5c6a6d3b0ba68ad",
                        "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg",
                        "isPro": false,
                        "fullname": "gmh",
                        "user": "menghao22",
                        "type": "user"
                    },
                    "name": "Meng-Hao Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:54.790Z",
                    "hidden": false
                },
                {
                    "_id": "694370d7542d62d58a7bf69d",
                    "name": "Shi-Min Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T08:19:04.000Z",
            "submittedOnDailyAt": "2025-12-18T00:44:13.153Z",
            "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models",
            "submittedOnDailyBy": {
                "_id": "6571b51fd5c6a6d3b0ba68ad",
                "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg",
                "isPro": false,
                "fullname": "gmh",
                "user": "menghao22",
                "type": "user"
            },
            "summary": "Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/",
            "upvotes": 37,
            "discussionId": "694370d8542d62d58a7bf69e",
            "projectPage": "https://czc726.github.io/DEER/",
            "ai_summary": "DEER framework uses diffusion large language models for efficient speculative decoding, overcoming the limitations of autoregressive drafters with better speed and draft quality.",
            "ai_keywords": [
                "autoregressive decoding",
                "speculative decoding",
                "diffusion large language model",
                "dLLM",
                "draft-verify scheme",
                "step-wise uncertainty accumulation",
                "parallel decoding",
                "two-stage training pipeline",
                "single-step decoding",
                "HumanEval",
                "Qwen3-30B-A3B"
            ]
        },
        "translation_title": "DEER: 확산으로 초안을 작성하고 자기 회귀 모델로 검증하기",
        "purpose": "LLM 기반 시스템의 효율성을 향상시키기 위한 새로운 스펙ulative decoding 프레임워크 개발",
        "method": [
            "DEER는 확산 대형 언어 모델을 사용해 초안을 작성하고, 자기 회귀 모델로 검증함(we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues).",
            "두 단계 훈련 파이프라인을 통해 dLLM 기반의 초안 작성기를 목표 AR 모델과 정렬함(DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model).",
            "단계 별로 디코딩을 수행해 긴 초안 세그먼트를 생성하는 방식을 채택함(further adopts single-step decoding to generate long draft segments)."
        ],
        "conclusion": "DEER는 초안 수용 길이에서 EAGLE-3보다 월등한 성능을 나타내며, 속도에서도 큰 개선을 보여줍니다.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.14681",
            "authors": [
                {
                    "_id": "69437375542d62d58a7bf6aa",
                    "name": "Lanxiang Hu",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6ab",
                    "name": "Siqi Kou",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6ac",
                    "name": "Yichao Fu",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6ad",
                    "name": "Samyam Rajbhandari",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6ae",
                    "name": "Tajana Rosing",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6af",
                    "name": "Yuxiong He",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6b0",
                    "name": "Zhijie Deng",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6b1",
                    "name": "Hao Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/X_SGgKHd6c9A0-I-9QNyn.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/MidLBXBtdUmG0fY7Wyh27.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/qVt5G_vvppSDGMXPPG8Sf.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/tJhlEhoQSGi5hoe8KvTId.gif"
            ],
            "publishedAt": "2025-12-16T18:45:18.000Z",
            "submittedOnDailyAt": "2025-12-18T00:57:47.270Z",
            "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
            "submittedOnDailyBy": {
                "_id": "6301d6455e305a35cb0846a7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
                "isPro": true,
                "fullname": "Lanxiang Hu",
                "user": "Snyhlxde",
                "type": "user"
            },
            "summary": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.",
            "upvotes": 33,
            "discussionId": "69437375542d62d58a7bf6b2",
            "githubRepo": "https://github.com/hao-ai-lab/JacobiForcing",
            "githubRepoAddedBy": "user",
            "ai_summary": "Jacobi Forcing is a progressive distillation method that enables efficient parallel decoding of transformer-based models while maintaining performance, significantly reducing inference latency.",
            "ai_keywords": [
                "Multi-token generation",
                "diffusion Large Language Models (dLLMs)",
                "parallel decoding",
                "inference latency",
                "adaptive autoregressive (AR) models",
                "masked data distribution",
                "bidirectional attention",
                "causal inference",
                "pretrained causal inference property",
                "trajectory characteristics",
                "multi-block decoding",
                "rejection recycling"
            ],
            "githubStars": 29
        },
        "translation_title": "Jacobi Forcing을 이용한 빠르고 정확한 인과적 병렬 디코딩",
        "purpose": "Transformer 기반 대규모 모델 추론의 지연 시간을 줄이기 위해 효율적인 병렬 디코딩 방법 연구",
        "method": [
            "Jacobi Forcing이라는 점진적 증류 패러다임을 도입하여, 모델이 자신의 생성된 병렬 디코딩 경로에서 훈련되도록 함(To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories.)",
            "Jacobi Forcing 모델은 코딩 및 수학 벤치마크에서 3.8배의 속도 향상을 달성하고 성능 손실을 최소화함.(The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance.)",
            "추가 계산으로 지연 시간을 줄이는 다중 블록 디코딩과 재사용 메커니즘을 도입함(Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup.)"
        ],
        "conclusion": "Jacobi Forcing을 통해 대규모 모델의 성능을 향상시키면서 효율적인 병렬 디코딩을 실현함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.14052",
            "authors": [
                {
                    "_id": "6942650d5d5b2dc1052749c8",
                    "name": "HyperAI Team",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749c9",
                    "name": "Yuchen Liu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749ca",
                    "name": "Kaiyang Han",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749cb",
                    "name": "Zhiqiang Xia",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749cc",
                    "name": "Yuhang Dong",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749cd",
                    "name": "Chen Song",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749ce",
                    "name": "Kangyu Tang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749cf",
                    "name": "Jiaming Xu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d0",
                    "name": "Xiushi Feng",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d1",
                    "name": "WenXuan Yu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d2",
                    "name": "Li Peng",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d3",
                    "name": "Mingyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d4",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d5",
                    "name": "Changpeng Yang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d6",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d7",
                    "name": "Haoyu Lu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d8",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d9",
                    "name": "Bingna Xu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749da",
                    "name": "Guangyao Liu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749db",
                    "name": "Long Huang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749dc",
                    "name": "Kaibin Guo",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749dd",
                    "name": "Jinyang Wu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749de",
                    "name": "Dan Wu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749df",
                    "name": "Hongzhen Wang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749e0",
                    "name": "Peng Zhou",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749e1",
                    "name": "Shuai Nie",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749e2",
                    "name": "Shande Wang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749e3",
                    "name": "Runyu Shi",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749e4",
                    "name": "Ying Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-16T03:36:41.000Z",
            "submittedOnDailyAt": "2025-12-18T05:34:36.317Z",
            "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
            "submittedOnDailyBy": {
                "_id": "6747de57f8cab58c22ec94a2",
                "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
                "isPro": false,
                "fullname": "Jinyang Wu",
                "user": "Jinyang23",
                "type": "user"
            },
            "summary": "Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.",
            "upvotes": 27,
            "discussionId": "6942650e5d5b2dc1052749e5",
            "ai_summary": "HyperVL, an efficient multimodal large language model for on-device inference, uses image tiling, Visual Resolution Compressor, and Dual Consistency Learning to reduce memory usage, latency, and power consumption while maintaining performance.",
            "ai_keywords": [
                "multimodal large language models",
                "Vision Transformer (ViT)",
                "image-tiling strategy",
                "Visual Resolution Compressor",
                "Dual Consistency Learning",
                "on-device inference",
                "latency",
                "power consumption",
                "benchmarks"
            ]
        },
        "translation_title": "HyperVL: 엣지 디바이스를 위한 효율적이고 동적인 다중 모달 대형 언어 모델",
        "purpose": "엣지 디바이스에서의 사용을 위해 메모리 사용량과 지연 시간을 줄인 다중 모달 대형 언어 모델 개발",
        "method": [
            "이미지를 타일 형태로 분할해 메모리 사용량을 제한함(To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference.)",
            "최적의 인코딩 해상도를 예측해 중복 계산을 제거하는 Visual Resolution Compressor(VRC)를 도입함(HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC)...)",
            "다양한 해상도의 ViT 인코더를 통합하는 Dual Consistency Learning(DCL)을 활용해 동적으로 전환 가능하게 함(2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework...)"
        ],
        "conclusion": "HyperVL은 유사한 크기의 모델 중 최고의 성능을 달성하고 실제 모바일 디바이스에서의 지연 시간과 전력 소모를 크게 줄여 실용성을 입증함.",
        "keywords": [
            "Multimodal Learning",
            "Large Language Models",
            "Image Classification"
        ]
    },
    {
        "paper": {
            "id": "2512.15635",
            "authors": [
                {
                    "_id": "6943902a542d62d58a7bf7a0",
                    "user": {
                        "_id": "64aa2210e04e7f92245f54d2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa2210e04e7f92245f54d2/OE43T22bLWBVgmqcJyUtu.png",
                        "isPro": false,
                        "fullname": "Li",
                        "user": "kotion",
                        "type": "user"
                    },
                    "name": "Yuanhang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:34.872Z",
                    "hidden": false
                },
                {
                    "_id": "6943902a542d62d58a7bf7a1",
                    "name": "Yiren Song",
                    "hidden": false
                },
                {
                    "_id": "6943902a542d62d58a7bf7a2",
                    "name": "Junzhe Bai",
                    "hidden": false
                },
                {
                    "_id": "6943902a542d62d58a7bf7a3",
                    "name": "Xinran Liang",
                    "hidden": false
                },
                {
                    "_id": "6943902a542d62d58a7bf7a4",
                    "name": "Hu Yang",
                    "hidden": false
                },
                {
                    "_id": "6943902a542d62d58a7bf7a5",
                    "name": "Libiao Jin",
                    "hidden": false
                },
                {
                    "_id": "6943902a542d62d58a7bf7a6",
                    "user": {
                        "_id": "6388a7e98a5dbe2f3dc61faa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg",
                        "isPro": false,
                        "fullname": "Qi Mao",
                        "user": "HelenMao",
                        "type": "user"
                    },
                    "name": "Qi Mao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:37.087Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T17:47:18.000Z",
            "submittedOnDailyAt": "2025-12-18T03:01:26.657Z",
            "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
            "submittedOnDailyBy": {
                "_id": "6388a7e98a5dbe2f3dc61faa",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg",
                "isPro": false,
                "fullname": "Qi Mao",
                "user": "HelenMao",
                "type": "user"
            },
            "summary": "We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning 15 high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.",
            "upvotes": 17,
            "discussionId": "6943902a542d62d58a7bf7a7",
            "projectPage": "https://cuc-mipg.github.io/IC-Effect/",
            "githubRepo": "https://github.com/CUC-MIPG/IC-Effect",
            "githubRepoAddedBy": "user",
            "ai_summary": "IC-Effect, an instruction-guided DiT-based framework, synthesizes complex video VFX effects while preserving spatial and temporal consistency using a two-stage training strategy and spatiotemporal sparse tokenization.",
            "ai_keywords": [
                "DiT",
                "few-shot video VFX editing",
                "flames",
                "particles",
                "cartoon characters",
                "spatial consistency",
                "temporal consistency",
                "DiT models",
                "contextual learning",
                "general editing adaptation",
                "Effect-LoRA",
                "spatiotemporal sparse tokenization",
                "VFX editing dataset"
            ],
            "githubStars": 15,
            "organization": {
                "_id": "67dab498ed21a53369f5de73",
                "name": "CUC-MIPG",
                "fullname": "Multimedia Intelligent Processing Group in Communication University of China",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640d704c8036cc2142299c19/B85B31gd7-0kjK_Rpvv3g.jpeg"
            }
        },
        "translation_title": "IC-Effect: 맥락 학습을 통한 정밀하고 효율적인 비디오 효과 편집",
        "purpose": "비디오 특수 효과 편집을 쉽게 하면서도 공간적 및 시간적 일관성을 철저히 유지하기 위한 프레임워크 개발",
        "method": [
            "DiT 기반의 프레임워크를 사용해, 지침에 따라 몇 가지 샷만으로 복잡한 비디오 VFX 편집을 가능하게 함(We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing) ",
            "광고하는 비디오의 배경을 깔끔하게 보존하고 자연스럽게 효과를 주입하도록 DiT 모델의 맥락적 학습 능력을 활용함(IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models)",
            "일반 편집 적응 후 효과 특정 학습을 위한 두 단계의 교육 전략을 채택하여 강력한 지침 준수와 견고한 효과 모델링을 보장함(A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA)",
            "효율성을 더 높이기 위해 시공간 희소 토큰화를 도입하여 계산량을 크게 줄임(we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation)"
        ],
        "conclusion": "IC-Effect는 고품질, 제어 가능하며 시간적으로 일관된 VFX 편집을 제공, 비디오 제작의 새로운 가능성을 열어줌.",
        "keywords": [
            "Video Generation",
            "Image Generation",
            "Computer Vision"
        ]
    }
]