[
    {
        "paper": {
            "id": "2507.18553",
            "authors": [
                {
                    "_id": "688389bb5d77c97374117c30",
                    "user": {
                        "_id": "6298d8dab58e71e2ac9e2967",
                        "avatarUrl": "/avatars/99fec4ba78ab61c5952f51e6ebf03ffa.svg",
                        "isPro": false,
                        "fullname": "Jiale Chen",
                        "user": "softmax",
                        "type": "user"
                    },
                    "name": "Jiale Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-28T08:45:38.736Z",
                    "hidden": false
                },
                {
                    "_id": "688389bb5d77c97374117c31",
                    "name": "Torsten Hoefler",
                    "hidden": false
                },
                {
                    "_id": "688389bb5d77c97374117c32",
                    "name": "Dan Alistarh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T16:22:18.000Z",
            "submittedOnDailyAt": "2025-07-28T08:41:14.809Z",
            "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm",
            "submittedOnDailyBy": {
                "_id": "6298d8dab58e71e2ac9e2967",
                "avatarUrl": "/avatars/99fec4ba78ab61c5952f51e6ebf03ffa.svg",
                "isPro": false,
                "fullname": "Jiale Chen",
                "user": "softmax",
                "type": "user"
            },
            "summary": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale. Yet, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: (i) the GPTQ error propagation\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\nupper bound of Babai's algorithm under the no-clipping condition. Taken\ntogether, these results place GPTQ on firm theoretical footing and open the\ndoor to importing decades of progress in lattice algorithms towards the design\nof future quantization algorithms for billion-parameter models.",
            "upvotes": 16,
            "discussionId": "688389bb5d77c97374117c33"
        },
        "translation_title": "LLM 양자화의 기하학: Babai의 최근접 평면 알고리즘으로서의 GPTQ",
        "purpose": "대규모 언어 모델(LLM)의 양자화를 통해 더 저렴한 가속기에 배포하기 위한 효과적인 방법을 제안하고자 함.",
        "method": [
            "GPTQ의 작동 방식을 마지막 차원에서 첫 번째 차원으로 실행했을 때 Babai의 최근접 평면 알고리즘과 수학적으로 동일함을 보임(we show that, when executed back-to-front (from the last to first dimension), GPTQ is mathematically identical to Babai's nearest plane algorithm).",
            "이 등가성을 바탕으로 GPTQ의 오차 전파 단계에 대한 기하학적 해석을 제공함(this equivalence is based on a sophisticated mathematical argument, and has two analytical consequences: (i) the GPTQ error propagation step gains an intuitive geometric interpretation).",
            "Babai 알고리즘의 오차 상한을 GPTQ가 상속받는다는 점을 설명함(ii) GPTQ inherits the error upper bound of Babai's algorithm under the no-clipping condition."
        ],
        "conclusion": "GPTQ는 이론적으로 확고한 기초를 가지며, 향후 양자화 알고리즘 설계에 대한 기하학적 해석을 제공하여 발전의 문이 열림.",
        "keywords": [
            "Large Language Models",
            "Quantization",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2507.16075",
            "authors": [
                {
                    "_id": "688501d57d7a19a208cdf03a",
                    "name": "Rujun Han",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf03b",
                    "name": "Yanfei Chen",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf03c",
                    "name": "Zoey CuiZhu",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf03d",
                    "name": "Lesly Miculicich",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf03e",
                    "name": "Guan Sun",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf03f",
                    "name": "Yuanjun Bi",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf040",
                    "name": "Weiming Wen",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf041",
                    "name": "Hui Wan",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf042",
                    "name": "Chunfeng Wen",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf043",
                    "name": "Solène Maître",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf044",
                    "name": "George Lee",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf045",
                    "name": "Vishy Tirumalashetty",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf046",
                    "name": "Emily Xue",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf047",
                    "name": "Zizhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf048",
                    "name": "Salem Haykal",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf049",
                    "name": "Burak Gokturk",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf04a",
                    "name": "Tomas Pfister",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf04b",
                    "name": "Chen-Yu Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-21T21:23:21.000Z",
            "submittedOnDailyAt": "2025-07-28T05:49:28.861Z",
            "title": "Deep Researcher with Test-Time Diffusion",
            "submittedOnDailyBy": {
                "_id": "62f32eab52ad88c930bb3f3b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
                "isPro": true,
                "fullname": "Asankhaya Sharma",
                "user": "codelion",
                "type": "user"
            },
            "summary": "Deep research agents, powered by Large Language Models (LLMs), are rapidly\nadvancing; yet, their performance often plateaus when generating complex,\nlong-form research reports using generic test-time scaling algorithms. Drawing\ninspiration from the iterative nature of human research, which involves cycles\nof searching, reasoning, and revision, we propose the Test-Time Diffusion Deep\nResearcher (TTD-DR). This novel framework conceptualizes research report\ngeneration as a diffusion process. TTD-DR initiates this process with a\npreliminary draft, an updatable skeleton that serves as an evolving foundation\nto guide the research direction. The draft is then iteratively refined through\na \"denoising\" process, which is dynamically informed by a retrieval mechanism\nthat incorporates external information at each step. The core process is\nfurther enhanced by a self-evolutionary algorithm applied to each component of\nthe agentic workflow, ensuring the generation of high-quality context for the\ndiffusion process. This draft-centric design makes the report writing process\nmore timely and coherent while reducing information loss during the iterative\nsearch process. We demonstrate that our TTD-DR achieves state-of-the-art\nresults on a wide array of benchmarks that require intensive search and\nmulti-hop reasoning, significantly outperforming existing deep research agents.",
            "upvotes": 13,
            "discussionId": "688501d67d7a19a208cdf04c",
            "githubRepo": "https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research",
            "ai_summary": "The Test-Time Diffusion Deep Researcher (TTD-DR) framework uses a diffusion process with iterative refinement and external information retrieval to generate high-quality research reports, outperforming existing methods.",
            "ai_keywords": [
                "Test-Time Diffusion Deep Researcher",
                "TTD-DR",
                "diffusion process",
                "preliminary draft",
                "denoising process",
                "retrieval mechanism",
                "self-evolutionary algorithm",
                "agentic workflow",
                "iterative search",
                "multi-hop reasoning"
            ],
            "githubStars": 2663
        },
        "translation_title": "테스트 시간 확산 기법을 활용한 심층 연구자",
        "purpose": "복잡한 연구 보고서를 더욱 효과적으로 생성하기 위한 새로운 프레임워크 개발",
        "method": [
            "사전 초안을 통해 확산 과정을 시작하고 이를 기반으로 연구 방향을 안내함(We propose the Test-Time Diffusion Deep Researcher (TTD-DR). This novel framework conceptualizes research report generation as a diffusion process.)",
            "초안을 'denoising' 과정을 통해 반복적으로 세련되게 만들고, 외부 정보를 통합한 검색 메커니즘을 활용하여 각 단계에서 동적으로 반영함(The draft is then iteratively refined through a 'denoising' process, which is dynamically informed by a retrieval mechanism.)",
            "자기 진화 알고리즘을 모든 구성 요소에 적용하여 고품질 맥락을 생성하도록 보장함(The core process is further enhanced by a self-evolutionary algorithm applied to each component of the agentic workflow.)"
        ],
        "conclusion": "TTD-DR은 광범위한 벤치마크에서 뛰어난 성능을 보여주며, 기존의 심층 연구자들보다 현저히 높은 성과를 기록함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2507.19478",
            "authors": [
                {
                    "_id": "68877a6bc8db48c925156eec",
                    "name": "Xuehui Wang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156eed",
                    "name": "Zhenyu Wu",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156eee",
                    "name": "JingJing Xie",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156eef",
                    "name": "Zichen Ding",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef0",
                    "name": "Bowen Yang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef1",
                    "name": "Zehao Li",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef2",
                    "name": "Zhaoyang Liu",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef3",
                    "name": "Qingyun Li",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef4",
                    "name": "Xuan Dong",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef5",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef6",
                    "name": "Weiyun Wang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef7",
                    "name": "Xiangyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef8",
                    "name": "Jixuan Chen",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef9",
                    "name": "Haodong Duan",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156efa",
                    "name": "Tianbao Xie",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156efb",
                    "name": "Chenyu Yang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156efc",
                    "name": "Shiqian Su",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156efd",
                    "name": "Yue Yu",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156efe",
                    "name": "Yuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156eff",
                    "name": "Yiqian Liu",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f00",
                    "name": "Xiao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f01",
                    "name": "Yanting Zhang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f02",
                    "name": "Xiangyu Yue",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f03",
                    "name": "Weijie Su",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f04",
                    "name": "Xizhou Zhu",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f05",
                    "name": "Wei Shen",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f06",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f07",
                    "name": "Wenhai Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-25T17:59:26.000Z",
            "submittedOnDailyAt": "2025-07-28T12:29:10.972Z",
            "title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI\n  Agents",
            "submittedOnDailyBy": {
                "_id": "649cf4ecdd87dd9ef76fe020",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/M7RpD_AcNewA2xADhhyCB.jpeg",
                "isPro": false,
                "fullname": "Xuehui Wang",
                "user": "huiserwang",
                "type": "user"
            },
            "summary": "We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI\nautomation agents across Windows, macOS, Linux, iOS, Android, and Web\nplatforms. It comprises four levels: GUI Content Understanding, Element\nGrounding, Task Automation, and Task Collaboration, covering essential skills\nfor GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)\nmetric to assess GUI agent execution efficiency in online automation scenarios.\nThrough MMBench-GUI, we identify accurate visual grounding as a critical\ndeterminant of overall task success, emphasizing the substantial benefits of\nmodular frameworks that integrate specialized grounding modules. Furthermore,\nto achieve reliable GUI automation, an agent requires strong task planning and\ncross-platform generalization abilities, with long-context memory, a broad\naction space, and long-term reasoning playing a critical role. More important,\ntask efficiency remains a critically underexplored dimension, and all models\nsuffer from substantial inefficiencies, with excessive redundant steps even\nwhen tasks are ultimately completed. The integration of precise localization,\neffective planning, and early stopping strategies is indispensable to enable\ntruly efficient and scalable GUI automation. Our benchmark code, evaluation\ndata, and running environment will be publicly available at\nhttps://github.com/open-compass/MMBench-GUI.",
            "upvotes": 12,
            "discussionId": "68877a6bc8db48c925156f08",
            "ai_summary": "A hierarchical benchmark evaluates GUI automation agents across multiple platforms, emphasizing key skills such as visual grounding, task planning, and efficiency.",
            "ai_keywords": [
                "GUI Content Understanding",
                "Element Grounding",
                "Task Automation",
                "Task Collaboration",
                "Efficiency-Quality Area",
                "EQA",
                "modular frameworks",
                "specialized grounding modules",
                "long-context memory",
                "broad action space",
                "long-term reasoning",
                "precise localization",
                "effective planning",
                "early stopping strategies"
            ]
        },
        "translation_title": "MMBench-GUI: GUI 에이전트를 위한 계층적 멀티 플랫폼 평가 프레임워크",
        "purpose": "GUI 자동화 에이전트를 다양한 플랫폼에서 평가하기 위한 프레임워크 개발",
        "method": [
            "Windows, macOS, Linux, iOS, Android, Web 등 여러 플랫폼에서 GUI 에이전트를 평가하기 위해 네 가지 수준의 계층적 벤치마크를 구축함(We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI automation agents across various platforms.)",
            "GUI 에이전트의 실행 효율성을 평가하기 위해 새로운 Efficiency-Quality Area (EQA) 메트릭을 제안함(In addition, we propose a novel Efficiency-Quality Area (EQA) metric to assess GUI agent execution efficiency.)",
            "정확한 시각적 그라운딩이 전체 작업 성공에 중요한 요인임을 확인하고, 전문화된 그라운딩 모듈을 통합하는 모듈식 프레임워크의 이점을 강조함(Through MMBench-GUI, we identify accurate visual grounding as a critical determinant of overall task success.)",
            "신뢰할 수 있는 GUI 자동화를 위해 에이전트는 강력한 작업 계획 및 크로스 플랫폼 일반화 능력이 필요하다고 주장함(Furthermore, to achieve reliable GUI automation, an agent requires strong task planning and cross-platform generalization abilities.)"
        ],
        "conclusion": "정확한 로컬라이제이션, 효과적인 계획 및 조기 정지 전략의 통합이 효율적이고 확장 가능한 GUI 자동화를 가능하게 하며, 모든 모델이 상당한 비효율성을 겪고 있음을 강조함.",
        "keywords": [
            "Computer Vision",
            "Task Automation",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2507.18392",
            "authors": [
                {
                    "_id": "68878f837e66e00ef8c1cbd3",
                    "name": "Asaf Yehudai",
                    "hidden": false
                },
                {
                    "_id": "68878f837e66e00ef8c1cbd4",
                    "name": "Lilach Eden",
                    "hidden": false
                },
                {
                    "_id": "68878f837e66e00ef8c1cbd5",
                    "name": "Yotam Perlitz",
                    "hidden": false
                },
                {
                    "_id": "68878f837e66e00ef8c1cbd6",
                    "name": "Roy Bar-Haim",
                    "hidden": false
                },
                {
                    "_id": "68878f837e66e00ef8c1cbd7",
                    "name": "Michal Shmueli-Scheuer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T13:15:21.000Z",
            "submittedOnDailyAt": "2025-07-28T13:26:30.022Z",
            "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy",
            "submittedOnDailyBy": {
                "_id": "638324f862badff43269e588",
                "avatarUrl": "/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg",
                "isPro": false,
                "fullname": "Asaf Yehudai",
                "user": "Asaf-Yehudai",
                "type": "user"
            },
            "summary": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study.",
            "upvotes": 3,
            "discussionId": "68878f837e66e00ef8c1cbd8",
            "githubRepo": "https://github.com/IBM/CLEAR",
            "ai_summary": "CLEAR is an interactive open-source package that provides detailed error analysis by generating per-instance feedback and system-level error issues, aiding in understanding and improving the performance of Large Language Models.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "CLEAR",
                "error analysis",
                "per-instance feedback",
                "system-level error issues",
                "interactive dashboard",
                "aggregate visualizations",
                "interactive filters",
                "RAG benchmarks",
                "Math benchmarks"
            ],
            "githubStars": 5
        },
        "translation_title": "CLEAR: LLM을 활용한 오류 분석을 쉽게 해주는 시스템",
        "purpose": "Large Language Models의 평가에서 오류 분석을 더 정확하고 유용하게 수행하기 위한 시스템 개발",
        "method": [
            "CLEAR라는 인터랙티브한 오픈소스 패키지를 소개함(we introduce CLEAR, an interactive, open-source package for LLM-based error analysis.)",
            "인스턴스별 텍스트 피드백을 생성하고 시스템 수준의 오류 문제를 식별하여 각 문제의 발생 빈도를 정량화함(CLEAR first generates per-instance textual feedback, then it creates a set of system-level error issues, and quantifies the prevalence of each identified issue.)",
            "인터랙티브 대시보드를 제공하여 종합적인 오류 분석을 할 수 있도록 함(Our package also provides users with an interactive dashboard that allows for a comprehensive error analysis through aggregate visualizations.)"
        ],
        "conclusion": "CLEAR는 LLM 평가에서 통찰력을 제공하며, 사용자 사례 연구를 통해 그 유용성을 입증함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Error Analysis"
        ]
    }
]