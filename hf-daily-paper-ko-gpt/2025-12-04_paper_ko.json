[
    {
        "paper": {
            "id": "2511.21631",
            "authors": [
                {
                    "_id": "692ffb1a26742347f61daf38",
                    "user": {
                        "_id": "63451cf0a05b51f7ded25505",
                        "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg",
                        "isPro": false,
                        "fullname": "shuai bai",
                        "user": "ShuaiBai623",
                        "type": "user"
                    },
                    "name": "Shuai Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-04T10:34:29.118Z",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf39",
                    "name": "Yuxuan Cai",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf3a",
                    "name": "Ruizhe Chen",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf3b",
                    "name": "Keqin Chen",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf3c",
                    "user": {
                        "_id": "63f30b870a16587ea970edfe",
                        "avatarUrl": "/avatars/b58ab2d8a85a6d83462c297de2714ce4.svg",
                        "isPro": false,
                        "fullname": "Xiong-Hui Chen",
                        "user": "xionghuichen",
                        "type": "user"
                    },
                    "name": "Xionghui Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-04T10:35:42.689Z",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf3d",
                    "user": {
                        "_id": "65b2529285b6c21448a10d65",
                        "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg",
                        "isPro": false,
                        "fullname": "Zesen Cheng",
                        "user": "ClownRat",
                        "type": "user"
                    },
                    "name": "Zesen Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-04T10:35:51.365Z",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf3e",
                    "name": "Lianghao Deng",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf3f",
                    "name": "Wei Ding",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf40",
                    "name": "Chang Gao",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf41",
                    "name": "Chunjiang Ge",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf42",
                    "name": "Wenbin Ge",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf43",
                    "name": "Zhifang Guo",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf44",
                    "user": {
                        "_id": "656f1b21b075b63c90ba02ee",
                        "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg",
                        "isPro": false,
                        "fullname": "Huang Qidong",
                        "user": "shikiw",
                        "type": "user"
                    },
                    "name": "Qidong Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-04T08:48:49.065Z",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf45",
                    "name": "Jie Huang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf46",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf47",
                    "name": "Binyuan Hui",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf48",
                    "name": "Shutong Jiang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf49",
                    "name": "Zhaohai Li",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf4a",
                    "name": "Mingsheng Li",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf4b",
                    "name": "Mei Li",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf4c",
                    "user": {
                        "_id": "6346be8f7fb9f11870c63998",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6346be8f7fb9f11870c63998/tFWawSkXL6bv1zgvzFWQd.png",
                        "isPro": false,
                        "fullname": "Kaixin Li",
                        "user": "likaixin",
                        "type": "user"
                    },
                    "name": "Kaixin Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-04T08:48:53.648Z",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf4d",
                    "user": {
                        "_id": "67a31313cf9d856beb7f9afb",
                        "avatarUrl": "/avatars/69395b134716f750545eab35a164e51f.svg",
                        "isPro": false,
                        "fullname": "Zicheng Lin",
                        "user": "etonlin",
                        "type": "user"
                    },
                    "name": "Zicheng Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-04T10:36:50.803Z",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf4e",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf4f",
                    "name": "Xuejing Liu",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf50",
                    "name": "Jiawei Liu",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf51",
                    "name": "Chenglong Liu",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf52",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf53",
                    "name": "Dayiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf54",
                    "user": {
                        "_id": "64e72776e9fc9d0475ef5188",
                        "avatarUrl": "/avatars/d32b9d4e1da5486c3d5f9b04fa29d167.svg",
                        "isPro": false,
                        "fullname": "Shixuan Liu",
                        "user": "liusx",
                        "type": "user"
                    },
                    "name": "Shixuan Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-04T08:48:58.470Z",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf55",
                    "name": "Dunjie Lu",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf56",
                    "name": "Ruilin Luo",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf57",
                    "name": "Chenxu Lv",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf58",
                    "name": "Rui Men",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf59",
                    "name": "Lingchen Meng",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf5a",
                    "name": "Xuancheng Ren",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf5b",
                    "name": "Xingzhang Ren",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf5c",
                    "name": "Sibo Song",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf5d",
                    "name": "Yuchong Sun",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf5e",
                    "name": "Jun Tang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf5f",
                    "name": "Jianhong Tu",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf60",
                    "name": "Jianqiang Wan",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf61",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf62",
                    "name": "Pengfei Wang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf63",
                    "name": "Qiuyue Wang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf64",
                    "name": "Yuxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf65",
                    "name": "Tianbao Xie",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf66",
                    "name": "Yiheng Xu",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf67",
                    "user": {
                        "_id": "645b10e80c73ea27d13f7aca",
                        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
                        "isPro": false,
                        "fullname": "xuhaiyang",
                        "user": "xhyandwyy",
                        "type": "user"
                    },
                    "name": "Haiyang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-04T08:48:51.583Z",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf68",
                    "name": "Jin Xu",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf69",
                    "name": "Zhibo Yang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf6a",
                    "name": "Mingkun Yang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf6b",
                    "name": "Jianxin Yang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf6c",
                    "name": "An Yang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf6d",
                    "name": "Bowen Yu",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf6e",
                    "name": "Fei Zhang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf6f",
                    "name": "Hang Zhang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf70",
                    "name": "Xi Zhang",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf71",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf72",
                    "name": "Humen Zhong",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf73",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf74",
                    "name": "Fan Zhou",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf75",
                    "name": "Jing Zhou",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf76",
                    "user": {
                        "_id": "627d2723401f42c57b6b7c0c",
                        "avatarUrl": "/avatars/6ff754e56aaee63d8572881a6a966171.svg",
                        "isPro": false,
                        "fullname": "Yuanzhi Zhu",
                        "user": "Yuanzhi",
                        "type": "user"
                    },
                    "name": "Yuanzhi Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-04T10:36:59.879Z",
                    "hidden": false
                },
                {
                    "_id": "692ffb1a26742347f61daf77",
                    "name": "Ke Zhu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"
            ],
            "publishedAt": "2025-11-26T17:59:08.000Z",
            "submittedOnDailyAt": "2025-12-04T01:02:46.772Z",
            "title": "Qwen3-VL Technical Report",
            "submittedOnDailyBy": {
                "_id": "63451cf0a05b51f7ded25505",
                "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg",
                "isPro": false,
                "fullname": "shuai bai",
                "user": "ShuaiBai623",
                "type": "user"
            },
            "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.",
            "upvotes": 76,
            "discussionId": "692ffb1b26742347f61daf78",
            "ai_summary": "Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.",
            "ai_keywords": [
                "vision-language model",
                "interleaved contexts",
                "multimodal benchmarks",
                "dense variants",
                "mixture-of-experts",
                "pure-text understanding",
                "long-context comprehension",
                "multimodal reasoning",
                "MMMU",
                "visual-math benchmarks",
                "interleaved-MRoPE",
                "DeepStack",
                "text-based time alignment",
                "T-RoPE",
                "explicit textual timestamp alignment",
                "vision-language alignment",
                "image-grounded reasoning",
                "agentic decision-making",
                "multimodal code intelligence"
            ],
            "organization": {
                "_id": "64c8b5837fe12ecd0a7e92eb",
                "name": "Qwen",
                "fullname": "Qwen",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
            }
        },
        "translation_title": "Qwen3-VL 기술 보고서",
        "purpose": "Qwen 시리즈에서 가장 강력한 vision-language 모델 개발과 다양한 multimodal 벤치마크에서의 성능 향상",
        "method": [
            "256K 토큰의 interleaved contexts를 지원하여 텍스트, 이미지, 비디오 통합을 가능하게 함(It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video.)",
            "Dense 및 mixture-of-experts 변형을 포함하여 다양한 지연-품질 상충을 수용함(The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs.)",
            "강화된 interleaved-MRoPE, DeepStack 통합 및 텍스트 기반 시간 정렬 같은 세 가지 주요 아키텍처 업그레이드를 도입함(Architecturally, we introduce three key upgrades: an enhanced interleaved-MRoPE, DeepStack integration, and text-based time alignment for video.)"
        ],
        "conclusion": "Qwen3-VL은 이미지 기반 추론, 주체적 의사결정 및 실제 작업 흐름에서의 multimodal 코드 인텔리전스를 위한 기본 엔진으로 발전할 것으로 기대됨.",
        "keywords": [
            "Multimodal Learning",
            "Vision-Language Models",
            "Video Understanding"
        ]
    },
    {
        "paper": {
            "id": "2512.02834",
            "authors": [
                {
                    "_id": "69313ee82d1e5b0a7d84da77",
                    "name": "Siyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "69313ee82d1e5b0a7d84da78",
                    "user": {
                        "_id": "667be442c8c087a184094892",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/667be442c8c087a184094892/rVn2sKB9TuABE-sIzxbAx.jpeg",
                        "isPro": false,
                        "fullname": "Yang Zhang",
                        "user": "breezeyoung",
                        "type": "user"
                    },
                    "name": "Yang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-04T08:43:32.201Z",
                    "hidden": false
                },
                {
                    "_id": "69313ee82d1e5b0a7d84da79",
                    "user": {
                        "_id": "6672937ceac0fb1b9e516595",
                        "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
                        "isPro": false,
                        "fullname": "haoran he",
                        "user": "haoranhe",
                        "type": "user"
                    },
                    "name": "Haoran He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-04T10:43:20.162Z",
                    "hidden": false
                },
                {
                    "_id": "69313ee82d1e5b0a7d84da7a",
                    "name": "Ling Pan",
                    "hidden": false
                },
                {
                    "_id": "69313ee82d1e5b0a7d84da7b",
                    "name": "Xiu Li",
                    "hidden": false
                },
                {
                    "_id": "69313ee82d1e5b0a7d84da7c",
                    "name": "Chenjia Bai",
                    "hidden": false
                },
                {
                    "_id": "69313ee82d1e5b0a7d84da7d",
                    "name": "Xuelong Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T14:42:54.000Z",
            "submittedOnDailyAt": "2025-12-04T06:26:21.029Z",
            "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
            "submittedOnDailyBy": {
                "_id": "667be442c8c087a184094892",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/667be442c8c087a184094892/rVn2sKB9TuABE-sIzxbAx.jpeg",
                "isPro": false,
                "fullname": "Yang Zhang",
                "user": "breezeyoung",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose TACO, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.",
            "upvotes": 28,
            "discussionId": "69313ee82d1e5b0a7d84da7e",
            "projectPage": "https://vla-anti-exploration.github.io/",
            "githubRepo": "https://github.com/breez3young/TACO",
            "ai_summary": "TACO, a test-time-scaling framework with a pseudo-count estimator, enhances the inference stability and success rates of Vision-Language-Action models in downstream tasks by preventing distribution shifts.",
            "ai_keywords": [
                "flow-matching",
                "diffusion objectives",
                "Vision-Language-Action models",
                "pre-training",
                "finetuning",
                "distribution shift",
                "test-time-scaling",
                "pseudo-count estimator",
                "action chunks",
                "anti-exploration principle",
                "offline reinforcement learning",
                "gradient-free",
                "denoising process",
                "RoboTwin2.0",
                "Robotwin",
                "LIBERO",
                "SimplerEnv",
                "dual-arm platform"
            ],
            "githubStars": 5
        },
        "translation_title": "Vision-Language-Action 모델을 anti-exploration으로 조절하기: 테스트 시 스케일링 접근법",
        "purpose": "Downstream 작업에서 성공적인 행동 모드를 보장하기 위해 VLA 모델의 추론 안정성을 향상시키기 위한 방법 제안",
        "method": [
            "TACO라는 테스트 시 스케일링(TTS) 프레임워크를 제안하여 가벼운 의사 수치 추정기를 사용함(we propose TACO, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks.)",
            "VLA 모델이 모든 샘플링된 행동 조각에서 최대 의사 수치를 실행할 수 있도록 함(the VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks.)",
            "추론 중에만 제약이 적용되어 VLA가 일반화 능력을 보존하도록 함(the constraint is applied only during inference.)"
        ],
        "conclusion": "TACO를 통한 방법은 추론 안정성 및 성공률을 크게 향상시킴.",
        "keywords": [
            "Vision-Language Models",
            "Robotics",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2512.03442",
            "authors": [
                {
                    "_id": "693104f92d1e5b0a7d84d9f2",
                    "user": {
                        "_id": "651c4e0bc0247c08a46ab2a6",
                        "avatarUrl": "/avatars/3396a34ffb400f576371afc8a5064783.svg",
                        "isPro": false,
                        "fullname": "xxr",
                        "user": "xrxing",
                        "type": "user"
                    },
                    "name": "Xingrun Xing",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-04T10:39:37.521Z",
                    "hidden": false
                },
                {
                    "_id": "693104f92d1e5b0a7d84d9f3",
                    "user": {
                        "_id": "65867b038dd42194878e08dc",
                        "avatarUrl": "/avatars/8965ac2f8e3ccb88759f6e4d84a30c2f.svg",
                        "isPro": false,
                        "fullname": "Zhiyuan Fan",
                        "user": "Zhiyuan-Fan",
                        "type": "user"
                    },
                    "name": "Zhiyuan Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-04T13:49:38.606Z",
                    "hidden": false
                },
                {
                    "_id": "693104f92d1e5b0a7d84d9f4",
                    "name": "Jie Lou",
                    "hidden": false
                },
                {
                    "_id": "693104f92d1e5b0a7d84d9f5",
                    "name": "Guoqi Li",
                    "hidden": false
                },
                {
                    "_id": "693104f92d1e5b0a7d84d9f6",
                    "name": "Jiajun Zhang",
                    "hidden": false
                },
                {
                    "_id": "693104f92d1e5b0a7d84d9f7",
                    "user": {
                        "_id": "64546be9548f22be59842cfe",
                        "avatarUrl": "/avatars/5582cf189b755d888b35f68a6e3e9bb8.svg",
                        "isPro": false,
                        "fullname": "zdb",
                        "user": "debingzhang",
                        "type": "user"
                    },
                    "name": "Debing Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-04T10:39:57.928Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-03T04:51:32.000Z",
            "submittedOnDailyAt": "2025-12-04T01:22:22.945Z",
            "title": "PretrainZero: Reinforcement Active Pretraining",
            "submittedOnDailyBy": {
                "_id": "651c4e0bc0247c08a46ab2a6",
                "avatarUrl": "/avatars/3396a34ffb400f576371afc8a5064783.svg",
                "isPro": false,
                "fullname": "xxr",
                "user": "xrxing",
                "type": "user"
            },
            "summary": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
            "upvotes": 24,
            "discussionId": "693104fa2d1e5b0a7d84d9f8",
            "ai_summary": "PretrainZero is a reinforcement active learning framework that enhances general reasoning capabilities by pretraining large models on a corpus without verifiable labels, improving performance on benchmarks compared to domain-specific training.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "active learning",
                "active pretraining",
                "self-supervised learning",
                "pretrained reward models",
                "Qwen3-4B-Base",
                "MMLU-Pro",
                "SuperGPQA",
                "math average benchmarks",
                "RLVR tasks"
            ]
        },
        "translation_title": "PretrainZero: 강화 학습을 통한 능동적 사전훈련",
        "purpose": "일반 경험을 통해 능동적으로 학습하고 인공지능 지능을 향상시키기 위한 학습 프레임워크 개발",
        "method": [
            "PretrainZero는 사전훈련 코퍼스를 기반으로 강화 학습을 일반화하여 도메인 특정 후훈련을 넘어서는 능동적 사전훈련 프레임워크를 제안함(we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining.)",
            "인간의 능동적 학습 능력에서 영감을 받아, 분포에서 합리적이고 유익한 내용을 능동적으로 식별하는 통합 추론 정책을 학습함(PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL.)",
            "검증 가능한 레이블이나 감독된 미세 조정 없이, 3B에서 30B의 기본 모델을 일반 위키피디아 코퍼스를 사용하여 강화 학습으로 직접 사전훈련함(we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning.)"
        ],
        "conclusion": "PretrainZero는 일반 추론 능력을 크게 향상시키고, 후훈련에서 사전훈련된 모델이 다운스트림 RLVR 작업의 추론 기초 모델로 기능할 수 있도록 함.",
        "keywords": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.03405",
            "authors": [
                {
                    "_id": "6930f7432d1e5b0a7d84d996",
                    "name": "Jiangtao Wu",
                    "hidden": false
                },
                {
                    "_id": "6930f7432d1e5b0a7d84d997",
                    "user": {
                        "_id": "67f9d060395fb1a0d7e4ae21",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GjpOfOuazN7IxcXBpVqRm.png",
                        "isPro": false,
                        "fullname": "Shihao Li",
                        "user": "Leexeo",
                        "type": "user"
                    },
                    "name": "Shihao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-04T08:43:59.201Z",
                    "hidden": false
                },
                {
                    "_id": "6930f7432d1e5b0a7d84d998",
                    "name": "Zhaozhou Bian",
                    "hidden": false
                },
                {
                    "_id": "6930f7432d1e5b0a7d84d999",
                    "user": {
                        "_id": "64241749a05235e2f8d34cb0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64241749a05235e2f8d34cb0/o6CY4xS22W8_DIqesFykM.jpeg",
                        "isPro": false,
                        "fullname": "Yuanxing Zhang",
                        "user": "LongoXC",
                        "type": "user"
                    },
                    "name": "Yuanxing Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-04T10:41:05.482Z",
                    "hidden": false
                },
                {
                    "_id": "6930f7432d1e5b0a7d84d99a",
                    "name": "Jialu Chen",
                    "hidden": false
                },
                {
                    "_id": "6930f7432d1e5b0a7d84d99b",
                    "user": {
                        "_id": "69059dc64c9138632afde265",
                        "avatarUrl": "/avatars/3ab6775359128d43bd28de006b94bd51.svg",
                        "isPro": false,
                        "fullname": "runzhe wen",
                        "user": "wrz123",
                        "type": "user"
                    },
                    "name": "Runzhe Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-04T10:41:12.901Z",
                    "hidden": false
                },
                {
                    "_id": "6930f7432d1e5b0a7d84d99c",
                    "name": "An Ping",
                    "hidden": false
                },
                {
                    "_id": "6930f7432d1e5b0a7d84d99d",
                    "user": {
                        "_id": "68f84bec0a277a9bf73f1e0d",
                        "avatarUrl": "/avatars/51cb0b5ba6393cbcf7ed79e66f160a7f.svg",
                        "isPro": false,
                        "fullname": "yiwen he",
                        "user": "heyween",
                        "type": "user"
                    },
                    "name": "Yiwen He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-04T10:41:19.015Z",
                    "hidden": false
                },
                {
                    "_id": "6930f7432d1e5b0a7d84d99e",
                    "user": {
                        "_id": "6771194d87c60cdabfa5bc3f",
                        "avatarUrl": "/avatars/5c3dc84196029661e3decee3641b4df6.svg",
                        "isPro": false,
                        "fullname": "Jiakai Wang",
                        "user": "jiakaiW",
                        "type": "user"
                    },
                    "name": "Jiakai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-04T10:41:28.040Z",
                    "hidden": false
                },
                {
                    "_id": "6930f7432d1e5b0a7d84d99f",
                    "name": "Jiaheng Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/gWX87F09tk4zl8cMMFIVi.png"
            ],
            "publishedAt": "2025-12-03T03:23:24.000Z",
            "submittedOnDailyAt": "2025-12-04T00:22:20.618Z",
            "title": "ViDiC: Video Difference Captioning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
            "upvotes": 23,
            "discussionId": "6930f7432d1e5b0a7d84d9a0",
            "projectPage": "https://vidic-1k.github.io/",
            "ai_summary": "The ViDiC task and ViDiC-1K dataset evaluate Multimodal Large Language Models' ability to describe differences between video pairs, addressing limitations in capturing motion continuity and event evolution.",
            "ai_keywords": [
                "Image Difference Captioning",
                "Video Difference Captioning",
                "ViDiC",
                "ViDiC-1K",
                "Multimodal Large Language Models",
                "checklist items",
                "dual-checklist framework",
                "LLM-as-a-Judge protocol",
                "video understanding",
                "edit awareness",
                "comparative reasoning"
            ]
        },
        "translation_title": "ViDiC: 비디오 차이 설명하기",
        "purpose": "비디오 쌍 간의 유사성과 차이를 세밀하게 설명하는 능력을 평가하기 위해 새로운 기준을 마련하고자 함",
        "method": [
            "ViDiC라는 새로운 과제와 ViDiC-1K 데이터세트를 소개하여 MLLM의 설명 능력을 평가함(ViDiC introduces the ViDiC task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs.)",
            "ViDiC-1K 데이터세트는 1,000개의 비디오 쌍과 4,000개가 넘는 비교 체크리스트 항목으로 구성됨(ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques.)",
            "신뢰할 수 있는 평가를 보장하기 위해 유사성과 차이를 별도로 측정하는 이중 체크리스트 프레임워크를 제안함(To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately.)",
            "19개의 대표적인 멀티모달 모델을 실험하여 비교 설명 능력의 성능 격차를 확인함(Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities.)"
        ],
        "conclusion": "ViDiC-1K는 비디오 이해, 편집 인식 및 비교 추론을 발전시키기 위한 도전적인 기준이 될 것으로 기대됨.",
        "keywords": [
            "Multimodal Learning",
            "Video Understanding",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2512.03043",
            "authors": [
                {
                    "_id": "692fd92426742347f61dad8d",
                    "name": "Kaituo Feng",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad8e",
                    "name": "Manyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad8f",
                    "name": "Hongyu Li",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad90",
                    "name": "Kaixuan Fan",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad91",
                    "name": "Shuang Chen",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad92",
                    "name": "Yilei Jiang",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad93",
                    "user": {
                        "_id": "67e60ae6ac37824273d74389",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YvPKZ_0gyJnvNwM1zK3JS.png",
                        "isPro": false,
                        "fullname": "Dian Zheng",
                        "user": "zhengli1013",
                        "type": "user"
                    },
                    "name": "Dian Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-04T08:49:05.327Z",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad94",
                    "name": "Peiwen Sun",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad95",
                    "name": "Yiyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad96",
                    "name": "Haoze Sun",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad97",
                    "name": "Yan Feng",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad98",
                    "name": "Peng Pei",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad99",
                    "name": "Xunliang Cai",
                    "hidden": false
                },
                {
                    "_id": "692fd92426742347f61dad9a",
                    "name": "Xiangyu Yue",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T18:59:52.000Z",
            "submittedOnDailyAt": "2025-12-04T00:30:53.335Z",
            "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
            "submittedOnDailyBy": {
                "_id": "67079840a9bcb7459b8d2a46",
                "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
                "isPro": false,
                "fullname": "Kaituo Feng",
                "user": "KaituoFeng",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.",
            "upvotes": 17,
            "discussionId": "692fd92426742347f61dad9b",
            "projectPage": "https://github.com/tulerfeng/OneThinker",
            "githubRepo": "https://github.com/tulerfeng/OneThinker",
            "ai_summary": "OneThinker, an all-in-one multimodal reasoning model, unifies image and video understanding across various tasks using RL and demonstrates strong performance and knowledge transfer.",
            "ai_keywords": [
                "Reinforcement learning",
                "Multimodal Large Language Models",
                "image and video reasoning",
                "question answering",
                "captioning",
                "spatial and temporal grounding",
                "tracking",
                "segmentation",
                "EMA-GRPO",
                "reward heterogeneity",
                "zero-shot generalization"
            ],
            "githubStars": 29
        },
        "translation_title": "OneThinker: 이미지 및 비디오를 위한 올인원 추론 모델",
        "purpose": "이미지와 비디오 이해를 통합하여 다양한 기본 시각 작업을 수행할 수 있는 모델 개발",
        "method": [
            "다양한 작업을 포함하는 OneThinker-600k 훈련 코퍼스를 구축함(To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks.)",
            "다양한 작업을 효율적으로 처리하기 위해 EMA-GRPO를 사용하여 보상 이질성을 처리함(Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization.)",
            "31개의 벤치마크에서 강력한 성능을 보여줌(Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks.)"
        ],
        "conclusion": "OneThinker는 강력한 성능과 지식 전이를 통해 통합된 멀티모달 추론 모델로 나아가는 단계를 제시함.",
        "keywords": [
            "Image Understanding",
            "Video Understanding",
            "Multimodal Learning"
        ]
    }
]