[
    {
        "paper": {
            "id": "2503.11647",
            "authors": [
                {
                    "_id": "67d785fa473d4edd330edee1",
                    "user": {
                        "_id": "6530bf50f145530101ec03a2",
                        "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
                        "isPro": false,
                        "fullname": "Jianhong Bai",
                        "user": "jianhongbai",
                        "type": "user"
                    },
                    "name": "Jianhong Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:47:22.245Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee2",
                    "user": {
                        "_id": "63401c89f81b9d101361f712",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665146415483-63401c89f81b9d101361f712.png",
                        "isPro": false,
                        "fullname": "Richard",
                        "user": "menghanxia",
                        "type": "user"
                    },
                    "name": "Menghan Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:47:41.792Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee3",
                    "name": "Xiao Fu",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee4",
                    "user": {
                        "_id": "60e272ca6c78a8c122b12127",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
                        "isPro": false,
                        "fullname": "Xintao Wang",
                        "user": "Xintao",
                        "type": "user"
                    },
                    "name": "Xintao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:47:51.145Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee5",
                    "user": {
                        "_id": "6672dd6d239ba86f129c5384",
                        "avatarUrl": "/avatars/6209afb551995b12d5e0d4d95e495694.svg",
                        "isPro": false,
                        "fullname": "Lianrui Mu",
                        "user": "Mu437",
                        "type": "user"
                    },
                    "name": "Lianrui Mu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:47:58.483Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee6",
                    "name": "Jinwen Cao",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee7",
                    "user": {
                        "_id": "6458b8d0990172cd1d703715",
                        "avatarUrl": "/avatars/55f0695e3cb9933c3903fde5a8f740d5.svg",
                        "isPro": false,
                        "fullname": "Zuozhu Liu",
                        "user": "Zuozhu",
                        "type": "user"
                    },
                    "name": "Zuozhu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:48:17.243Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee8",
                    "user": {
                        "_id": "66c46129d67297a9b93e03c5",
                        "avatarUrl": "/avatars/cffd8b07fa3655e240efc8e81f99d97d.svg",
                        "isPro": false,
                        "fullname": "Haoji Hu",
                        "user": "garland1979",
                        "type": "user"
                    },
                    "name": "Haoji Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:48:23.846Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee9",
                    "user": {
                        "_id": "641790e2f1e86908935d82a0",
                        "avatarUrl": "/avatars/ced7a137c6344c74b7ac0d5c84833fc8.svg",
                        "isPro": false,
                        "fullname": "Xiang Bai",
                        "user": "baixianger",
                        "type": "user"
                    },
                    "name": "Xiang Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:48:29.804Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edeea",
                    "user": {
                        "_id": "662f93942510ef5735d7ad00",
                        "avatarUrl": "/avatars/dc9486db75869ce902d0a638eea126bd.svg",
                        "isPro": false,
                        "fullname": "magicwpf",
                        "user": "magicwpf",
                        "type": "user"
                    },
                    "name": "Pengfei Wan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T14:42:32.134Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edeeb",
                    "user": {
                        "_id": "644c8324f02250233d0d67d9",
                        "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "dizhang",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:48:49.559Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/RzQL-WqDDCxBy_j4rJuMl.mp4"
            ],
            "publishedAt": "2025-03-14T17:59:31.000Z",
            "submittedOnDailyAt": "2025-03-17T00:50:10.251Z",
            "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
            "submittedOnDailyBy": {
                "_id": "6530bf50f145530101ec03a2",
                "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
                "isPro": false,
                "fullname": "Jianhong Bai",
                "user": "jianhongbai",
                "type": "user"
            },
            "summary": "Camera control has been actively studied in text or image conditioned video\ngeneration tasks. However, altering camera trajectories of a given video\nremains under-explored, despite its importance in the field of video creation.\nIt is non-trivial due to the extra constraints of maintaining multiple-frame\nappearance and dynamic synchronization. To address this, we present\nReCamMaster, a camera-controlled generative video re-rendering framework that\nreproduces the dynamic scene of an input video at novel camera trajectories.\nThe core innovation lies in harnessing the generative capabilities of\npre-trained text-to-video models through a simple yet powerful video\nconditioning mechanism -- its capability often overlooked in current research.\nTo overcome the scarcity of qualified training data, we construct a\ncomprehensive multi-camera synchronized video dataset using Unreal Engine 5,\nwhich is carefully curated to follow real-world filming characteristics,\ncovering diverse scenes and camera movements. It helps the model generalize to\nin-the-wild videos. Lastly, we further improve the robustness to diverse inputs\nthrough a meticulously designed training strategy. Extensive experiments tell\nthat our method substantially outperforms existing state-of-the-art approaches\nand strong baselines. Our method also finds promising applications in video\nstabilization, super-resolution, and outpainting. Project page:\nhttps://jianhongbai.github.io/ReCamMaster/",
            "upvotes": 71,
            "discussionId": "67d785fb473d4edd330edf77",
            "ai_keywords": [
                "ReCamMaster",
                "text-to-video models",
                "video conditioning mechanism",
                "multi-camera synchronized video dataset",
                "Unreal Engine 5",
                "video stabilization",
                "super-resolution",
                "outpainting"
            ]
        },
        "translation_title": "ReCamMaster: 단일 비디오로부터 카메라 제어 생성 렌더링",
        "purpose": "주어진 비디오의 카메라 궤적을 수정하여 새로운 카메라 시점에서 동적으로 장면을 재생성하는 것",
        "method": [
            "기존 텍스트-비디오 모델의 생성 능력을 활용하기 위한 비디오 조정 메커니즘을 도입함 (the core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through a simple yet powerful video conditioning mechanism.)",
            "Unreal Engine 5를 사용하여 실제 촬영 특성을 따르는 다중 카메라 동기화 비디오 데이터세트를 구축함 (we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics.)",
            "다양한 입력에 대한 강건성을 향상시키기 위해 정밀하게 설계된 훈련 전략을 사용함 (we further improve the robustness to diverse inputs through a meticulously designed training strategy.)"
        ],
        "conclusion": "우리의 방법은 기존 최첨단 접근 방식을 크게 초월하며, 비디오 안정화, 초해상도, 아웃페인팅과 같은 유망한 응용 프로그램을 발견할 수 있음을 보임.",
        "keywords": [
            "Video Generation",
            "Image Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2503.07677",
            "authors": [
                {
                    "_id": "67d2ca0767366130cccad93d",
                    "user": {
                        "_id": "63973ee44e7b4959dc98028f",
                        "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
                        "isPro": false,
                        "fullname": "Kwanyoung",
                        "user": "kwanyoung",
                        "type": "user"
                    },
                    "name": "Kwanyoung Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:58:03.528Z",
                    "hidden": false
                },
                {
                    "_id": "67d2ca0767366130cccad93e",
                    "user": {
                        "_id": "668377232d89090894bea7b4",
                        "avatarUrl": "/avatars/1a74a08d645a352db4a460036b9fb6db.svg",
                        "isPro": false,
                        "fullname": "byeongsu sim",
                        "user": "byeongsus",
                        "type": "user"
                    },
                    "name": "Byeongsu Sim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:47:11.835Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T07:23:19.000Z",
            "submittedOnDailyAt": "2025-03-17T00:44:05.364Z",
            "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity",
            "submittedOnDailyBy": {
                "_id": "63973ee44e7b4959dc98028f",
                "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
                "isPro": false,
                "fullname": "Kwanyoung",
                "user": "kwanyoung",
                "type": "user"
            },
            "summary": "Diffusion models have shown impressive results in generating high-quality\nconditional samples using guidance techniques such as Classifier-Free Guidance\n(CFG). However, existing methods often require additional training or neural\nfunction evaluations (NFEs), making them incompatible with guidance-distilled\nmodels. Also, they rely on heuristic approaches that need identifying target\nlayers. In this work, we propose a novel and efficient method, termed PLADIS,\nwhich boosts pre-trained models (U-Net/Transformer) by leveraging sparse\nattention. Specifically, we extrapolate query-key correlations using softmax\nand its sparse counterpart in the cross-attention layer during inference,\nwithout requiring extra training or NFEs. By leveraging the noise robustness of\nsparse attention, our PLADIS unleashes the latent potential of text-to-image\ndiffusion models, enabling them to excel in areas where they once struggled\nwith newfound effectiveness. It integrates seamlessly with guidance techniques,\nincluding guidance-distilled models. Extensive experiments show notable\nimprovements in text alignment and human preference, offering a highly\nefficient and universally applicable solution.",
            "upvotes": 64,
            "discussionId": "67d2ca0b67366130cccada34",
            "ai_keywords": [
                "diffusion models",
                "Classifier-Free Guidance (CFG)",
                "neural function evaluations (NFEs)",
                "guidance-distilled models",
                "PLADIS",
                "pre-trained models (U-Net/Transformer)",
                "sparse attention",
                "query-key correlations",
                "softmax",
                "cross-attention layer",
                "noise robustness",
                "text-to-image diffusion models",
                "text alignment",
                "human preference"
            ]
        },
        "translation_title": "PLADIS: 희소성을 활용하여 추론 시 확산 모델에서 주의력의 한계를 극복하기",
        "purpose": "기존 확산 모델의 한계를 개선하고 효과를 극대화하기 위한 효율적인 방법 개발",
        "method": [
            "PLADIS라는 새로운 방법론을 제안하여 사전 학습된 모델을 희소 주의력으로 향상시킴(we propose a novel and efficient method, termed PLADIS, which boosts pre-trained models by leveraging sparse attention.)",
            "추론 단계에서 교차 주의력 레이어의 softmax와 희소 softmax를 활용하여 쿼리-키 상관관계를 추출함(we extrapolate query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference.)",
            "PLADIS는 추가 훈련이나 신경 함수 평가 없이 작동하여 텍스트-이미지 확산 모델의 잠재력을 극대화함(without requiring extra training or NFEs, our PLADIS unleashes the latent potential of text-to-image diffusion models.)"
        ],
        "conclusion": "PLADIS는 텍스트 정렬과 인간 선호에서 뚜렷한 개선을 보여주는 효과적이고 보편적으로 적용 가능한 솔루션임.",
        "keywords": [
            "Image Generation",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2503.11646",
            "authors": [
                {
                    "_id": "67d78c194fd0e3fa3a082f8d",
                    "user": {
                        "_id": "634e4120038b5879133552f5",
                        "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
                        "isPro": true,
                        "fullname": "Siyuan",
                        "user": "SiyuanH",
                        "type": "user"
                    },
                    "name": "Siyuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:21.620Z",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f8e",
                    "user": {
                        "_id": "670f827bb94a3734d270f707",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/D6qCPBMJAUgozfG7YTwky.png",
                        "isPro": false,
                        "fullname": "Yue Liao",
                        "user": "morninghaze",
                        "type": "user"
                    },
                    "name": "Yue Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:49:27.927Z",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f8f",
                    "user": {
                        "_id": "620326e962b2b0e46e79971b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620326e962b2b0e46e79971b/1FVPRpsWng5q3An4qbuYQ.jpeg",
                        "isPro": false,
                        "fullname": "Siyuan Feng",
                        "user": "Eralien",
                        "type": "user"
                    },
                    "name": "Siyuan Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:19.837Z",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f90",
                    "user": {
                        "_id": "666c463389e21df7d4a34d03",
                        "avatarUrl": "/avatars/8f73e78d740ace263961de1a2896fc09.svg",
                        "isPro": false,
                        "fullname": "姜姝",
                        "user": "jiang12345",
                        "type": "user"
                    },
                    "name": "Shu Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T14:42:21.098Z",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f91",
                    "name": "Si Liu",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f92",
                    "user": {
                        "_id": "65c04e9c27a5fdca81abcbd9",
                        "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
                        "isPro": false,
                        "fullname": "Hongsheng LI",
                        "user": "hsli-cuhk",
                        "type": "user"
                    },
                    "name": "Hongsheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:49:51.674Z",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f93",
                    "user": {
                        "_id": "67739bfa64e8b7438ae68eb4",
                        "avatarUrl": "/avatars/15193bfbce487b2de4ce8c86bd18885a.svg",
                        "isPro": false,
                        "fullname": "Maoqing Yao",
                        "user": "AutobotZero",
                        "type": "user"
                    },
                    "name": "Maoqing Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:49:59.798Z",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f94",
                    "user": {
                        "_id": "646ec9b135f55eb49e405faa",
                        "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
                        "isPro": false,
                        "fullname": "Guanghui Ren",
                        "user": "sundrops",
                        "type": "user"
                    },
                    "name": "Guanghui Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:50:05.432Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/2Cb7g14KRbbgg6yotocsP.mp4"
            ],
            "publishedAt": "2025-03-14T17:59:07.000Z",
            "submittedOnDailyAt": "2025-03-17T01:30:24.394Z",
            "title": "Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning",
            "submittedOnDailyBy": {
                "_id": "634e4120038b5879133552f5",
                "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
                "isPro": true,
                "fullname": "Siyuan",
                "user": "SiyuanH",
                "type": "user"
            },
            "summary": "The pursuit of data efficiency, where quality outweighs quantity, has emerged\nas a cornerstone in robotic manipulation, especially given the high costs\nassociated with real-world data collection. We propose that maximizing the\ninformational density of individual demonstrations can dramatically reduce\nreliance on large-scale datasets while improving task performance. To this end,\nwe introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework\nthat redefines robotic data acquisition through real-time, bidirectional\nhuman-environment interactions. Unlike conventional pipelines that passively\nrecord static demonstrations, ADC adopts a collaborative perturbation paradigm:\nduring a single episode, an adversarial operator dynamically alters object\nstates, environmental conditions, and linguistic commands, while the\ntele-operator adaptively adjusts actions to overcome these evolving challenges.\nThis process compresses diverse failure-recovery behaviors, compositional task\nvariations, and environmental perturbations into minimal demonstrations. Our\nexperiments demonstrate that ADC-trained models achieve superior compositional\ngeneralization to unseen task instructions, enhanced robustness to perceptual\nperturbations, and emergent error recovery capabilities. Strikingly, models\ntrained with merely 20% of the demonstration volume collected through ADC\nsignificantly outperform traditional approaches using full datasets. These\nadvances bridge the gap between data-centric learning paradigms and practical\nrobotic deployment, demonstrating that strategic data acquisition, not merely\npost-hoc processing, is critical for scalable, real-world robot learning.\nAdditionally, we are curating a large-scale ADC-Robotics dataset comprising\nreal-world manipulation tasks with adversarial perturbations. This benchmark\nwill be open-sourced to facilitate advancements in robotic imitation learning.",
            "upvotes": 30,
            "discussionId": "67d78c1b4fd0e3fa3a08301c",
            "projectPage": " https://sites.google.com/view/adc-robot",
            "ai_keywords": [
                "Adversarial Data Collection",
                "Human-in-the-Loop (HiL)",
                "real-time, bidirectional human-environment interactions",
                "collaborative perturbation paradigm",
                "adversarial operator",
                "tele-operator",
                "compositional generalization",
                "perceptual perturbations",
                "error recovery capabilities",
                "ADC-trained models",
                "ADC-Robotics dataset",
                "robotic imitation learning"
            ]
        },
        "translation_title": "대적 데이터 수집: 효율적이고 강인한 로봇 모방 학습을 위한 인간 협력적 변화",
        "purpose": "로봇 조작에서 데이터 효율성을 극대화하여 대규모 데이터 세트에 대한 의존성을 줄이고 작업 성능을 향상시키기 위한 연구",
        "method": [
            "대적 데이터 수집(Adversarial Data Collection)이라는 Human-in-the-Loop(HiL) 프레임워크를 도입함(we introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework).",
            "적대적 작동자가 동적으로 객체 상태, 환경 조건 및 언어적 명령을 변경하고, 원거리 작업자가 적응하여 행동을 조정함(Unlike conventional pipelines that passively record static demonstrations, ADC adopts a collaborative perturbation paradigm).",
            "다양한 실패-복구 행동과 환경 변화가 담긴 최소한의 시연으로 데이터 효율성을 높임(This process compresses diverse failure-recovery behaviors, compositional task variations, and environmental perturbations into minimal demonstrations.)"
        ],
        "conclusion": "20%의 데이터로 훈련된 모델이 전통적인 방식보다 훨씬 뛰어난 성능을 보이며, 데이터 중심 학습과 실용적인 로봇 배치 간의 격차를 해소하는 데 기여함.",
        "keywords": [
            "Robotics",
            "Data Efficiency",
            "Imitation Learning"
        ]
    },
    {
        "paper": {
            "id": "2503.11224",
            "authors": [
                {
                    "_id": "67d788b6ba098a0651e1e235",
                    "user": {
                        "_id": "663f07d029be04778ba97871",
                        "avatarUrl": "/avatars/fb7c9d4a2c537d918a3267e7cbc03f04.svg",
                        "isPro": false,
                        "fullname": "Xingtai Lv",
                        "user": "XingtaiHF",
                        "type": "user"
                    },
                    "name": "Xingtai Lv",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:34.410Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e236",
                    "user": {
                        "_id": "679ce8c048ebd7903d76a832",
                        "avatarUrl": "/avatars/5f3fecaacfee6e2d5a72dd19fe87055a.svg",
                        "isPro": false,
                        "fullname": "Youbang Sun",
                        "user": "Youbang",
                        "type": "user"
                    },
                    "name": "Youbang Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:50:17.568Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e237",
                    "user": {
                        "_id": "60bc94cd85a3ab33829b6211",
                        "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
                        "isPro": false,
                        "fullname": "Kaiyan Zhang",
                        "user": "iseesaw",
                        "type": "user"
                    },
                    "name": "Kaiyan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:26.057Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e238",
                    "user": {
                        "_id": "65597738deee83130a1301d5",
                        "avatarUrl": "/avatars/9bcc40aebe4db079927675d95c00463c.svg",
                        "isPro": false,
                        "fullname": "Shang (Lindsay) Qu",
                        "user": "lindsay-qu",
                        "type": "user"
                    },
                    "name": "Shang Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T14:42:23.487Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e239",
                    "user": {
                        "_id": "647ffddeb82adfa7cc1a10d9",
                        "avatarUrl": "/avatars/26aa168d6b2068298ebb16584aa52b6c.svg",
                        "isPro": false,
                        "fullname": "zhu",
                        "user": "xuekai",
                        "type": "user"
                    },
                    "name": "Xuekai Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:50:38.118Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e23a",
                    "user": {
                        "_id": "672c2d7816766a76a747b7b5",
                        "avatarUrl": "/avatars/12c7b26d2b81721ccac3a5c71e32a1a1.svg",
                        "isPro": false,
                        "fullname": "Yuchen Fan",
                        "user": "yuchenFan",
                        "type": "user"
                    },
                    "name": "Yuchen Fan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:50:54.445Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e23b",
                    "name": "Yi Wu",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e23c",
                    "user": {
                        "_id": "6445fa2ffc22e309d78bef3e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fa2ffc22e309d78bef3e/FQaINLd0PjgY9EnK_APRk.jpeg",
                        "isPro": false,
                        "fullname": "Messi Hua",
                        "user": "Messi-Hua",
                        "type": "user"
                    },
                    "name": "Ermo Hua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:30.639Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e23d",
                    "user": {
                        "_id": "667e577139b49eba118d569f",
                        "avatarUrl": "/avatars/1a26dd96b4b352b8968561750ecae9a7.svg",
                        "isPro": false,
                        "fullname": "Xinwei Long",
                        "user": "xinwei666",
                        "type": "user"
                    },
                    "name": "Xinwei Long",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:51:02.068Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e23e",
                    "user": {
                        "_id": "677b80e31ad30ab2c798e776",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/X8IFnIK3TDHOGKZCzLTe8.jpeg",
                        "isPro": false,
                        "fullname": "Ning Ding",
                        "user": "BradPitt2025",
                        "type": "user"
                    },
                    "name": "Ning Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:51:08.621Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e23f",
                    "user": {
                        "_id": "669f614b59adf5b56e05bce3",
                        "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
                        "isPro": false,
                        "fullname": "BowenZhou",
                        "user": "bowenZhou",
                        "type": "user"
                    },
                    "name": "Bowen Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:51:15.825Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-14T09:20:31.000Z",
            "submittedOnDailyAt": "2025-03-17T01:26:02.931Z",
            "title": "Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models",
            "submittedOnDailyBy": {
                "_id": "6445fa2ffc22e309d78bef3e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fa2ffc22e309d78bef3e/FQaINLd0PjgY9EnK_APRk.jpeg",
                "isPro": false,
                "fullname": "Messi Hua",
                "user": "Messi-Hua",
                "type": "user"
            },
            "summary": "State Space Models (SSMs) have emerged as a promising alternative to the\npopular transformer-based models and have been increasingly gaining attention.\nCompared to transformers, SSMs excel at tasks with sequential data or longer\ncontexts, demonstrating comparable performances with significant efficiency\ngains. In this survey, we provide a coherent and systematic overview for SSMs,\nincluding their theoretical motivations, mathematical formulations, comparison\nwith existing model classes, and various applications. We divide the SSM series\ninto three main sections, providing a detailed introduction to the original\nSSM, the structured SSM represented by S4, and the selective SSM typified by\nMamba. We put an emphasis on technicality, and highlight the various key\ntechniques introduced to address the effectiveness and efficiency of SSMs. We\nhope this manuscript serves as an introduction for researchers to explore the\ntheoretical foundations of SSMs.",
            "upvotes": 20,
            "discussionId": "67d788b7ba098a0651e1e2a4",
            "ai_keywords": [
                "State Space Models (SSMs)",
                "transformer-based models",
                "sequential data",
                "theoretical motivations",
                "mathematical formulations",
                "comparison",
                "model classes",
                "original SSM",
                "structured SSM",
                "S4",
                "selective SSM",
                "Mamba",
                "effectiveness",
                "efficiency"
            ]
        },
        "translation_title": "효율성과 효과성에 대한 기술: 상태 공간 모델 조사",
        "purpose": "상태 공간 모델의 이론적 기초와 효율성에 대한 체계적인 개요 제공",
        "method": [
            "상태 공간 모델(SSM)의 이론적 동기와 수학적 공식, 기존 모델과의 비교 등을 포괄적으로 검토함(We provide a coherent and systematic overview for SSMs, including their theoretical motivations, mathematical formulations, comparison with existing model classes, and various applications.)",
            "SSM 시리즈를 원래 SSM, 구조화된 SSM(S4), 선택적 SSM(Mamba)으로 나누어 자세히 소개함(We divide the SSM series into three main sections, providing a detailed introduction to the original SSM, the structured SSM represented by S4, and the selective SSM typified by Mamba.)",
            "효율성과 효과성을 고려한 다양한 기술들을 강조함(We put an emphasis on technicality, and highlight the various key techniques introduced to address the effectiveness and efficiency of SSMs.)"
        ],
        "conclusion": "이 논문은 연구자들이 상태 공간 모델의 이론적 기초를 탐구할 수 있는 기초 자료로 활용될 수 있기를 바랍니다.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2503.11069",
            "authors": [
                {
                    "_id": "67d785458678eaf139e3c594",
                    "user": {
                        "_id": "654dbac9938fbf1e696be8aa",
                        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
                        "isPro": false,
                        "fullname": "Chaoyun Zhang",
                        "user": "vyokky",
                        "type": "user"
                    },
                    "name": "Chaoyun Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:51:30.401Z",
                    "hidden": false
                },
                {
                    "_id": "67d785458678eaf139e3c595",
                    "user": {
                        "_id": "62c6df026a092eda1f1ab6e5",
                        "avatarUrl": "/avatars/d58fff1a157b189ce2617889ef5f6e2f.svg",
                        "isPro": false,
                        "fullname": "Shilin He",
                        "user": "shilhe",
                        "type": "user"
                    },
                    "name": "Shilin He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:51:37.539Z",
                    "hidden": false
                },
                {
                    "_id": "67d785458678eaf139e3c596",
                    "user": {
                        "_id": "666933c97bf97e24f7b5266e",
                        "avatarUrl": "/avatars/283961b37d463a386b08ad33dacca0f4.svg",
                        "isPro": false,
                        "fullname": "Liqun Li",
                        "user": "liqul",
                        "type": "user"
                    },
                    "name": "Liqun Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:51:57.886Z",
                    "hidden": false
                },
                {
                    "_id": "67d785458678eaf139e3c597",
                    "user": {
                        "_id": "67481846f47628abdd8c4397",
                        "avatarUrl": "/avatars/b43f2988ac17bd2bb2369133934ce75d.svg",
                        "isPro": false,
                        "fullname": "Si Qin",
                        "user": "SiQin88",
                        "type": "user"
                    },
                    "name": "Si Qin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:52:05.644Z",
                    "hidden": false
                },
                {
                    "_id": "67d785458678eaf139e3c598",
                    "name": "Yu Kang",
                    "hidden": false
                },
                {
                    "_id": "67d785458678eaf139e3c599",
                    "user": {
                        "_id": "652fc9f39bc50a6c0e435224",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fc9f39bc50a6c0e435224/70OBVDHHBsxG2giJ-E3_1.jpeg",
                        "isPro": false,
                        "fullname": "Lin Qingwei",
                        "user": "Eliblo1969",
                        "type": "user"
                    },
                    "name": "Qingwei Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:52:17.826Z",
                    "hidden": false
                },
                {
                    "_id": "67d785458678eaf139e3c59a",
                    "user": {
                        "_id": "66473d2c7abe6ad66e81a3dd",
                        "avatarUrl": "/avatars/82f40244806c06ffeaa1c4265e9725ea.svg",
                        "isPro": false,
                        "fullname": "ZHANGDONGMEI",
                        "user": "ZDM6426",
                        "type": "user"
                    },
                    "name": "Dongmei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:52:31.623Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-14T04:26:21.000Z",
            "submittedOnDailyAt": "2025-03-17T00:43:33.225Z",
            "title": "API Agents vs. GUI Agents: Divergence and Convergence",
            "submittedOnDailyBy": {
                "_id": "654dbac9938fbf1e696be8aa",
                "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
                "isPro": false,
                "fullname": "Chaoyun Zhang",
                "user": "vyokky",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications.",
            "upvotes": 17,
            "discussionId": "67d785468678eaf139e3c5ee"
        },
        "translation_title": "API 에이전트와 GUI 에이전트: 분기와 융합",
        "purpose": "API 기반과 GUI 기반의 LLM 에이전트를 비교하고, 두 가지 접근 방법의 장점을 활용하여 효율적인 작업 자동화를 위한 가이드 제공",
        "method": [
            "API 기반 LLM 에이전트와 GUI 기반 LLM 에이전트를 체계적으로 비교하고 분석함(We present the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence).",
            "하이브리드 접근 방법의 장점을 활용할 수 있는 시나리오를 강조함(We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths).",
            "명확한 결정 기준을 제안하고 실용적인 사용 사례를 설명함(By proposing clear decision criteria and illustrating practical use cases)"
        ],
        "conclusion": "LLM 기반 자동화의 지속적인 혁신은 API와 GUI 에이전트 간의 경계를 흐리게 하여 더욱 유연하고 적응 가능한 솔루션을 만들 것으로 기대됨.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Robotics"
        ]
    }
]