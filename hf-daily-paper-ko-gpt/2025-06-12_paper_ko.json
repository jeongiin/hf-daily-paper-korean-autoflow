[
    {
        "paper": {
            "id": "2506.06395",
            "authors": [
                {
                    "_id": "68492dcf42e4f9106973f437",
                    "user": {
                        "_id": "6734e315c1aadce903f73aea",
                        "avatarUrl": "/avatars/95d95c49419372debc201cb63c354b86.svg",
                        "isPro": false,
                        "fullname": "Li Pengyi",
                        "user": "LiPengyi29",
                        "type": "user"
                    },
                    "name": "Pengyi Li",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-11T07:18:40.287Z",
                    "hidden": false
                },
                {
                    "_id": "68492dcf42e4f9106973f438",
                    "user": {
                        "_id": "6626c5d0a329de26e7eb16fa",
                        "avatarUrl": "/avatars/124f389f768fb666efd8b5a9b54c3b3c.svg",
                        "isPro": false,
                        "fullname": "Matvey Skripkin",
                        "user": "barracuda049",
                        "type": "user"
                    },
                    "name": "Matvey Skripkin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:44.217Z",
                    "hidden": false
                },
                {
                    "_id": "68492dcf42e4f9106973f439",
                    "name": "Alexander Zubrey",
                    "hidden": false
                },
                {
                    "_id": "68492dcf42e4f9106973f43a",
                    "user": {
                        "_id": "643984dceb7c5616ef3f5d54",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
                        "isPro": false,
                        "fullname": "Andrey Kuznetsov",
                        "user": "kuznetsoffandrey",
                        "type": "user"
                    },
                    "name": "Andrey Kuznetsov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:42.242Z",
                    "hidden": false
                },
                {
                    "_id": "68492dcf42e4f9106973f43b",
                    "user": {
                        "_id": "6169a581d05945bfd8718dfa",
                        "avatarUrl": "/avatars/1892ab06a7ddb557232777de3cbec470.svg",
                        "isPro": false,
                        "fullname": "Ivan Oseledets",
                        "user": "oseledets",
                        "type": "user"
                    },
                    "name": "Ivan Oseledets",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:40.519Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg"
            ],
            "publishedAt": "2025-06-05T19:55:15.000Z",
            "submittedOnDailyAt": "2025-06-12T07:02:06.762Z",
            "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models",
            "submittedOnDailyBy": {
                "_id": "643984dceb7c5616ef3f5d54",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
                "isPro": false,
                "fullname": "Andrey Kuznetsov",
                "user": "kuznetsoffandrey",
                "type": "user"
            },
            "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\nsimple, scalable post-training method for inference models, requiring only a\nsmall number of samples and unlabelled supervision.",
            "upvotes": 56,
            "discussionId": "68492dd042e4f9106973f43c",
            "ai_summary": "Reinforcement Learning via Self-Confidence (RLSC) improves large language model accuracy using the model's confidence as a reward signal, eliminating the need for human labels or reward engineering.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Large language models",
                "self-confidence",
                "RLSC"
            ]
        },
        "translation_title": "확신이 전부다: 언어 모델의 Few-Shot RL 미세 조정",
        "purpose": "언어 모델을 특정 작업 목표에 맞게 조정하기 위한 새로운 RL 방법 제안",
        "method": [
            "모델의 자체 신뢰도를 보상 신호로 사용하는 Reinforcement Learning via Self-Confidence (RLSC) 제안(We propose Reinforcement Learning via Self-Confidence (RLSC), which uses the model's own confidence as reward signals-eliminating the need for labels, preference models, or reward engineering.)",
            "Qwen2.5-Math-7B 모델에 대해 단 16개의 샘플과 10 또는 20개의 학습 단계를 사용하여 RLSC 적용함(Applied to Qwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps.)",
            "RLSC를 통해 여러 데이터셋에서 정확도 향상(개선된 성능 설명)(RLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23.)"
        ],
        "conclusion": "RLSC는 소량의 샘플과 비표시 감독만으로도 효율적인 후속 훈련 방법을 제공한다.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Reinforcement Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.09113",
            "authors": [
                {
                    "_id": "684a3b0a9b38e1e5a33a683f",
                    "user": {
                        "_id": "6614f2dca37a503c2320b44e",
                        "avatarUrl": "/avatars/4bee18c49eff253d6eeb9a1f1509b68b.svg",
                        "isPro": false,
                        "fullname": "gaoyu",
                        "user": "gaooyu1520",
                        "type": "user"
                    },
                    "name": "Yu Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:01.191Z",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6840",
                    "name": "Haoyuan Guo",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6841",
                    "name": "Tuyen Hoang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6842",
                    "name": "Weilin Huang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6843",
                    "name": "Lu Jiang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6844",
                    "name": "Fangyuan Kong",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6845",
                    "name": "Huixia Li",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6846",
                    "name": "Jiashi Li",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6847",
                    "name": "Liang Li",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6848",
                    "name": "Xiaojie Li",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6849",
                    "name": "Xunsong Li",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a684a",
                    "name": "Yifu Li",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a684b",
                    "name": "Shanchuan Lin",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a684c",
                    "name": "Zhijie Lin",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a684d",
                    "user": {
                        "_id": "63049b95dae2eb7d083f1bf3",
                        "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg",
                        "isPro": false,
                        "fullname": "Jiawei Liu",
                        "user": "jwliu-cc",
                        "type": "user"
                    },
                    "name": "Jiawei Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:41:55.061Z",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a684e",
                    "name": "Shu Liu",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a684f",
                    "name": "Xiaonan Nie",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6850",
                    "name": "Zhiwu Qing",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6851",
                    "name": "Yuxi Ren",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6852",
                    "name": "Li Sun",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6853",
                    "name": "Zhi Tian",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6854",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6855",
                    "name": "Sen Wang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6856",
                    "name": "Guoqiang Wei",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6857",
                    "name": "Guohong Wu",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6858",
                    "user": {
                        "_id": "6381c5d63680a7cf34e08ca9",
                        "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
                        "isPro": false,
                        "fullname": "wujie10558@gmail.com",
                        "user": "wujie10",
                        "type": "user"
                    },
                    "name": "Jie Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:05.891Z",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6859",
                    "name": "Ruiqi Xia",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a685a",
                    "name": "Fei Xiao",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a685b",
                    "name": "Xuefeng Xiao",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a685c",
                    "name": "Jiangqiao Yan",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a685d",
                    "name": "Ceyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a685e",
                    "name": "Jianchao Yang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a685f",
                    "name": "Runkai Yang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6860",
                    "name": "Tao Yang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6861",
                    "name": "Yihang Yang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6862",
                    "user": {
                        "_id": "65b62ab582d3845134ef0aab",
                        "avatarUrl": "/avatars/fb1c22b2937e86f668b06fedb48e57e0.svg",
                        "isPro": false,
                        "fullname": "Zilyu Ye",
                        "user": "YeLuoSuiYou",
                        "type": "user"
                    },
                    "name": "Zilyu Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:41:58.552Z",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6863",
                    "name": "Xuejiao Zeng",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6864",
                    "name": "Yan Zeng",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6865",
                    "name": "Heng Zhang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6866",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6867",
                    "name": "Xiaozheng Zheng",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6868",
                    "name": "Peihao Zhu",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6869",
                    "name": "Jiaxin Zou",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a686a",
                    "name": "Feilong Zuo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T17:56:11.000Z",
            "submittedOnDailyAt": "2025-06-12T01:08:56.090Z",
            "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
            "submittedOnDailyBy": {
                "_id": "6381c5d63680a7cf34e08ca9",
                "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
                "isPro": false,
                "fullname": "wujie10558@gmail.com",
                "user": "wujie10",
                "type": "user"
            },
            "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.",
            "upvotes": 42,
            "discussionId": "684a3b0b9b38e1e5a33a686b",
            "projectPage": "https://seed.bytedance.com/seedance",
            "ai_summary": "Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.",
            "ai_keywords": [
                "diffusion modeling",
                "multi-source data curation",
                "precision and meaningful video captioning",
                "efficient architecture",
                "training paradigm",
                "multi-shot generation",
                "text-to-video",
                "image-to-video",
                "fine-grained supervised fine-tuning",
                "video-specific RLHF",
                "multi-dimensional reward mechanisms",
                "multi-stage distillation strategies",
                "model acceleration",
                "spatiotemporal fluidity",
                "structural stability",
                "instruction adherence",
                "multi-shot narrative coherence",
                "consistent subject representation"
            ]
        },
        "translation_title": "Seedance 1.0: 비디오 생성 모델의 한계 탐구",
        "purpose": "비디오 생성의 성능을 향상시키고 다양한 시나리오에서 학습할 수 있는 모델 개발",
        "method": [
            "다양한 소스의 데이터 수집과 의미 있는 비디오 캡션을 통해 포괄적인 학습을 가능하게 함(augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios.)",
            "multi-shot 생성과 text-to-video 및 image-to-video 작업을 함께 학습할 수 있는 효율적인 아키텍처 설계 제안(efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks.)",
            "세밀한 감독 하에 파인 튜닝과 비디오 특화 RLHF 접근법을 통해 성능 개선을 최적화함(carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements.)",
            "다단계 증류 전략과 시스템 최적화를 통해 약 10배의 추론 속도 향상 달성(excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations.)"
        ],
        "conclusion": "Seedance 1.0은 1080p 해상도로 5초 비디오를 41.4초 만에 생성할 수 있으며, 고품질과 빠른 비디오 생성, 우수한 시공간 흐름과 구조적 안정성을 자랑함.",
        "keywords": [
            "Video Generation",
            "Machine Learning",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2506.09350",
            "authors": [
                {
                    "_id": "684a79ca9b38e1e5a33a68bf",
                    "user": {
                        "_id": "645863f7dc18eb1a9b5d29df",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645863f7dc18eb1a9b5d29df/t49Nnyl4tbkUn7CmQqKZh.jpeg",
                        "isPro": false,
                        "fullname": "Peter Lin",
                        "user": "PeterL1n",
                        "type": "user"
                    },
                    "name": "Shanchuan Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:40:47.762Z",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c0",
                    "name": "Ceyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c1",
                    "name": "Hao He",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c2",
                    "name": "Jianwen Jiang",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c3",
                    "name": "Yuxi Ren",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c4",
                    "name": "Xin Xia",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c5",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c6",
                    "name": "Xuefeng Xiao",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c7",
                    "name": "Lu Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T03:04:23.000Z",
            "submittedOnDailyAt": "2025-06-12T05:25:53.654Z",
            "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
            "upvotes": 27,
            "discussionId": "684a79ca9b38e1e5a33a68c8",
            "projectPage": "https://seaweed-apt.com/2",
            "ai_summary": "Autoregressive adversarial post-training transforms a pre-trained latent video diffusion model into a real-time, interactive video generator with efficient one-step generation and reduced error accumulation.",
            "ai_keywords": [
                "autoregressive adversarial post-training",
                "latent video diffusion model",
                "single neural function evaluation",
                "KV cache",
                "student-forcing",
                "real-time video generation",
                "24fps",
                "736x416 resolution",
                "1280x720 resolution",
                "H100 GPUs"
            ]
        },
        "translation_title": "실시간 상호작용 비디오 생성을 위한 자율 회귀 적대적 후속 학습",
        "purpose": "기존의 대규모 비디오 생성 모델의 연산량을 줄이고 실시간 상호작용 애플리케이션에서 사용할 수 있도록 개선",
        "method": [
            "사전 학습된 잠재 비디오 확산 모델을 자율 회귀 적대적 후속 학습(AAPT)을 통해 실시간 상호작용 비디오 생성기로 변환함(In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator.)",
            "모델이 한 번에 하나의 잠재 이미지를 자율 회귀적으로 생성하며, 이를 통해 실시간으로 사용자에게 결과를 스트리밍할 수 있도록 함(Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE).)",
            "적대적 훈련을 활용하여 보다 효율적인 아키텍처를 설계하고, 모델의 오류 축적을 줄이는 훈련 방식(student-forcing)을 적용함(This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation.)"
        ],
        "conclusion": "8B 모델은 단일 H100에서 736x416 해상도로 실시간 24fps 비디오 생성을 달성하며, 최대 1분 길이의 비디오를 생성할 수 있음.",
        "keywords": [
            "Video Generation",
            "Natural Language Processing",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2506.09790",
            "authors": [
                {
                    "_id": "684a33989b38e1e5a33a6804",
                    "user": {
                        "_id": "639c379cdb7c5f35004066cb",
                        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
                        "isPro": false,
                        "fullname": "Zhenran Xu",
                        "user": "imryanxu",
                        "type": "user"
                    },
                    "name": "Zhenran Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:13.515Z",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a6805",
                    "user": {
                        "_id": "6309995efa440d8b5bd5ddc2",
                        "avatarUrl": "/avatars/e7350783a8edc5660afd5173818f02f2.svg",
                        "isPro": false,
                        "fullname": "Yiyu Wang",
                        "user": "Curya",
                        "type": "user"
                    },
                    "name": "Yiyu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:11.706Z",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a6806",
                    "name": "Xue Yang",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a6807",
                    "name": "Longyue Wang",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a6808",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a6809",
                    "name": "Kaifu Zhang",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a680a",
                    "name": "Baotian Hu",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a680b",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T14:35:15.000Z",
            "submittedOnDailyAt": "2025-06-12T00:38:07.422Z",
            "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
            "submittedOnDailyBy": {
                "_id": "639c379cdb7c5f35004066cb",
                "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
                "isPro": false,
                "fullname": "Zhenran Xu",
                "user": "imryanxu",
                "type": "user"
            },
            "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.",
            "upvotes": 25,
            "discussionId": "684a33989b38e1e5a33a680c",
            "projectPage": "https://github.com/AIDC-AI/ComfyUI-Copilot",
            "githubRepo": "https://github.com/AIDC-AI/ComfyUI-Copilot",
            "ai_summary": "ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.",
            "ai_keywords": [
                "modular workflows",
                "ComfyUI",
                "large reasoning model",
                "automated workflow generation",
                "chain-of-thought (CoT) reasoning",
                "node selection",
                "workflow planning",
                "code-level workflow representation",
                "CoT fine-tuning",
                "reinforcement learning",
                "fine-grained rule-metric hybrid reward",
                "format validity",
                "structural integrity",
                "node-level fidelity",
                "GPT-4o",
                "Claude series",
                "pass rate",
                "node-level F1 scores",
                "graph-level F1 scores",
                "intricate workflows",
                "diverse nodes",
                "qualitative comparison",
                "AI art creation"
            ]
        },
        "translation_title": "ComfyUI-R1: 워크플로우 생성을 위한 추론 모델 탐색",
        "purpose": "AI 기반 창작 파이프라인에서 사용자 친화적인 자동화된 워크플로우 생성을 위한 연구",
        "method": [
            "4K 워크플로우 데이터셋을 바탕으로 node 선택, 워크플로우 계획 및 코드 수준 워크플로우 표현을 포함한 긴 추론 데이터(CoT) 구축(Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation.)",
            "두 단계 프레임워크를 통해 모델을 훈련함: 첫 번째 단계에서는 ComfyUI 도메인에 모델을 조정하기 위한 CoT 미세 조정 진행(ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain;)",
            "두 번째 단계에서는 세부 기준 메트릭 하이브리드 보상에 의해 추론 능력을 장려하기 위한 강화 학습 적용(2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity.)"
        ],
        "conclusion": "ComfyUI-R1은 97%의 형식 유효성 비율을 기록했으며, 기존의 최첨단 방법을 크게 초월하는 성과를 보여줍니다.",
        "keywords": [
            "Natural Language Processing",
            "Robotics",
            "Workflow Generation"
        ]
    },
    {
        "paper": {
            "id": "2506.09995",
            "authors": [
                {
                    "_id": "684a39639b38e1e5a33a6837",
                    "name": "Yuanpeng Tu",
                    "hidden": false
                },
                {
                    "_id": "684a39639b38e1e5a33a6838",
                    "name": "Hao Luo",
                    "hidden": false
                },
                {
                    "_id": "684a39639b38e1e5a33a6839",
                    "user": {
                        "_id": "644a1b6401e18bf93a6f45c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
                        "isPro": false,
                        "fullname": "xichen",
                        "user": "xichenhku",
                        "type": "user"
                    },
                    "name": "Xi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:07.608Z",
                    "hidden": false
                },
                {
                    "_id": "684a39639b38e1e5a33a683a",
                    "name": "Xiang Bai",
                    "hidden": false
                },
                {
                    "_id": "684a39639b38e1e5a33a683b",
                    "name": "Fan Wang",
                    "hidden": false
                },
                {
                    "_id": "684a39639b38e1e5a33a683c",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:59:53.000Z",
            "submittedOnDailyAt": "2025-06-12T00:50:19.796Z",
            "title": "PlayerOne: Egocentric World Simulator",
            "submittedOnDailyBy": {
                "_id": "644a1b6401e18bf93a6f45c1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
                "isPro": false,
                "fullname": "xichen",
                "user": "xichenhku",
                "type": "user"
            },
            "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.",
            "upvotes": 23,
            "discussionId": "684a39639b38e1e5a33a683d",
            "projectPage": "https://playerone-hku.github.io/",
            "ai_summary": "PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.",
            "ai_keywords": [
                "egocentric realistic world simulator",
                "coarse-to-fine pipeline",
                "pretraining",
                "finetuning",
                "synchronous motion-video data",
                "automatic construction pipeline",
                "part-disentangled motion injection",
                "joint reconstruction framework",
                "4D scene",
                "video frames",
                "scene consistency",
                "long-form video generation",
                "worldconsistent modeling"
            ]
        },
        "translation_title": "PlayerOne: 사용자의 시점을 고려한 현실적 세계 시뮬레이터",
        "purpose": "사용자가 몰입하고 제한 없이 탐험할 수 있는 현실적인 세계 시뮬레이터 개발",
        "method": [
            "사용자로부터 받은 에고센트릭 장면 이미지를 기반으로 실제 세계를 구성하고 에고센트릭 비디오를 생성함.(Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos...)",
            "대규모 에고센트릭 텍스트-비디오 쌍을 활용해 사전 훈련을 한 후, 동일한 동작을 가진 비디오 데이터로 미세 조정하는 방법을 사용함.(PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs...)",
            "부품 수준의 운동 제어를 가능하게 하는 부품 분리 운동 주입 방식을 설계함.(we design a part-disentangled motion injection scheme, enabling precise control of part-level movements.)"
        ],
        "conclusion": "PlayerOne은 에고센트릭 실세계 시뮬레이션의 첫 번째 사례로, 다양한 응용 분야에 대한 새로운 가능성을 열어줌.",
        "keywords": [
            "Computer Vision",
            "Video Generation",
            "Robotics"
        ]
    }
]