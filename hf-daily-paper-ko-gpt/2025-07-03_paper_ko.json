[
    {
        "paper": {
            "id": "2507.01949",
            "authors": [
                {
                    "_id": "6865e6218c83dab5f72d1e47",
                    "name": "Kwai Keye Team",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e48",
                    "name": "Biao Yang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e49",
                    "name": "Bin Wen",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e4a",
                    "name": "Changyi Liu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e4b",
                    "name": "Chenglong Chu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e4c",
                    "name": "Chengru Song",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e4d",
                    "name": "Chongling Rao",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e4e",
                    "name": "Chuan Yi",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e4f",
                    "name": "Da Li",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e50",
                    "name": "Dunju Zang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e51",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e52",
                    "name": "Guorui Zhou",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e53",
                    "name": "Hao Peng",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e54",
                    "name": "Haojie Ding",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e55",
                    "name": "Jiaming Huang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e56",
                    "user": {
                        "_id": "65802e81b701ff85a37caba8",
                        "avatarUrl": "/avatars/b2e9726893caa7e62aad83b1d02e5b41.svg",
                        "isPro": false,
                        "fullname": "jiangxia cao",
                        "user": "caojiangxia",
                        "type": "user"
                    },
                    "name": "Jiangxia Cao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:44.694Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e57",
                    "name": "Jiankang Chen",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e58",
                    "user": {
                        "_id": "61540338e5b9ae6774201e58",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61540338e5b9ae6774201e58/p_minqil1sdiqg5wEVxT5.jpeg",
                        "isPro": false,
                        "fullname": "jingyun",
                        "user": "hjy",
                        "type": "user"
                    },
                    "name": "Jingyun Hua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:15:01.253Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e59",
                    "name": "Jin Ouyang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e5a",
                    "name": "Kaibing Chen",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e5b",
                    "name": "Kaiyu Jiang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e5c",
                    "name": "Kaiyu Tang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e5d",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e5e",
                    "name": "Shengnan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e5f",
                    "name": "Siyang Mao",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e60",
                    "name": "Sui Huang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e61",
                    "name": "Tianke Zhang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e62",
                    "user": {
                        "_id": "68652063e29f1407b58da60f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hTgS65zsKRPxELjSMDSNm.png",
                        "isPro": false,
                        "fullname": "tingting gao",
                        "user": "TinaGao",
                        "type": "user"
                    },
                    "name": "Tingting Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:47:43.679Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e63",
                    "name": "Wei Chen",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e64",
                    "user": {
                        "_id": "65423daba385933e812516d5",
                        "avatarUrl": "/avatars/d18b85b4206ab5905ef5bc95622dff3e.svg",
                        "isPro": false,
                        "fullname": "wei yuan",
                        "user": "yw95",
                        "type": "user"
                    },
                    "name": "Wei Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:49.219Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e65",
                    "name": "Xiangyu Wu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e66",
                    "user": {
                        "_id": "64a4dba8fe950993d2d89113",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a4dba8fe950993d2d89113/yukb9NNeVspIX7eFTreUq.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Hu",
                        "user": "huxiao09",
                        "type": "user"
                    },
                    "name": "Xiao Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:42.172Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e67",
                    "user": {
                        "_id": "673597cfc2424474d12ca58c",
                        "avatarUrl": "/avatars/f748f19619b07838a66bc419a7a6db9d.svg",
                        "isPro": false,
                        "fullname": "xingyulu",
                        "user": "Xingyulu47",
                        "type": "user"
                    },
                    "name": "Xingyu Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:46.882Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e68",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e69",
                    "user": {
                        "_id": "623d8ca4c29adf5ef6175615",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                        "isPro": false,
                        "fullname": "Yi-Fan Zhang",
                        "user": "yifanzhang114",
                        "type": "user"
                    },
                    "name": "Yi-Fan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:15:06.461Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e6a",
                    "name": "Yiping Yang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e6b",
                    "name": "Yulong Chen",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e6c",
                    "name": "Zhenhua Wu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e6d",
                    "name": "Zhenyu Li",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e6e",
                    "user": {
                        "_id": "641948b7d13ffa40812eb239",
                        "avatarUrl": "/avatars/65a0262fea6907bec48ddc1d966742da.svg",
                        "isPro": false,
                        "fullname": "Zhixin Ling",
                        "user": "NamingIsTroublesome",
                        "type": "user"
                    },
                    "name": "Zhixin Ling",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:15:03.454Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e6f",
                    "name": "Ziming Li",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e70",
                    "name": "Dehua Ma",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e71",
                    "name": "Di Xu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e72",
                    "name": "Haixuan Gao",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e73",
                    "name": "Hang Li",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e74",
                    "name": "Jiawei Guo",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e75",
                    "name": "Jing Wang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e76",
                    "name": "Lejian Ren",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e77",
                    "name": "Muhao Wei",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e78",
                    "name": "Qianqian Wang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e79",
                    "name": "Qigen Hu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e7a",
                    "name": "Shiyao Wang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e7b",
                    "name": "Tao Yu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e7c",
                    "name": "Xinchen Luo",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e7d",
                    "name": "Yan Li",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e7e",
                    "name": "Yiming Liang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e7f",
                    "name": "Yuhang Hu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e80",
                    "name": "Zeyi Lu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e81",
                    "name": "Zhuoran Yang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e82",
                    "name": "Zixing Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-02T17:57:28.000Z",
            "submittedOnDailyAt": "2025-07-03T00:40:58.759Z",
            "title": "Kwai Keye-VL Technical Report",
            "submittedOnDailyBy": {
                "_id": "623d8ca4c29adf5ef6175615",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                "isPro": false,
                "fullname": "Yi-Fan Zhang",
                "user": "yifanzhang114",
                "type": "user"
            },
            "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce Kwai Keye-VL, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the KC-MMBench, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
            "upvotes": 91,
            "discussionId": "6865e6218c83dab5f72d1e83",
            "projectPage": "https://kwai-keye.github.io/",
            "githubRepo": "https://github.com/Kwai-Keye/Keye",
            "githubStars": 382
        },
        "translation_title": "Kwai Keye-VL 기술 보고서",
        "purpose": "짧은 동영상 이해를 위한 성능을 높이기 위해 멀티모달 큰 언어 모델을 개발하고자 함.",
        "method": [
            "600억 이상의 토큰으로 구성된 고품질 데이터셋을 사용하여 훈련함(We introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities.)",
            "비전-언어 정렬을 위한 4단계 사전 훈련과 2단계 후 훈련 과정을 포함한 혁신적인 훈련 레시피를 개발함(This recipe features a four-stage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process.)",
            "2단계 후 훈련에서 5가지 모드의 '콜드 스타트' 데이터 혼합을 통해 모델의 추론 능력을 자극함(This mixture teaches the model to decide when and how to reason.)"
        ],
        "conclusion": "Keye-VL은 공공 비디오 벤치마크에서 최첨단 성과를 달성하고, 일반 이미지 기반 작업에서도 높은 경쟁력을 유지하며, 실제 짧은 동영상 시나리오를 위한 새로운 벤치마크인 KC-MMBench에서 뛰어난 장점을 보임.",
        "keywords": [
            "Multimodal Learning",
            "Video Understanding",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2507.01945",
            "authors": [
                {
                    "_id": "6865e4b88c83dab5f72d1e41",
                    "user": {
                        "_id": "6629d7c9fa14eaccf07d8633",
                        "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
                        "isPro": false,
                        "fullname": "Nan Chen",
                        "user": "CNcreator0331",
                        "type": "user"
                    },
                    "name": "Nan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:51.437Z",
                    "hidden": false
                },
                {
                    "_id": "6865e4b88c83dab5f72d1e42",
                    "name": "Mengqi Huang",
                    "hidden": false
                },
                {
                    "_id": "6865e4b88c83dab5f72d1e43",
                    "name": "Yihao Meng",
                    "hidden": false
                },
                {
                    "_id": "6865e4b88c83dab5f72d1e44",
                    "name": "Zhendong Mao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
            ],
            "publishedAt": "2025-07-02T17:55:50.000Z",
            "submittedOnDailyAt": "2025-07-03T00:57:17.831Z",
            "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
            "submittedOnDailyBy": {
                "_id": "6629d7c9fa14eaccf07d8633",
                "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
                "isPro": false,
                "fullname": "Nan Chen",
                "user": "CNcreator0331",
                "type": "user"
            },
            "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
            "upvotes": 58,
            "discussionId": "6865e4b88c83dab5f72d1e45",
            "projectPage": "https://cn-makers.github.io/long_animation_web/",
            "githubRepo": "https://github.com/CN-makers/LongAnimation",
            "githubStars": 82
        },
        "translation_title": "LongAnimation: 동적 글로벌-로컬 메모리를 이용한 장편 애니메이션 생성",
        "purpose": "장편 애니메이션 컬러라이제이션의 자동화 연구를 통해 색상 일관성을 높이려는 목표",
        "method": [
            "장편 애니메이션 컬러라이제이션을 위해 동적 글로벌-로컬 패러다임을 제안함(We argue that ideal long-term color consistency can be achieved through a dynamic global-local paradigm.)",
            "LongAnimation 프레임워크를 제안하며, SketchDiT, Dynamic Global-Local Memory (DGLM), Color Consistency Reward로 구성함(Specifically, we propose LongAnimation, a novel framework, which mainly includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color Consistency Reward.)",
            "DGLM 모듈은 기존의 글로벌 히스토리 기능을 동적으로 압축하고 현재 생성 기능과 적응적으로 결합함(The DGLM module employs a long video understanding model to dynamically compress global historical features and adaptively fuse them with the current generation features.)",
            "컬러 일관성을 향상시키기 위해 컬러 일관성 보상을 도입함(To refine the color consistency, we introduce a Color Consistency Reward.)"
        ],
        "conclusion": "LongAnimation은 장단기 색상 일관성을 유지하는 데 효과적이며, 공개 도메인 애니메이션 컬러라이제이션 작업에 적합하다는 것을 입증함.",
        "keywords": [
            "Video Generation",
            "Image Understanding",
            "Color Consistency"
        ]
    },
    {
        "paper": {
            "id": "2507.01634",
            "authors": [
                {
                    "_id": "6865e04b8c83dab5f72d1e2d",
                    "user": {
                        "_id": "66ef2611fcc1c455f8dce832",
                        "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
                        "isPro": false,
                        "fullname": "Boyuan Sun",
                        "user": "BBBBCHAN",
                        "type": "user"
                    },
                    "name": "Boyuan Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:54.412Z",
                    "hidden": false
                },
                {
                    "_id": "6865e04b8c83dab5f72d1e2e",
                    "name": "Modi Jin",
                    "hidden": false
                },
                {
                    "_id": "6865e04b8c83dab5f72d1e2f",
                    "name": "Bowen Yin",
                    "hidden": false
                },
                {
                    "_id": "6865e04b8c83dab5f72d1e30",
                    "name": "Qibin Hou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-02T12:05:57.000Z",
            "submittedOnDailyAt": "2025-07-03T00:16:17.577Z",
            "title": "Depth Anything at Any Condition",
            "submittedOnDailyBy": {
                "_id": "66ef2611fcc1c455f8dce832",
                "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
                "isPro": false,
                "fullname": "Boyuan Sun",
                "user": "BBBBCHAN",
                "type": "user"
            },
            "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC",
            "upvotes": 31,
            "discussionId": "6865e04b8c83dab5f72d1e31",
            "projectPage": "https://ghost233lism.github.io/depthanything-AC-page/",
            "githubRepo": "https://github.com/HVision-NKU/DepthAnythingAC",
            "githubStars": 104
        },
        "translation_title": "어떠한 조건에서도 깊이 추정하기 (Depth Anything at Any Condition)",
        "purpose": "다양한 환경에서 깊이 정보를 정확히 추정할 수 있는 모델 개발",
        "method": [
            "복잡한 오픈 월드 환경에서도 잘 작동할 수 있는 깊이 추정 모델인 DepthAnything-AC를 제안함(We present Depth Anything at Any Condition (DepthAnything-AC), a foundation monocular depth estimation (MDE) model capable of handling diverse environmental conditions.)",
            "언라벨 데이터만으로도 효과적인 성능을 낼 수 있도록 비지도 일관성 정규화 파라다임을 제안함(To overcome the challenges of data scarcity and the inability of generating high-quality pseudo-labels from corrupted images, we propose an unsupervised consistency regularization finetuning paradigm that requires only a relatively small amount of unlabeled data.)",
            "모델이 패치 수준의 상대적 관계를 학습하게 하기 위해 Spatial Distance Constraint를 적용하여 더 명확한 의미 경계와 정확한 세부 정보를 얻음(Furthermore, we propose the Spatial Distance Constraint to explicitly enforce the model to learn patch-level relative relationships, resulting in clearer semantic boundaries and more accurate details.)"
        ],
        "conclusion": "DepthAnything-AC는 다양한 벤치마크에서 제로샷 성능을 보이며, 실제 환경에서도 강인하게 작동함.",
        "keywords": [
            "Computer Vision",
            "Image Understanding",
            "Depth Estimation"
        ]
    },
    {
        "paper": {
            "id": "2507.01925",
            "authors": [
                {
                    "_id": "686600cf8c83dab5f72d1ed0",
                    "name": "Yifan Zhong",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed1",
                    "name": "Fengshuo Bai",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed2",
                    "user": {
                        "_id": "6578459d62d3ac1817ed79fe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578459d62d3ac1817ed79fe/AXDJuwLUoEOb4Fj3U0Xxo.jpeg",
                        "isPro": false,
                        "fullname": "Shaofei Cai",
                        "user": "phython96",
                        "type": "user"
                    },
                    "name": "Shaofei Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:14:49.923Z",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed3",
                    "user": {
                        "_id": "66ee5cf1801ea45d7a44a542",
                        "avatarUrl": "/avatars/04bafbcbf1aea3920a79bddbd1a18f42.svg",
                        "isPro": false,
                        "fullname": "XUCHUAN HUANG",
                        "user": "Feernnn",
                        "type": "user"
                    },
                    "name": "Xuchuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:14:52.141Z",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed4",
                    "name": "Zhang Chen",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed5",
                    "name": "Xiaowei Zhang",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed6",
                    "name": "Yuanfei Wang",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed7",
                    "name": "Shaoyang Guo",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed8",
                    "name": "Tianrui Guan",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed9",
                    "name": "Ka Nam Lui",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1eda",
                    "name": "Zhiquan Qi",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1edb",
                    "name": "Yitao Liang",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1edc",
                    "name": "Yuanpei Chen",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1edd",
                    "name": "Yaodong Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-02T17:34:52.000Z",
            "submittedOnDailyAt": "2025-07-03T03:39:33.625Z",
            "title": "A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective",
            "submittedOnDailyBy": {
                "_id": "655d9f43b5da99edaf3f2f81",
                "avatarUrl": "/avatars/c7225b3ed54d099a4fd87682427fb5bf.svg",
                "isPro": false,
                "fullname": "Yifan Zhong",
                "user": "Yifan-Zhong",
                "type": "user"
            },
            "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof action tokens that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.",
            "upvotes": 16,
            "discussionId": "686600cf8c83dab5f72d1ede",
            "githubRepo": "https://github.com/Psi-Robot/Awesome-VLA-Papers",
            "githubStars": 32
        },
        "translation_title": "행동 토큰화 관점에서의 비전-언어-행동 모델에 대한 조사",
        "purpose": "비전-언어-행동(VLA) 모델의 행동 토큰화에 대한 이해를 높이고, 각 토큰 유형의 강점과 한계를 분석하여 향후 연구 방향을 제시하는 것",
        "method": [
            "VLA 모델의 행동 토큰을 언어 설명, 코드, 가능성(affordance), 경로(trajectory), 목표 상태(goal state), 잠재 표현(latent representation), 원시 행동(raw action), 추론(reasoning)으로 분류하여 연구함.(we further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning.)",
            "기존 VLA 연구를 행동 토큰화 관점에서 분류하고 해석하여 각 토큰 유형의 강점과 한계를 정리함.(Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement.)",
            "체계적인 검토 및 분석을 통해 VLA 모델의 발전을 종합적으로 이해하고, 향후 연구를 위한 지침을 제공함.(Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research.)"
        ],
        "conclusion": "이 조사를 통해 VLA 모델의 행동 토큰화에 대한 포괄적인 이해를 제시하여, 일반적인 지능으로 발전하는 데 기여하고자 함.",
        "keywords": [
            "Multimodal Learning",
            "Vision-Language Models",
            "Action Tokenization"
        ]
    },
    {
        "paper": {
            "id": "2507.01953",
            "authors": [
                {
                    "_id": "686601648c83dab5f72d1ee0",
                    "name": "Yukang Cao",
                    "hidden": false
                },
                {
                    "_id": "686601648c83dab5f72d1ee1",
                    "name": "Chenyang Si",
                    "hidden": false
                },
                {
                    "_id": "686601648c83dab5f72d1ee2",
                    "name": "Jinghao Wang",
                    "hidden": false
                },
                {
                    "_id": "686601648c83dab5f72d1ee3",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-02T17:58:20.000Z",
            "submittedOnDailyAt": "2025-07-03T02:40:23.949Z",
            "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
            "submittedOnDailyBy": {
                "_id": "63a07c3ab5515dccd40fdb71",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
                "isPro": false,
                "fullname": "Yukang Cao",
                "user": "yukangcao",
                "type": "user"
            },
            "summary": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.",
            "upvotes": 11,
            "discussionId": "686601658c83dab5f72d1ee4",
            "projectPage": "https://yukangcao.github.io/FreeMorph/",
            "githubRepo": "https://github.com/yukangcao/FreeMorph",
            "githubStars": 15
        },
        "translation_title": "FreeMorph: 튜닝 없는 일반화된 이미지 모핑을 위한 확산 모델",
        "purpose": "이미지 모핑의 품질을 유지하면서도 입력 이미지의 서로 다른 의미나 레이아웃을 처리하려는 방법의 개발",
        "method": [
            "가이드를 반영한 구형 보간 설계를 제안하고, 입력 이미지에서 명시적인 가이드를 반영하도록 자기 주의 모듈을 수정함으로써 정체성 손실 문제를 해결하고 방향 전환을 개선함(We first propose a guidance-aware spherical interpolation design that incorporates explicit guidance from the input images by modifying the self-attention modules.)",
            "각 입력 이미지의 자기 주의 모듈을 혼합하여 제어된 일관된 전환을 달성할 수 있는 단계 지향 변동 추세를 도입함(We further introduce a step-oriented variation trend that blends self-attention modules derived from each input image.)"
        ],
        "conclusion": "FreeMorph는 기존 방법들보다 10배에서 50배 더 빠르며 이미지 모핑의 새로운 최첨단 기술을 수립함.",
        "keywords": [
            "Image Generation",
            "Image Understanding",
            "Computer Vision"
        ]
    }
]