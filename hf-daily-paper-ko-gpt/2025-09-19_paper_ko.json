[
    {
        "paper": {
            "id": "2509.15221",
            "authors": [
                {
                    "_id": "68ccb2e43df9ac65e93dc5b9",
                    "user": {
                        "_id": "6432c6152bfb2b0ec7572479",
                        "avatarUrl": "/avatars/7c2f908f511cdfa38a19e967d86f9c40.svg",
                        "isPro": false,
                        "fullname": "Zhaoyang Liu",
                        "user": "zyliu",
                        "type": "user"
                    },
                    "name": "Zhaoyang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:49:00.775Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5ba",
                    "user": {
                        "_id": "6502f241b1792803da7e8def",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png",
                        "isPro": false,
                        "fullname": "JingJing Xie",
                        "user": "ownerEli",
                        "type": "user"
                    },
                    "name": "JingJing Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:58.643Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5bb",
                    "user": {
                        "_id": "642b9861bb77f8456634b048",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg",
                        "isPro": false,
                        "fullname": "Zichen Ding",
                        "user": "heroding77",
                        "type": "user"
                    },
                    "name": "Zichen Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:49:03.065Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5bc",
                    "name": "Zehao Li",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5bd",
                    "name": "Bowen Yang",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5be",
                    "name": "Zhenyu Wu",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5bf",
                    "name": "Xuehui Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c0",
                    "user": {
                        "_id": "6064a0eeb1703ddba0d458b9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
                        "isPro": false,
                        "fullname": "Qiushi",
                        "user": "QiushiSun",
                        "type": "user"
                    },
                    "name": "Qiushi Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T10:25:21.623Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c1",
                    "name": "Shi Liu",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c2",
                    "user": {
                        "_id": "619507e7b74b6c591f794340",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                        "isPro": false,
                        "fullname": "Weiyun Wang",
                        "user": "Weiyun1025",
                        "type": "user"
                    },
                    "name": "Weiyun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:10:25.791Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c3",
                    "user": {
                        "_id": "64804866c7f87934d082bb25",
                        "avatarUrl": "/avatars/41761226c79ac16e48d4c4cb84362adb.svg",
                        "isPro": false,
                        "fullname": "Yeshenglong",
                        "user": "Yeshenglong",
                        "type": "user"
                    },
                    "name": "Shenglong Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:10:17.583Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c4",
                    "name": "Qingyun Li",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c5",
                    "name": "Zeyue Tian",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c6",
                    "name": "Gen Luo",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c7",
                    "user": {
                        "_id": "666a8f24e2990b0cb16b7bf9",
                        "avatarUrl": "/avatars/fcbaf8f1e3e53a2a4a819b7cb2c53aa4.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Yue",
                        "user": "xyyue",
                        "type": "user"
                    },
                    "name": "Xiangyu Yue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:10:44.945Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c8",
                    "user": {
                        "_id": "645d9c3058f9ee315148116d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645d9c3058f9ee315148116d/uBoAWgrF2Di4WcXVGW9fP.jpeg",
                        "isPro": false,
                        "fullname": "Biqing Qi",
                        "user": "jackqi7",
                        "type": "user"
                    },
                    "name": "Biqing Qi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:10:56.050Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5c9",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5ca",
                    "name": "Bowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5cb",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5cc",
                    "name": "Qifeng Chen",
                    "hidden": false
                },
                {
                    "_id": "68ccb2e43df9ac65e93dc5cd",
                    "name": "Wenhai Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T17:59:22.000Z",
            "submittedOnDailyAt": "2025-09-19T00:03:42.415Z",
            "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.",
            "upvotes": 75,
            "discussionId": "68ccb2e43df9ac65e93dc5ce",
            "githubRepo": "https://github.com/OpenGVLab/ScaleCUA",
            "ai_summary": "ScaleCUA, a large-scale dataset and model for computer use agents, achieves state-of-the-art performance across multiple platforms and tasks by leveraging data-driven scaling.",
            "ai_keywords": [
                "Vision-Language Models",
                "computer use agents",
                "GUIs",
                "closed-loop pipeline",
                "automated agents",
                "human experts",
                "WebArena-Lite-v2",
                "ScreenSpot-Pro",
                "MMBench-GUI L1-Hard",
                "OSWorld-G"
            ],
            "githubStars": 60
        },
        "translation_title": "ScaleCUA: 크로스 플랫폼 데이터로 오픈 소스 컴퓨터 사용 에이전트를 확장하기",
        "purpose": "오픈 소스 컴퓨터 사용 에이전트(CUAs)를 확장하기 위한 대규모 데이터셋 및 데이터 기반 모델 연구",
        "method": [
            "자동화된 에이전트와 인간 전문가를 결합한 폐쇄 루프 파이프라인을 통해 6개의 운영 체제와 3개의 작업 도메인에 걸친 대규모 데이터셋을 구축함(It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts.)",
            "이 데이터를 활용하여 ScaleCUA를 훈련시켜 플랫폼 간 원활하게 작동할 수 있도록 함(Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms.)",
            "Baseline과 비교해 WebArena-Lite-v2에서 +26.6, ScreenSpot-Pro에서 +10.7의 성장을 보임(Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro))."
        ],
        "conclusion": "ScaleCUA는 데이터 기반 확장을 통해 일반적인 컴퓨터 사용 에이전트의 성능을 높이며, 새로운 최첨단 결과를 달성함.",
        "keywords": [
            "Computer Vision",
            "Multimodal Learning",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2509.15207",
            "authors": [
                {
                    "_id": "68ccb7983df9ac65e93dc626",
                    "user": {
                        "_id": "647ffddeb82adfa7cc1a10d9",
                        "avatarUrl": "/avatars/26aa168d6b2068298ebb16584aa52b6c.svg",
                        "isPro": false,
                        "fullname": "zhu",
                        "user": "xuekai",
                        "type": "user"
                    },
                    "name": "Xuekai Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:45.314Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc627",
                    "user": {
                        "_id": "649e6761f9134a06ed1e0cea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg",
                        "isPro": false,
                        "fullname": "Daixuan Cheng",
                        "user": "daixuancheng",
                        "type": "user"
                    },
                    "name": "Daixuan Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:48.340Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc628",
                    "user": {
                        "_id": "64384e18d221ff12edae4c75",
                        "avatarUrl": "/avatars/3a5d2400af0f26091c233d63984df412.svg",
                        "isPro": false,
                        "fullname": "Dinghuai Zhang",
                        "user": "Dinghuai",
                        "type": "user"
                    },
                    "name": "Dinghuai Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:11:24.546Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc629",
                    "name": "Hengli Li",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc62a",
                    "name": "Kaiyan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc62b",
                    "name": "Che Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc62c",
                    "name": "Youbang Sun",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc62d",
                    "name": "Ermo Hua",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc62e",
                    "user": {
                        "_id": "622474f38dc6b0b64f5e903d",
                        "avatarUrl": "/avatars/d6b60a014277a8ec7d564163c5f644aa.svg",
                        "isPro": false,
                        "fullname": "Yuxin Zuo",
                        "user": "yuxinzuo",
                        "type": "user"
                    },
                    "name": "Yuxin Zuo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:11:54.405Z",
                    "hidden": true
                },
                {
                    "_id": "68ccb7983df9ac65e93dc62f",
                    "user": {
                        "_id": "663f07d029be04778ba97871",
                        "avatarUrl": "/avatars/fb7c9d4a2c537d918a3267e7cbc03f04.svg",
                        "isPro": false,
                        "fullname": "Xingtai Lv",
                        "user": "XingtaiHF",
                        "type": "user"
                    },
                    "name": "Xingtai Lv",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:11:46.119Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc630",
                    "user": {
                        "_id": "6663e5b10c54dffcd2a921ca",
                        "avatarUrl": "/avatars/2a4589fef05306ccde06728c752e5601.svg",
                        "isPro": false,
                        "fullname": "Qizheng Zhang",
                        "user": "qizhengz",
                        "type": "user"
                    },
                    "name": "Qizheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:12:02.816Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc631",
                    "name": "Lin Chen",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc632",
                    "name": "Fanghao Shao",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc633",
                    "name": "Bo Xue",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc634",
                    "name": "Yunchong Song",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc635",
                    "user": {
                        "_id": "65574c70d16b524f1d2345c1",
                        "avatarUrl": "/avatars/8edbe9ebea85c570bc19b63bf3a727d9.svg",
                        "isPro": false,
                        "fullname": "Zhenjie Yang",
                        "user": "jayyoung0802",
                        "type": "user"
                    },
                    "name": "Zhenjie Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:37.248Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc636",
                    "user": {
                        "_id": "650eba9555dc1e841746f132",
                        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
                        "isPro": false,
                        "fullname": "Ganqu Cui",
                        "user": "ganqu",
                        "type": "user"
                    },
                    "name": "Ganqu Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:12:20.955Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc637",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc638",
                    "user": {
                        "_id": "641904caf9d6f1d772ec7af7",
                        "avatarUrl": "/avatars/4a63eac71eb30f70b1a0e9d4708f26c1.svg",
                        "isPro": false,
                        "fullname": "Jianfeng Gao",
                        "user": "wyngjf",
                        "type": "user"
                    },
                    "name": "Jianfeng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:12:29.442Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc639",
                    "name": "Xiaodong Liu",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc63a",
                    "name": "Bowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc63b",
                    "name": "Hongyuan Mei",
                    "hidden": false
                },
                {
                    "_id": "68ccb7983df9ac65e93dc63c",
                    "name": "Zhouhan Lin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/wtc3vGNJQlj3FdOsBXwp3.png"
            ],
            "publishedAt": "2025-09-18T17:56:36.000Z",
            "submittedOnDailyAt": "2025-09-19T00:24:38.079Z",
            "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "649e6761f9134a06ed1e0cea",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg",
                "isPro": false,
                "fullname": "Daixuan Cheng",
                "user": "daixuancheng",
                "type": "user"
            },
            "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.",
            "upvotes": 63,
            "discussionId": "68ccb7983df9ac65e93dc63d",
            "githubRepo": "https://github.com/Xuekai-Zhu/FlowRL",
            "ai_summary": "FlowRL enhances LLM reinforcement learning by matching the full reward distribution through flow balancing, improving diversity and performance over reward-maximizing methods.",
            "ai_keywords": [
                "FlowRL",
                "reward distribution",
                "flow balancing",
                "reinforcement learning",
                "reward-maximizing methods",
                "PPO",
                "GRPO",
                "normalized target distribution",
                "learnable partition function",
                "reverse KL divergence",
                "diverse exploration",
                "generalizable reasoning trajectories",
                "math reasoning",
                "code reasoning"
            ],
            "githubStars": 20
        },
        "translation_title": "FlowRL: LLM 추론을 위한 보상 분포 일치 기법",
        "purpose": "보상 분포를 일치시켜 LLM의 추론 다양성을 향상시키기 위한 방법 연구",
        "method": [
            "보상 극대화 대신 흐름 균형을 통해 전체 보상 분포를 일치시키는 방법 제안(We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning.)",
            "스칼라 보상을 학습 가능한 분할 함수로 정규화된 목표 분포로 변환하고, 정책과 목표 분포 간의 역 KL 발산을 최소화함(we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution.)",
            "흐름 균형 최적화 방법을 구현하여 다양한 탐색과 일반화 가능한 추론 경로를 촉진함(We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories.)"
        ],
        "conclusion": "FlowRL은 수학 벤치마크에서 GRPO 대비 10.0%, PPO 대비 5.1%의 평균 개선을 달성하였고, 코드 추론 작업에서도 일관된 성과를 보임.",
        "keywords": [
            "Large Language Models",
            "Reinforcement Learning",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2509.14760",
            "authors": [
                {
                    "_id": "68ccc6a13df9ac65e93dc66a",
                    "user": {
                        "_id": "689ec537196ab997b13dc977",
                        "avatarUrl": "/avatars/28e0ba07ca26056833e1197a6aa0d14f.svg",
                        "isPro": false,
                        "fullname": "Haoran Zhang",
                        "user": "zzzhr97",
                        "type": "user"
                    },
                    "name": "Haoran Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:23.170Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc6a13df9ac65e93dc66b",
                    "user": {
                        "_id": "63f3502a520c14618925825a",
                        "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
                        "isPro": false,
                        "fullname": "Yafu Li",
                        "user": "yaful",
                        "type": "user"
                    },
                    "name": "Yafu Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:12:52.499Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc6a13df9ac65e93dc66c",
                    "user": {
                        "_id": "6498fde776d49ee00f79cbfe",
                        "avatarUrl": "/avatars/4c284a71080150e6cb3b9632dfccef60.svg",
                        "isPro": false,
                        "fullname": "Xuyang Hu",
                        "user": "huxy912",
                        "type": "user"
                    },
                    "name": "Xuyang Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:13:07.959Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc6a13df9ac65e93dc66d",
                    "user": {
                        "_id": "657fe7a8504da7f6f30a2832",
                        "avatarUrl": "/avatars/65987e3cba449b5d250616510ee11f33.svg",
                        "isPro": false,
                        "fullname": "Dongrui Liu",
                        "user": "Max9803",
                        "type": "user"
                    },
                    "name": "Dongrui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-19T13:13:15.250Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc6a13df9ac65e93dc66e",
                    "name": "Zhilin Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccc6a13df9ac65e93dc66f",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "68ccc6a13df9ac65e93dc670",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T09:08:53.000Z",
            "submittedOnDailyAt": "2025-09-19T02:48:26.705Z",
            "title": "Reasoning over Boundaries: Enhancing Specification Alignment via\n  Test-time Delibration",
            "submittedOnDailyBy": {
                "_id": "63f3502a520c14618925825a",
                "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
                "isPro": false,
                "fullname": "Yafu Li",
                "user": "yaful",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly applied in diverse real-world\nscenarios, each governed by bespoke behavioral and safety specifications (spec)\ncustom-tailored by users or organizations. These spec, categorized into\nsafety-spec and behavioral-spec, vary across scenarios and evolve with changing\npreferences and requirements. We formalize this challenge as specification\nalignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec\nfrom both behavioral and safety perspectives. To address this challenge, we\npropose Align3, a lightweight method that employs Test-Time Deliberation (TTD)\nwith hierarchical reflection and revision to reason over the specification\nboundaries. We further present SpecBench, a unified benchmark for measuring\nspecification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts.\nExperiments on 15 reasoning and 18 instruct models with several TTD methods,\nincluding Self-Refine, TPO, and MoreThink, yield three key findings: (i)\ntest-time deliberation enhances specification alignment; (ii) Align3 advances\nthe safety-helpfulness trade-off frontier with minimal overhead; (iii)\nSpecBench effectively reveals alignment gaps. These results highlight the\npotential of test-time deliberation as an effective strategy for reasoning over\nthe real-world specification boundaries.",
            "upvotes": 42,
            "discussionId": "68ccc6a13df9ac65e93dc671",
            "githubRepo": "https://github.com/zzzhr97/SpecBench",
            "ai_summary": "Align3, a lightweight method using Test-Time Deliberation, enhances specification alignment in large language models across diverse scenarios with minimal overhead.",
            "ai_keywords": [
                "Large language models",
                "specification alignment",
                "Test-Time Deliberation",
                "hierarchical reflection",
                "revision",
                "SpecBench",
                "safety-helpfulness trade-off",
                "Self-Refine",
                "TPO",
                "MoreThink"
            ],
            "githubStars": 13
        },
        "translation_title": "경계에 대한 추론: 테스트 시간 변화를 통한 명세 정렬 향상",
        "purpose": "다양한 시나리오에서 대형 언어 모델(LLMs)이 사용자나 조직별 맞춤형 명세를 따르는 능력 향상",
        "method": [
            "외부 명세에 대처하기 위한 가벼운 방법 Align3를 제안함(We propose Align3, a lightweight method that employs Test-Time Deliberation (TTD))",
            "Hierarchical reflection과 revision을 통해 명세 경계를 추론함(to reason over the specification boundaries)",
            "Align3의 성능을 평가하기 위해 5개의 시나리오와 103개의 명세, 1,500개의 프롬프트로 구성된 SpecBench를 개발함(We further present SpecBench, a unified benchmark for measuring specification alignment)."
        ],
        "conclusion": "이 방법을 통해 테스트 시간 변화를 이용한 추론이 명세 정렬을 효과적으로 향상시키고, Align3는 안전성과 유용성의 균형을 최소한의 부담으로 개선함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2509.15194",
            "authors": [
                {
                    "_id": "68ccb4383df9ac65e93dc5d0",
                    "user": {
                        "_id": "65a05abf07184d32fa002d41",
                        "avatarUrl": "/avatars/3a23e7e568d2024381ed31b56c1c461a.svg",
                        "isPro": false,
                        "fullname": "Yujun Zhou",
                        "user": "yujunzhou",
                        "type": "user"
                    },
                    "name": "Yujun Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:53.689Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d1",
                    "user": {
                        "_id": "62ffa3f8311cad266f9af236",
                        "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
                        "isPro": false,
                        "fullname": "Zhenwen Liang",
                        "user": "invokerliang",
                        "type": "user"
                    },
                    "name": "Zhenwen Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:56.281Z",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d2",
                    "name": "Haolin Liu",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d3",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d4",
                    "name": "Kishan Panaganti",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d5",
                    "name": "Linfeng Song",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d6",
                    "name": "Dian Yu",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d7",
                    "name": "Xiangliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d8",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "68ccb4383df9ac65e93dc5d9",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T17:50:04.000Z",
            "submittedOnDailyAt": "2025-09-19T00:09:24.304Z",
            "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
            "submittedOnDailyBy": {
                "_id": "5feab3a28a3201f8e554c969",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
                "isPro": false,
                "fullname": "Wenhao Yu",
                "user": "wyu1",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability.",
            "upvotes": 28,
            "discussionId": "68ccb4383df9ac65e93dc5da",
            "githubRepo": "https://github.com/YujunZhou/EVOL-RL",
            "ai_summary": "EVOL-RL, a label-free reinforcement learning method, enhances large language models by balancing stability and variation, preventing entropy collapse and improving generalization.",
            "ai_keywords": [
                "reinforcement learning from verifiable rewards",
                "RLVR",
                "label-free methods",
                "confidence minimization",
                "self-consistency",
                "majority-vote objectives",
                "entropy collapse",
                "Test-Time Reinforcement Learning",
                "TTRL",
                "EVolution-Oriented and Label-free Reinforcement Learning",
                "EVOL-RL",
                "GRPO",
                "asymmetric clipping",
                "entropy regularizer",
                "pass@1",
                "pass@n",
                "AIME24",
                "Qwen3-4B-Base",
                "AIME25",
                "GPQA"
            ],
            "githubStars": 8
        },
        "translation_title": "레이블 없이 언어 모델 발전하기: 다수결이 선택을 이끌고, 새로움이 변화를 촉진하다",
        "purpose": "레이블이나 외부 판단 없이 자연어 처리 모델이 스스로 개선될 수 있도록 하는 방법 연구",
        "method": [
            "EVOL-RL이라는 새로운 규칙을 제안하여, 안정성과 변화를 레이블 없이 결합함(EVOL-RL, a simple rule that couples stability with variation under a label-free setting).",
            "다수결을 안정적인 기준으로 삼고, 기존 응답과 다른 추론을 갖춘 응답에 보상을 주는 시스템을 구현함(EVOL-RL keeps the majority-voted answer as a stable anchor while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced).",
            "GRPO를 통해 EVOL-RL을 실행하고, 비대칭 클리핑과 엔트로피 조절기를 사용하여 신호 강도를 유지하고 탐색을 지속함(Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search)."
        ],
        "conclusion": "EVOL-RL은 모델의 다양성을 유지하고, 사고의 연쇄를 길고 정보있게 유지하며, 여러 영역에서 강력한 일반화 능력을 보여줌.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2509.15185",
            "authors": [
                {
                    "_id": "68ccc7243df9ac65e93dc679",
                    "user": {
                        "_id": "6662a450b1fff5575fdf0fbd",
                        "avatarUrl": "/avatars/2a6065269f1980213625a9cfd8d42fbd.svg",
                        "isPro": false,
                        "fullname": "Xiaoyu Yue",
                        "user": "YueXY233",
                        "type": "user"
                    },
                    "name": "Xiaoyu Yue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T06:48:19.187Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc67a",
                    "user": {
                        "_id": "64b7aa374df206a3ed1947d2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7aa374df206a3ed1947d2/Ostk72ehOR6yUX-PhUvyQ.jpeg",
                        "isPro": false,
                        "fullname": "wzd",
                        "user": "GoodEnough",
                        "type": "user"
                    },
                    "name": "Zidong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-19T07:00:14.082Z",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc67b",
                    "name": "Yuqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc67c",
                    "name": "Wenlong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc67d",
                    "name": "Xihui Liu",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc67e",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc67f",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "68ccc7243df9ac65e93dc680",
                    "name": "Luping Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-18T17:47:40.000Z",
            "submittedOnDailyAt": "2025-09-19T01:31:06.584Z",
            "title": "Understand Before You Generate: Self-Guided Training for Autoregressive\n  Image Generation",
            "submittedOnDailyBy": {
                "_id": "6662a450b1fff5575fdf0fbd",
                "avatarUrl": "/avatars/2a6065269f1980213625a9cfd8d42fbd.svg",
                "isPro": false,
                "fullname": "Xiaoyu Yue",
                "user": "YueXY233",
                "type": "user"
            },
            "summary": "Recent studies have demonstrated the importance of high-quality visual\nrepresentations in image generation and have highlighted the limitations of\ngenerative models in image understanding. As a generative paradigm originally\ndesigned for natural language, autoregressive models face similar challenges.\nIn this work, we present the first systematic investigation into the mechanisms\nof applying the next-token prediction paradigm to the visual domain. We\nidentify three key properties that hinder the learning of high-level visual\nsemantics: local and conditional dependence, inter-step semantic inconsistency,\nand spatial invariance deficiency. We show that these issues can be effectively\naddressed by introducing self-supervised objectives during training, leading to\na novel training framework, Self-guided Training for AutoRegressive models\n(ST-AR). Without relying on pre-trained representation models, ST-AR\nsignificantly enhances the image understanding ability of autoregressive models\nand leads to improved generation quality. Specifically, ST-AR brings\napproximately 42% FID improvement for LlamaGen-L and 49% FID improvement for\nLlamaGen-XL, while maintaining the same sampling strategy.",
            "upvotes": 22,
            "discussionId": "68ccc7253df9ac65e93dc681",
            "ai_summary": "Self-guided Training for AutoRegressive models (ST-AR) enhances image understanding and generation quality in autoregressive models by addressing key visual semantics challenges through self-supervised objectives.",
            "ai_keywords": [
                "autoregressive models",
                "next-token prediction",
                "high-level visual semantics",
                "local and conditional dependence",
                "inter-step semantic inconsistency",
                "spatial invariance deficiency",
                "self-supervised objectives",
                "Self-guided Training for AutoRegressive models",
                "ST-AR",
                "FID improvement",
                "LlamaGen-L",
                "LlamaGen-XL"
            ]
        },
        "translation_title": "생성하기 전에 이해하기: 자가 지도 학습을 통한 자기 회귀 이미지 생성",
        "purpose": "자가 지도 학습을 통해 자기 회귀 모델의 이미지 이해 능력을 향상시키는 새로운 훈련 프레임워크 개발",
        "method": [
            "시각 영역에 다음 토큰 예측 패러다임을 적용하는 방법에 대한 체계적인 연구를 진행함(we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain.)",
            "학습 과정에서 자가 감독 목표를 도입하여 고차원 시각 의미의 학습을 방해하는 문제를 해결함(we show that these issues can be effectively addressed by introducing self-supervised objectives during training.)",
            "사전 학습된 표현 모델에 의존하지 않고 ST-AR을 통해 자기 회귀 모델의 이미지 이해 능력을 크게 향상시킴(without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models.)"
        ],
        "conclusion": "ST-AR은 LlamaGen-L에 대해 약 42%, LlamaGen-XL에 대해 49%의 FID 개선을 이루어냈으며, 이는 이미지 생성 품질을 향상시킴.",
        "keywords": [
            "Image Generation",
            "Image Understanding",
            "Multimodal Learning"
        ]
    }
]