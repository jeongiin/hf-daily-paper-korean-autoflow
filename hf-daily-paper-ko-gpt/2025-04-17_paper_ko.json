[
    {
        "paper": {
            "id": "2504.10514",
            "authors": [
                {
                    "_id": "67ffedb8b0c26d6ec0b608cf",
                    "name": "Yijun Liang",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d0",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d1",
                    "user": {
                        "_id": "64a8121e35fab7cd04c30ed0",
                        "avatarUrl": "/avatars/48849b84703158772f1022932331b143.svg",
                        "isPro": false,
                        "fullname": "Chenrui Fan",
                        "user": "Fcr09",
                        "type": "user"
                    },
                    "name": "Chenrui Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-17T08:05:05.662Z",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d2",
                    "name": "Ziyue Li",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d3",
                    "name": "Dang Nguyen",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d4",
                    "user": {
                        "_id": "63f546e0fcf95ecac2b0ee3e",
                        "avatarUrl": "/avatars/02a401bcff91cc473d9946bbb771a985.svg",
                        "isPro": false,
                        "fullname": "Kwesi Cobbina",
                        "user": "kweCobi",
                        "type": "user"
                    },
                    "name": "Kwesi Cobbina",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:08:36.225Z",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d5",
                    "user": {
                        "_id": "639d4b8d860db464ae35c3ab",
                        "avatarUrl": "/avatars/ec0fa3e91593a03fc9fb611e66b30553.svg",
                        "isPro": false,
                        "fullname": "Shweta Bhardwaj",
                        "user": "shweta12",
                        "type": "user"
                    },
                    "name": "Shweta Bhardwaj",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:08:29.680Z",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d6",
                    "user": {
                        "_id": "6393847e3e30234ae798b7be",
                        "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
                        "isPro": true,
                        "fullname": "JiuhaiChen",
                        "user": "jiuhai",
                        "type": "user"
                    },
                    "name": "Jiuhai Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:08:22.786Z",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d7",
                    "name": "Fuxiao Liu",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d8",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-17T08:05:07.396Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T16:36:26.000Z",
            "submittedOnDailyAt": "2025-04-17T00:58:33.032Z",
            "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.",
            "upvotes": 24,
            "discussionId": "67ffedbeb0c26d6ec0b60a5b",
            "projectPage": "https://huggingface.co/datasets/umd-zhou-lab/ColorBench",
            "githubRepo": "https://github.com/tianyi-lab/ColorBench",
            "ai_keywords": [
                "vision-language models (VLMs)",
                "color understanding",
                "color perception",
                "color-based cues",
                "color transformations",
                "scaling law",
                "language model",
                "vision encoder",
                "CoT reasoning",
                "multimodal AI"
            ]
        },
        "translation_title": "ColorBench: VLM이 색채를 보고 이해할 수 있는가? 색 인식, 추론 및 견고성을 평가하기 위한 종합 벤치마크",
        "purpose": "VLM의 색 이해 능력을 평가하고 개선하기 위한 기준 마련",
        "method": [
            "다양한 테스트 시나리오를 구성하여 VLM의 색 인식 및 추론 능력을 평가함(ColorBench evaluates how these models perceive colors, infer meanings from color-based cues.)",
            "32개의 VLM을 다양한 언어 모델과 비전 인코더로 포괄적으로 평가함(Through an extensive evaluation of 32 VLMs with varying language models and vision encoders.)",
            "코어 각 모델의 성능 차이를 분석하고 색 이해를 향상시키기 위한 요구 사항을 강조함(These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension.)"
        ],
        "conclusion": "ColorBench를 통해 VLM의 색 이해가 부족하다는 것을 발견하고, 이를 개선하기 위한 기초 도구로 활용될 수 있음을 시사함.",
        "keywords": [
            "Vision-Language Models",
            "Color Perception",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2504.12285",
            "authors": [
                {
                    "_id": "68006e6e175c8dce4ec17f7a",
                    "user": {
                        "_id": "613f07f40153aafa379775f2",
                        "avatarUrl": "/avatars/3965175b320d753d9a5ccb0c7d9298a4.svg",
                        "isPro": false,
                        "fullname": "Shuming Ma",
                        "user": "shumingma",
                        "type": "user"
                    },
                    "name": "Shuming Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:13:17.676Z",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f7b",
                    "user": {
                        "_id": "63f71771d36951307fcb4dcd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
                        "isPro": false,
                        "fullname": "Hongyu Wang",
                        "user": "hongyuw",
                        "type": "user"
                    },
                    "name": "Hongyu Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:14:45.597Z",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f7c",
                    "user": {
                        "_id": "632bd2f72d6a805eeb4bc601",
                        "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
                        "isPro": false,
                        "fullname": "HUANG SHAOHAN",
                        "user": "buaahsh",
                        "type": "user"
                    },
                    "name": "Shaohan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:14:59.750Z",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f7d",
                    "user": {
                        "_id": "64abbcff6cadc7aca584f71b",
                        "avatarUrl": "/avatars/fc6e85ad4a8befd133a37b411712c648.svg",
                        "isPro": false,
                        "fullname": "Xingxing Zhang",
                        "user": "THU-CHUNXIA",
                        "type": "user"
                    },
                    "name": "Xingxing Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:15:06.359Z",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f7e",
                    "name": "Ying Hu",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f7f",
                    "name": "Ting Song",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f80",
                    "name": "Yan Xia",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f81",
                    "user": {
                        "_id": "6368c512fbfe97c16a40baba",
                        "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
                        "isPro": false,
                        "fullname": "Furu Wei",
                        "user": "thegenerality",
                        "type": "user"
                    },
                    "name": "Furu Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:15:42.250Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-16T17:51:43.000Z",
            "submittedOnDailyAt": "2025-04-17T01:29:56.744Z",
            "title": "BitNet b1.58 2B4T Technical Report",
            "submittedOnDailyBy": {
                "_id": "63f71771d36951307fcb4dcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
                "isPro": false,
                "fullname": "Hongyu Wang",
                "user": "hongyuw",
                "type": "user"
            },
            "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
            "upvotes": 19,
            "discussionId": "68006e70175c8dce4ec17fc0",
            "ai_keywords": [
                "BitNet b1.58 2B4T",
                "Large Language Model (LLM)",
                "1-bit Large Language Model",
                "Train",
                "Corpus",
                "4 trillion tokens",
                "Benchmarks",
                "Language understanding",
                "Mathematical reasoning",
                "Coding proficiency",
                "Conversational ability"
            ]
        },
        "translation_title": "BitNet b1.58 2B4T 기술 보고서",
        "purpose": "1비트 대규모 언어 모델(LLM)의 효율성과 성능을 높이기 위한 연구",
        "method": [
            "2억 파라미터를 가진 최초의 오픈 소스, 네이티브 1비트 LLM인 BitNet b1.58 2B4T를 소개함.(We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale.)",
            "4조 토큰으로 구성된 말뭉치에서 모델을 훈련하고, 언어 이해, 수학적 추론, 코딩 능력, 대화 능력에 대한 다양한 기준으로 엄격하게 평가함.(Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability.)",
            "BitNet b1.58 2B4T가 유사한 크기의 최신 오픈 가중치 LLM과 동등한 성능을 달성하며, 연산 효율성에서 유의미한 장점을 제공함을 입증함.(Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency.)"
        ],
        "conclusion": "BitNet b1.58 2B4T는 메모리 사용량과 전력 소모를 크게 줄이면서도 높은 성능을 발휘하며, Hugging Face를 통해 모델 가중치와 오픈 소스 추론 구현을 제공하여 연구와 채택을 촉진함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Computational Efficiency"
        ]
    },
    {
        "paper": {
            "id": "2504.12240",
            "authors": [
                {
                    "_id": "680057b49031335df49732fc",
                    "user": {
                        "_id": "64970d3d9c3b29dca8633f87",
                        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
                        "isPro": false,
                        "fullname": "JunhaoZhuang",
                        "user": "JunhaoZhuang",
                        "type": "user"
                    },
                    "name": "Junhao Zhuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-17T08:04:09.222Z",
                    "hidden": false
                },
                {
                    "_id": "680057b49031335df49732fd",
                    "user": {
                        "_id": "66837d3c48edefb453b0640a",
                        "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
                        "isPro": false,
                        "fullname": "Lingen Li",
                        "user": "l-li",
                        "type": "user"
                    },
                    "name": "Lingen Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:09:48.492Z",
                    "hidden": false
                },
                {
                    "_id": "680057b49031335df49732fe",
                    "user": {
                        "_id": "62d4577bc85b0fcf7fde39bb",
                        "avatarUrl": "/avatars/a3a5729e33ae89ce9ba408830db3c835.svg",
                        "isPro": false,
                        "fullname": "Xuan Ju",
                        "user": "juxuan27",
                        "type": "user"
                    },
                    "name": "Xuan Ju",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:09:55.870Z",
                    "hidden": false
                },
                {
                    "_id": "680057b49031335df49732ff",
                    "name": "Zhaoyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "680057b49031335df4973300",
                    "name": "Chun Yuan",
                    "hidden": false
                },
                {
                    "_id": "680057b49031335df4973301",
                    "user": {
                        "_id": "63ca3ddc04c979828310bfcb",
                        "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
                        "isPro": false,
                        "fullname": "Ying Shan",
                        "user": "yshan2u",
                        "type": "user"
                    },
                    "name": "Ying Shan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:12:47.014Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
            ],
            "publishedAt": "2025-04-16T16:45:19.000Z",
            "submittedOnDailyAt": "2025-04-17T00:32:24.205Z",
            "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
            "submittedOnDailyBy": {
                "_id": "64970d3d9c3b29dca8633f87",
                "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
                "isPro": false,
                "fullname": "JunhaoZhuang",
                "user": "JunhaoZhuang",
                "type": "user"
            },
            "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
            "upvotes": 17,
            "discussionId": "680057b89031335df497343e",
            "projectPage": "https://zhuang2002.github.io/Cobra/",
            "githubRepo": "https://github.com/zhuang2002/Cobra",
            "ai_keywords": [
                "diffusion models",
                "line art colorization",
                "contextual image guidance",
                "color hints",
                "Causal Sparse DiT architecture",
                "positional encodings",
                "causal sparse attention",
                "Key-Value Cache",
                "long-context references",
                "color identity consistency"
            ]
        },
        "translation_title": "코브라: BRoAder References를 활용한 효율적인 선화 색칠",
        "purpose": "만화 제작 산업에서 높은 정확도, 효율성, 맥락 일관성 및 유연한 제어를 가진 참조 기반의 선화 색칠 기술을 개발하기 위함",
        "method": [
            "Cobra라는 효율적이고 다재다능한 방법을 소개하여, 200개 이상의 참조 이미지를 활용하면서도 낮은 지연 시간을 유지함(To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency.)",
            "Cobra의 중앙 구조로 Causal Sparse DiT 아키텍처를 사용하여 긴 맥락 참조를 효과적으로 관리하고 색상 일관성을 보장함(Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency.)"
        ],
        "conclusion": "Cobra는 방대한 맥락 참조를 통해 정확한 선화 색칠을 달성하고, 추론 속도와 상호작용성을 크게 향상시켜 산업의 중요한 요구 사항을 충족시킴.",
        "keywords": [
            "Image Generation",
            "Image Understanding",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2504.10326",
            "authors": [
                {
                    "_id": "67ff772061373fdf16ce1d38",
                    "user": {
                        "_id": "66486ba1640bc89c93bcc8a2",
                        "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
                        "isPro": false,
                        "fullname": "Yangshen Deng",
                        "user": "YangshenDeng",
                        "type": "user"
                    },
                    "name": "Yangshen Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T11:57:58.302Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d39",
                    "name": "Zhengxin You",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d3a",
                    "user": {
                        "_id": "647dd5a6becb41a272925b5e",
                        "avatarUrl": "/avatars/9ec9a3357ac43dcbe350fb3b72f3dbc4.svg",
                        "isPro": false,
                        "fullname": "Long Xiang",
                        "user": "BenjaminXIANG",
                        "type": "user"
                    },
                    "name": "Long Xiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-17T11:10:34.914Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d3b",
                    "user": {
                        "_id": "66d6be0bddf54fd90923c727",
                        "avatarUrl": "/avatars/7bb82c8c339db944d79d47b3b9b35aa8.svg",
                        "isPro": false,
                        "fullname": "Li",
                        "user": "Qilong00",
                        "type": "user"
                    },
                    "name": "Qilong Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:16:16.665Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d3c",
                    "user": {
                        "_id": "66efe50667c4ce2c9024cd45",
                        "avatarUrl": "/avatars/7b7f650953c371f08a5beecc500b6a43.svg",
                        "isPro": false,
                        "fullname": "peiqiyuan",
                        "user": "YuanPeiqi",
                        "type": "user"
                    },
                    "name": "Peiqi Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:16:22.829Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d3d",
                    "name": "Zhaoyang Hong",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d3e",
                    "user": {
                        "_id": "66d6c339b61dd11022907252",
                        "avatarUrl": "/avatars/d2ed3cc003e94b2e5204ce0f8a481dcf.svg",
                        "isPro": false,
                        "fullname": "Yitao Zheng",
                        "user": "FeTieTer",
                        "type": "user"
                    },
                    "name": "Yitao Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:16:36.808Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d3f",
                    "name": "Wanting Li",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d40",
                    "user": {
                        "_id": "671a7bce2b10d343bab18637",
                        "avatarUrl": "/avatars/af1c10a59236d953b42e67d3955eecc4.svg",
                        "isPro": false,
                        "fullname": "runzhong",
                        "user": "runzhongli",
                        "type": "user"
                    },
                    "name": "Runzhong Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:17:02.031Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d41",
                    "user": {
                        "_id": "63898b61ec1f539adc0f4da2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674248167280-63898b61ec1f539adc0f4da2.jpeg",
                        "isPro": false,
                        "fullname": "Haotian Liu",
                        "user": "liuhaotian",
                        "type": "user"
                    },
                    "name": "Haotian Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:17:25.728Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d42",
                    "name": "Kyriakos Mouratidis",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d43",
                    "name": "Man Lung Yiu",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d44",
                    "name": "Huan Li",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d45",
                    "name": "Qiaomu Shen",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d46",
                    "name": "Rui Mao",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d47",
                    "user": {
                        "_id": "66b02a2642c34e7a212133c0",
                        "avatarUrl": "/avatars/737a69b095e8c427ecd08f870b173635.svg",
                        "isPro": false,
                        "fullname": "Bo Tang",
                        "user": "BO1022",
                        "type": "user"
                    },
                    "name": "Bo Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:18:01.460Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T15:34:26.000Z",
            "submittedOnDailyAt": "2025-04-17T06:00:20.705Z",
            "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
            "submittedOnDailyBy": {
                "_id": "66486ba1640bc89c93bcc8a2",
                "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
                "isPro": false,
                "fullname": "Yangshen Deng",
                "user": "YangshenDeng",
                "type": "user"
            },
            "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
            "upvotes": 16,
            "discussionId": "67ff772261373fdf16ce1d93",
            "ai_keywords": [
                "vector database",
                "long-context inference",
                "Large Language Models (LLMs)",
                "KV cache",
                "attention computation",
                "Model as a Service (MaaS)",
                "Service Level Objectives (SLOs)",
                "KV cache disaggregation",
                "retrieval-based sparse attention",
                "query processing procedure",
                "native query optimizer",
                "LLM inference benchmarks"
            ]
        },
        "translation_title": "AlayaDB: 효율적이고 효과적인 장기 컨텍스트 LLM 추론을 위한 데이터 기초",
        "purpose": "Large Language Models(LLMs)의 장기 컨텍스트 추론을 위한 새로운 벡터 데이터베이스 시스템 개발",
        "method": [
            "KV 캐시와 주의(attention) 계산을 LLM 추론 시스템에서 분리하고 이를 새로운 벡터 데이터베이스 시스템으로 캡슐화함( Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into a novel vector database system.)",
            "하드웨어 자원을 적게 소모하고 다양한 서비스 수준 목표(Service Level Objectives, SLOs)에 대해 높은 생성 품질을 제공함(AlayaDB consumes fewer hardware resources and offers higher generation quality for various workloads with different kinds of Service Level Objectives (SLOs), when comparing with the existing alternative solutions.)",
            "주의 계산과 캐시 관리를 쿼리 처리 절차로 추상화하고, 이를 통해 성능을 최적화함(The crux of AlayaDB is that it abstracts the attention computation and cache management for LLM inference into a query processing procedure, and optimizes the performance via a native query optimizer.)"
        ],
        "conclusion": "AlayaDB는 LLM 추론 벤치마크에서 효과성을 입증하고, 산업 파트너들의 세 가지 사용 사례를 통해 성능 개선을 보여줌.",
        "keywords": [
            "Large Language Models",
            "Machine Learning",
            "Query Processing"
        ]
    },
    {
        "paper": {
            "id": "2504.11536",
            "authors": [
                {
                    "_id": "6800cc7159e20f50cc282e87",
                    "name": "Jiazhan Feng",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e88",
                    "user": {
                        "_id": "64ce05c631c655ff8a2e183c",
                        "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
                        "isPro": false,
                        "fullname": "Shijue Huang",
                        "user": "JoeYing",
                        "type": "user"
                    },
                    "name": "Shijue Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:11:05.968Z",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e89",
                    "name": "Xingwei Qu",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e8a",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:11:27.962Z",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e8b",
                    "user": {
                        "_id": "643f37cce9d063936912048b",
                        "avatarUrl": "/avatars/25822ea5676a79b2e1ddf08d5fc2226c.svg",
                        "isPro": false,
                        "fullname": "Yujia Qin",
                        "user": "YujiaHi",
                        "type": "user"
                    },
                    "name": "Yujia Qin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:11:35.665Z",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e8c",
                    "name": "Baoquan Zhong",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e8d",
                    "user": {
                        "_id": "6698fe04a188ffb7e412deb7",
                        "avatarUrl": "/avatars/e389e72e37916b74efc14724d56a0cf1.svg",
                        "isPro": false,
                        "fullname": "Chengquan Jiang",
                        "user": "imjcqt",
                        "type": "user"
                    },
                    "name": "Chengquan Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:11:51.371Z",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e8e",
                    "user": {
                        "_id": "671ca8f1299315f77400863a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lB32V9C2R2D1rAXhmz73w.png",
                        "isPro": false,
                        "fullname": "Jinxin Chi",
                        "user": "chijx",
                        "type": "user"
                    },
                    "name": "Jinxin Chi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:11:57.367Z",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e8f",
                    "user": {
                        "_id": "643f956635e2b54a42e7feba",
                        "avatarUrl": "/avatars/c6185f81ae8499ae866ad451c1cbf43b.svg",
                        "isPro": false,
                        "fullname": "Wanjun Zhong",
                        "user": "WanjunZhong",
                        "type": "user"
                    },
                    "name": "Wanjun Zhong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:12:03.535Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T18:10:22.000Z",
            "submittedOnDailyAt": "2025-04-17T08:10:39.383Z",
            "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.",
            "upvotes": 15,
            "discussionId": "6800cc7359e20f50cc282f43",
            "ai_keywords": [
                "reinforcement learning",
                "dynamic interleaving",
                "real-time code execution",
                "natural language reasoning processes",
                "automated RL paradigm",
                "policy rollouts",
                "multi-turn real-time code execution",
                "synthetic cold-start data generation",
                "code-augmented long-form reasoning traces",
                "fine-tuning",
                "RL training",
                "task outcomes as rewards",
                "autonomous discovery",
                "optimal tool invocation patterns",
                "MATH Olympiad benchmark",
                "AIME",
                "accuracy",
                "training steps",
                "OpenAI's o1-preview",
                "code self-correction",
                "adaptive tool use",
                "hybrid neuro-symbolic systems"
            ]
        },
        "translation_title": "ReTool: LLM에서 전략적 도구 사용을 위한 강화 학습",
        "purpose": "구조화된 문제 해결이 필요한 상황에서 LLM의 긴 글 추론 성능을 향상시키고자 함",
        "method": [
            "자연어 추론 과정에 실시간 코드 실행을 동적으로 섞는 기능을 도입함 (including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes)",
            "다단계 실시간 코드 실행을 통해 도구 호출 시점과 방법을 학습하는 자동 강화 학습(RL) 패러다임을 도입함 (and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback)",
            "합성된 데이터 생성을 통해 코드 향상된 긴 글 추론 흔적을 생성하고 이를 기반 모델 미세 조정에 활용함 (beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models)",
            "RL 훈련을 통해 작업 성과를 보상으로 활용하고 모델의 도구 사용 전략을 점진적으로 개선함 (Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy)"
        ],
        "conclusion": "ReTool는 복잡한 수학적 추론을 향상시키고, 도구 통합의 결과 기반 접근 방식의 가능성을 보여줌.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Robotics"
        ]
    }
]