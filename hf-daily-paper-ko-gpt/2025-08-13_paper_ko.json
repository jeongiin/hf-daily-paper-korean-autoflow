[
    {
        "paper": {
            "id": "2508.05748",
            "authors": [
                {
                    "_id": "689c0152fab6fdd2e52ac85d",
                    "user": {
                        "_id": "682b22ebac526172e1b4ed1b",
                        "avatarUrl": "/avatars/a9e486bf72d27013e6c1903b64a7754c.svg",
                        "isPro": false,
                        "fullname": "Geng Xinyu",
                        "user": "Ornamentt",
                        "type": "user"
                    },
                    "name": "Xinyu Geng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:48.684Z",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac85e",
                    "user": {
                        "_id": "643e9ee6f6bb3c31a26e7bc4",
                        "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
                        "isPro": false,
                        "fullname": "Peng Xia",
                        "user": "richardxp888",
                        "type": "user"
                    },
                    "name": "Peng Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:38.450Z",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac85f",
                    "user": {
                        "_id": "646abd0f8dfd6ff79b6cfbb9",
                        "avatarUrl": "/avatars/3de3b7cbeda95c2b4f460f87f8e9a1f7.svg",
                        "isPro": false,
                        "fullname": "ZhenZhang",
                        "user": "zhzhen23",
                        "type": "user"
                    },
                    "name": "Zhen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:20:47.118Z",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac860",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac861",
                    "name": "Qiuchen Wang",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac862",
                    "name": "Ruixue Ding",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac863",
                    "name": "Chenxi Wang",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac864",
                    "user": {
                        "_id": "644a4fbc2166258fccc664bc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                        "isPro": false,
                        "fullname": "Jialong Wu",
                        "user": "callanwu",
                        "type": "user"
                    },
                    "name": "Jialong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:43.643Z",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac865",
                    "user": {
                        "_id": "66e4019518a1920fb7ca19d7",
                        "avatarUrl": "/avatars/4f8be2f3244239510235c2ec6e8d38a6.svg",
                        "isPro": false,
                        "fullname": "Yida Zhao",
                        "user": "zhaoyd",
                        "type": "user"
                    },
                    "name": "Yida Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:34.963Z",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac866",
                    "name": "Kuan Li",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac867",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac868",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac869",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac86a",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/1DotdrNI5gc_sKLMxtlnq.png",
                "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/VqGc9-ADKeMvExKGjvaWU.png"
            ],
            "publishedAt": "2025-08-07T18:03:50.000Z",
            "submittedOnDailyAt": "2025-08-13T02:05:16.133Z",
            "title": "WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent",
            "submittedOnDailyBy": {
                "_id": "643e9ee6f6bb3c31a26e7bc4",
                "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
                "isPro": false,
                "fullname": "Peng Xia",
                "user": "richardxp888",
                "type": "user"
            },
            "summary": "Web agents such as Deep Research have demonstrated superhuman cognitive\nabilities, capable of solving highly challenging information-seeking problems.\nHowever, most research remains primarily text-centric, overlooking visual\ninformation in the real world. This makes multimodal Deep Research highly\nchallenging, as such agents require much stronger reasoning abilities in\nperception, logic, knowledge, and the use of more sophisticated tools compared\nto text-based agents. To address this limitation, we introduce WebWatcher, a\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\nreasoning capabilities. It leverages high-quality synthetic multimodal\ntrajectories for efficient cold start training, utilizes various tools for deep\nreasoning, and further enhances generalization through reinforcement learning.\nTo better evaluate the capabilities of multimodal agents, we propose\nBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\ninformation retrieval involving both visual and textual information.\nExperimental results show that WebWatcher significantly outperforms proprietary\nbaseline, RAG workflow and open-source agents in four challenging VQA\nbenchmarks, which paves the way for solving complex multimodal\ninformation-seeking tasks.",
            "upvotes": 78,
            "discussionId": "689c0152fab6fdd2e52ac86b",
            "githubRepo": "https://github.com/Alibaba-NLP/WebAgent//",
            "ai_summary": "WebWatcher, a multimodal agent with enhanced visual-language reasoning, outperforms existing agents in complex visual and textual information retrieval tasks using synthetic trajectories and reinforcement learning.",
            "ai_keywords": [
                "multimodal",
                "visual-language reasoning",
                "high-quality synthetic multimodal trajectories",
                "reinforcement learning",
                "BrowseComp-VL",
                "VQA benchmarks"
            ],
            "githubStars": 5989
        },
        "translation_title": "WebWatcher: 비전-언어 심층 연구 에이전트의 새로운 경계를 열다",
        "purpose": "비주얼 정보를 포함한 다중 모달 Deep Research 시스템의 성능 향상",
        "method": [
            "WebWatcher라는 다중 모달 에이전트를 소개함으로써 비전-언어 추론 능력을 개선함(To address this limitation, we introduce WebWatcher, a multi-modal Agent for Deep Research equipped with enhanced visual-language reasoning capabilities.)",
            "고품질 합성 다중 모달 경로를 사용하여 효율적인 초급 훈련을 수행함(It leverages high-quality synthetic multimodal trajectories for efficient cold start training.)",
            "강화 학습을 통해 일반화 능력을 더욱 향상시킴(and further enhances generalization through reinforcement learning.)",
            "비주얼 및 텍스트 정보가 모두 포함된 복잡한 정보 검색을 요구하는 BrowseComp-VL 벤치마크를 제안함(To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark with BrowseComp-style...)."
        ],
        "conclusion": "WebWatcher는 어려운 VQA 벤치마크에서 기존의 솔루션보다 뛰어난 성능을 보여 다중 모달 정보 탐색 과제를 해결하는 새로운 길을 열었다.",
        "keywords": [
            "Multimodal Learning",
            "Vision-Language Models",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2508.08086",
            "authors": [
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e0",
                    "name": "Zhongqi Yang",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e1",
                    "name": "Wenhang Ge",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e2",
                    "name": "Yuqi Li",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e3",
                    "name": "Jiaqi Chen",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e4",
                    "name": "Haoyuan Li",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e5",
                    "user": {
                        "_id": "67a9664af5f1253c64259c50",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Jk9W1C8704pqxNOsqZ0d9.png",
                        "isPro": false,
                        "fullname": "an",
                        "user": "dearamy",
                        "type": "user"
                    },
                    "name": "Mengyin An",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:19:49.471Z",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e6",
                    "user": {
                        "_id": "67a9b36a2fbf63093c19d3de",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a9b36a2fbf63093c19d3de/LbB2XH0ezcJ_tdu4hU0Of.png",
                        "isPro": false,
                        "fullname": "老k",
                        "user": "kangfei",
                        "type": "user"
                    },
                    "name": "Fei Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:19:46.490Z",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e7",
                    "name": "Hua Xue",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e8",
                    "name": "Baixin Xu",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e9",
                    "name": "Yuyang Yin",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4ea",
                    "name": "Eric Li",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4eb",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4ec",
                    "name": "Yikai Wang",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4ed",
                    "name": "Hao-Xiang Guo",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4ee",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64758ee7d815855e4efa206b/gD6oavgX7cDu24qQLbXir.png"
            ],
            "publishedAt": "2025-08-11T15:29:57.000Z",
            "submittedOnDailyAt": "2025-08-13T00:45:23.058Z",
            "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation",
            "submittedOnDailyBy": {
                "_id": "64758ee7d815855e4efa206b",
                "avatarUrl": "/avatars/105ada08a9a982fc7b723bdc678f7e72.svg",
                "isPro": false,
                "fullname": "wenhang ge",
                "user": "spongy",
                "type": "user"
            },
            "summary": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.",
            "upvotes": 45,
            "discussionId": "689ac0a0fab6fdd2e52ac4ef",
            "projectPage": "https://matrix-3d.github.io/",
            "githubRepo": "https://github.com/SkyworkAI/Matrix-3D/tree/main",
            "ai_summary": "Matrix-3D generates wide-coverage 3D worlds from single images or text using panoramic video diffusion and reconstruction models.",
            "ai_keywords": [
                "panoramic representation",
                "wide-coverage",
                "omnidirectional",
                "explorable 3D world generation",
                "conditional video generation",
                "panoramic 3D reconstruction",
                "trajectory-guided",
                "panoramic video diffusion model",
                "scene mesh renders",
                "feed-forward large panorama reconstruction model",
                "optimization-based pipeline",
                "Matrix-Pano dataset",
                "panoramic video generation",
                "3D world generation"
            ],
            "githubStars": 204
        },
        "translation_title": "Matrix-3D: 다방향 탐색 가능한 3D 월드 생성",
        "purpose": "단일 이미지 또는 텍스트 프롬프트를 통해 넓은 범위의 탐색 가능한 3D 월드를 생성하기 위한 방법론 개발",
        "method": [
            "파노라마 표현을 활용한 넓은 범위의 3D 월드 생성을 위해 조건부 비디오 생성 및 파노라마 3D 재구성을 결합하는 Matrix-3D 프레임워크를 제안함(In this work, we propose Matrix-3D, a framework that utilize panoramic representation for wide-coverage omnidirectional explorable 3D world generation that combines conditional video generation and panoramic 3D reconstruction.)",
            "장면 메시 렌더를 조건으로 사용하는 궤적 유도 파노라마 비디오 확산 모델을 훈련하여 고품질 및 기하학적으로 일관된 장면 비디오 생성이 가능함(We first train a trajectory-guided panoramic video diffusion model that employs scene mesh renders as condition, to enable high-quality and geometrically consistent scene video generation.)",
            "3D 장면 재구성을 위해 두 가지 방법을 제안함: (1) 빠른 3D 장면 재구성을 위한 피드포워드 대형 파노라마 재구성 모델과 (2) 정확하고 세부적인 3D 장면 재구성을 위한 최적화 기반 파이프라인을 작성함(To lift the panorama scene video to 3D world, we propose two separate methods: (1) a feed-forward large panorama reconstruction model for rapid 3D scene reconstruction and (2) an optimization-based pipeline for accurate and detailed 3D scene reconstruction.)",
            "116K개의 고품질 정적 파노라마 비디오 시퀀스를 포함한 Matrix-Pano 데이터셋을 도입하여 효과적인 훈련을 지원함(To facilitate effective training, we also introduce the Matrix-Pano dataset, the first large-scale synthetic collection comprising 116K high-quality static panoramic video sequences with depth and trajectory annotations.)"
        ],
        "conclusion": "제안한 프레임워크는 파노라마 비디오 생성 및 3D 월드 생성에서 최첨단 성능을 달성함.",
        "keywords": [
            "3D Vision",
            "Video Generation",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2508.07976",
            "authors": [
                {
                    "_id": "689bf8c2fab6fdd2e52ac825",
                    "name": "Jiaxuan Gao",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac826",
                    "name": "Wei Fu",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac827",
                    "name": "Minyang Xie",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac828",
                    "name": "Shusheng Xu",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac829",
                    "name": "Chuyi He",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac82a",
                    "name": "Zhiyu Mei",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac82b",
                    "name": "Banghua Zhu",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac82c",
                    "name": "Yi Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-11T13:36:57.000Z",
            "submittedOnDailyAt": "2025-08-13T01:34:49.634Z",
            "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL",
            "submittedOnDailyBy": {
                "_id": "63159678915d0b80682fe9f9",
                "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
                "isPro": false,
                "fullname": "Shusheng Xu",
                "user": "xssstory",
                "type": "user"
            },
            "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
            "upvotes": 28,
            "discussionId": "689bf8c2fab6fdd2e52ac82d",
            "githubRepo": "https://github.com/inclusionAI/ASearcher",
            "ai_summary": "ASearcher is an open-source project that uses scalable asynchronous RL training to enhance search agents, achieving high performance on QA tasks with long-horizon search capabilities.",
            "ai_keywords": [
                "LLM-based agents",
                "search tools",
                "Search Intelligence",
                "open-source agents",
                "online RL methods",
                "fully asynchronous RL training",
                "prompt-based LLM agent",
                "QA dataset",
                "xBench",
                "GAIA",
                "Avg@4",
                "tool calls",
                "output tokens",
                "ASearcher-Web-QwQ"
            ],
            "githubStars": 77
        },
        "translation_title": "Beyond Ten Turns: 대규모 비동기 RL로 긴 수평 에이전트 검색 가능성 열기",
        "purpose": "전문가 수준의 Search Intelligence를 달성하기 위해 대규모 RL 훈련을 통해 검색 에이전트를 개선하려는 목표",
        "method": [
            "대규모 비동기 RL 훈련을 도입하여 긴 수평 검색을 가능하게 하면서도 높은 훈련 효율성을 유지함(Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency.)",
            "프롬프트 기반 LLM 에이전트를 통해 고품질의 도전적인 QA를 자동으로 종합하여 대규모 QA 데이터셋을 생성함(2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset.)",
            "RL 훈련을 통해 QwQ-32B 에이전트는 xBench와 GAIA에서 각각 46.7% 및 20.8% Avg@4 개선을 달성함(Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively.)"
        ],
        "conclusion": "ASearcher-Web-QwQ는 기존 오픈소스 32B 에이전트를 초월하는 성능을 기록하며 모델, 훈련 데이터, 코드를 오픈소스화 함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.07409",
            "authors": [
                {
                    "_id": "689c1928fab6fdd2e52ac8a0",
                    "user": {
                        "_id": "663b5f08389689b517109b77",
                        "avatarUrl": "/avatars/b8d66083b9e15c7f7924b4bbc8ca3861.svg",
                        "isPro": false,
                        "fullname": "Gaojunyao",
                        "user": "Gaojunyao",
                        "type": "user"
                    },
                    "name": "Junyao Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:20.003Z",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a1",
                    "user": {
                        "_id": "65464049e70ffa3c07f22e92",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EAYwKThe-BGeza-CmzIdx.jpeg",
                        "isPro": false,
                        "fullname": " Li Jiaxing",
                        "user": "LiJiaxing",
                        "type": "user"
                    },
                    "name": "Jiaxing Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:23.461Z",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a2",
                    "name": "Wenran Liu",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a3",
                    "name": "Yanhong Zeng",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a4",
                    "name": "Fei Shen",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a5",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a6",
                    "name": "Yanan Sun",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a7",
                    "name": "Cairong Zhao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/663b5f08389689b517109b77/unLyeVoJYZogIaDHKufWQ.gif"
            ],
            "publishedAt": "2025-08-10T16:15:04.000Z",
            "submittedOnDailyAt": "2025-08-13T03:19:56.895Z",
            "title": "CharacterShot: Controllable and Consistent 4D Character Animation",
            "submittedOnDailyBy": {
                "_id": "663b5f08389689b517109b77",
                "avatarUrl": "/avatars/b8d66083b9e15c7f7924b4bbc8ca3861.svg",
                "isPro": false,
                "fullname": "Gaojunyao",
                "user": "Gaojunyao",
                "type": "user"
            },
            "summary": "In this paper, we propose CharacterShot, a controllable and\nconsistent 4D character animation framework that enables any individual\ndesigner to create dynamic 3D characters (i.e., 4D character animation) from a\nsingle reference character image and a 2D pose sequence. We begin by\npretraining a powerful 2D character animation model based on a cutting-edge\nDiT-based image-to-video model, which allows for any 2D pose sequnce as\ncontrollable signal. We then lift the animation model from 2D to 3D through\nintroducing dual-attention module together with camera prior to generate\nmulti-view videos with spatial-temporal and spatial-view consistency. Finally,\nwe employ a novel neighbor-constrained 4D gaussian splatting optimization on\nthese multi-view videos, resulting in continuous and stable 4D character\nrepresentations. Moreover, to improve character-centric performance, we\nconstruct a large-scale dataset Character4D, containing 13,115 unique\ncharacters with diverse appearances and motions, rendered from multiple\nviewpoints. Extensive experiments on our newly constructed benchmark,\nCharacterBench, demonstrate that our approach outperforms current\nstate-of-the-art methods. Code, models, and datasets will be publicly available\nat https://github.com/Jeoyal/CharacterShot.",
            "upvotes": 27,
            "discussionId": "689c1928fab6fdd2e52ac8a8",
            "githubRepo": "https://github.com/Jeoyal/CharacterShot",
            "ai_summary": "CharacterShot is a 4D character animation framework that uses a DiT-based model and dual-attention module to generate consistent 3D animations from a single image and 2D pose sequence.",
            "ai_keywords": [
                "DiT-based image-to-video model",
                "dual-attention module",
                "camera prior",
                "neighbor-constrained 4D gaussian splatting",
                "Character4D dataset",
                "CharacterBench benchmark"
            ],
            "githubStars": 20
        },
        "translation_title": "CharacterShot: 제어 가능하며 일관된 4D 캐릭터 애니메이션",
        "purpose": "디자이너가 단일 참조 이미지와 2D 포즈 시퀀스를 통해 동적인 3D 캐릭터를 생성할 수 있는 방법 연구",
        "method": [
            "최신 DiT 기반 이미지-비디오 모델을 바탕으로 강력한 2D 캐릭터 애니메이션 모델을 사전 훈련하여, 모든 2D 포즈 시퀀스를 제어 신호로 활용함(we begin by pretraining a powerful 2D character animation model based on a cutting-edge DiT-based image-to-video model, which allows for any 2D pose sequence as controllable signal).",
            "이중 주의 모듈과 카메라 프라이어를 도입하여 애니메이션 모델을 2D에서 3D로 변환함(we then lift the animation model from 2D to 3D through introducing dual-attention module together with camera prior).",
            "다중 관점 비디오에서 연속적이고 안정적인 4D 캐릭터 표현을 위한 새로운 이웃 제약 4D 가우시안 스플래팅 최적화를 적용함(finally, we employ a novel neighbor-constrained 4D gaussian splatting optimization on these multi-view videos)."
        ],
        "conclusion": "우리의 방법은 현재 최고 성능 방법들을 능가하며, 웹에서 코드와 모델, 데이터셋을 공개할 예정임.",
        "keywords": [
            "Computer Vision",
            "Video Generation",
            "3D Vision"
        ]
    },
    {
        "paper": {
            "id": "2508.09138",
            "authors": [
                {
                    "_id": "689befdbfab6fdd2e52ac80c",
                    "name": "Wen Wang",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac80d",
                    "name": "Bozhen Fang",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac80e",
                    "name": "Chenchen Jing",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac80f",
                    "user": {
                        "_id": "5e1058e9fcf41d740b69966d",
                        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
                        "isPro": false,
                        "fullname": "Yongliang Shen",
                        "user": "tricktreat",
                        "type": "user"
                    },
                    "name": "Yongliang Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:13:02.765Z",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac810",
                    "name": "Yangyi Shen",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac811",
                    "user": {
                        "_id": "64981bea09cea550852652af",
                        "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg",
                        "isPro": false,
                        "fullname": "Qiuyu Wang",
                        "user": "qiuyuu",
                        "type": "user"
                    },
                    "name": "Qiuyu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:13:07.409Z",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac812",
                    "name": "Hao Ouyang",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac813",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac814",
                    "name": "Chunhua Shen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63f089456309c84d5f47f951/znHPdJqj7uNrvTS4acpqM.png"
            ],
            "publishedAt": "2025-08-12T17:59:57.000Z",
            "submittedOnDailyAt": "2025-08-13T00:24:54.833Z",
            "title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "63f089456309c84d5f47f951",
                "avatarUrl": "/avatars/04b926a7f2ad091ee00fef0c59903492.svg",
                "isPro": false,
                "fullname": "Wen Wang",
                "user": "wwen1997",
                "type": "user"
            },
            "summary": "Diffusion large language models (dLLMs) generate text through iterative\ndenoising, yet current decoding strategies discard rich intermediate\npredictions in favor of the final output. Our work here reveals a critical\nphenomenon, temporal oscillation, where correct answers often emerge in the\nmiddle process, but are overwritten in later denoising steps. To address this\nissue, we introduce two complementary methods that exploit temporal\nconsistency: 1) Temporal Self-Consistency Voting, a training-free, test-time\ndecoding strategy that aggregates predictions across denoising steps to select\nthe most consistent output; and 2) a post-training method termed Temporal\nConsistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a\nmeasure of semantic stability across intermediate predictions, as a reward\nsignal to encourage stable generations. Empirical results across multiple\nbenchmarks demonstrate the effectiveness of our approach. Using the negative\nTSE reward alone, we observe a remarkable average improvement of 24.7% on the\nCountdown dataset over an existing dLLM. Combined with the accuracy reward, we\nachieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and\n25.3% on Countdown, respectively. Our findings underscore the untapped\npotential of temporal dynamics in dLLMs and offer two simple yet effective\ntools to harness them.",
            "upvotes": 23,
            "discussionId": "689befdbfab6fdd2e52ac815",
            "projectPage": "https://aim-uofa.github.io/dLLM-MidTruth/",
            "githubRepo": "https://github.com/aim-uofa/dLLM-MidTruth",
            "ai_summary": "Two methods, Temporal Self-Consistency Voting and Temporal Consistency Reinforcement, improve diffusion large language models by leveraging temporal consistency in intermediate predictions.",
            "ai_keywords": [
                "diffusion large language models",
                "denoising",
                "temporal oscillation",
                "Temporal Self-Consistency Voting",
                "Temporal Consistency Reinforcement",
                "Temporal Semantic Entropy",
                "semantic stability",
                "Countdown dataset",
                "GSM8K",
                "MATH500",
                "SVAMP"
            ],
            "githubStars": 29
        },
        "translation_title": "시간은 특징이다: 확산 언어 모델에서 시간적 동력학을 활용하기",
        "purpose": "확산 언어 모델의 중간 예측을 활용하여 출력의 일관성을 높이고 성능을 개선하기 위한 연구",
        "method": [
            "Temporal Self-Consistency Voting을 도입하여 여러 denoising 단계에서 예측을 집계하여 가장 일관된 출력을 선택함.(we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output.)",
            "Temporal Consistency Reinforcement라는 후처리 방법을 사용하여 중간 예측 간의 의미 안정성을 측정하는 Temporal Semantic Entropy (TSE)를 보상 신호로 활용함.(and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations.)",
            "여러 벤치마크에서 경험적인 결과를 통해 접근 방식의 효과성을 입증함.(Empirical results across multiple benchmarks demonstrate the effectiveness of our approach.)"
        ],
        "conclusion": "이 연구는 확산 언어 모델에서 시간적 동력학의 잠재력을 강조하며, 이를 활용하기 위한 두 가지 간단하면서도 효과적인 도구를 제공함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    }
]