[
    {
        "paper": {
            "id": "2512.11558",
            "authors": [
                {
                    "_id": "693f7554f516c693246811d3",
                    "name": "Zhenyang Cai",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811d4",
                    "name": "Jiaming Zhang",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811d5",
                    "name": "Junjie Zhao",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811d6",
                    "user": {
                        "_id": "660244de381ff94818335b67",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660244de381ff94818335b67/Vwt4S739tpURyqgP3IDgd.jpeg",
                        "isPro": false,
                        "fullname": "ZiyiZENG",
                        "user": "CocoNutZENG",
                        "type": "user"
                    },
                    "name": "Ziyi Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-15T08:08:39.120Z",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811d7",
                    "name": "Yanchao Li",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811d8",
                    "name": "Jingyi Liang",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811d9",
                    "name": "Junying Chen",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811da",
                    "name": "Yunjin Yang",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811db",
                    "name": "Jiajun You",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811dc",
                    "name": "Shuzhi Deng",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811dd",
                    "name": "Tongfei Wang",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811de",
                    "name": "Wanting Chen",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811df",
                    "name": "Chunxiu Hao",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811e0",
                    "name": "Ruiqi Xie",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811e1",
                    "name": "Zhenwei Wen",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811e2",
                    "name": "Xiangyi Feng",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811e3",
                    "name": "Zou Ting",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811e4",
                    "name": "Jin Zou Lin",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811e5",
                    "name": "Jianquan Li",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811e6",
                    "name": "Guangjun Yu",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811e7",
                    "name": "Liangyi Chen",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811e8",
                    "name": "Junwen Wang",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811e9",
                    "name": "Shan Jiang",
                    "hidden": false
                },
                {
                    "_id": "693f7554f516c693246811ea",
                    "name": "Benyou Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-12T13:42:57.000Z",
            "submittedOnDailyAt": "2025-12-15T00:14:07.445Z",
            "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
            "submittedOnDailyBy": {
                "_id": "64f1a34f2c5c8b767916447e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1a34f2c5c8b767916447e/uak2CsMAnxW8q4dwyAOBN.jpeg",
                "isPro": false,
                "fullname": "Zhenyang Cai",
                "user": "Eric3200",
                "type": "user"
            },
            "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
            "upvotes": 37,
            "discussionId": "693f7555f516c693246811eb",
            "ai_summary": "DentalGPT, a specialized dental multimodal large language model, achieves superior performance in disease classification and dental VQA tasks through high-quality domain knowledge injection and reinforcement learning.",
            "ai_keywords": [
                "multimodal large language models",
                "dentalGPT",
                "domain knowledge injection",
                "reinforcement learning",
                "annotated multimodal dataset",
                "dental images",
                "visual understanding",
                "multimodal complex reasoning",
                "intraoral benchmarks",
                "panoramic benchmarks",
                "medical VQA benchmarks",
                "disease classification",
                "dental VQA",
                "parameters"
            ]
        },
        "translation_title": "DentalGPT: 치과 분야의 다중 모달 복합 추론 증진",
        "purpose": "치과 진료에서 다중 모달 데이터를 신뢰성 있게 해석하고, 자동화된 구강 건강 관리를 위한 모델 개발",
        "method": [
            "120,000개 이상의 치과 이미지와 상세 설명을 결합하여 가장 큰 주석이 달린 다중 모달 데이터셋을 구축함(we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning.)",
            "해당 데이터셋에서 훈련하여 MLLM의 치과 상태에 대한 시각적 이해를 향상시킴(Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions.)",
            "강화 학습 단계를 통해 다중 모달 복합 추론 능력을 강화함(the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning.)"
        ],
        "conclusion": "DentalGPT는 치과 질병 분류 및 VQA 작업에서 우수한 성능을 보여주며, 고품질 치과 데이터와 단계적 적응 방식이 전문화된 치과 MLLM 구축에 효과적임을 입증함.",
        "keywords": [
            "Multimodal Learning",
            "Large Language Models",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2512.08269",
            "authors": [
                {
                    "_id": "693bc3359874a2a5e4ffb3e3",
                    "user": {
                        "_id": "664df2176bc1025819f81caf",
                        "avatarUrl": "/avatars/964e2e4612fe05fa0cbadf6139c63077.svg",
                        "isPro": false,
                        "fullname": "taewoongkang",
                        "user": "Keh0t0",
                        "type": "user"
                    },
                    "name": "Taewoong Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:18.431Z",
                    "hidden": false
                },
                {
                    "_id": "693bc3359874a2a5e4ffb3e4",
                    "name": "Kinam Kim",
                    "hidden": false
                },
                {
                    "_id": "693bc3359874a2a5e4ffb3e5",
                    "user": {
                        "_id": "681afd78ba7048bcef707648",
                        "avatarUrl": "/avatars/ef0a55c648634f8496e00e29b74f78bc.svg",
                        "isPro": false,
                        "fullname": "Dohyeon Kim",
                        "user": "kdh8156",
                        "type": "user"
                    },
                    "name": "Dohyeon Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-15T08:13:59.047Z",
                    "hidden": false
                },
                {
                    "_id": "693bc3359874a2a5e4ffb3e6",
                    "name": "Minho Park",
                    "hidden": false
                },
                {
                    "_id": "693bc3359874a2a5e4ffb3e7",
                    "name": "Junha Hyung",
                    "hidden": false
                },
                {
                    "_id": "693bc3359874a2a5e4ffb3e8",
                    "name": "Jaegul Choo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/664df2176bc1025819f81caf/JNNjfUSkmFz_bX8SwSwOg.mp4"
            ],
            "publishedAt": "2025-12-09T05:53:39.000Z",
            "submittedOnDailyAt": "2025-12-15T02:29:24.566Z",
            "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
            "submittedOnDailyBy": {
                "_id": "664df2176bc1025819f81caf",
                "avatarUrl": "/avatars/964e2e4612fe05fa0cbadf6139c63077.svg",
                "isPro": false,
                "fullname": "taewoongkang",
                "user": "Keh0t0",
                "type": "user"
            },
            "summary": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.",
            "upvotes": 30,
            "discussionId": "693bc3369874a2a5e4ffb3e9",
            "projectPage": "https://keh0t0.github.io/EgoX/",
            "githubRepo": "https://github.com/KEH0T0/EgoX",
            "githubRepoAddedBy": "user",
            "ai_summary": "EgoX framework generates egocentric videos from exocentric inputs using video diffusion models with LoRA adaptation, unified conditioning, and geometry-guided self-attention for coherence and visual fidelity.",
            "ai_keywords": [
                "video diffusion models",
                "LoRA adaptation",
                "unified conditioning",
                "width and channel wise concatenation",
                "geometry-guided self-attention",
                "egocentric videos",
                "exocentric videos"
            ],
            "githubStars": 10,
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "translation_title": "EgoX: 단일 외관 비디오로부터 자아 중심 비디오 생성",
        "purpose": "자아 중심 비디오 생성을 통해 몰입감 있는 이해를 가능하게 하기 위한 방법 연구",
        "method": [
            "EgoX라는 새로운 프레임워크를 제시해 단일 외관 입력에서 자아 중심 비디오를 생성함(To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input.)",
            "사전 학습된 스페이치오-템포랄 지식을 활용하여 경량화된 LoRA 적응을 통해 효율적으로 처리함(EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation.)",
            "외관과 자아 중심 사전 지식을 결합하는 통일된 조건 부여 전략을 도입함(A unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation is introduced.)",
            "형상 유도 자기 주의 메커니즘을 사용해 시각적으로 관련된 지역에 선택적으로 집중함(A geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity.)"
        ],
        "conclusion": "우리의 접근법은 일관되고 사실적인 자아 중심 비디오 생성을 달성하며, 보이지 않는 비디오와 실세계 비디오에서도 강한 확장성과 강인성을 보여줌.",
        "keywords": [
            "Video Generation",
            "Image Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.11749",
            "authors": [
                {
                    "_id": "693f754ef516c693246811c3",
                    "user": {
                        "_id": "662887715d246621f33d2ce6",
                        "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
                        "isPro": false,
                        "fullname": "Shi Minglei",
                        "user": "MingleiShi",
                        "type": "user"
                    },
                    "name": "Minglei Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-15T08:08:57.314Z",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811c4",
                    "user": {
                        "_id": "641d373d353524fe41f1d453",
                        "avatarUrl": "/avatars/6fa9a0a4ba9818a221f835174a14be2d.svg",
                        "isPro": false,
                        "fullname": "Haolin Wang",
                        "user": "howlin",
                        "type": "user"
                    },
                    "name": "Haolin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-15T08:08:54.781Z",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811c5",
                    "name": "Borui Zhang",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811c6",
                    "name": "Wenzhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811c7",
                    "name": "Bohan Zeng",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811c8",
                    "name": "Ziyang Yuan",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811c9",
                    "name": "Xiaoshi Wu",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811ca",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811cb",
                    "name": "Huan Yang",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811cc",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811cd",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811ce",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811cf",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "693f754ef516c693246811d0",
                    "name": "Jiwen Lu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/V8NQYwKC-8n2SbplK5Ag7.png"
            ],
            "publishedAt": "2025-12-12T17:45:03.000Z",
            "submittedOnDailyAt": "2025-12-15T00:19:12.400Z",
            "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
            "upvotes": 27,
            "discussionId": "693f754ff516c693246811d1",
            "githubRepo": "https://github.com/KlingTeam/SVG-T2I",
            "githubRepoAddedBy": "user",
            "ai_summary": "SVG-T2I, a scaled SVG framework, enables high-quality text-to-image synthesis directly in the Visual Foundation Model feature domain, achieving competitive performance in generative tasks.",
            "ai_keywords": [
                "Visual Foundation Model",
                "VFM",
                "SVG",
                "Self-supervised representations for Visual Generation",
                "SVG-T2I",
                "text-to-image diffusion",
                "GenEval",
                "DPG-Bench",
                "autoencoder",
                "generation model"
            ],
            "githubStars": 32,
            "organization": {
                "_id": "662c559b322afcbae51b3c8b",
                "name": "KlingTeam",
                "fullname": "Kling Team",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
            }
        },
        "translation_title": "SVG-T2I: 변별적 오토인코더 없이 Text-to-Image Latent Diffusion Model 확장하기",
        "purpose": "고품질 text-to-image 생성 지원을 위한 VFM(feature domain)에서의 대규모 diffusion 모델 학습 연구",
        "method": [
            "SVG 프레임워크를 확장하여 VFM 특징 도메인에서 직접 text-to-image 합성을 지원하는 SVG-T2I를 제안함(To bridge this gap, we scale the SVG framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain.)",
            "표준 text-to-image diffusion 파이프라인을 활용하여 competitive 성능을 달성함(By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance...)"
        ],
        "conclusion": "SVG-T2I는 VFM의 본질적인 표현력을 검증하며, 모델 관련 자료를 오픈 소스하여 추가 연구를 장려함.",
        "keywords": [
            "Computer Vision",
            "Image Generation",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.11799",
            "authors": [
                {
                    "_id": "693f741df516c693246811ad",
                    "name": "Ye Fang",
                    "hidden": false
                },
                {
                    "_id": "693f741df516c693246811ae",
                    "name": "Tong Wu",
                    "hidden": false
                },
                {
                    "_id": "693f741df516c693246811af",
                    "name": "Valentin Deschaintre",
                    "hidden": false
                },
                {
                    "_id": "693f741df516c693246811b0",
                    "name": "Duygu Ceylan",
                    "hidden": false
                },
                {
                    "_id": "693f741df516c693246811b1",
                    "name": "Iliyan Georgiev",
                    "hidden": false
                },
                {
                    "_id": "693f741df516c693246811b2",
                    "name": "Chun-Hao Paul Huang",
                    "hidden": false
                },
                {
                    "_id": "693f741df516c693246811b3",
                    "name": "Yiwei Hu",
                    "hidden": false
                },
                {
                    "_id": "693f741df516c693246811b4",
                    "name": "Xuelin Chen",
                    "hidden": false
                },
                {
                    "_id": "693f741df516c693246811b5",
                    "name": "Tuanfeng Yang Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IKbwcaT_GNej00AHtNUA1.mp4"
            ],
            "publishedAt": "2025-12-12T18:59:54.000Z",
            "submittedOnDailyAt": "2025-12-15T00:06:37.373Z",
            "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
            "upvotes": 23,
            "discussionId": "693f741ef516c693246811b6",
            "projectPage": "https://aleafy.github.io/vrgbx/",
            "githubRepo": "https://github.com/Aleafy/V-RGBX",
            "githubRepoAddedBy": "user",
            "ai_summary": "V-RGBX is an end-to-end framework for intrinsic-aware video editing that combines video inverse rendering, photorealistic synthesis, and keyframe-based editing to produce consistent and physically plausible edits.",
            "ai_keywords": [
                "video inverse rendering",
                "intrinsic channels",
                "photorealistic video synthesis",
                "keyframe-based video editing",
                "interleaved conditioning mechanism",
                "object appearance editing",
                "scene-level relighting"
            ],
            "githubStars": 35,
            "organization": {
                "_id": "61e5d14f77496de0a6d95c6b",
                "name": "adobe",
                "fullname": "Adobe",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
            }
        },
        "translation_title": "V-RGBX: 내재적 속성에 대한 정확한 제어를 통한 비디오 편집",
        "purpose": "내재적 장면 속성을 이해하고 이를 기반으로 비디오 합성을 지원하는 편집 가능한 비디오 편집 프레임워크 연구",
        "method": [
            "내재적 채널로 비디오 역 렌더링을 통한 기능 구현(1. video inverse rendering into intrinsic channels)",
            "이 내재적 표현에서 포토리얼리스틱 비디오 합성을 수행함(2. photorealistic video synthesis from these intrinsic representations)",
            "내재적 채널에 따라 키프레임 기반 비디오 편집을 지원함(3. keyframe-based video editing conditioned on intrinsic channels)"
        ],
        "conclusion": "V-RGBX는 물리적으로 믿을 수 있는 방식으로 키프레임 편집을 전파하며, 다양한 응용 프로그램에서 이전 방법보다 우수한 성능을 보여준다.",
        "keywords": [
            "Video Generation",
            "Video Understanding",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2512.10411",
            "authors": [
                {
                    "_id": "693bca119874a2a5e4ffb3fb",
                    "name": "Yijiong Yu",
                    "hidden": false
                },
                {
                    "_id": "693bca119874a2a5e4ffb3fc",
                    "name": "Jiale Liu",
                    "hidden": false
                },
                {
                    "_id": "693bca119874a2a5e4ffb3fd",
                    "name": "Qingyun Wu",
                    "hidden": false
                },
                {
                    "_id": "693bca119874a2a5e4ffb3fe",
                    "name": "Huazheng Wang",
                    "hidden": false
                },
                {
                    "_id": "693bca119874a2a5e4ffb3ff",
                    "name": "Ji Pei",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6374c494958cd71fa7ea0a9d/uufd1O2gmYQsB47R_HezI.png"
            ],
            "publishedAt": "2025-12-11T08:21:24.000Z",
            "submittedOnDailyAt": "2025-12-15T03:40:32.426Z",
            "title": "Sliding Window Attention Adaptation",
            "submittedOnDailyBy": {
                "_id": "6374c494958cd71fa7ea0a9d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/b2SjfvbjYqPCW38LzkzWl.jpeg",
                "isPro": false,
                "fullname": "yuyijiong",
                "user": "yuyijiong",
                "type": "user"
            },
            "summary": "The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation",
            "upvotes": 12,
            "discussionId": "693bca119874a2a5e4ffb400",
            "ai_summary": "Sliding Window Attention Adaptation (SWAA) enables Transformer-based Large Language Models (LLMs) to use sliding window attention without retraining, recovering long-context performance through a combination of adaptation techniques.",
            "ai_keywords": [
                "self-attention mechanism",
                "Transformer-based Large Language Models (LLMs)",
                "sliding window attention (SWA)",
                "full attention (FA)",
                "Sliding Window Attention Adaptation (SWAA)",
                "prefilling",
                "sink tokens",
                "chain-of-thought (CoT)"
            ],
            "organization": {
                "_id": "6897df91ad3033f4085e432c",
                "name": "OregonStateUniversity",
                "fullname": "Oregon State University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6897df118dbb78d2e8837335/ssdqjm2xjvu285uuDBZbd.png"
            }
        },
        "translation_title": "슬라이딩 윈도우 어텐션 적응",
        "purpose": "길고 복잡한 입력에 대해 Transformer 기반의 Large Language Models(LLMs)를 효과적으로 적응시키기 위한 방법을 연구",
        "method": [
            "SLWA를 프리필링 중에만 적용하는 방법을 사용함 (applying SWA only during prefilling;)",
            "모델의 'sink' 토큰을 보존하는 방식으로 접근함 (preserving 'sink' tokens;)",
            "FA와 SWA 레이어를 교차 배치하는 방식을 실험함 (interleaving FA/SWA layers;)",
            "사고의 연쇄(chain-of-thought; CoT) 방식을 포함하여 여러 접근 방식을 결합함 (chain-of-thought;)",
            "미세 조정(fine-tuning)을 통해 성능을 극대화함 (fine-tuning.)"
        ],
        "conclusion": "SLWA는 효과적으로 길고 복잡한 입력자의 성능을 회복할 수 있으며, 특정 조합이 효과적이라는 것을 발견함으로써 여러 상황에 맞춘 권장 레시피를 제공함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    }
]