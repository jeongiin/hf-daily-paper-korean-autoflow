[
    {
        "paper": {
            "id": "2508.11737",
            "authors": [
                {
                    "_id": "68a3e2c0b65388761d07448f",
                    "user": {
                        "_id": "658a8a837959448ef5500ce5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
                        "isPro": false,
                        "fullname": "Shiyin Lu",
                        "user": "runninglsy",
                        "type": "user"
                    },
                    "name": "Shiyin Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:04:40.445Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074490",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074491",
                    "user": {
                        "_id": "637aebed7ce76c3b834cea37",
                        "avatarUrl": "/avatars/78d6dd02d900e4a4b4fd89776b01f4fe.svg",
                        "isPro": false,
                        "fullname": "RainingXY",
                        "user": "xxyyy123",
                        "type": "user"
                    },
                    "name": "Yu Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T10:48:49.351Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074492",
                    "name": "Yuwei Hu",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074493",
                    "user": {
                        "_id": "66ab4c8a1703f12f49583c6d",
                        "avatarUrl": "/avatars/59c77d4556edc049bb410e180813d5e3.svg",
                        "isPro": false,
                        "fullname": "zss",
                        "user": "Suikong",
                        "type": "user"
                    },
                    "name": "Shanshan Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T10:48:43.080Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074494",
                    "name": "Yanqing Ma",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074495",
                    "user": {
                        "_id": "6621ae16e774284ec1714f41",
                        "avatarUrl": "/avatars/0c91af9b001e810dc6b9748fadc9f734.svg",
                        "isPro": false,
                        "fullname": "Zhichao wei",
                        "user": "wzcjojo",
                        "type": "user"
                    },
                    "name": "Zhichao Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-19T14:07:22.136Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074496",
                    "user": {
                        "_id": "66842ad1eef2f5be9e113fb9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SaEuI2ddyT6jzz_i84Jzp.png",
                        "isPro": false,
                        "fullname": "Yinglun Li",
                        "user": "Geralt-Lee",
                        "type": "user"
                    },
                    "name": "Yinglun Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-19T14:07:28.303Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074497",
                    "user": {
                        "_id": "65bef1410a16cbc80b905cbe",
                        "avatarUrl": "/avatars/6482efa09de4a089d4bcef83ab689421.svg",
                        "isPro": false,
                        "fullname": "Lunhao Duan",
                        "user": "Tumen04",
                        "type": "user"
                    },
                    "name": "Lunhao Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-19T14:07:34.397Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074498",
                    "name": "Jianshan Zhao",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074499",
                    "name": "Yuxuan Han",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d07449a",
                    "name": "Haijun Li",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d07449b",
                    "user": {
                        "_id": "6443cc8a9c1bd83bd19c90bf",
                        "avatarUrl": "/avatars/2da4f7930e77737da66082ef36e1e9b0.svg",
                        "isPro": false,
                        "fullname": "WANYING CHEN",
                        "user": "WANYING",
                        "type": "user"
                    },
                    "name": "Wanying Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-19T14:07:56.418Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d07449c",
                    "user": {
                        "_id": "64eda513fdab28c1058e68a8",
                        "avatarUrl": "/avatars/4149c680062eb1e855ed014439155d6e.svg",
                        "isPro": false,
                        "fullname": "Tjunke",
                        "user": "junketang",
                        "type": "user"
                    },
                    "name": "Junke Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-19T14:08:04.775Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d07449d",
                    "name": "Chengkun Hou",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d07449e",
                    "name": "Zhixing Du",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d07449f",
                    "name": "Tianli Zhou",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a0",
                    "name": "Wenjie Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a1",
                    "name": "Huping Ding",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a2",
                    "name": "Jiahe Li",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a3",
                    "name": "Wen Li",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a4",
                    "name": "Gui Hu",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a5",
                    "name": "Yiliang Gu",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a6",
                    "name": "Siran Yang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a7",
                    "name": "Jiamang Wang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a8",
                    "name": "Hailong Sun",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a9",
                    "name": "Yibo Wang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744aa",
                    "name": "Hui Sun",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744ab",
                    "name": "Jinlong Huang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744ac",
                    "name": "Yuping He",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744ad",
                    "name": "Shengze Shi",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744ae",
                    "name": "Weihong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744af",
                    "name": "Guodong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b0",
                    "name": "Junpeng Jiang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b1",
                    "name": "Sensen Gao",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b2",
                    "name": "Yi-Feng Wu",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b3",
                    "name": "Sijia Chen",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b4",
                    "name": "Yuhui Chen",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b5",
                    "user": {
                        "_id": "63e1fde9419922d5a6d41a73",
                        "avatarUrl": "/avatars/8964cbbc64578f839a5a2773dcd565dd.svg",
                        "isPro": false,
                        "fullname": "Qing-Guo Chen",
                        "user": "cqgwin",
                        "type": "user"
                    },
                    "name": "Qing-Guo Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T10:48:46.841Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b6",
                    "name": "Zhao Xu",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b7",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b8",
                    "name": "Kaifu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-15T17:01:08.000Z",
            "submittedOnDailyAt": "2025-08-19T01:10:24.109Z",
            "title": "Ovis2.5 Technical Report",
            "submittedOnDailyBy": {
                "_id": "658a8a837959448ef5500ce5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
                "isPro": false,
                "fullname": "Shiyin Lu",
                "user": "runninglsy",
                "type": "user"
            },
            "summary": "We present Ovis2.5, a successor to Ovis2 designed for native-resolution\nvisual perception and strong multimodal reasoning. Ovis2.5 integrates a\nnative-resolution vision transformer that processes images at their native,\nvariable resolutions, avoiding the degradation from fixed-resolution tiling and\npreserving both fine detail and global layout -- crucial for visually dense\ncontent like complex charts. To strengthen reasoning, we train the model to\nmove beyond linear chain-of-thought and perform reflection -- including\nself-checking and revision. This advanced capability is exposed as an optional\n\"thinking mode\" at inference time, allowing users to trade latency for enhanced\naccuracy on difficult inputs. The model is trained via a comprehensive\nfive-phase curriculum that progressively builds its skills. The process begins\nwith foundational visual and multimodal pretraining, advances through\nlarge-scale instruction tuning, and culminates in alignment and reasoning\nenhancement using DPO and GRPO. To scale these upgrades efficiently, we employ\nmultimodal data packing and hybrid parallelism, yielding a significant\nend-to-end speedup. We release two open-source models: Ovis2.5-9B and\nOvis2.5-2B. The latter continues the \"small model, big performance\" philosophy\nof Ovis2, making it ideal for resource-constrained, on-device scenarios. On the\nOpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a\nsubstantial improvement over its predecessor, Ovis2-8B, and achieving\nstate-of-the-art results among open-source MLLMs in the sub-40B parameter\nrange; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate\nscores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong\ncapabilities on grounding and video tasks, and achieves open-source SOTA at its\nscale for complex chart analysis.",
            "upvotes": 70,
            "discussionId": "68a3e2c0b65388761d0744b9",
            "githubRepo": "https://github.com/AIDC-AI/Ovis",
            "ai_summary": "Ovis2.5, a native-resolution vision transformer with multimodal reasoning, achieves state-of-the-art performance on various benchmarks through advanced training techniques and efficient scaling methods.",
            "ai_keywords": [
                "vision transformer",
                "native-resolution",
                "multimodal reasoning",
                "linear chain-of-thought",
                "reflection",
                "thinking mode",
                "five-phase curriculum",
                "DPO",
                "GRPO",
                "multimodal data packing",
                "hybrid parallelism",
                "OpenCompass",
                "MLLMs",
                "STEM benchmarks",
                "grounding",
                "video tasks",
                "complex chart analysis"
            ],
            "githubStars": 1140
        },
        "translation_title": "Ovis2.5 기술 보고서",
        "purpose": "네이티브 해상도에서의 비주얼 인식과 강력한 멀티모달 추론 능력 향상",
        "method": [
            "네이티브 해상도 비전 트랜스포머를 통합해 원본 해상도 이미지를 처리하고, 고정 해상도 타일링으로 인한 저하를 피함(we integrate a native-resolution vision transformer that processes images at their native, variable resolutions, avoiding the degradation from fixed-resolution tiling).",
            "모델 훈련을 통해 선형 사고를 넘고 스스로 점검하고 수정하는 반사를 수행함(this advanced capability is exposed as an optional 'thinking mode' at inference time, allowing users to trade latency for enhanced accuracy on difficult inputs).",
            "종합적인 다섯 단계 커리큘럼을 통해 시각적 및 멀티모달 프리트레이닝, 대규모 지시 조정, 정렬 및 추론 향상을 수행함(the model is trained via a comprehensive five-phase curriculum that progressively builds its skills)."
        ],
        "conclusion": "Ovis2.5는 멀티모달 리더보드에서 큰 성과를 보여주며, 복잡한 차트 분석에서 오픈소스 SOTA를 달성함.",
        "keywords": [
            "Multimodal Learning",
            "Image Understanding",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2508.10419",
            "authors": [
                {
                    "_id": "68a3e682b65388761d0744d6",
                    "name": "Juyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744d7",
                    "name": "Rongchen Zhao",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744d8",
                    "name": "Wei Wei",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744d9",
                    "name": "Yufeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744da",
                    "name": "Mo Yu",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744db",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744dc",
                    "name": "Jin Xu",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744dd",
                    "user": {
                        "_id": "650f0fac11f3210cf7a8a849",
                        "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
                        "isPro": false,
                        "fullname": "Liyan Xu",
                        "user": "lxucs",
                        "type": "user"
                    },
                    "name": "Liyan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T10:48:40.740Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-14T07:52:09.000Z",
            "submittedOnDailyAt": "2025-08-19T01:21:34.597Z",
            "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long\n  Narrative Reasoning",
            "submittedOnDailyBy": {
                "_id": "650f0fac11f3210cf7a8a849",
                "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
                "isPro": false,
                "fullname": "Liyan Xu",
                "user": "lxucs",
                "type": "user"
            },
            "summary": "Narrative comprehension on long stories and novels has been a challenging\ndomain attributed to their intricate plotlines and entangled, often evolving\nrelations among characters and entities. Given the LLM's diminished reasoning\nover extended context and high computational cost, retrieval-based approaches\nremain a pivotal role in practice. However, traditional RAG methods can fall\nshort due to their stateless, single-step retrieval process, which often\noverlooks the dynamic nature of capturing interconnected relations within\nlong-range context. In this work, we propose ComoRAG, holding the principle\nthat narrative reasoning is not a one-shot process, but a dynamic, evolving\ninterplay between new evidence acquisition and past knowledge consolidation,\nanalogous to human cognition when reasoning with memory-related signals in the\nbrain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes\niterative reasoning cycles while interacting with a dynamic memory workspace.\nIn each cycle, it generates probing queries to devise new exploratory paths,\nthen integrates the retrieved evidence of new aspects into a global memory\npool, thereby supporting the emergence of a coherent context for the query\nresolution. Across four challenging long-context narrative benchmarks (200K+\ntokens), ComoRAG outperforms strong RAG baselines with consistent relative\ngains up to 11% compared to the strongest baseline. Further analysis reveals\nthat ComoRAG is particularly advantageous for complex queries requiring global\ncomprehension, offering a principled, cognitively motivated paradigm for\nretrieval-based long context comprehension towards stateful reasoning. Our code\nis publicly released at https://github.com/EternityJune25/ComoRAG",
            "upvotes": 45,
            "discussionId": "68a3e683b65388761d0744de",
            "githubRepo": "https://github.com/EternityJune25/ComoRAG",
            "ai_summary": "ComoRAG, an iterative retrieval-based approach, enhances long-context narrative comprehension by dynamically updating memory and generating probing queries, outperforming traditional RAG methods.",
            "ai_keywords": [
                "retrieval-based approaches",
                "RAG methods",
                "ComoRAG",
                "iterative reasoning cycles",
                "dynamic memory workspace",
                "probing queries",
                "global memory pool",
                "long-context narrative comprehension",
                "stateful reasoning"
            ],
            "githubStars": 57
        },
        "translation_title": "ComoRAG: 상태 기반 장기 내러티브 추론을 위한 인지 영감을 받은 메모리 조직 RAG",
        "purpose": "복잡한 내러티브의 이해를 개선하기 위한 효율적인 메모리 조직을 통해 장기 내러티브 추론의 정확도를 높이는 것.",
        "method": [
            "ComoRAG는 내러티브 추론이 비단 일회성이 아니라 새로운 증거를 얻고 이전 지식을 통합하는 동적인 프로세스라고 주장함 (we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation.)",
            "추론 정체 상황에서 iterative reasoning cycle을 수행하며 동적인 메모리 작업 공간과 상호작용함 (when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace.)",
            "각 사이클에서 새로운 탐색 경로를 개발하기 위해 쿼리를 생성하고, 새로운 증거를 글로벌 메모리 풀에 통합함 (in each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool.)"
        ],
        "conclusion": "ComoRAG는 200K+ 토큰의 긴 내러티브 환경에서 기존 RAG 방법보다 최대 11%의 성능 향상을 보여 주며, 복잡한 쿼리에 대해 효과적이고 일관된 성과를 달성함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.13154",
            "authors": [
                {
                    "_id": "68a3dfefb65388761d074471",
                    "name": "Zhaoxi Chen",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074472",
                    "name": "Tianqi Liu",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074473",
                    "name": "Long Zhuo",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074474",
                    "name": "Jiawei Ren",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074475",
                    "name": "Zeng Tao",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074476",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074477",
                    "name": "Fangzhou Hong",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074478",
                    "name": "Liang Pan",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074479",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66d347eebb76fb26eedb256e/ll8ni6nZZorhKA7jCZgT_.mp4"
            ],
            "publishedAt": "2025-08-18T17:59:55.000Z",
            "submittedOnDailyAt": "2025-08-19T00:55:49.901Z",
            "title": "4DNeX: Feed-Forward 4D Generative Modeling Made Easy",
            "submittedOnDailyBy": {
                "_id": "66d347eebb76fb26eedb256e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d347eebb76fb26eedb256e/iCPF7GkmZu--XCsWzoucl.jpeg",
                "isPro": false,
                "fullname": "tianqi liu",
                "user": "tqliu",
                "type": "user"
            },
            "summary": "We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,\ndynamic 3D) scene representations from a single image. In contrast to existing\nmethods that rely on computationally intensive optimization or require\nmulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D\ngeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)\nto alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale\ndataset with high-quality 4D annotations generated using advanced\nreconstruction approaches. 2) we introduce a unified 6D video representation\nthat jointly models RGB and XYZ sequences, facilitating structured learning of\nboth appearance and geometry. 3) we propose a set of simple yet effective\nadaptation strategies to repurpose pretrained video diffusion models for 4D\nmodeling. 4DNeX produces high-quality dynamic point clouds that enable\nnovel-view video synthesis. Extensive experiments demonstrate that 4DNeX\noutperforms existing 4D generation methods in efficiency and generalizability,\noffering a scalable solution for image-to-4D modeling and laying the foundation\nfor generative 4D world models that simulate dynamic scene evolution.",
            "upvotes": 41,
            "discussionId": "68a3dfefb65388761d07447a",
            "projectPage": "https://4dnex.github.io/",
            "githubRepo": "https://github.com/3DTopia/4DNeX",
            "ai_summary": "4DNeX generates high-quality dynamic 3D scene representations from a single image using a fine-tuned pretrained video diffusion model, outperforming existing methods in efficiency and generalizability.",
            "ai_keywords": [
                "feed-forward framework",
                "4D scene representations",
                "video diffusion model",
                "4DNeX-10M",
                "6D video representation",
                "RGB",
                "XYZ sequences",
                "dynamic point clouds",
                "novel-view video synthesis",
                "generative 4D world models"
            ],
            "githubStars": 81
        },
        "translation_title": "4DNeX: 간편한 Feed-Forward 4D 생성 모델링",
        "purpose": "단일 이미지에서 4D(동적 3D) 장면 표현을 효율적으로 생성하기 위한 새로운 프레임워크 개발",
        "method": [
            "기존의 계산 집약적 최적화나 멀티 프레임 비디오 입력을 요구하지 않고, 사전 훈련된 비디오 디퓨전 모델을 미세 조정하여 이미지에서 4D 생성을 가능하게 함(4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model.)",
            "고품질 4D 주석이 포함된 대규모 데이터셋인 4DNeX-10M을 구축하여 4D 데이터 부족 문제를 완화함(To alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale dataset with high-quality 4D annotations.).",
            "RGB 및 XYZ 시퀀스를 공동으로 모델링하는 통합된 6D 비디오 표현을 도입하여 형태와 기하학적 구조 학습을 촉진함(we introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry.)",
            "사전 훈련된 비디오 디퓨전 모델을 4D 모델링에 재사용하기 위한 간단하지만 효과적인 적응 전략을 제안함(we propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for 4D modeling.)"
        ],
        "conclusion": "4DNeX는 고품질의 동적 포인트 클라우드를 생성하여 새로운 비디오 합성을 가능하게 하며, 기존 4D 생성 방법들보다 효율성과 일반화에서 우수한 성능을 보여줌.",
        "keywords": [
            "4D Generation",
            "Video Synthesis",
            "Dynamic Scene Representation"
        ]
    },
    {
        "paper": {
            "id": "2508.12811",
            "authors": [
                {
                    "_id": "68a3e68db65388761d0744e0",
                    "user": {
                        "_id": "63463bc4547c70e4b7d3009f",
                        "avatarUrl": "/avatars/6e5350fd998f0a7a4143d7504218164a.svg",
                        "isPro": false,
                        "fullname": "Yikai Wang",
                        "user": "yikaiwang",
                        "type": "user"
                    },
                    "name": "Yikai Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:04:19.759Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e68db65388761d0744e1",
                    "name": "Zhouxia Wang",
                    "hidden": false
                },
                {
                    "_id": "68a3e68db65388761d0744e2",
                    "name": "Zhonghua Wu",
                    "hidden": false
                },
                {
                    "_id": "68a3e68db65388761d0744e3",
                    "name": "Qingyi Tao",
                    "hidden": false
                },
                {
                    "_id": "68a3e68db65388761d0744e4",
                    "name": "Kang Liao",
                    "hidden": false
                },
                {
                    "_id": "68a3e68db65388761d0744e5",
                    "name": "Chen Change Loy",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63463bc4547c70e4b7d3009f/Oc1rdp6mW2ISy-bEA9Uqy.mp4"
            ],
            "publishedAt": "2025-08-18T10:47:37.000Z",
            "submittedOnDailyAt": "2025-08-19T02:48:33.504Z",
            "title": "Next Visual Granularity Generation",
            "submittedOnDailyBy": {
                "_id": "63463bc4547c70e4b7d3009f",
                "avatarUrl": "/avatars/6e5350fd998f0a7a4143d7504218164a.svg",
                "isPro": false,
                "fullname": "Yikai Wang",
                "user": "yikaiwang",
                "type": "user"
            },
            "summary": "We propose a novel approach to image generation by decomposing an image into\na structured sequence, where each element in the sequence shares the same\nspatial resolution but differs in the number of unique tokens used, capturing\ndifferent level of visual granularity. Image generation is carried out through\nour newly introduced Next Visual Granularity (NVG) generation framework, which\ngenerates a visual granularity sequence beginning from an empty image and\nprogressively refines it, from global layout to fine details, in a structured\nmanner. This iterative process encodes a hierarchical, layered representation\nthat offers fine-grained control over the generation process across multiple\ngranularity levels. We train a series of NVG models for class-conditional image\ngeneration on the ImageNet dataset and observe clear scaling behavior. Compared\nto the VAR series, NVG consistently outperforms it in terms of FID scores (3.30\n-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to\nshowcase the capability and potential of the NVG framework. Our code and models\nwill be released.",
            "upvotes": 35,
            "discussionId": "68a3e68db65388761d0744e6",
            "projectPage": "https://yikai-wang.github.io/nvg/",
            "githubRepo": "https://github.com/Yikai-Wang/nvg",
            "ai_summary": "A novel Next Visual Granularity (NVG) framework generates images by iteratively refining a sequence of visual granularities, outperforming existing methods in class-conditional image generation.",
            "ai_keywords": [
                "Next Visual Granularity (NVG)",
                "visual granularity sequence",
                "global layout",
                "fine details",
                "hierarchical",
                "layered representation",
                "class-conditional image generation",
                "ImageNet dataset",
                "FID scores"
            ],
            "githubStars": 5
        },
        "translation_title": "다음 비주얼 세분화 생성",
        "purpose": "이미지 생성을 위해 다양한 수준의 비주얼 세분화를 캡처하는 새로운 접근법 연구",
        "method": [
            "이미지를 구조화된 시퀀스로 분해하여 이미지 생성을 수행함(We propose a novel approach to image generation by decomposing an image into a structured sequence.)",
            "Next Visual Granularity (NVG) 생성을 통해 공백 이미지에서 시작하여 점진적으로 정제하는 프로세스를 적용함(This iterative process encodes a hierarchical, layered representation that offers fine-grained control over the generation process across multiple granularity levels.)",
            "이미지넷 데이터셋을 사용하여 클래스 조건 이미지 생성을 위한 NVG 모델을 훈련함(We train a series of NVG models for class-conditional image generation on the ImageNet dataset.)"
        ],
        "conclusion": "NVG는 기존 VAR 시리즈에 비해 이미지 생성을 더 향상시키며, 모델의 성능을 명확히 개선함.",
        "keywords": [
            "Image Generation",
            "Image Understanding",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2508.09834",
            "authors": [
                {
                    "_id": "68a31244b65388761d074306",
                    "name": "Weigao Sun",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074307",
                    "name": "Jiaxi Hu",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074308",
                    "name": "Yucheng Zhou",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074309",
                    "name": "Jusen Du",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d07430a",
                    "user": {
                        "_id": "66ea643899af9ac3463639b1",
                        "avatarUrl": "/avatars/252d470e761a57834dee3dbc60dfefed.svg",
                        "isPro": false,
                        "fullname": "Disen Lan",
                        "user": "landisen",
                        "type": "user"
                    },
                    "name": "Disen Lan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:05:04.185Z",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d07430b",
                    "name": "Kexin Wang",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d07430c",
                    "name": "Tong Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d07430d",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d07430e",
                    "name": "Yu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d07430f",
                    "name": "Xiaoyu Mo",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074310",
                    "name": "Daizong Liu",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074311",
                    "name": "Yuxuan Liang",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074312",
                    "name": "Wenliang Chen",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074313",
                    "name": "Guoqi Li",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074314",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T14:13:46.000Z",
            "submittedOnDailyAt": "2025-08-19T01:03:34.076Z",
            "title": "Speed Always Wins: A Survey on Efficient Architectures for Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "6246bb33da617c00b48e4d92",
                "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
                "isPro": false,
                "fullname": "Weigao Sun",
                "user": "weigao266",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.",
            "upvotes": 31,
            "discussionId": "68a31244b65388761d074315",
            "projectPage": "https://github.com/weigao266/Awesome-Efficient-Arch",
            "githubRepo": "https://github.com/weigao266/Awesome-Efficient-Arch",
            "ai_summary": "This survey examines innovative architectures for large language models to enhance efficiency, covering linear and sparse sequence modeling, efficient attention mechanisms, sparse mixture-of-experts, hybrid models, and diffusion LLMs.",
            "ai_keywords": [
                "transformer models",
                "linear sequence modeling",
                "sparse sequence modeling",
                "efficient full attention",
                "sparse mixture-of-experts",
                "hybrid model architectures",
                "diffusion LLMs"
            ],
            "githubStars": 104
        },
        "translation_title": "속도의 승리: 대규모 언어 모델을 위한 효율적인 아키텍처 조사",
        "purpose": "대규모 언어 모델의 효율성을 높이는 새로운 아키텍처를 조사하기 위한 연구",
        "method": [
            "전통적인 transformer 아키텍처의 한계를 해결하며 효율성을 높이는 다양한 LLM 아키텍처를 체계적으로 검토함(This survey offers a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency.)",
            "언어 모델링을 시작으로 선형 및 희소 시퀀스 모델링, 효율적인 전체 주의 변형 및 혼합 전문가 모델 아키텍처를 다룸(Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, and sparse mixture-of-experts.)",
            "신기술의 적용을 다른 모달리티에 논의하고, 확장 가능한 기초 모델의 개발을 위해 이들이 미치는 넓은 의의에 대해 고려함(Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models.)"
        ],
        "conclusion": "이 조사는 현대의 효율적인 LLM 아키텍처에 대한 청사진을 제시하며, 향후 연구가 보다 효율적이고 다재다능한 AI 시스템으로 나아가는 데 기여할 것을 희망함.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Efficient Architectures"
        ]
    }
]