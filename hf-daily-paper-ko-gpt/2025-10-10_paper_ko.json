[
    {
        "paper": {
            "id": "2510.08558",
            "authors": [
                {
                    "_id": "68e86eaf95e8e6771df38925",
                    "user": {
                        "_id": "63e0a50242591dda0b9dca5c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e0a50242591dda0b9dca5c/c7cBPEBWQDFYimfGnO_SI.png",
                        "isPro": false,
                        "fullname": "Kai Zhang",
                        "user": "drogozhang",
                        "type": "user"
                    },
                    "name": "Kai Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:42.421Z",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38926",
                    "name": "Xiangchao Chen",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38927",
                    "user": {
                        "_id": "635e3a76106f984574c36409",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
                        "isPro": false,
                        "fullname": "Bo Liu",
                        "user": "Benjamin-eecs",
                        "type": "user"
                    },
                    "name": "Bo Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:47.903Z",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38928",
                    "name": "Tianci Xue",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38929",
                    "name": "Zeyi Liao",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3892a",
                    "name": "Zhihan Liu",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3892b",
                    "user": {
                        "_id": "655fed9fdef5905d38b84af3",
                        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
                        "isPro": false,
                        "fullname": "Xiyao Wang",
                        "user": "russwang",
                        "type": "user"
                    },
                    "name": "Xiyao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:45.647Z",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3892c",
                    "user": {
                        "_id": "65ace92f64c9b93eca5c2bce",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ace92f64c9b93eca5c2bce/pG0JRXH-8zEy0IoaEnMNw.jpeg",
                        "isPro": false,
                        "fullname": "Yuting Ning",
                        "user": "nnnyt",
                        "type": "user"
                    },
                    "name": "Yuting Ning",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T09:12:27.032Z",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3892d",
                    "name": "Zhaorun Chen",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3892e",
                    "name": "Xiaohan Fu",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3892f",
                    "name": "Jian Xie",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38930",
                    "name": "Yuxuan Sun",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38931",
                    "user": {
                        "_id": "6500870f1e14749e84f8f887",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6500870f1e14749e84f8f887/wfvx4BZvh2OyW-vpq5jEy.jpeg",
                        "isPro": false,
                        "fullname": "Boyu Gou",
                        "user": "BoyuNLP",
                        "type": "user"
                    },
                    "name": "Boyu Gou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:38.644Z",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38932",
                    "name": "Qi Qi",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38933",
                    "name": "Zihang Meng",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38934",
                    "name": "Jianwei Yang",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38935",
                    "name": "Ning Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38936",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38937",
                    "name": "Ashish Shah",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38938",
                    "name": "Dat Huynh",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38939",
                    "name": "Hengduo Li",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3893a",
                    "name": "Zi Yang",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3893b",
                    "name": "Sara Cao",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3893c",
                    "name": "Lawrence Jang",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3893d",
                    "name": "Shuyan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3893e",
                    "name": "Jiacheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df3893f",
                    "name": "Huan Sun",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38940",
                    "name": "Jason Weston",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38941",
                    "name": "Yu Su",
                    "hidden": false
                },
                {
                    "_id": "68e86eaf95e8e6771df38942",
                    "name": "Yifan Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:59:17.000Z",
            "submittedOnDailyAt": "2025-10-10T00:55:59.528Z",
            "title": "Agent Learning via Early Experience",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "A long-term goal of language agents is to learn and improve through their own\nexperience, ultimately outperforming humans in complex, real-world tasks.\nHowever, training agents from experience data with reinforcement learning\nremains difficult in many environments, which either lack verifiable rewards\n(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn\ntool use). As a result, most current agents rely on supervised fine-tuning on\nexpert data, which is challenging to scale and generalizes poorly. This\nlimitation stems from the nature of expert demonstrations: they capture only a\nnarrow range of scenarios and expose the agent to limited environment\ndiversity. We address this limitation with a middle-ground paradigm we call\nearly experience: interaction data generated by the agent's own actions, where\nthe resulting future states serve as supervision without reward signals. Within\nthis paradigm we study two strategies of using such data: (1) Implicit world\nmodeling, which uses collected states to ground the policy in environment\ndynamics; and (2) Self-reflection, where the agent learns from its suboptimal\nactions to improve reasoning and decision-making. We evaluate across eight\ndiverse environments and multiple model families. Our approaches consistently\nimprove effectiveness and out-of-domain generalization, highlighting the value\nof early experience. Moreover, in environments with verifiable rewards, our\nresults provide promising signals that early experience offers a strong\nfoundation for subsequent reinforcement learning, positioning it as a practical\nbridge between imitation learning and fully experience-driven agents.",
            "upvotes": 105,
            "discussionId": "68e86eaf95e8e6771df38943",
            "ai_summary": "Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.",
            "ai_keywords": [
                "reinforcement learning",
                "early experience",
                "implicit world modeling",
                "self-reflection",
                "out-of-domain generalization"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "translation_title": "초기 경험을 통한 에이전트 학습",
        "purpose": "에이전트가 자신의 경험을 통해 학습하고 개선하여 복잡한 과제를 수행할 수 있도록 하기 위해 초기 경험 데이터 활용 방안 연구",
        "method": [
            "에이전트의 행동으로 생성된 상호작용 데이터를 이용하여 보상 신호 없이 미래 상태를 감독으로 활용함(we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals.)",
            "수집된 상태를 사용하여 정책을 환경 동역학에 맞추는 암묵적 세계 모델링 전략을 활용함(Implicit world modeling, which uses collected states to ground the policy in environment dynamics;)",
            "에이전트가 비효율적인 행동에서 학습하여 추론과 의사결정을 개선하는 자기 반성 전략을 적용함(Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making.)"
        ],
        "conclusion": "제안한 접근 방식을 통해 각종 환경에서 에이전트의 효과성과 도메인 외 일반화를 지속적으로 개선할 수 있었으며, 초기 경험은 모방 학습과 완전 경험 기반 에이전트 사이의 실용적인 연결 고리 역할을 할 수 있음을 보였다.",
        "keywords": [
            "Reinforcement Learning",
            "Agent Learning",
            "Self-Reflection"
        ]
    },
    {
        "paper": {
            "id": "2510.03279",
            "authors": [
                {
                    "_id": "68e88ed495e8e6771df38b2c",
                    "name": "Youjin Wang",
                    "hidden": false
                },
                {
                    "_id": "68e88ed495e8e6771df38b2d",
                    "name": "Yangjingyi Chen",
                    "hidden": false
                },
                {
                    "_id": "68e88ed495e8e6771df38b2e",
                    "name": "Jiahao Yan",
                    "hidden": false
                },
                {
                    "_id": "68e88ed495e8e6771df38b2f",
                    "name": "Jiaxuan Lu",
                    "hidden": false
                },
                {
                    "_id": "68e88ed495e8e6771df38b30",
                    "name": "Xiao Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T14:40:58.000Z",
            "submittedOnDailyAt": "2025-10-10T03:13:32.550Z",
            "title": "MemMamba: Rethinking Memory Patterns in State Space Model",
            "submittedOnDailyBy": {
                "_id": "67e655f7d6b8333a8f78eadf",
                "avatarUrl": "/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg",
                "isPro": false,
                "fullname": "Jiaxuan Lu",
                "user": "Blue-Giant",
                "type": "user"
            },
            "summary": "With the explosive growth of data, long-sequence modeling has become\nincreasingly important in tasks such as natural language processing and\nbioinformatics. However, existing methods face inherent trade-offs between\nefficiency and memory. Recurrent neural networks suffer from gradient vanishing\nand explosion, making them hard to scale. Transformers can model global\ndependencies but are constrained by quadratic complexity. Recently, selective\nstate-space models such as Mamba have demonstrated high efficiency with O(n)\ntime and O(1) recurrent inference, yet their long-range memory decays\nexponentially. In this work, we conduct mathematical derivations and\ninformation-theoretic analysis to systematically uncover the memory decay\nmechanism of Mamba, answering a fundamental question: what is the nature of\nMamba's long-range memory and how does it retain information? To quantify key\ninformation loss, we further introduce horizontal-vertical memory fidelity\nmetrics that capture degradation both within and across layers. Inspired by how\nhumans distill and retain salient information when reading long documents, we\npropose MemMamba, a novel architectural framework that integrates state\nsummarization mechanism together with cross-layer and cross-token attention,\nwhich alleviates long-range forgetting while preserving linear complexity.\nMemMamba achieves significant improvements over existing Mamba variants and\nTransformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,\nwhile delivering a 48% speedup in inference efficiency. Both theoretical\nanalysis and empirical results demonstrate that MemMamba achieves a\nbreakthrough in the complexity-memory trade-off, offering a new paradigm for\nultra-long sequence modeling.",
            "upvotes": 47,
            "discussionId": "68e88ed495e8e6771df38b31",
            "ai_summary": "MemMamba, a novel architecture integrating state summarization and cross-attention, improves long-range memory and efficiency in sequence modeling compared to Mamba and Transformers.",
            "ai_keywords": [
                "recurrent neural networks",
                "gradient vanishing",
                "gradient explosion",
                "Transformers",
                "quadratic complexity",
                "selective state-space models",
                "Mamba",
                "memory decay",
                "information-theoretic analysis",
                "horizontal-vertical memory fidelity",
                "cross-layer attention",
                "cross-token attention",
                "PG19",
                "Passkey Retrieval",
                "ultra-long sequence modeling"
            ]
        },
        "translation_title": "MemMamba: 상태 공간 모델에서의 메모리 패턴 재고찰",
        "purpose": "자연어 처리와 바이오 인포매틱스와 같은 긴 시퀀스 모델링에서 메모리와 효율성의 균형을 맞추기 위한 연구",
        "method": [
            "수학적 유도 및 정보 이론적 분석을 통해 Mamba의 메모리 감소 메커니즘을 체계적으로 밝혀냄(we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba.)",
            "인간이 긴 문서를 읽을 때 중요한 정보를 유지하는 방법에 영감을 받아, 상태 요약 메커니즘과 교차 레이어 및 교차 토큰 주의를 통합한 새로운 아키텍처인 MemMamba를 제안함(we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention.)",
            "MemMamba가 긴 시퀀스 벤치마크에서 기존 Mamba 변형 및 Transformers보다 상당한 성과를 내고, 추론 효율성을 48% 향상시키는 것을 확인함(MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks and delivers a 48% speedup in inference efficiency.)"
        ],
        "conclusion": "MemMamba는 복잡성과 메모리 간의 균형을 혁신적으로 개선하여 초-long 시퀀스 모델링에 대한 새로운 패러다임을 제공함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2509.23768",
            "authors": [
                {
                    "_id": "68e88b7695e8e6771df38af4",
                    "user": {
                        "_id": "67c443afb753bd020f9c97d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xbACBNLSopWmN5G1K8h_Y.png",
                        "isPro": false,
                        "fullname": "Cheng",
                        "user": "YangC777",
                        "type": "user"
                    },
                    "name": "Cheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:55:23.886Z",
                    "hidden": false
                },
                {
                    "_id": "68e88b7695e8e6771df38af5",
                    "name": "Jiaxuan Lu",
                    "hidden": false
                },
                {
                    "_id": "68e88b7695e8e6771df38af6",
                    "user": {
                        "_id": "65a8bcb717d869bb7487c2a1",
                        "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg",
                        "isPro": false,
                        "fullname": "Haiyuan Wan",
                        "user": "haiyuanwan",
                        "type": "user"
                    },
                    "name": "Haiyuan Wan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T08:55:13.044Z",
                    "hidden": false
                },
                {
                    "_id": "68e88b7695e8e6771df38af7",
                    "name": "Junchi Yu",
                    "hidden": false
                },
                {
                    "_id": "68e88b7695e8e6771df38af8",
                    "name": "Feiwei Qin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-28T09:34:35.000Z",
            "submittedOnDailyAt": "2025-10-10T03:09:29.020Z",
            "title": "From What to Why: A Multi-Agent System for Evidence-based Chemical\n  Reaction Condition Reasoning",
            "submittedOnDailyBy": {
                "_id": "67e655f7d6b8333a8f78eadf",
                "avatarUrl": "/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg",
                "isPro": false,
                "fullname": "Jiaxuan Lu",
                "user": "Blue-Giant",
                "type": "user"
            },
            "summary": "The chemical reaction recommendation is to select proper reaction condition\nparameters for chemical reactions, which is pivotal to accelerating chemical\nscience. With the rapid development of large language models (LLMs), there is\ngrowing interest in leveraging their reasoning and planning capabilities for\nreaction condition recommendation. Despite their success, existing methods\nrarely explain the rationale behind the recommended reaction conditions,\nlimiting their utility in high-stakes scientific workflows. In this work, we\npropose ChemMAS, a multi-agent system that reframes condition prediction as an\nevidence-based reasoning task. ChemMAS decomposes the task into mechanistic\ngrounding, multi-channel recall, constraint-aware agentic debate, and rationale\naggregation. Each decision is backed by interpretable justifications grounded\nin chemical knowledge and retrieved precedents. Experiments show that ChemMAS\nachieves 20-35% gains over domain-specific baselines and outperforms\ngeneral-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable,\nhuman-trustable rationales, which establishes a new paradigm for explainable AI\nin scientific discovery.",
            "upvotes": 41,
            "discussionId": "68e88b7795e8e6771df38af9",
            "ai_summary": "ChemMAS, a multi-agent system, improves reaction condition recommendation by providing interpretable rationales, outperforming existing methods in accuracy and explainability.",
            "ai_keywords": [
                "multi-agent system",
                "evidence-based reasoning",
                "mechanistic grounding",
                "multi-channel recall",
                "constraint-aware agentic debate",
                "rationale aggregation",
                "explainable AI"
            ]
        },
        "translation_title": "무엇에서 왜로: 증거 기반 화학 반응 조건 추론을 위한 다중 에이전트 시스템",
        "purpose": "화학 반응 추천에서 추천 조건의 근거를 제공하여 과학적 작업의 신뢰성을 높이기 위한 연구",
        "method": [
            "ChemMAS라는 다중 에이전트 시스템을 제안해 조건 예측을 증거 기반 추론 태스크로 재구성함(we propose ChemMAS, a multi-agent system that reframes condition prediction as an evidence-based reasoning task.)",
            "작업을 기계적 기초, 다채널 회상, 제약 인식 에이전트 논쟁 및 근거 집합으로 분해함(ChemMAS decomposes the task into mechanistic grounding, multi-channel recall, constraint-aware agentic debate, and rationale aggregation.)",
            "각 결정은 화학적 지식과 이전 사례에 기반한 해석 가능한 근거로 뒷받침됨(Each decision is backed by interpretable justifications grounded in chemical knowledge and retrieved precedents.)"
        ],
        "conclusion": "ChemMAS는 도메인 특화 기준선보다 20-35% 이상의 성과를 달성하고 일반적인 LLM보다 10-15% 더 높은 정확도를 보여주며, 신뢰할 수 있는 근거를 제공하여 과학 발견에서 설명 가능한 AI의 새로운 패러다임을 제시함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Explainable AI"
        ]
    },
    {
        "paper": {
            "id": "2510.08377",
            "authors": [
                {
                    "_id": "68e8705d95e8e6771df38965",
                    "user": {
                        "_id": "64f8e358766ff9f3d2b0de84",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
                        "isPro": true,
                        "fullname": "Cong Wei",
                        "user": "CongWei1230",
                        "type": "user"
                    },
                    "name": "Cong Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-10T04:32:20.837Z",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df38966",
                    "name": "Quande Liu",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df38967",
                    "name": "Zixuan Ye",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df38968",
                    "name": "Qiulin Wang",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df38969",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df3896a",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df3896b",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "68e8705d95e8e6771df3896c",
                    "name": "Wenhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T16:01:30.000Z",
            "submittedOnDailyAt": "2025-10-10T01:03:12.654Z",
            "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Unified multimodal models have shown promising results in multimodal content\ngeneration and editing but remain largely limited to the image domain. In this\nwork, we present UniVideo, a versatile framework that extends unified modeling\nto the video domain. UniVideo adopts a dual-stream design, combining a\nMultimodal Large Language Model (MLLM) for instruction understanding with a\nMultimodal DiT (MMDiT) for video generation. This design enables accurate\ninterpretation of complex multimodal instructions while preserving visual\nconsistency. Built on this architecture, UniVideo unifies diverse video\ngeneration and editing tasks under a single multimodal instruction paradigm and\nis jointly trained across them. Extensive experiments demonstrate that UniVideo\nmatches or surpasses state-of-the-art task-specific baselines in\ntext/image-to-video generation, in-context video generation and in-context\nvideo editing. Notably, the unified design of UniVideo enables two forms of\ngeneralization. First, UniVideo supports task composition, such as combining\nediting with style transfer, by integrating multiple capabilities within a\nsingle instruction. Second, even without explicit training on free-form video\nediting, UniVideo transfers its editing capability from large-scale image\nediting data to this setting, handling unseen instructions such as\ngreen-screening characters or changing materials within a video. Beyond these\ncore capabilities, UniVideo also supports visual-prompt-based video generation,\nwhere the MLLM interprets visual prompts and guides the MMDiT during synthesis.\nTo foster future research, we will release our model and code.",
            "upvotes": 40,
            "discussionId": "68e8705d95e8e6771df3896d",
            "projectPage": "https://congwei1230.github.io/UniVideo/",
            "ai_summary": "UniVideo, a dual-stream framework combining a Multimodal Large Language Model and a Multimodal DiT, extends unified modeling to video generation and editing, achieving state-of-the-art performance and supporting task composition and generalization.",
            "ai_keywords": [
                "Multimodal Large Language Model",
                "Multimodal DiT",
                "dual-stream design",
                "text/image-to-video generation",
                "in-context video generation",
                "in-context video editing",
                "task composition",
                "style transfer",
                "visual-prompt-based video generation"
            ]
        },
        "translation_title": "UniVideo: 비디오에 대한 이해, 생성 및 편집 통합",
        "purpose": "영상 도메인에서 통합 모델링을 확장하고 다양한 비디오 생성 및 편집 작업을 하나의 패러다임 아래 통합하기 위함",
        "method": [
            "Multimodal Large Language Model (MLLM)과 Multimodal DiT (MMDiT)를 결합한 이중 스트림 설계를 채택함(We present UniVideo, a versatile framework that extends unified modeling to the video domain.)",
            "단일 다중 모달 지침 패러다임 아래에서 다양한 비디오 생성 및 편집 작업을 통합하여 공동 훈련함(This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency.)",
            "광범위한 실험을 통해 UniVideo가 텍스트/이미지-비디오 생성 및 편집에서 최첨단의 성능을 초과함(Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines.)"
        ],
        "conclusion": "UniVideo는 비디오 생성 및 편집을 위한 강력한 프레임워크를 제공하며, 향후 연구에 기여하기 위해 모델과 코드를 공개할 예정이다.",
        "keywords": [
            "Video Generation",
            "Multimodal Learning",
            "Video Understanding"
        ]
    }
]