[
    {
        "paper": {
            "id": "2508.13167",
            "authors": [
                {
                    "_id": "68a535f16cf0bf898542ec6c",
                    "name": "Weizhen Li",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec6d",
                    "name": "Jianbo Lin",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec6e",
                    "name": "Zhuosong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec6f",
                    "name": "Jingyi Cao",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec70",
                    "name": "Xinpeng Liu",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec71",
                    "name": "Jiayu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec72",
                    "name": "Zhenqiang Huang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec73",
                    "name": "Qianben Chen",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec74",
                    "name": "Weichen Sun",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec75",
                    "name": "Qiexiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec76",
                    "name": "Hongxuan Lu",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec77",
                    "user": {
                        "_id": "64301abe450c0de9a1d3d18e",
                        "avatarUrl": "/avatars/01b284874dadc7d21d656c53dcb77e42.svg",
                        "isPro": false,
                        "fullname": "tianrui",
                        "user": "tianyue818",
                        "type": "user"
                    },
                    "name": "Tianrui Qin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:31.629Z",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec78",
                    "name": "Chenghao Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec79",
                    "name": "Yi Yao",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec7a",
                    "name": "Shuying Fan",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec7b",
                    "user": {
                        "_id": "68022a47806e99452af4d05e",
                        "avatarUrl": "/avatars/4546956c787242b9950a05cd6ad26a21.svg",
                        "isPro": false,
                        "fullname": "wanwan",
                        "user": "wanwan1212",
                        "type": "user"
                    },
                    "name": "Xiaowan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:19.190Z",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec7c",
                    "name": "Tiannan Wang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec7d",
                    "name": "Pai Liu",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec7e",
                    "user": {
                        "_id": "6578265ddea7e2122d02f6ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578265ddea7e2122d02f6ba/Bh6JjoVF5ceLSjV7Z7nTk.jpeg",
                        "isPro": false,
                        "fullname": "kang zhu",
                        "user": "kangz",
                        "type": "user"
                    },
                    "name": "King Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:21.203Z",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec7f",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec80",
                    "user": {
                        "_id": "657c1f7e688f1a0f7ecfe264",
                        "avatarUrl": "/avatars/265afcb7b0eeddbcf66ec4cdd4920dd3.svg",
                        "isPro": false,
                        "fullname": "Dingfeng Shi",
                        "user": "hugteste",
                        "type": "user"
                    },
                    "name": "Dingfeng Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:24.492Z",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec81",
                    "name": "Piaohong Wang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec82",
                    "name": "Yeyi Guan",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec83",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec84",
                    "user": {
                        "_id": "6417d9ea8f689506e7148417",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
                        "isPro": false,
                        "fullname": "minghao",
                        "user": "Liam-Liu",
                        "type": "user"
                    },
                    "name": "Minghao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:27.711Z",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec85",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec86",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec87",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec88",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:36.844Z",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec89",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T17:01:02.000Z",
            "submittedOnDailyAt": "2025-08-20T01:12:39.713Z",
            "title": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent\n  Distillation and Agentic RL",
            "submittedOnDailyBy": {
                "_id": "628c8598ef14f971b698107f",
                "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
                "isPro": false,
                "fullname": "Zhou",
                "user": "Wangchunshu",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) and multi-agent systems have\ndemonstrated remarkable capabilities in complex problem-solving tasks such as\ndeep research, vibe coding, and mathematical reasoning. However, most existing\nmulti-agent systems are built upon manual prompt/workflow engineering with\nsophisticated agent frameworks, making them computationally inefficient, less\ncapable, and can not benefit from data-centric learning. In this work, we\nintroduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables\nnative end-to-end complex problem-solving in the same way as a multi-agent\nsystem (i.e., multi-turn problem solving with multiple tools and multiple\nagents) within one model. In chain-of-agents problem-solving, the model\ndynamically activates different tool agents and role-playing agents to simulate\nmulti-agent collaboration in an end-to-end fashion. To elicit end-to-end\nchain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent\ndistillation framework to distill state-of-the-art multi-agent systems into\nchain-of-agents trajectories for agentic supervised fine-tuning. We then use\nagentic reinforcement learning on verifiable agentic tasks to further improve\nthe models' capabilities on chain-of-agents problem solving. We call the\nresulting models Agent Foundation Models (AFMs). Our empirical studies\ndemonstrate that AFM establishes new state-of-the-art performance across\ndiverse benchmarks in both web agent and code agent settings. We make the\nentire research, including the model weights, code for training and evaluation,\nand the training data, fully open-sourced, which offers a solid starting point\nfor future research on agent models and agentic RL.",
            "upvotes": 55,
            "discussionId": "68a535f16cf0bf898542ec8a",
            "projectPage": "https://chain-of-agents-afm.github.io/",
            "githubRepo": "https://github.com/OPPO-PersonalAI/Agent_Foundation_Models",
            "ai_summary": "Chain-of-Agents (CoA) paradigm enables end-to-end complex problem-solving in LLMs through dynamic agent activation, improving performance via multi-agent distillation and agentic reinforcement learning.",
            "ai_keywords": [
                "large language models",
                "multi-agent systems",
                "deep research",
                "vibe coding",
                "mathematical reasoning",
                "prompt/workflow engineering",
                "agent frameworks",
                "chain-of-agents",
                "tool agents",
                "role-playing agents",
                "multi-agent distillation",
                "agentic supervised fine-tuning",
                "agentic reinforcement learning",
                "Agent Foundation Models",
                "AFMs",
                "web agent",
                "code agent",
                "verifiable agentic tasks"
            ],
            "githubStars": 58
        },
        "translation_title": "Chain-of-Agents: 다중 에이전트를 통한 엔드 투 엔드 에이전트 기초 모델",
        "purpose": "LLM이 복잡한 문제를 효율적으로 해결할 수 있는 새로운 방식의 연구",
        "method": [
            "Chain-of-Agents(Coa)라는 새로운 LLM 추론 패러다임을 도입하여 다중 에이전트 시스템과 유사한 방식으로 복잡한 문제를 해결하도록 함(we introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables native end-to-end complex problem-solving in the same way as a multi-agent system.)",
            "Multi-agent distillation 프레임워크를 사용해 최첨단 다중 에이전트 시스템을 CoA 경로로 증류함(To elicit end-to-end chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent distillation framework to distill state-of-the-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning.)",
            "검증 가능한 에이전트 작업에서 에이전틱 강화 학습을 통해 모델의 능력을 향상시킴(We then use agentic reinforcement learning on verifiable agentic tasks to further improve the models' capabilities on chain-of-agents problem solving.)"
        ],
        "conclusion": "AFM을 통해 다양한 벤치마크에서 새로운 최첨단 성능을 달성하였으며, 연구 결과와 모델 가중치를 공개하여 향후 에이전트 모델 및 에이전틱 RL 연구의 출발점을 제공함.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2508.14041",
            "authors": [
                {
                    "_id": "68a5270e6cf0bf898542ec12",
                    "user": {
                        "_id": "666afb91e936f6cbcfc8b50c",
                        "avatarUrl": "/avatars/a618c074c9e11e6b9444d0e366efbbdf.svg",
                        "isPro": false,
                        "fullname": "LIN, CHIN-YANG",
                        "user": "linjohnss",
                        "type": "user"
                    },
                    "name": "Chin-Yang Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:57.117Z",
                    "hidden": false
                },
                {
                    "_id": "68a5270e6cf0bf898542ec13",
                    "name": "Cheng Sun",
                    "hidden": false
                },
                {
                    "_id": "68a5270e6cf0bf898542ec14",
                    "name": "Fu-En Yang",
                    "hidden": false
                },
                {
                    "_id": "68a5270e6cf0bf898542ec15",
                    "user": {
                        "_id": "64ae22dd1aee69ece065cdcd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                        "isPro": false,
                        "fullname": "Min-Hung Chen",
                        "user": "cmhungsteve",
                        "type": "user"
                    },
                    "name": "Min-Hung Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:58.995Z",
                    "hidden": false
                },
                {
                    "_id": "68a5270e6cf0bf898542ec16",
                    "name": "Yen-Yu Lin",
                    "hidden": false
                },
                {
                    "_id": "68a5270e6cf0bf898542ec17",
                    "name": "Yu-Lun Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/dCg9OGuyLHLqwelz7AgUB.mp4"
            ],
            "publishedAt": "2025-08-19T17:59:56.000Z",
            "submittedOnDailyAt": "2025-08-20T01:41:48.509Z",
            "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
            "submittedOnDailyBy": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
            },
            "summary": "LongSplat addresses critical challenges in novel view synthesis (NVS) from\ncasually captured long videos characterized by irregular camera motion, unknown\ncamera poses, and expansive scenes. Current methods often suffer from pose\ndrift, inaccurate geometry initialization, and severe memory limitations. To\naddress these issues, we introduce LongSplat, a robust unposed 3D Gaussian\nSplatting framework featuring: (1) Incremental Joint Optimization that\nconcurrently optimizes camera poses and 3D Gaussians to avoid local minima and\nensure global consistency; (2) a robust Pose Estimation Module leveraging\nlearned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that\nconverts dense point clouds into anchors based on spatial density. Extensive\nexperiments on challenging benchmarks demonstrate that LongSplat achieves\nstate-of-the-art results, substantially improving rendering quality, pose\naccuracy, and computational efficiency compared to prior approaches. Project\npage: https://linjohnss.github.io/longsplat/",
            "upvotes": 38,
            "discussionId": "68a5270e6cf0bf898542ec18",
            "projectPage": "https://linjohnss.github.io/longsplat/",
            "githubRepo": "https://github.com/NVlabs/LongSplat",
            "ai_summary": "LongSplat improves novel view synthesis from long videos with irregular motion through joint optimization, robust pose estimation, and efficient anchor formation.",
            "ai_keywords": [
                "novel view synthesis",
                "3D Gaussian Splatting",
                "Incremental Joint Optimization",
                "camera poses",
                "3D Gaussians",
                "Pose Estimation Module",
                "3D priors",
                "Octree Anchor Formation",
                "dense point clouds",
                "anchors",
                "rendering quality",
                "pose accuracy",
                "computational efficiency"
            ],
            "githubStars": 84
        },
        "translation_title": "LongSplat: 비정형 긴 비디오를 위한 강력한 무위치 3D Gaussian Splatting",
        "purpose": "비정형 긴 비디오에서의 새로운 시점 합성 문제를 해결하고자 함",
        "method": [
            "Incremental Joint Optimization을 도입하여 카메라 위치와 3D Gaussian을 동시에 최적화함으로써 지역 최소값을 피하고 글로벌 일관성을 보장함(We introduce LongSplat, a robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency;)",
            "학습된 3D 선행 지식을 활용한 정교한 Pose Estimation Module을 구축함(2) a robust Pose Estimation Module leveraging learned 3D priors;)",
            "밀집 포인트 클라우드를 공간 밀도에 따라 앵커로 변환하는 효율적인 Octree Anchor Formation 메커니즘을 구현함(3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density."
        ],
        "conclusion": "LongSplat은 렌더링 품질, 포즈 정확성, 계산 효율성을 크게 향상시켜 이전 접근 방식에 비해 최첨단 결과를 달성함.",
        "keywords": [
            "3D Vision",
            "Pose Estimation",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2508.13948",
            "authors": [
                {
                    "_id": "68a5366b6cf0bf898542ec8c",
                    "name": "Yuge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a5366b6cf0bf898542ec8d",
                    "name": "Nan Chen",
                    "hidden": false
                },
                {
                    "_id": "68a5366b6cf0bf898542ec8e",
                    "user": {
                        "_id": "62abdf657b037eafffc48808",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655430982462-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Jiahang Xu",
                        "user": "Jiahang",
                        "type": "user"
                    },
                    "name": "Jiahang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:16.884Z",
                    "hidden": false
                },
                {
                    "_id": "68a5366b6cf0bf898542ec8f",
                    "name": "Yuqing Yang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6466d323ac657f60661d2778/rnVtFjaxiAUmMm2vyb7Uu.mp4"
            ],
            "publishedAt": "2025-08-19T15:37:29.000Z",
            "submittedOnDailyAt": "2025-08-20T01:28:00.780Z",
            "title": "Prompt Orchestration Markup Language",
            "submittedOnDailyBy": {
                "_id": "6466d323ac657f60661d2778",
                "avatarUrl": "/avatars/62f70630cdf1c252b80b4d5eaa5a4150.svg",
                "isPro": false,
                "fullname": "Yuge Zhang",
                "user": "ultmaster",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios.",
            "upvotes": 20,
            "discussionId": "68a5366c6cf0bf898542ec90",
            "projectPage": "https://microsoft.github.io/poml/",
            "githubRepo": "https://github.com/microsoft/poml",
            "ai_summary": "POML addresses challenges in prompting Large Language Models by providing a structured, data-integrated, and format-sensitive markup language with templating and developer tools.",
            "ai_keywords": [
                "POML",
                "Prompt Orchestration Markup Language",
                "component-based markup",
                "specialized tags",
                "CSS-like styling system",
                "templating",
                "developer toolkit",
                "IDE support",
                "SDKs",
                "version control",
                "collaboration",
                "PomLink",
                "TableQA"
            ],
            "githubStars": 3502
        },
        "translation_title": "프롬프트 오케스트레이션 마크업 언어",
        "purpose": "대규모 언어 모델의 프롬프트 구조, 데이터 통합, 형식 감수성을 개선하기 위한 시스템 구축",
        "method": [
            "POML(프롬프트 오케스트레이션 마크업 언어)를 소개하고, 구성 요소 기반 마크업 방식을 사용해 논리적인 구조를 구현함(we introduce POML (Prompt Orchestration Markup Language).)",
            "특수 태그를 통해 다양한 데이터 통합을 원활하게 처리하도록 함(POML employs specialized tags for seamless data integration.)",
            "CSS와 유사한 스타일링 시스템을 통해 콘텐츠와 프레젠테이션을 분리해 형식 감수성을 감소시킴(and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity.)",
            "템플릿 기능과 개발자 도구 키트를 포함해 버전 관리 및 협업을 개선함(It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration.)"
        ],
        "conclusion": "POML은 복잡한 애플리케이션 통합과 정확성 수행에 긍정적인 영향을 미쳤으며, 실제 개발 시나리오에서도 효과적이라는 것을 확인함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.06905",
            "authors": [
                {
                    "_id": "689d51b2b083e610d741e9e4",
                    "name": "Ruoxi Chen",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9e5",
                    "name": "Dongping Chen",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9e6",
                    "name": "Siyuan Wu",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9e7",
                    "user": {
                        "_id": "67b589ab0914159f2e33f7d2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qZRV_ePJ1zaRxZrg7Ea4K.jpeg",
                        "isPro": false,
                        "fullname": "Sinan Wang",
                        "user": "wsnHowest",
                        "type": "user"
                    },
                    "name": "Sinan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:53:24.787Z",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9e8",
                    "name": "Shiyun Lang",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9e9",
                    "name": "Petr Sushko",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9ea",
                    "name": "Gaoyang Jiang",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9eb",
                    "name": "Yao Wan",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9ec",
                    "name": "Ranjay Krishna",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-09T09:36:21.000Z",
            "submittedOnDailyAt": "2025-08-20T03:20:45.314Z",
            "title": "MultiRef: Controllable Image Generation with Multiple Visual References",
            "submittedOnDailyBy": {
                "_id": "643be8879f5d314db2d9ed23",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
                "isPro": false,
                "fullname": "Chen Dongping",
                "user": "shuaishuaicdp",
                "type": "user"
            },
            "summary": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/.",
            "upvotes": 13,
            "discussionId": "689d51b2b083e610d741e9ed",
            "ai_summary": "Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.",
            "ai_keywords": [
                "MultiRef-bench",
                "RefBlend",
                "MultiRef",
                "OmniGen",
                "ACE",
                "Show-o",
                "ChatDiT",
                "LLM + SD"
            ]
        },
        "translation_title": "MultiRef: 여러 시각 참조를 활용한 제어 가능한 이미지 생성",
        "purpose": "여러 시각 참조를 사용하여 이미지 생성을 제어하기 위한 방법론 개발",
        "method": [
            "MultiRef-bench라는 990개의 합성 샘플과 1,000개의 실제 샘플을 포함하는 평가 프레임워크를 소개함(We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images.)",
            "RefBlend라는 데이터 엔진을 사용해 10가지 참조 유형과 33가지 참조 조합으로 합성 샘플을 생성함(The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations.)",
            "MultiRef라는 이름의 38,000개의 고품질 이미지를 포함하는 데이터셋을 구축하여 추가 연구를 촉진함(Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research.)"
        ],
        "conclusion": "현재의 최첨단 시스템도 다중 참조 조건화에서 어려움을 겪고 있으며, 이러한 결과는 보다 유연하고 인간과 같은 창의적 도구 개발에 귀중한 방향을 제시함.",
        "keywords": [
            "Image Generation",
            "Image Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.08777",
            "authors": [
                {
                    "_id": "68a5825738160bfd39426a54",
                    "user": {
                        "_id": "65eed0a600f1a613dae4d9be",
                        "avatarUrl": "/avatars/a420d0e2c77a482eb5022deeccb2ebf5.svg",
                        "isPro": false,
                        "fullname": "Francesco Fabbri",
                        "user": "frafabbri",
                        "type": "user"
                    },
                    "name": "Francesco Fabbri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:50:51.265Z",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a55",
                    "name": "Gustavo Penha",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a56",
                    "name": "Edoardo D'Amico",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a57",
                    "name": "Alice Wang",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a58",
                    "user": {
                        "_id": "626193a918176506fab35c62",
                        "avatarUrl": "/avatars/44a34405e4821fa9047cfa635e198f61.svg",
                        "isPro": false,
                        "fullname": "Marco De Nadai",
                        "user": "marcodena",
                        "type": "user"
                    },
                    "name": "Marco De Nadai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:50:59.013Z",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a59",
                    "name": "Jackie Doremus",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a5a",
                    "name": "Paul Gigioli",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a5b",
                    "name": "Andreas Damianou",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a5c",
                    "name": "Oskar Stal",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a5d",
                    "name": "Mounia Lalmas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T09:23:35.000Z",
            "submittedOnDailyAt": "2025-08-20T06:38:30.390Z",
            "title": "Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge",
            "submittedOnDailyBy": {
                "_id": "626193a918176506fab35c62",
                "avatarUrl": "/avatars/44a34405e4821fa9047cfa635e198f61.svg",
                "isPro": false,
                "fullname": "Marco De Nadai",
                "user": "marcodena",
                "type": "user"
            },
            "summary": "Evaluating personalized recommendations remains a central challenge,\nespecially in long-form audio domains like podcasts, where traditional offline\nmetrics suffer from exposure bias and online methods such as A/B testing are\ncostly and operationally constrained. In this paper, we propose a novel\nframework that leverages Large Language Models (LLMs) as offline judges to\nassess the quality of podcast recommendations in a scalable and interpretable\nmanner. Our two-stage profile-aware approach first constructs natural-language\nuser profiles distilled from 90 days of listening history. These profiles\nsummarize both topical interests and behavioral patterns, serving as compact,\ninterpretable representations of user preferences. Rather than prompting the\nLLM with raw data, we use these profiles to provide high-level, semantically\nrich context-enabling the LLM to reason more effectively about alignment\nbetween a user's interests and recommended episodes. This reduces input\ncomplexity and improves interpretability. The LLM is then prompted to deliver\nfine-grained pointwise and pairwise judgments based on the profile-episode\nmatch. In a controlled study with 47 participants, our profile-aware judge\nmatched human judgments with high fidelity and outperformed or matched a\nvariant using raw listening histories. The framework enables efficient,\nprofile-aware evaluation for iterative testing and model selection in\nrecommender systems.",
            "upvotes": 10,
            "discussionId": "68a5826638160bfd39426a5e",
            "ai_summary": "A novel framework uses Large Language Models to evaluate podcast recommendations by constructing user profiles and providing context for the LLM to make judgments, improving efficiency and interpretability.",
            "ai_keywords": [
                "Large Language Models",
                "profile-aware",
                "natural-language user profiles",
                "semantic context",
                "pointwise judgments",
                "pairwise judgments",
                "recommender systems"
            ]
        },
        "translation_title": "프로필 인식을 통한 팟캐스트 추천 평가를 위한 LLM 활용",
        "purpose": "개인화된 팟캐스트 추천의 품질을 평가하고 개선하기 위한 새로운 프레임워크 개발",
        "method": [
            "90일 청취 기록을 기반으로 자연어 사용자 프로필을 생성하여 사용자 선호도를 요약함(Our two-stage profile-aware approach first constructs natural-language user profiles distilled from 90 days of listening history.)",
            "사용자 프로필을 활용해 LLM이 추천 에피소드와 사용자 관심사 간의 정렬을 더 잘 이해하도록 함(We use these profiles to provide high-level, semantically rich context-enabling the LLM to reason more effectively about alignment between a user's interests and recommended episodes.)",
            "47명의 참가자를 대상으로 한 연구에서 LLM 기반 평가가 인간 평가와 높은 일치를 보임(In a controlled study with 47 participants, our profile-aware judge matched human judgments with high fidelity.)"
        ],
        "conclusion": "이 프레임워크는 추천 시스템에서 효율적이고 프로필 인식 기반의 평가를 가능하게 하여 반복적인 테스트와 모델 선택에 기여함.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Recommender Systems"
        ]
    }
]