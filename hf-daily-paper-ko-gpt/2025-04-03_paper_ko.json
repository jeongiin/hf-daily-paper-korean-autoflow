[
    {
        "paper": {
            "id": "2504.00999",
            "authors": [
                {
                    "_id": "67ecc3973d267d266649e075",
                    "user": {
                        "_id": "640f7083208821a59b74c757",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678735253848-640f7083208821a59b74c757.jpeg",
                        "isPro": false,
                        "fullname": "Siyuan Li",
                        "user": "Lupin1998",
                        "type": "user"
                    },
                    "name": "Siyuan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T08:28:38.819Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e076",
                    "user": {
                        "_id": "671b4781d2f774c5ec9ebd62",
                        "avatarUrl": "/avatars/b4f1cbaa6e092eda005f81f199a35e19.svg",
                        "isPro": false,
                        "fullname": "Luyuan Zhang",
                        "user": "LuyuanZhang01",
                        "type": "user"
                    },
                    "name": "Luyuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T08:28:41.242Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e077",
                    "user": {
                        "_id": "6594d390674349122ce6f368",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/luDBiSMX_9l8QEpAQu3HJ.jpeg",
                        "isPro": false,
                        "fullname": "Zedong Wang",
                        "user": "ZedongWangAI",
                        "type": "user"
                    },
                    "name": "Zedong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T07:52:39.150Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e078",
                    "user": {
                        "_id": "670880950e79a8b46f7ff9dd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
                        "isPro": false,
                        "fullname": "Juanxi Tian",
                        "user": "Juanxi",
                        "type": "user"
                    },
                    "name": "Juanxi Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T08:28:43.436Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e079",
                    "name": "Cheng Tan",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e07a",
                    "user": {
                        "_id": "67ee7eef2a8e2fd1445407ab",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/TelMtjU8Ki8ulQU4-b0He.jpeg",
                        "isPro": false,
                        "fullname": "Zicheng Liu",
                        "user": "MarcusB3n",
                        "type": "user"
                    },
                    "name": "Zicheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:34:22.460Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e07b",
                    "user": {
                        "_id": "63578f79a1f8ad1c31bd2148",
                        "avatarUrl": "/avatars/e91cf0a7c71a9533556267e67bf0610f.svg",
                        "isPro": false,
                        "fullname": "Y",
                        "user": "CY-7",
                        "type": "user"
                    },
                    "name": "Chang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T13:33:08.711Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e07c",
                    "user": {
                        "_id": "64184911db24526c7c9ca035",
                        "avatarUrl": "/avatars/2be15485bdd9e00857d4bff107d1577e.svg",
                        "isPro": false,
                        "fullname": "qingsong xie",
                        "user": "kongming11",
                        "type": "user"
                    },
                    "name": "Qingsong Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:34:47.666Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e07d",
                    "user": {
                        "_id": "603864181b642aef9350af81",
                        "avatarUrl": "/avatars/27ec0b885cbedef007876c8c98c69d15.svg",
                        "isPro": false,
                        "fullname": "Haonan Lu",
                        "user": "luhaonan20",
                        "type": "user"
                    },
                    "name": "Haonan Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:34:57.853Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e07e",
                    "name": "Haoqian Wang",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e07f",
                    "name": "Zhen Lei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T17:39:19.000Z",
            "submittedOnDailyAt": "2025-04-03T00:15:32.614Z",
            "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
            "submittedOnDailyBy": {
                "_id": "670880950e79a8b46f7ff9dd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
                "isPro": false,
                "fullname": "Juanxi Tian",
                "user": "Juanxi",
                "type": "user"
            },
            "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
            "upvotes": 55,
            "discussionId": "67ecc3993d267d266649e10c",
            "projectPage": "https://apexgen-x.github.io/MergeVQ/",
            "githubRepo": "https://github.com/ApexGen-X/MergeVQ",
            "ai_keywords": [
                "Masked Image Modeling (MIM)",
                "Vector Quantization (VQ)",
                "shared latent space",
                "generation quality",
                "representation learning",
                "token merging",
                "generative models",
                "token merge module",
                "self-attention blocks",
                "encoder",
                "Look-up Free Quantization (LFQ)",
                "global alignment",
                "cross-attention",
                "decoder",
                "reconstruction",
                "MergeAR",
                "KV Cache compression",
                "raster-order prediction",
                "AR generative model",
                "ImageNet",
                "token efficiency",
                "inference speed"
            ]
        },
        "translation_title": "MergeVQ: 시각적 생성 및 표현을 위한 통합 프레임워크 (토큰 병합 및 양자화를 통한)",
        "purpose": "이미지 생성과 시각적 표현 학습 간의 균형을 맞추기 위해 새로운 모델 제안",
        "method": [
            "MergeVQ는 VQ 기반 생성 모델에 토큰 병합 기법을 도입하여 이미지 생성과 시각적 표현 학습을 통합하는 아키텍처를 개발함.(we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture.)",
            "MergeVQ는 인코더에서 셀프 어텐션 블록 이후 토큰 병합 모듈을 사용하여 상위 k 의미를 분리하고, 디코더에서 크로스 어텐션을 통해 세부 정보를 복원함.(MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction.)",
            "MergeAR을 통해 효율적인 레스터 순서 예측을 위한 KV Cache 압축을 수행함.(As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction.)"
        ],
        "conclusion": "MergeVQ는 이미지넷에서 시각적 표현 학습과 이미지 생성 작업 모두에서 경쟁력 있는 성능을 달성하며, 토큰 효율성과 추론 속도를 유지함.",
        "keywords": [
            "Image Generation",
            "Image Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2504.00883",
            "authors": [
                {
                    "_id": "67edf28e042e8ba3e95d1960",
                    "user": {
                        "_id": "67ed94169723a841a7128c6f",
                        "avatarUrl": "/avatars/950397084cc48c0885cf85bd3e5260b1.svg",
                        "isPro": false,
                        "fullname": "ZhenYi Liao",
                        "user": "ZhenYiao",
                        "type": "user"
                    },
                    "name": "Zhenyi Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:44:50.157Z",
                    "hidden": false
                },
                {
                    "_id": "67edf28e042e8ba3e95d1961",
                    "user": {
                        "_id": "64184911db24526c7c9ca035",
                        "avatarUrl": "/avatars/2be15485bdd9e00857d4bff107d1577e.svg",
                        "isPro": false,
                        "fullname": "qingsong xie",
                        "user": "kongming11",
                        "type": "user"
                    },
                    "name": "Qingsong Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:45:08.446Z",
                    "hidden": false
                },
                {
                    "_id": "67edf28e042e8ba3e95d1962",
                    "user": {
                        "_id": "6527849a5fc08c60dc6a3120",
                        "avatarUrl": "/avatars/54651dc48acd119c012520a616285e7a.svg",
                        "isPro": false,
                        "fullname": "Yanhao Zhang",
                        "user": "YanhaoZhang",
                        "type": "user"
                    },
                    "name": "Yanhao Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:45:23.767Z",
                    "hidden": false
                },
                {
                    "_id": "67edf28e042e8ba3e95d1963",
                    "name": "Zijian Kong",
                    "hidden": false
                },
                {
                    "_id": "67edf28e042e8ba3e95d1964",
                    "user": {
                        "_id": "603864181b642aef9350af81",
                        "avatarUrl": "/avatars/27ec0b885cbedef007876c8c98c69d15.svg",
                        "isPro": false,
                        "fullname": "Haonan Lu",
                        "user": "luhaonan20",
                        "type": "user"
                    },
                    "name": "Haonan Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:45:39.183Z",
                    "hidden": false
                },
                {
                    "_id": "67edf28e042e8ba3e95d1965",
                    "user": {
                        "_id": "63f53e493aa49d8cb97b0aad",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f53e493aa49d8cb97b0aad/QJDAwDaf2jryMXyDr4LTK.png",
                        "isPro": false,
                        "fullname": "Zhen-Yu Yang",
                        "user": "zhenyuyang",
                        "type": "user"
                    },
                    "name": "Zhenyu Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:45:45.712Z",
                    "hidden": false
                },
                {
                    "_id": "67edf28e042e8ba3e95d1966",
                    "user": {
                        "_id": "64bba541da140e461924dfed",
                        "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
                        "isPro": false,
                        "fullname": "zhijie deng",
                        "user": "zhijie3",
                        "type": "user"
                    },
                    "name": "Zhijie Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T07:52:14.204Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T15:11:11.000Z",
            "submittedOnDailyAt": "2025-04-03T01:03:18.798Z",
            "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
            "submittedOnDailyBy": {
                "_id": "64bba541da140e461924dfed",
                "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
                "isPro": false,
                "fullname": "zhijie deng",
                "user": "zhijie3",
                "type": "user"
            },
            "summary": "Increasing attention has been placed on improving the reasoning capacities of\nmulti-modal large language models (MLLMs). As the cornerstone for AI agents\nthat function in the physical realm, video-based visual-spatial intelligence\n(VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This\nwork conducts a first, in-depth study on improving the visual-spatial reasoning\nof MLLMs via R1-Zero-like training. Technically, we first identify that the\nvisual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models\ncannot be activated via Chain of Thought (CoT) prompts. We then incorporate\nGRPO training for improved visual-spatial reasoning, using the carefully\ncurated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation,\nwe identify the necessity to keep the KL penalty (even with a small value) in\nGRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from\nQwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o.\nMoreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves\nperformance comparable to that of the best open-source model\nLLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning\nand direct preference optimization baselines and observe strong performance\nsuperiority. The code and dataset will be available soon.",
            "upvotes": 41,
            "discussionId": "67edf28f042e8ba3e95d1a60",
            "githubRepo": "https://github.com/zhijie-group/R1-Zero-VSI",
            "ai_keywords": [
                "multi-modal large language models (MLLMs)",
                "video-based visual-spatial intelligence (VSI)",
                "Chain of Thought (CoT)",
                "GRPO training",
                "VSI-100k dataset",
                "DeepSeek-R1-Zero",
                "KL penalty",
                "vsGRPO-2B model",
                "Qwen2-VL-2B",
                "vsGRPO-7B model",
                "Qwen2-VL-7B",
                "LLaVA-NeXT-Video-72B",
                "supervised fine-tuning",
                "direct preference optimization"
            ]
        },
        "translation_title": "R1-Zero와 유사한 훈련을 통한 시각-공간 추론 개선",
        "purpose": "다중 모달 대형 언어 모델의 시각-공간 추론 능력을 개선하기 위한 연구",
        "method": [
            "Qwen2-VL 모델의 시각-공간 추론 능력이 Chain of Thought (CoT) 프롬프트로 활성화되지 않음을 확인함(we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts).",
            "GRPO 훈련을 도입하여 시각-공간 추론을 개선하고, carefully curated VSI-100k 데이터셋을 활용함(we then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset).",
            "KL 페널티를 유지하는 필요성을 발견하고, 120 GPU 시간 만에 vsGRPO-2B 모델이 Qwen2-VL-2B에서 fine-tune됨(vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1%)."
        ],
        "conclusion": "vsGRPO 모델은 기존 모델에 비해 성능이 우수하며, LLaVA-NeXT-Video-72B와 유사한 성과를 달성함.",
        "keywords": [
            "Video Understanding",
            "Multimodal Learning",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2504.01014",
            "authors": [
                {
                    "_id": "67eca389e14049f5ff064ea6",
                    "user": {
                        "_id": "6506b77a773ceaa8d52ecea1",
                        "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
                        "isPro": false,
                        "fullname": "CJH",
                        "user": "Howe666",
                        "type": "user"
                    },
                    "name": "Junhao Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:27.228Z",
                    "hidden": false
                },
                {
                    "_id": "67eca389e14049f5ff064ea7",
                    "user": {
                        "_id": "6455cc8f654d8bccae50e4d4",
                        "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
                        "isPro": false,
                        "fullname": "Yuying Ge",
                        "user": "tttoaster",
                        "type": "user"
                    },
                    "name": "Yuying Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:45:57.084Z",
                    "hidden": false
                },
                {
                    "_id": "67eca389e14049f5ff064ea8",
                    "user": {
                        "_id": "640e9762b03f4cd29f58d982",
                        "avatarUrl": "/avatars/81da37d628163fe3e094b247c7c3a3b5.svg",
                        "isPro": false,
                        "fullname": "Yixiao Ge",
                        "user": "yxgeee",
                        "type": "user"
                    },
                    "name": "Yixiao Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:46:03.701Z",
                    "hidden": false
                },
                {
                    "_id": "67eca389e14049f5ff064ea9",
                    "user": {
                        "_id": "65e77726767bfc7d109c45bf",
                        "avatarUrl": "/avatars/24e68c86e06055ea1209598ba49ce8b9.svg",
                        "isPro": false,
                        "fullname": "Jing Liao",
                        "user": "CeciliaJL",
                        "type": "user"
                    },
                    "name": "Jing Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:46:09.694Z",
                    "hidden": false
                },
                {
                    "_id": "67eca389e14049f5ff064eaa",
                    "user": {
                        "_id": "63ca3ddc04c979828310bfcb",
                        "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
                        "isPro": false,
                        "fullname": "Ying Shan",
                        "user": "yshan2u",
                        "type": "user"
                    },
                    "name": "Ying Shan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:46:15.314Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T17:57:18.000Z",
            "submittedOnDailyAt": "2025-04-03T01:15:35.152Z",
            "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction",
            "submittedOnDailyBy": {
                "_id": "6506b77a773ceaa8d52ecea1",
                "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
                "isPro": false,
                "fullname": "CJH",
                "user": "Howe666",
                "type": "user"
            },
            "summary": "Recent advancements in image and video synthesis have opened up new promise\nin generative games. One particularly intriguing application is transforming\ncharacters from anime films into interactive, playable entities. This allows\nplayers to immerse themselves in the dynamic anime world as their favorite\ncharacters for life simulation through language instructions. Such games are\ndefined as infinite game since they eliminate predetermined boundaries and\nfixed gameplay rules, where players can interact with the game world through\nopen-ended language and experience ever-evolving storylines and environments.\nRecently, a pioneering approach for infinite anime life simulation employs\nlarge language models (LLMs) to translate multi-turn text dialogues into\nlanguage instructions for image generation. However, it neglects historical\nvisual context, leading to inconsistent gameplay. Furthermore, it only\ngenerates static images, failing to incorporate the dynamics necessary for an\nengaging gaming experience. In this work, we propose AnimeGamer, which is built\nupon Multimodal Large Language Models (MLLMs) to generate each game state,\nincluding dynamic animation shots that depict character movements and updates\nto character states, as illustrated in Figure 1. We introduce novel\naction-aware multimodal representations to represent animation shots, which can\nbe decoded into high-quality video clips using a video diffusion model. By\ntaking historical animation shot representations as context and predicting\nsubsequent representations, AnimeGamer can generate games with contextual\nconsistency and satisfactory dynamics. Extensive evaluations using both\nautomated metrics and human evaluations demonstrate that AnimeGamer outperforms\nexisting methods in various aspects of the gaming experience. Codes and\ncheckpoints are available at https://github.com/TencentARC/AnimeGamer.",
            "upvotes": 27,
            "discussionId": "67eca39ce14049f5ff06535b",
            "projectPage": "https://howe125.github.io/AnimeGamer.github.io/",
            "githubRepo": "https://github.com/TencentARC/AnimeGamer",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "video diffusion model",
                "action-aware multimodal representations",
                "automated metrics",
                "human evaluations"
            ]
        },
        "translation_title": "AnimeGamer: 다음 게임 상태 예측을 통한 무한 애니메이션 생활 시뮬레이션",
        "purpose": "애니메이션 캐릭터를 상호작용 가능한 존재로 변환하여, 플레이어들이 언어 지시를 통해 동적인 애니메이션 세계에 몰입할 수 있도록 하는 무한 게임 개발",
        "method": [
            "Multimodal Large Language Models(MLLMs)를 사용하여 게임 상태를 생성함(we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state)",
            "이전 애니메이션 샷 표현을 역사적 컨텍스트로 활용하여 일관성 있는 게임 생성과 동적인 애니메이션 샷을 예측함(AnimeGamer can generate games with contextual consistency and satisfactory dynamics by taking historical animation shot representations as context and predicting subsequent representations)",
            "비디오 확산 모델을 사용하여 고품질 비디오 클립으로 디코딩할 수 있는 새로운 액션 인식 다중 모달 표현을 도입함(We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model)"
        ],
        "conclusion": "AnimeGamer는 기존 방법들보다 다양한 측면에서 게임 경험을 향상시키며, 동적인 애니메이션을 포함한 일관성 있는 게임을 생성할 수 있음을 입증하였음.",
        "keywords": [
            "Video Generation",
            "Multimodal Learning",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2504.01956",
            "authors": [
                {
                    "_id": "67ee01265839c8a023344aee",
                    "user": {
                        "_id": "65c38f6c137aba2aee524989",
                        "avatarUrl": "/avatars/a93e29f55876df3e65e2532972e057e4.svg",
                        "isPro": false,
                        "fullname": "Hanyang Wang",
                        "user": "hanyang-21",
                        "type": "user"
                    },
                    "name": "Hanyang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T07:52:06.284Z",
                    "hidden": false
                },
                {
                    "_id": "67ee01265839c8a023344aef",
                    "user": {
                        "_id": "6505a02f9310ce8c400edc63",
                        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
                        "isPro": false,
                        "fullname": "Fangfu Liu",
                        "user": "Liuff23",
                        "type": "user"
                    },
                    "name": "Fangfu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:46:26.790Z",
                    "hidden": false
                },
                {
                    "_id": "67ee01265839c8a023344af0",
                    "user": {
                        "_id": "66e14cf2fb009ef598305fe5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/etohfcD9GKaRPir1HRP-5.jpeg",
                        "isPro": false,
                        "fullname": "Jiawei Chi",
                        "user": "isaac158",
                        "type": "user"
                    },
                    "name": "Jiawei Chi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:46:47.647Z",
                    "hidden": false
                },
                {
                    "_id": "67ee01265839c8a023344af1",
                    "user": {
                        "_id": "66c8131afafc0fc87ca99650",
                        "avatarUrl": "/avatars/a6eeba2ccf011d5c9964fd38f85bd671.svg",
                        "isPro": false,
                        "fullname": "Yueqi Duan",
                        "user": "duanyueqi",
                        "type": "user"
                    },
                    "name": "Yueqi Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:46:55.497Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65c38f6c137aba2aee524989/JKAKb_7rnf6eZT56AF6aM.mp4"
            ],
            "publishedAt": "2025-04-02T17:59:21.000Z",
            "submittedOnDailyAt": "2025-04-03T02:07:36.716Z",
            "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step",
            "submittedOnDailyBy": {
                "_id": "65c38f6c137aba2aee524989",
                "avatarUrl": "/avatars/a93e29f55876df3e65e2532972e057e4.svg",
                "isPro": false,
                "fullname": "Hanyang Wang",
                "user": "hanyang-21",
                "type": "user"
            },
            "summary": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene",
            "upvotes": 22,
            "discussionId": "67ee012a5839c8a023344bdb",
            "projectPage": "https://hanyang-21.github.io/VideoScene",
            "githubRepo": "https://github.com/hanyang-21/VideoScene",
            "ai_keywords": [
                "video generative models",
                "video diffusion models",
                "3D scenes",
                "sparse views",
                "geometry regularization",
                "feed-forward model",
                "video generative prior",
                "inference time",
                "3D constraint",
                "reconstruction artifacts",
                "VideoScene",
                "3D-aware leap flow distillation",
                "dynamic denoising policy network"
            ]
        },
        "translation_title": "VideoScene: 비디오 확산 모델로 3D 장면을 한 단계에서 생성하기",
        "purpose": "비디오에서 3D 장면을 효율적이고 효과적으로 생성하기 위한 도구 개발",
        "method": [
            "비디오 확산 모델을 증류하여 3D 장면을 한 번에 생성하는 VideoScene을 제안함(we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step.)",
            "시간 소모적인 중복 정보를 건너뛰기 위한 3D 인식 leap flow 증류 전략을 설계함(Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information.)",
            "최적의 leap timestep을 적응적으로 결정하는 동적 노이즈 제거 정책 네트워크를 훈련함(and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference.)"
        ],
        "conclusion": "VideoScene은 이전 비디오 확산 모델보다 더 빠르고 뛰어난 3D 장면 생성 성과를 보여주며, 미래의 비디오에서 3D 애플리케이션에 유용한 도구로서의 가능성을 강조함.",
        "keywords": [
            "3D Vision",
            "Video Generation",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2504.01724",
            "authors": [
                {
                    "_id": "67edf7b6d277de0ec2aa5b6b",
                    "name": "Yuxuan Luo",
                    "hidden": false
                },
                {
                    "_id": "67edf7b6d277de0ec2aa5b6c",
                    "name": "Zhengkun Rong",
                    "hidden": false
                },
                {
                    "_id": "67edf7b6d277de0ec2aa5b6d",
                    "user": {
                        "_id": "63451d2dfeba4bdba59cf9b1",
                        "avatarUrl": "/avatars/ff5dd2f3b502c54e7c3e2512c3e98b28.svg",
                        "isPro": false,
                        "fullname": "Lizhen Wang",
                        "user": "wanglz14",
                        "type": "user"
                    },
                    "name": "Lizhen Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:47:42.725Z",
                    "hidden": false
                },
                {
                    "_id": "67edf7b6d277de0ec2aa5b6e",
                    "user": {
                        "_id": "602e78801f993496bc14d9c9",
                        "avatarUrl": "/avatars/eff73f60e6652d107cebeda3b2b87fe1.svg",
                        "isPro": false,
                        "fullname": "Longhao Zhang",
                        "user": "zhanglonghao",
                        "type": "user"
                    },
                    "name": "Longhao Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:47:48.593Z",
                    "hidden": false
                },
                {
                    "_id": "67edf7b6d277de0ec2aa5b6f",
                    "name": "Tianshu Hu",
                    "hidden": false
                },
                {
                    "_id": "67edf7b6d277de0ec2aa5b70",
                    "user": {
                        "_id": "668f66c56575435392165c25",
                        "avatarUrl": "/avatars/4ebe10abc29229e7862ada6891d1ccbe.svg",
                        "isPro": false,
                        "fullname": "Zhu",
                        "user": "YongmingZhu",
                        "type": "user"
                    },
                    "name": "Yongming Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:48:03.944Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T13:30:32.000Z",
            "submittedOnDailyAt": "2025-04-03T01:22:04.548Z",
            "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/.",
            "upvotes": 22,
            "discussionId": "67edf7bcd277de0ec2aa5d7b",
            "ai_keywords": [
                "diffusion transformer (DiT)",
                "hybrid guidance",
                "implicit facial representations",
                "3D head spheres",
                "3D body skeletons",
                "facial expressions",
                "body movements",
                "expressive animations",
                "identity-preserving animations",
                "progressive training strategy",
                "varying resolutions",
                "varying scales",
                "motion patterns",
                "sequential frames",
                "visual references",
                "long-term temporal coherence",
                "long-term consistency",
                "expressive results",
                "upper-body generation",
                "full-body generation"
            ]
        },
        "translation_title": "DreamActor-M1: 다각적이고 표현력이 뛰어난 강력한 인간 이미지 애니메이션을 위한 하이브리드 가이던스",
        "purpose": "인간 애니메이션의 표현력과 강건성을 높이기 위한 프레임워크 개발",
        "method": [
            "Diffusion transformer(DiT) 기반의 프레임워크를 제안하며, 하이브리드 가이던스를 통해 제어의 한계를 극복함(We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations.)",
            "표정과 몸 동작을 강하게 제어하기 위해 암묵적인 얼굴 표현, 3D 헤드 구체 및 3D 신체 스켈레톤을 통합한 하이브리드 제어 신호를 사용함(Our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements.)",
            "다양한 신체 자세와 이미지 크기를 다루기 위해 변화하는 해상도와 크기를 가지는 데이터로 점진적 훈련 전략을 채택함(We employ a progressive training strategy using data with varying resolutions and scales to handle various body poses and image scales.)",
            "복잡한 움직임 동안 보이지 않는 영역에 대한 장기적 시간적 일관성을 보장하기 위해 연속 프레임의 모션 패턴과 보완적인 시각적 참조를 통합함(We integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements.)"
        ],
        "conclusion": "우리의 방법은 기존 기술보다 우수한 성능을 발휘하며, 표현력이 뛰어난 포트레이트, 상체 및 전신 생성 결과를 제공함.",
        "keywords": [
            "Image Generation",
            "Pose Estimation",
            "3D Vision"
        ]
    }
]