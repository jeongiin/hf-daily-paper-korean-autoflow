[
    {
        "paper": {
            "id": "2508.02193",
            "authors": [
                {
                    "_id": "6892c77d8da45ffb0a2b2471",
                    "name": "Yuxuan Song",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2472",
                    "name": "Zheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2473",
                    "name": "Cheng Luo",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2474",
                    "name": "Pengyang Gao",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2475",
                    "name": "Fan Xia",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2476",
                    "name": "Hao Luo",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2477",
                    "name": "Zheng Li",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2478",
                    "name": "Yuehang Yang",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2479",
                    "name": "Hongli Yu",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b247a",
                    "name": "Xingwei Qu",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b247b",
                    "name": "Yuwei Fu",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b247c",
                    "name": "Jing Su",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b247d",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b247e",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b247f",
                    "name": "Mingxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2480",
                    "name": "Lin Yan",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2481",
                    "name": "Xiaoying Jia",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2482",
                    "name": "Jingjing Liu",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2483",
                    "name": "Wei-Ying Ma",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2484",
                    "name": "Ya-Qin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2485",
                    "name": "Yonghui Wu",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2486",
                    "name": "Hao Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66275b53d138af2d2eeb9326/dFE-umzBWqWmGHWbKItOo.png"
            ],
            "publishedAt": "2025-08-04T08:43:01.000Z",
            "submittedOnDailyAt": "2025-08-06T02:15:50.872Z",
            "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference",
            "submittedOnDailyBy": {
                "_id": "66275b53d138af2d2eeb9326",
                "avatarUrl": "/avatars/1103f68b978238d5bce604294d467e00.svg",
                "isPro": false,
                "fullname": "Yuxuan Song",
                "user": "yxsong",
                "type": "user"
            },
            "summary": "We present Seed Diffusion Preview, a large-scale language model based on\ndiscrete-state diffusion, offering remarkably fast inference speed. Thanks to\nnon-sequential, parallel generation, discrete diffusion models provide a\nnotable speedup to mitigate the inherent latency of token-by-token decoding, as\ndemonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion\nPreview achieves an inference speed of 2,146 token/s over H20 GPUs while\nmaintaining competitive performance across a sweep of standard code evaluation\nbenchmarks, significantly faster than contemporary Mercury and Gemini\nDiffusion, establishing new state of the art on the speed-quality Pareto\nfrontier for code models.",
            "upvotes": 50,
            "discussionId": "6892c77d8da45ffb0a2b2487",
            "projectPage": "https://seed.bytedance.com/en/seed_diffusion",
            "ai_summary": "Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.",
            "ai_keywords": [
                "discrete-state diffusion",
                "non-sequential",
                "parallel generation",
                "token-by-token decoding",
                "Seed Diffusion Preview",
                "H20 GPUs",
                "code evaluation benchmarks",
                "speed-quality Pareto frontier"
            ]
        },
        "translation_title": "Seed Diffusion: 고속 추론을 위한 대규모 Diffusion 언어 모델",
        "purpose": "코드 모델의 빠른 추론 속도와 경쟁력 있는 성능을 달성하기 위한 대규모 언어 모델 개발",
        "method": [
            "비연속 상태 diffusion 기반의 언어 모델을 제안하고, 병렬 생성 방식으로 고속 추론을 가능하게 함(We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed.)",
            "비순차적 방식으로 토큰을 생성하여 기존의 토큰별 디코딩의 지연을 줄임(Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding.)",
            "H20 GPU에서 초당 2,146 토큰의 추론 속도를 달성하면서 표준 코드 평가 벤치마크에서 좋은 성능을 유지함(Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks.)"
        ],
        "conclusion": "Seed Diffusion Preview는 코드 모델의 속도와 품질 모두에서 새로운 최첨단 성과를 설정함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.03320",
            "authors": [
                {
                    "_id": "6892bd5c8da45ffb0a2b23fe",
                    "name": "Peiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b23ff",
                    "name": "Yi Peng",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2400",
                    "name": "Yimeng Gan",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2401",
                    "name": "Liang Hu",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2402",
                    "name": "Tianyidan Xie",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2403",
                    "name": "Xiaokun Wang",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2404",
                    "name": "Yichen Wei",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2405",
                    "name": "Chuanxin Tang",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2406",
                    "name": "Bo Zhu",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2407",
                    "name": "Changshi Li",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2408",
                    "name": "Hongyang Wei",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2409",
                    "name": "Eric Li",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b240a",
                    "name": "Xuchen Song",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b240b",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b240c",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T10:59:01.000Z",
            "submittedOnDailyAt": "2025-08-06T01:57:00.887Z",
            "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding\n  and Generation",
            "submittedOnDailyBy": {
                "_id": "620f5a1c3f76c50e6458a9b6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
                "isPro": false,
                "fullname": "Peiyu Wang",
                "user": "OrlandoHugBot",
                "type": "user"
            },
            "summary": "We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model\nthat unifies image understanding, text-to-image generation, and image editing\nwithin a single architecture-eliminating the need for task-specific adapters or\ninter-module connectors-and demonstrate that compact multimodal systems can\nachieve state-of-the-art performance on commodity hardware. Skywork UniPic\nachieves a GenEval score of 0.86, surpassing most existing unified models; sets\na new DPG-Bench complex-generation record of 85.5; attains 5.83 on\nGEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x\n1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled\nencoding strategy that leverages a masked autoregressive encoder for synthesis\nand a SigLIP2 encoder for understanding, all feeding a shared autoregressive\ndecoder; (2) a progressive, resolution-aware training schedule scaling from 256\nx 256 to 1024 x 1024 while dynamically unfreezing parameters to balance\ncapacity and stability; and (3) meticulously curated, 100 million-scale\ndatasets augmented with task-specific reward models to refine generation and\nediting objectives. By demonstrating that high-fidelity multimodal integration\nneed not incur prohibitive resource demands, Skywork UniPic establishes a\npractical paradigm for deployable, high-fidelity multimodal AI. Code and\nweights are publicly available at\nhttps://huggingface.co/Skywork/Skywork-UniPic-1.5B.",
            "upvotes": 38,
            "discussionId": "6892bd5c8da45ffb0a2b240d",
            "githubRepo": "https://github.com/SkyworkAI/UniPic",
            "ai_summary": "Skywork UniPic, a 1.5 billion-parameter autoregressive model, unifies image understanding, text-to-image generation, and image editing with state-of-the-art performance on commodity hardware.",
            "ai_keywords": [
                "autoregressive model",
                "image understanding",
                "text-to-image generation",
                "image editing",
                "GenEval",
                "DPG-Bench",
                "GEditBench-EN",
                "ImgEdit-Bench",
                "decoupled encoding strategy",
                "masked autoregressive encoder",
                "SigLIP2 encoder",
                "shared autoregressive decoder",
                "progressive training schedule",
                "resolution-aware training",
                "parameter unfreezing",
                "high-fidelity multimodal integration"
            ],
            "githubStars": 370
        },
        "translation_title": "Skywork UniPic: 시각 이해 및 생성을 위한 통합 자기 회귀 모델링",
        "purpose": "이미지 이해, 텍스트-이미지 생성, 이미지 편집을 통합하여 고성능의 다중 모드 시스템 구축",
        "method": [
            "자기 회귀 디코더를 공유하며 아카이브된 encoding 전략을 활용하여 이미지 합성과 이해를 위한 인코더를 분리함.(a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder.)",
            "256x256에서 1024x1024로 확장되는 단계적인 훈련 일정을 설정하여 용량과 안정성을 조화롭게 조정함.(a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability.)",
            "작업별 보상 모델로 보강된 1억 규모의 데이터를 통해 생성 및 편집 목표를 개선함.(meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives.)"
        ],
        "conclusion": "Skywork UniPic은 고품질 다중 모드 통합이 자원의 부담을 주지 않음을 증명하여 실제로 사용 가능한 고충실도 다중 모드 AI의 패러다임을 확립함.",
        "keywords": [
            "Image Generation",
            "Image Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.03694",
            "authors": [
                {
                    "_id": "6892b64d8da45ffb0a2b23d4",
                    "name": "Jianxiong Gao",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23d5",
                    "name": "Zhaoxi Chen",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23d6",
                    "name": "Xian Liu",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23d7",
                    "name": "Jianfeng Feng",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23d8",
                    "name": "Chenyang Si",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23d9",
                    "name": "Yanwei Fu",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23da",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23db",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T17:59:58.000Z",
            "submittedOnDailyAt": "2025-08-06T00:30:01.624Z",
            "title": "LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation",
            "submittedOnDailyBy": {
                "_id": "643815c4961bb61e463c5896",
                "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg",
                "isPro": false,
                "fullname": "Jianxiong Gao",
                "user": "Jianxiong",
                "type": "user"
            },
            "summary": "Controllable ultra-long video generation is a fundamental yet challenging\ntask. Although existing methods are effective for short clips, they struggle to\nscale due to issues such as temporal inconsistency and visual degradation. In\nthis paper, we initially investigate and identify three key factors: separate\nnoise initialization, independent control signal normalization, and the\nlimitations of single-modality guidance. To address these issues, we propose\nLongVie, an end-to-end autoregressive framework for controllable long video\ngeneration. LongVie introduces two core designs to ensure temporal consistency:\n1) a unified noise initialization strategy that maintains consistent generation\nacross clips, and 2) global control signal normalization that enforces\nalignment in the control space throughout the entire video. To mitigate visual\ndegradation, LongVie employs 3) a multi-modal control framework that integrates\nboth dense (e.g., depth maps) and sparse (e.g., keypoints) control signals,\ncomplemented by 4) a degradation-aware training strategy that adaptively\nbalances modality contributions over time to preserve visual quality. We also\nintroduce LongVGenBench, a comprehensive benchmark consisting of 100\nhigh-resolution videos spanning diverse real-world and synthetic environments,\neach lasting over one minute. Extensive experiments show that LongVie achieves\nstate-of-the-art performance in long-range controllability, consistency, and\nquality.",
            "upvotes": 31,
            "discussionId": "6892b64d8da45ffb0a2b23dc",
            "projectPage": "https://vchitect.github.io/LongVie-project/",
            "githubRepo": "https://github.com/Vchitect/LongVie",
            "ai_summary": "LongVie, an end-to-end autoregressive framework, addresses temporal consistency and visual degradation in ultra-long video generation through unified noise initialization, global control signal normalization, multi-modal control, and degradation-aware training.",
            "ai_keywords": [
                "autoregressive framework",
                "temporal consistency",
                "visual degradation",
                "unified noise initialization",
                "global control signal normalization",
                "multi-modal control",
                "degradation-aware training",
                "LongVGenBench"
            ],
            "githubStars": 21
        },
        "translation_title": "LongVie: 다중 모달 유도 제어 가능한 초장기 비디오 생성",
        "purpose": "제어 가능한 초장기 비디오 생성을 위한 새로운 프레임워크 연구",
        "method": [
            "세 가지 주요 요소(분리된 노이즈 초기화, 독립적 제어 신호 정규화, 단일 모달리티 유도의 한계)를 분석함(we initially investigate and identify three key factors: separate noise initialization, independent control signal normalization, and the limitations of single-modality guidance.)",
            "LongVie라는 종단 간 자회귀 프레임워크를 제안하고, 일관된 노이즈 초기화 전략과 전역 제어 신호 정규화 방식을 도입함(To address these issues, we propose LongVie, an end-to-end autoregressive framework for controllable long video generation.)",
            "다중 모달 제어 프레임워크를 활용하여 시각적 품질을 유지함(LongVie employs a multi-modal control framework that integrates both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals.)",
            "100개의 고해상도 비디오로 구성된 종합 벤치마크인 LongVGenBench를 도입함(We also introduce LongVGenBench, a comprehensive benchmark consisting of 100 high-resolution videos spanning diverse real-world and synthetic environments.)"
        ],
        "conclusion": "LongVie는 긴 범위의 제어 가능성, 일관성 및 품질 측면에서 최첨단 성능을 달성함.",
        "keywords": [
            "Video Generation",
            "Multimodal Learning",
            "3D Vision"
        ]
    },
    {
        "paper": {
            "id": "2508.03686",
            "authors": [
                {
                    "_id": "6892f2468da45ffb0a2b2500",
                    "name": "Shudong Liu",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2501",
                    "name": "Hongwei Liu",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2502",
                    "name": "Junnan Liu",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2503",
                    "name": "Linchen Xiao",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2504",
                    "name": "Songyang Gao",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2505",
                    "name": "Chengqi Lyu",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2506",
                    "name": "Yuzhe Gu",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2507",
                    "name": "Wenwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2508",
                    "name": "Derek F. Wong",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2509",
                    "name": "Songyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b250a",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T17:55:24.000Z",
            "submittedOnDailyAt": "2025-08-06T04:43:01.913Z",
            "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward",
            "submittedOnDailyBy": {
                "_id": "630716d11801ecc7d2595021",
                "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
                "isPro": false,
                "fullname": "Songyang Zhang",
                "user": "zsytony",
                "type": "user"
            },
            "summary": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier.",
            "upvotes": 21,
            "discussionId": "6892f2468da45ffb0a2b250b",
            "githubRepo": "https://github.com/open-compass/CompassVerifier",
            "ai_summary": "CompassVerifier is a lightweight, robust model for verifying LLM outputs across various domains, supported by VerifierBench, a comprehensive benchmark dataset.",
            "ai_keywords": [
                "LLMs",
                "answer verification",
                "reward model",
                "evaluation frameworks",
                "regex rules",
                "evaluation prompts",
                "benchmarks",
                "verifier development",
                "multi-domain competency",
                "math",
                "knowledge",
                "reasoning tasks",
                "multi-subproblems",
                "formulas",
                "sequence answers",
                "abnormal/invalid responses",
                "VerifierBench"
            ],
            "githubStars": 16
        },
        "translation_title": "CompassVerifier: LLM 평가 및 결과 보상을 위한 통합적이고 강력한 검증기",
        "purpose": "LLMs의 성능 평가 및 최적화를 위한 효과적인 답변 검증 모델을 개발하고자 함.",
        "method": [
            "경량 검증기 모델인 CompassVerifier를 개발함(we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward.)",
            "다양한 도메인에서 수학, 지식, 추론 작업을 포함하여 여러 답변 유형 처리 능력을 입증함(it demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks.)",
            "여러 데이터 출처에서 수집한 모델 출력을 포함한 VerifierBench 벤치마크를 도입하고, 메타 오류 패턴의 수동 분석을 통해 CompassVerifier를 강화함(We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier.)"
        ],
        "conclusion": "CompassVerifier와 VerifierBench는 답변 검증 및 평가 프로토콜, 강화학습 연구에 기여할 것으로 기대됨.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Reinforcement Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.03012",
            "authors": [
                {
                    "_id": "6892c65c8da45ffb0a2b245e",
                    "name": "Zexiong Ma",
                    "hidden": false
                },
                {
                    "_id": "6892c65c8da45ffb0a2b245f",
                    "name": "Chao Peng",
                    "hidden": false
                },
                {
                    "_id": "6892c65c8da45ffb0a2b2460",
                    "name": "Qunhong Zeng",
                    "hidden": false
                },
                {
                    "_id": "6892c65c8da45ffb0a2b2461",
                    "name": "Pengfei Gao",
                    "hidden": false
                },
                {
                    "_id": "6892c65c8da45ffb0a2b2462",
                    "name": "Yanzhen Zou",
                    "hidden": false
                },
                {
                    "_id": "6892c65c8da45ffb0a2b2463",
                    "name": "Bing Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T02:44:21.000Z",
            "submittedOnDailyAt": "2025-08-06T01:36:11.665Z",
            "title": "Tool-integrated Reinforcement Learning for Repo Deep Search",
            "submittedOnDailyBy": {
                "_id": "654da66fb36f85a025bc24b6",
                "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
                "isPro": false,
                "fullname": "Zexiong Ma",
                "user": "mizersy",
                "type": "user"
            },
            "summary": "Issue localization, the process of identifying code locations that need\nmodification to resolve software issues, is a critical yet challenging task in\nsoftware development. The semantic gap between natural language issue\ndescriptions and faulty code requires complex multi-hop reasoning through code\ndependencies. Existing LLM-based agents attempt to address this by integrating\nrepository retrieval tools. However, this transforms issue localization into a\ndemanding task we call Repo Deep Search, which requires the LLM to effectively\nutilize various repository retrieval tools throughout a multi-step reasoning\nand navigation process. To tackle this challenge, we present ToolTrain, a\ntwo-stage tool-integrated training framework combining rejection-sampled\nsupervised fine-tuning and tool-integrated reinforcement learning to enhance\nLLMs' ability to use retrieval tools for issue localization. Experimental\nresults show that ToolTrain-trained models achieve state-of-the-art\nperformance, with our 32B model even surpassing Claude-3.7 on function-level\nlocalization. The results also show that improved localization performance\ntranslates to better end-to-end issue resolution performance. This further\ndemonstrates that training for issue localization is a viable and effective\nstrategy for improving automated software development.",
            "upvotes": 7,
            "discussionId": "6892c65d8da45ffb0a2b2464",
            "githubRepo": "https://github.com/Mizersy/RepoDeepSearch",
            "ai_summary": "ToolTrain, a two-stage training framework combining supervised fine-tuning and reinforcement learning, enhances LLMs for issue localization by integrating repository retrieval tools, achieving state-of-the-art performance.",
            "ai_keywords": [
                "LLM-based agents",
                "Repo Deep Search",
                "rejection-sampled supervised fine-tuning",
                "tool-integrated reinforcement learning",
                "function-level localization",
                "Claude-3.7",
                "automated software development"
            ],
            "githubStars": 4
        },
        "translation_title": "도구 통합 강화 학습을 통한 Repo Deep Search",
        "purpose": "소프트웨어 문제 해결을 위한 코드 위치 식별 효율성을 높이기 위한 연구",
        "method": [
            "이슈 로컬라이제이션을 도와주는 도구 통합 교육 프레임워크인 ToolTrain을 제안함(To tackle this challenge, we present ToolTrain, a two-stage tool-integrated training framework combining rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning).",
            "ToolTrain을 통해 LLM이 다양한 리포지토리 검색 도구를 효과적으로 활용하도록 학습함(we develop a framework to enhance LLMs' ability to use retrieval tools for issue localization).",
            "실험 결과 ToolTrain 훈련 모델이 최신 성능을 기록하며, 특정 모델이 Claude-3.7보다 더 나은 성능을 보임(Experimental results show that ToolTrain-trained models achieve state-of-the-art performance, with our 32B model even surpassing Claude-3.7 on function-level localization)."
        ],
        "conclusion": "ToolTrain을 통한 교육이 자동화된 소프트웨어 개발에서 효과적인 전략임을 입증하며, 이는 문제 해결 성능 향상으로 이어짐.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Robotics"
        ]
    }
]