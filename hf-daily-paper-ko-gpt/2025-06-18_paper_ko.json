[
    {
        "paper": {
            "id": "2506.14028",
            "authors": [
                {
                    "_id": "685240aa0164cd131671056a",
                    "name": "Xueqing Peng",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671056b",
                    "name": "Lingfei Qian",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671056c",
                    "name": "Yan Wang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671056d",
                    "name": "Ruoyu Xiang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671056e",
                    "name": "Yueru He",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671056f",
                    "name": "Yang Ren",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710570",
                    "name": "Mingyang Jiang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710571",
                    "name": "Jeff Zhao",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710572",
                    "name": "Huan He",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710573",
                    "name": "Yi Han",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710574",
                    "name": "Yun Feng",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710575",
                    "name": "Yuechen Jiang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710576",
                    "name": "Yupeng Cao",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710577",
                    "name": "Haohang Li",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710578",
                    "name": "Yangyang Yu",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710579",
                    "name": "Xiaoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671057a",
                    "name": "Penglei Gao",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671057b",
                    "name": "Shengyuan Lin",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671057c",
                    "name": "Keyi Wang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671057d",
                    "name": "Shanshan Yang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671057e",
                    "user": {
                        "_id": "62f662bcc58915315c4eccea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                        "isPro": true,
                        "fullname": "Yilun Zhao",
                        "user": "yilunzhao",
                        "type": "user"
                    },
                    "name": "Yilun Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:58:59.111Z",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671057f",
                    "name": "Zhiwei Liu",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710580",
                    "name": "Peng Lu",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710581",
                    "name": "Jerry Huang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710582",
                    "name": "Suyuchen Wang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710583",
                    "name": "Triantafillos Papadopoulos",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710584",
                    "name": "Polydoros Giannouris",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710585",
                    "name": "Efstathia Soufleri",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710586",
                    "name": "Nuo Chen",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710587",
                    "name": "Guojun Xiong",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710588",
                    "name": "Zhiyang Deng",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710589",
                    "name": "Yijia Zhao",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671058a",
                    "name": "Mingquan Lin",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671058b",
                    "name": "Meikang Qiu",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671058c",
                    "name": "Kaleb E Smith",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671058d",
                    "name": "Arman Cohan",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671058e",
                    "name": "Xiao-Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671058f",
                    "name": "Jimin Huang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710590",
                    "name": "Alejandro Lopez-Lira",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710591",
                    "name": "Xi Chen",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710592",
                    "name": "Junichi Tsujii",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710593",
                    "name": "Jian-Yun Nie",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710594",
                    "name": "Sophia Ananiadou",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710595",
                    "name": "Qianqian Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T22:01:49.000Z",
            "submittedOnDailyAt": "2025-06-18T13:36:29.873Z",
            "title": "MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\n  for Financial LLM Evaluation",
            "submittedOnDailyBy": {
                "_id": "63a0c0803c8841cfe2cd1f15",
                "avatarUrl": "/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg",
                "isPro": false,
                "fullname": "Xueqing Peng",
                "user": "Xueqing",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) have accelerated progress in\nfinancial NLP and applications, yet existing benchmarks remain limited to\nmonolingual and unimodal settings, often over-relying on simple tasks and\nfailing to reflect the complexity of real-world financial communication. We\nintroduce MultiFinBen, the first multilingual and multimodal benchmark tailored\nto the global financial domain, evaluating LLMs across modalities (text,\nvision, audio) and linguistic settings (monolingual, bilingual, multilingual)\non domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy\nand PolyFiQA-Expert, the first multilingual financial benchmarks requiring\nmodels to perform complex reasoning over mixed-language inputs; and EnglishOCR\nand SpanishOCR, the first OCR-embedded financial QA tasks challenging models to\nextract and reason over information from visual-text financial documents.\nMoreover, we propose a dynamic, difficulty-aware selection mechanism and curate\na compact, balanced benchmark rather than simple aggregation existing datasets.\nExtensive evaluation of 22 state-of-the-art models reveals that even the\nstrongest models, despite their general multimodal and multilingual\ncapabilities, struggle dramatically when faced with complex cross-lingual and\nmultimodal tasks in financial domain. MultiFinBen is publicly released to\nfoster transparent, reproducible, and inclusive progress in financial studies\nand applications.",
            "upvotes": 47,
            "discussionId": "685240ab0164cd1316710596",
            "ai_summary": "MultiFinBen is a multilingual and multimodal benchmark for financial domain tasks, evaluating LLMs across modalities and linguistic settings, revealing challenges in complex cross-lingual and multimodal financial reasoning.",
            "ai_keywords": [
                "LLMs",
                "financial NLP",
                "multilingual",
                "multimodal",
                "benchmark",
                "domain-specific tasks",
                "PolyFiQA-Easy",
                "PolyFiQA-Expert",
                "EnglishOCR",
                "SpanishOCR",
                "dynamic selection mechanism",
                "difficulty-aware",
                "OCR-embedded",
                "financial QA"
            ]
        },
        "translation_title": "MultiFinBen: 다국어, 다중 모드 및 난이도 인식 재무 LLM 평가 벤치마크",
        "purpose": "재무 NLP와 응용 프로그램을 위한 다국어 및 다중 모드 재무 LLM 평가를 목표로 하는 벤치마크 개발",
        "method": [
            "MultiFinBen이라는 첫 번째 다국어 및 다중 모드 벤치마크를 도입함(We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain.)",
            "PolyFiQA-Easy 및 PolyFiQA-Expert라는 두 가지 새로운 작업을 포함하여 복합 언어 입력에 대한 복잡한 추론을 요구함(We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs.)",
            "다ynamic, difficulty-aware 선택 메커니즘을 제안하고, 기존 데이터 세트를 단순하게 집계하기보다는 균형 잡힌 벤치마크를 큐레이팅함(we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets.)"
        ],
        "conclusion": "MultiFinBen은 재무 분야의 복잡한 언어 및 다중 모드 작업에 대해 어려움을 겪고 있음에도 불구하고, 모든 최신 모델이 성능을 발휘할 수 있는 투명하고 재현 가능한 발전을 촉진하는 데 기여할 것이다.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.12928",
            "authors": [
                {
                    "_id": "6851dd060164cd13167103d7",
                    "name": "King Zhu",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103d8",
                    "name": "Hanhao Li",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103d9",
                    "name": "Siwei Wu",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103da",
                    "name": "Tianshun Xing",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103db",
                    "name": "Dehua Ma",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103dc",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103dd",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103de",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103df",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103e0",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103e1",
                    "name": "Changwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103e2",
                    "name": "Chenghua Lin",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103e3",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103e4",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:16:44.662Z",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103e5",
                    "user": {
                        "_id": "628c8598ef14f971b698107f",
                        "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Wangchunshu",
                        "type": "user"
                    },
                    "name": "Wangchunshu Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:16:42.693Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-15T17:59:47.000Z",
            "submittedOnDailyAt": "2025-06-18T04:21:02.464Z",
            "title": "Scaling Test-time Compute for LLM Agents",
            "submittedOnDailyBy": {
                "_id": "638efcf4c67af472d316d424",
                "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                "isPro": false,
                "fullname": "Ge Zhang",
                "user": "zhangysk",
                "type": "user"
            },
            "summary": "Scaling test time compute has shown remarkable success in improving the\nreasoning abilities of large language models (LLMs). In this work, we conduct\nthe first systematic exploration of applying test-time scaling methods to\nlanguage agents and investigate the extent to which it improves their\neffectiveness. Specifically, we explore different test-time scaling strategies,\nincluding: (1) parallel sampling algorithms; (2) sequential revision\nstrategies; (3) verifiers and merging methods; (4)strategies for diversifying\nrollouts.We carefully analyze and ablate the impact of different design\nstrategies on applying test-time scaling on language agents, and have follow\nfindings: 1. Scaling test time compute could improve the performance of agents.\n2. Knowing when to reflect is important for agents. 3. Among different\nverification and result merging approaches, the list-wise method performs best.\n4. Increasing diversified rollouts exerts a positive effect on the agent's task\nperformance.",
            "upvotes": 33,
            "discussionId": "6851dd060164cd13167103e6",
            "ai_summary": "Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.",
            "ai_keywords": [
                "parallel sampling algorithms",
                "sequential revision strategies",
                "verifiers",
                "merging methods",
                "diversified rollouts",
                "test-time scaling",
                "large language models"
            ]
        },
        "translation_title": "LLM 에이전트를 위한 테스트 시간 계산 확장",
        "purpose": "테스트 시간 계산 확장을 통해 LLM의 추론 능력을 개선하고자 함",
        "method": [
            "언어 에이전트에 적용할 테스트 시간 확장 방법을 체계적으로 탐색함.(In this work, we conduct the first systematic exploration of applying test-time scaling methods to language agents and investigate the extent to which it improves their effectiveness.)",
            "다양한 테스트 시간 확장 전략을 탐색함(예: 병렬 샘플링 알고리즘, 순차 수정 전략 등)(Specifically, we explore different test-time scaling strategies, including: (1) parallel sampling algorithms; (2) sequential revision strategies; (3) verifiers and merging methods; (4) strategies for diversifying rollouts.)",
            "다양한 설계 전략의 영향을 분석함.(We carefully analyze and ablate the impact of different design strategies on applying test-time scaling on language agents.)"
        ],
        "conclusion": "테스트 시간 계산 확장이 에이전트의 성능을 개선하고, 리스트 방식의 검증 방법이 가장 효과적임을 확인함.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.14429",
            "authors": [
                {
                    "_id": "68521a9a0164cd131671045c",
                    "user": {
                        "_id": "64f033ef82c6eea604c4da8b",
                        "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
                        "isPro": false,
                        "fullname": "Liu Xiaoran",
                        "user": "LiuXR",
                        "type": "user"
                    },
                    "name": "Xiaoran Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:16:22.707Z",
                    "hidden": false
                },
                {
                    "_id": "68521a9a0164cd131671045d",
                    "name": "Zhigeng Liu",
                    "hidden": false
                },
                {
                    "_id": "68521a9a0164cd131671045e",
                    "name": "Zengfeng Huang",
                    "hidden": false
                },
                {
                    "_id": "68521a9a0164cd131671045f",
                    "name": "Qipeng Guo",
                    "hidden": false
                },
                {
                    "_id": "68521a9a0164cd1316710460",
                    "name": "Ziwei He",
                    "hidden": false
                },
                {
                    "_id": "68521a9a0164cd1316710461",
                    "name": "Xipeng Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T11:45:37.000Z",
            "submittedOnDailyAt": "2025-06-18T00:18:23.135Z",
            "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs",
            "submittedOnDailyBy": {
                "_id": "64f033ef82c6eea604c4da8b",
                "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
                "isPro": false,
                "fullname": "Liu Xiaoran",
                "user": "LiuXR",
                "type": "user"
            },
            "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textit{stable perplexity} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textit{local perception}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs.",
            "upvotes": 30,
            "discussionId": "68521a9a0164cd1316710462",
            "ai_summary": "This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.",
            "ai_keywords": [
                "diffusion LLMs",
                "auto-regressive LLMs",
                "stable perplexity",
                "local perception",
                "Rotary Position Embedding (RoPE) scaling theory",
                "LongLLaDA",
                "NTK-based RoPE extrapolation",
                "context extrapolation scaling laws",
                "long-context tasks"
            ]
        },
        "translation_title": "LongLLaDA: 확산 LLM의 긴 문맥 기능 해제",
        "purpose": "Diffusion LLM의 긴 문맥 성능을 이해하고 문맥 확장 방법을 제시하기 위한 체계적인 연구",
        "method": [
            "Diffusion LLM과 전통적인 auto-regressive LLM의 긴 문맥 성능을 비교하는 체계적인 조사를 수행함(we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs.)",
            "Diffusion LLM의 특별한 특징으로 안정적인 perplexity를 유지함을 확인함(unlike auto-regressive LLMs, they maintain remarkably stable perplexity during direct context extrapolation.)",
            "RoPE 기반의 방법을 LongLLaDA에 통합하여 훈련 없이 문맥 확장을 가능하게 하는 방법을 제안함(we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation.)"
        ],
        "conclusion": "이 연구는 diffusion LLM을 위한 최초의 문맥 확장 방법을 확립하고, 긴 문맥에서의 성능 차이를 확인함으로써 향후 연구에 기초 자료를 제공함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.12285",
            "authors": [
                {
                    "_id": "6852b1597eb90a35d35de603",
                    "name": "Yinghao Ma",
                    "hidden": false
                },
                {
                    "_id": "6852b1597eb90a35d35de604",
                    "name": "Siyou Li",
                    "hidden": false
                },
                {
                    "_id": "6852b1597eb90a35d35de605",
                    "name": "Juntao Yu",
                    "hidden": false
                },
                {
                    "_id": "6852b1597eb90a35d35de606",
                    "name": "Emmanouil Benetos",
                    "hidden": false
                },
                {
                    "_id": "6852b1597eb90a35d35de607",
                    "name": "Akira Maezawa",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-14T00:18:44.000Z",
            "submittedOnDailyAt": "2025-06-18T13:02:05.187Z",
            "title": "CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction\n  Following",
            "submittedOnDailyBy": {
                "_id": "6410665d5364a661bee22524",
                "avatarUrl": "/avatars/f1cb0e07f36933187ceccbd5dcbeff79.svg",
                "isPro": false,
                "fullname": "Yinghao Ma",
                "user": "nicolaus625",
                "type": "user"
            },
            "summary": "Recent advances in audio-text large language models (LLMs) have opened new\npossibilities for music understanding and generation. However, existing\nbenchmarks are limited in scope, often relying on simplified tasks or\nmulti-choice evaluations that fail to reflect the complexity of real-world\nmusic analysis. We reinterpret a broad range of traditional MIR annotations as\ninstruction-following formats and introduce CMI-Bench, a comprehensive music\ninstruction following benchmark designed to evaluate audio-text LLMs on a\ndiverse set of music information retrieval (MIR) tasks. These include genre\nclassification, emotion regression, emotion tagging, instrument classification,\npitch estimation, key detection, lyrics transcription, melody extraction, vocal\ntechnique recognition, instrument performance technique detection, music\ntagging, music captioning, and (down)beat tracking: reflecting core challenges\nin MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized\nevaluation metrics consistent with previous state-of-the-art MIR models,\nensuring direct comparability with supervised approaches. We provide an\nevaluation toolkit supporting all open-source audio-textual LLMs, including\nLTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant\nperformance gaps between LLMs and supervised models, along with their culture,\nchronological and gender bias, highlighting the potential and limitations of\ncurrent models in addressing MIR tasks. CMI-Bench establishes a unified\nfoundation for evaluating music instruction following, driving progress in\nmusic-aware LLMs.",
            "upvotes": 26,
            "discussionId": "6852b15a7eb90a35d35de608",
            "githubRepo": "https://github.com/nicolaus625/CMI-bench",
            "ai_summary": "CMI-Bench introduces a comprehensive instruction-following benchmark for audio-text LLMs to evaluate them on a diverse range of music information retrieval tasks.",
            "ai_keywords": [
                "audio-text large language models",
                "LLMs",
                "music information retrieval",
                "MIR",
                "genre classification",
                "emotion regression",
                "instrument classification",
                "pitch estimation",
                "key detection",
                "lyrics transcription",
                "melody extraction",
                "vocal technique recognition",
                "instrument performance technique detection",
                "music tagging",
                "music captioning",
                "beat tracking",
                "evaluation metrics",
                "cultural bias",
                "chronological bias",
                "gender bias"
            ]
        },
        "translation_title": "CMI-Bench: 음악 지시 평가를 위한 포괄적 벤치마크",
        "purpose": "음악 이해 및 생성을 평가하기 위한 포괄적이고 다양한 음악 정보 검색(MIR) 작업 기준 제공",
        "method": [
            "다양한 전통적 MIR 주석들을 지침 따르기 형식으로 재해석하고 CMI-Bench를 도입함(We reinterpret a broad range of traditional MIR annotations as instruction-following formats and introduce CMI-Bench.)",
            "CMI-Bench는 장르 분류, 감정 회귀, 악기 분류 등 다양한 MIR 작업을 포함함(These include genre classification, emotion regression, instrument classification, pitch estimation, etc.)",
            "표준화된 평가 지표를 채택하여 이전 최첨단 MIR 모델들과 직접 비교 가능하게 함(Unlike previous benchmarks, CMI-Bench adopts standardized evaluation metrics consistent with previous state-of-the-art MIR models.)"
        ],
        "conclusion": "CMI-Bench는 음악 지시 평가를 위한 통합 기반을 제공하고, 음악 인식 LLM의 발전을 촉진함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Music Information Retrieval"
        ]
    },
    {
        "paper": {
            "id": "2506.14245",
            "authors": [
                {
                    "_id": "68521c2a0164cd131671046b",
                    "user": {
                        "_id": "669f940b3b09946711e20c52",
                        "avatarUrl": "/avatars/27044caec57d8d68d700208fae78b6c6.svg",
                        "isPro": false,
                        "fullname": "XumengWen",
                        "user": "XumengWen",
                        "type": "user"
                    },
                    "name": "Xumeng Wen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:15:48.128Z",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd131671046c",
                    "name": "Zihan Liu",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd131671046d",
                    "user": {
                        "_id": "64a7a2bad001860e0c34f7f2",
                        "avatarUrl": "/avatars/2433104071e4ae1c3e2d755d81d7964b.svg",
                        "isPro": false,
                        "fullname": "Shun Zheng",
                        "user": "shun-zheng",
                        "type": "user"
                    },
                    "name": "Shun Zheng",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-18T02:37:06.379Z",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd131671046e",
                    "user": {
                        "_id": "67d7c0e0cb3e80af0d13660a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hCmEo__IWO_R_Ps8Q52Os.png",
                        "isPro": false,
                        "fullname": "zhijianxu",
                        "user": "VEWOXIC",
                        "type": "user"
                    },
                    "name": "Zhijian Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:15:46.044Z",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd131671046f",
                    "name": "Shengyu Ye",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710470",
                    "name": "Zhirong Wu",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710471",
                    "user": {
                        "_id": "6560763e152b659e623865ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liang",
                        "user": "MasterVito",
                        "type": "user"
                    },
                    "name": "Xiao Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:16:18.590Z",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710472",
                    "user": {
                        "_id": "604714a0c82d59b7347b55ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604714a0c82d59b7347b55ae/WZFKDDUPi8JS0BK8t7mIv.jpeg",
                        "isPro": false,
                        "fullname": "YangWang92",
                        "user": "yangwang92",
                        "type": "user"
                    },
                    "name": "Yang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:16:20.452Z",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710473",
                    "name": "Junjie Li",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710474",
                    "name": "Ziming Miao",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710475",
                    "name": "Jiang Bian",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710476",
                    "name": "Mao Yang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a7a2bad001860e0c34f7f2/zpklFaaRznQyEa0t9Ji70.png"
            ],
            "publishedAt": "2025-06-17T07:06:56.000Z",
            "submittedOnDailyAt": "2025-06-18T01:24:22.712Z",
            "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs",
            "submittedOnDailyBy": {
                "_id": "64a7a2bad001860e0c34f7f2",
                "avatarUrl": "/avatars/2433104071e4ae1c3e2d755d81d7964b.svg",
                "isPro": false,
                "fullname": "Shun Zheng",
                "user": "shun-zheng",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npromising paradigm for advancing the reasoning capabilities of Large Language\nModels (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned\nmodels often underperform their base models on the Pass@K metric for\nsolution-finding, leading to the hypothesis that RLVR merely re-weights\nexisting reasoning paths at the cost of reasoning diversity. In this work, we\nresolve this contradiction by identifying the source of the problem: the\nPass@K metric itself is a flawed measure of reasoning, as it credits correct\nfinal answers that probably arise from inaccurate or incomplete chains of\nthought (CoTs). To address this, we introduce a more precise evaluation metric,\nCoT-Pass@K, which mandates that both the reasoning path and the final\nanswer be correct. We provide a new theoretical foundation that formalizes how\nRLVR, unlike traditional RL, is uniquely structured to incentivize logical\nintegrity. Our empirical results are supportive: using CoT-Pass@K, we\nobserve that RLVR can incentivize the generalization of correct reasoning for\nall values of K. Furthermore, by analyzing the training dynamics, we find\nthat this enhanced reasoning capability emerges early in the training process\nand smoothly generalizes. Our work provides a clear perspective on the role of\nRLVR, offers a more reliable method for its evaluation, and confirms its\npotential to genuinely advance machine reasoning.",
            "upvotes": 22,
            "discussionId": "68521c2a0164cd1316710477",
            "ai_summary": "RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "LLMS",
                "Pass@K",
                "chains of thought",
                "CoT-Pass@K",
                "logical integrity",
                "machine reasoning",
                "training dynamics"
            ]
        },
        "translation_title": "검증 가능한 보상으로 강화 학습이 기본 LLM의 올바른 추론을 촉진한다",
        "purpose": "기존 언어 모델보다 더 우수한 추론 능력을 갖춘 모델을 만들기 위해 RLVR의 효과성을 높이고 평가 기준을 개선하려는 목표",
        "method": [
            "기존 Pass@K 메트릭의 문제점을 밝힘(we identify the source of the problem: the Pass@K metric itself is a flawed measure of reasoning)",
            "더 정확한 평가 기준인 CoT-Pass@K를 도입하여 추론 경로와 최종 답변이 모두 올바르게 요구되도록 함(To address this, we introduce a more precise evaluation metric, CoT-Pass@K)",
            "RLVR의 구조가 전통적인 RL과 다르게 논리적 일관성을 장려하는 방법을 이론적으로 정립함(we provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity)"
        ],
        "conclusion": "RLVR을 사용하면 CoT-Pass@K에서 모든 K 값에 대해 올바른 추론을 일반화할 수 있으며, 훈련 초기부터 향상된 추론 능력이 나타나고 부드럽게 일반화됨을 확인함.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Reinforcement Learning"
        ]
    }
]