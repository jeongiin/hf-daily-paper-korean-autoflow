[
    {
        "paper": {
            "id": "2502.18411",
            "authors": [
                {
                    "_id": "67be834ae7b05f9e43b172b2",
                    "user": {
                        "_id": "6530e62f536dbca918e71c3e",
                        "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Z",
                        "user": "PhoenixZ",
                        "type": "user"
                    },
                    "name": "Xiangyu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:26:02.247Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b3",
                    "user": {
                        "_id": "646cd947da8e99940b6e55cf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
                        "isPro": false,
                        "fullname": "Shengyuan Ding",
                        "user": "ChrisDing1105",
                        "type": "user"
                    },
                    "name": "Shengyuan Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:25:59.887Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b4",
                    "user": {
                        "_id": "675aa937ab6aa7ecd09341ce",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/d_CNUsNOw92pg7MVhf9Vm.png",
                        "isPro": false,
                        "fullname": "Zicheng Zhang",
                        "user": "UniverseCA",
                        "type": "user"
                    },
                    "name": "Zicheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:49:10.028Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b5",
                    "name": "Haian Huang",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b6",
                    "name": "Maosong Cao",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b7",
                    "user": {
                        "_id": "619507e7b74b6c591f794340",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                        "isPro": false,
                        "fullname": "Weiyun Wang",
                        "user": "Weiyun1025",
                        "type": "user"
                    },
                    "name": "Weiyun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:48:45.520Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b8",
                    "user": {
                        "_id": "64638c4d51fa6e63060521b5",
                        "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
                        "isPro": false,
                        "fullname": "JIaqi",
                        "user": "Jiaqiwang",
                        "type": "user"
                    },
                    "name": "Jiaqi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:48:38.876Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b9",
                    "user": {
                        "_id": "64f5f8dd9b17cd59c453c57f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
                        "isPro": false,
                        "fullname": "Xinyu Fang",
                        "user": "nebulae09",
                        "type": "user"
                    },
                    "name": "Xinyu Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:26:04.433Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172ba",
                    "user": {
                        "_id": "64d1c560c0c627dfa71bdbe0",
                        "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
                        "isPro": false,
                        "fullname": "wenhai.wang",
                        "user": "wangwhcore",
                        "type": "user"
                    },
                    "name": "Wenhai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:48:28.151Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172bb",
                    "name": "Guangtao Zhai",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172bc",
                    "user": {
                        "_id": "63ee1379190ddd6214efd73a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:48:20.155Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172bd",
                    "name": "Hua Yang",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172be",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T18:05:14.000Z",
            "title": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference",
            "summary": "Recent advancements in open-source multi-modal large language models (MLLMs)\nhave primarily focused on enhancing foundational capabilities, leaving a\nsignificant gap in human preference alignment. This paper introduces\nOmniAlign-V, a comprehensive dataset of 200K high-quality training samples\nfeaturing diverse images, complex questions, and varied response formats to\nimprove MLLMs' alignment with human preferences. We also present MM-AlignBench,\na human-annotated benchmark specifically designed to evaluate MLLMs' alignment\nwith human values. Experimental results show that finetuning MLLMs with\nOmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference\nOptimization (DPO), significantly enhances human preference alignment while\nmaintaining or enhancing performance on standard VQA benchmarks, preserving\ntheir fundamental capabilities. Our datasets, benchmark, code and checkpoints\nhave been released at https://github.com/PhoenixZ810/OmniAlign-V.",
            "upvotes": 52,
            "discussionId": "67be834ce7b05f9e43b1730a"
        },
        "translation_title": "OmniAlign-V: 인간 선호와의 향상된 정렬을 향하여",
        "purpose": "MLLMs의 인간 선호와의 정렬을 개선하기 위한 고품질 데이터세트 및 벤치마크 연구",
        "method": [
            "200K개의 다양한 이미지와 복잡한 질문, 다양한 응답 형식을 포함한 고품질 훈련 샘플 데이터세트를 생성함(To improve MLLMs' alignment with human preferences, this paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats.)",
            "MLLMs의 인간 가치 정렬을 평가하기 위해 MM-AlignBench라는 인간 주석 기반 벤치마크를 도입함(We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values.)",
            "Supervised Fine-Tuning (SFT) 또는 Direct Preference Optimization (DPO)를 사용하여 OmniAlign-V로 MLLMs를 미세 조정함(Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment.)"
        ],
        "conclusion": "OmniAlign-V를 사용한 MLLMs의 훈련은 인간 선호와의 정렬을 크게 향상시키며, 표준 VQA 벤치마크에서의 성능을 유지하거나 향상시킴.",
        "keywords": [
            "Natural Language Processing",
            "Multimodal Learning",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2502.18137",
            "authors": [
                {
                    "_id": "67be8443ed8e258c0f70063a",
                    "user": {
                        "_id": "66c0a08bac74db25de8427ec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
                        "isPro": false,
                        "fullname": "Jintao Zhang",
                        "user": "jt-zhang",
                        "type": "user"
                    },
                    "name": "Jintao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:25:57.704Z",
                    "hidden": false
                },
                {
                    "_id": "67be8443ed8e258c0f70063b",
                    "user": {
                        "_id": "6329bdbbde087eac2921e6a9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663679904323-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xiangchendong",
                        "user": "Xiang-cd",
                        "type": "user"
                    },
                    "name": "Chendong Xiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:49:29.341Z",
                    "hidden": false
                },
                {
                    "_id": "67be8443ed8e258c0f70063c",
                    "name": "Haofeng Huang",
                    "hidden": false
                },
                {
                    "_id": "67be8443ed8e258c0f70063d",
                    "name": "Jia Wei",
                    "hidden": false
                },
                {
                    "_id": "67be8443ed8e258c0f70063e",
                    "user": {
                        "_id": "65d5a000ec7e31555e4db57e",
                        "avatarUrl": "/avatars/aab8319fbaffdd53faff59a40ca5a5ea.svg",
                        "isPro": false,
                        "fullname": "Haocheng Xi",
                        "user": "hxi0408",
                        "type": "user"
                    },
                    "name": "Haocheng Xi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:49:45.446Z",
                    "hidden": false
                },
                {
                    "_id": "67be8443ed8e258c0f70063f",
                    "name": "Jun Zhu",
                    "hidden": false
                },
                {
                    "_id": "67be8443ed8e258c0f700640",
                    "user": {
                        "_id": "65fcad0ba0d7adc40b54fac2",
                        "avatarUrl": "/avatars/7564b5642378fddb46ec3b5ae57c0402.svg",
                        "isPro": false,
                        "fullname": "Jianfei Chen",
                        "user": "surfingtomchen",
                        "type": "user"
                    },
                    "name": "Jianfei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:49:52.550Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T12:02:17.000Z",
            "title": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference",
            "summary": "An efficient attention implementation is essential for large models due to\nits quadratic time complexity. Fortunately, attention commonly exhibits\nsparsity, i.e., many values in the attention map are near zero, allowing for\nthe omission of corresponding computations. Many studies have utilized the\nsparse pattern to accelerate attention. However, most existing works focus on\noptimizing attention within specific models by exploiting certain sparse\npatterns of the attention map. A universal sparse attention that guarantees\nboth the speedup and end-to-end performance of diverse models remains elusive.\nIn this paper, we propose SpargeAttn, a universal sparse and quantized\nattention for any model. Our method uses a two-stage online filter: in the\nfirst stage, we rapidly and accurately predict the attention map, enabling the\nskip of some matrix multiplications in attention. In the second stage, we\ndesign an online softmax-aware filter that incurs no extra overhead and further\nskips some matrix multiplications. Experiments show that our method\nsignificantly accelerates diverse models, including language, image, and video\ngeneration, without sacrificing end-to-end metrics. The codes are available at\nhttps://github.com/thu-ml/SpargeAttn.",
            "upvotes": 36,
            "discussionId": "67be8447ed8e258c0f70075f"
        },
        "translation_title": "SpargeAttn: 모든 모델 추론을 가속화하는 정확한 희소 주의 메커니즘",
        "purpose": "다양한 모델의 추론 속도를 높이고 성능을 유지하기 위한 보편적인 희소 주의 메커니즘 개발",
        "method": [
            "희소 패턴을 활용하여 주의 맵을 신속하게 예측하고 일부 행렬 곱셈을 생략할 수 있도록 하는 두 단계 온라인 필터를 사용함(In the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention.)",
            "온라인 소프트맥스 인식 필터를 설계하여 추가 오버헤드 없이 또 다른 행렬 곱셈을 생략할 수 있도록 함(In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications.)"
        ],
        "conclusion": "SpargeAttn은 다양한 모델, 즉 언어, 이미지 및 비디오 생성 모델을 획기적으로 가속화하며, 성능 저하 없이 우수한 결과를 제공함.",
        "keywords": [
            "Natural Language Processing",
            "Computer Vision",
            "Video Generation"
        ]
    },
    {
        "paper": {
            "id": "2502.17363",
            "authors": [
                {
                    "_id": "67bd6d2bbf6d46017e619f31",
                    "user": {
                        "_id": "66078994c50f8393c56ed837",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/aYYde45zaFACRllyEhJyU.jpeg",
                        "isPro": true,
                        "fullname": "Tianrui Zhu",
                        "user": "xilluill",
                        "type": "user"
                    },
                    "name": "Tianrui Zhu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-25T07:24:35.845Z",
                    "hidden": false
                },
                {
                    "_id": "67bd6d2bbf6d46017e619f32",
                    "user": {
                        "_id": "6315d306a9456afe2b9bf34a",
                        "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
                        "isPro": false,
                        "fullname": "ElevenZ",
                        "user": "shiyi0408",
                        "type": "user"
                    },
                    "name": "Shiyi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:30:48.262Z",
                    "hidden": false
                },
                {
                    "_id": "67bd6d2bbf6d46017e619f33",
                    "user": {
                        "_id": "646c6985d072747f7ebf352a",
                        "avatarUrl": "/avatars/8aaf92045687b21b56c257db62bf4fa5.svg",
                        "isPro": false,
                        "fullname": "Jiawei Shao",
                        "user": "jewelshaw",
                        "type": "user"
                    },
                    "name": "Jiawei Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:50:09.030Z",
                    "hidden": false
                },
                {
                    "_id": "67bd6d2bbf6d46017e619f34",
                    "name": "Yansong Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-24T17:40:09.000Z",
            "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
            "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to O(1) using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
            "upvotes": 25,
            "discussionId": "67bd6d2dbf6d46017e619f99"
        },
        "translation_title": "KV-Edit: 정밀 배경 보존을 위한 비훈련형 이미지 편집",
        "purpose": "이미지 편집에서 원본 이미지의 유사성을 유지하면서 목표에 맞는 콘텐츠를 생성하는 데 경험하는 과제를 해결하기 위해.",
        "method": [
            "KV cache를 사용하여 배경 일관성을 유지하는 비훈련형 접근 방식인 KV-Edit 제안을 함(Hence, we propose KV-Edit, a training-free approach that uses KV cache in DiTs to maintain background consistency.)",
            "편집 중 KV cache의 메모리 소비를 탐색하고 공간 복잡성을 O(1)로 최적화함(We further explore the memory consumption of the KV cache during editing and optimize the space complexity to O(1) using an inversion-free method.)",
            "추가 훈련 없이 모든 DiT 기반 생성 모델과 호환됨(Our approach is compatible with any DiT-based generative model without additional training.)"
        ],
        "conclusion": "KV-Edit는 기존 접근 방식에 비해 배경과 이미지 품질 측면에서 상당한 향상을 보이며, 훈련 기반 방법보다도 뛰어난 성능을 발휘함.",
        "keywords": [
            "Image Generation",
            "Image Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2502.18364",
            "authors": [
                {
                    "_id": "67be81414084d82ee69ad4a2",
                    "user": {
                        "_id": "647e83257f9ad5e44babe82a",
                        "avatarUrl": "/avatars/2d9593775c49856fe5dfa5bd23dfcda7.svg",
                        "isPro": false,
                        "fullname": "yifan pu",
                        "user": "yifanpu001",
                        "type": "user"
                    },
                    "name": "Yifan Pu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:58:24.942Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a3",
                    "user": {
                        "_id": "65e78ebf24a38e0fc5e149e6",
                        "avatarUrl": "/avatars/d05e267f29de7de226c4fc0ae37c95ff.svg",
                        "isPro": false,
                        "fullname": "Yiming Zhao",
                        "user": "2JZ",
                        "type": "user"
                    },
                    "name": "Yiming Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:58:30.860Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a4",
                    "name": "Zhicong Tang",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a5",
                    "name": "Ruihong Yin",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a6",
                    "user": {
                        "_id": "65229f2f6b01183a67e86370",
                        "avatarUrl": "/avatars/b218207fce28497b30e22c807d44b2d2.svg",
                        "isPro": false,
                        "fullname": "Haoxing Ye",
                        "user": "131131yhx",
                        "type": "user"
                    },
                    "name": "Haoxing Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:58:52.821Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a7",
                    "name": "Yuhui Yuan",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a8",
                    "user": {
                        "_id": "666470a28f5513b0cf11e850",
                        "avatarUrl": "/avatars/7beea758882677ad32a12ce56d4d084a.svg",
                        "isPro": false,
                        "fullname": "Dong Chen",
                        "user": "DongChen06",
                        "type": "user"
                    },
                    "name": "Dong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:59:16.526Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a9",
                    "user": {
                        "_id": "646b2f4bb1202bc77c0fb396",
                        "avatarUrl": "/avatars/6b09dec5d5affe817ad6acda60f61740.svg",
                        "isPro": false,
                        "fullname": "Jianmin_bao",
                        "user": "JianminBao",
                        "type": "user"
                    },
                    "name": "Jianmin Bao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:59:22.654Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4aa",
                    "user": {
                        "_id": "64f7f119a92703ef65d9a717",
                        "avatarUrl": "/avatars/118524faab66cecba6d4da622034b44b.svg",
                        "isPro": false,
                        "fullname": "Sirui Zhang",
                        "user": "zsr200901",
                        "type": "user"
                    },
                    "name": "Sirui Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:59:30.766Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4ab",
                    "user": {
                        "_id": "67965a5a9f57883759a6efc3",
                        "avatarUrl": "/avatars/9138a879fbe1f60c2f4720810bfdfda6.svg",
                        "isPro": false,
                        "fullname": "Yanbin Wang",
                        "user": "yanbinwang",
                        "type": "user"
                    },
                    "name": "Yanbin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:59:38.138Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4ac",
                    "name": "Lin Liang",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4ad",
                    "user": {
                        "_id": "6672e20d1dbdf7da8310dd92",
                        "avatarUrl": "/avatars/5d2fb23f92a7f9ff025a5be17a26de4d.svg",
                        "isPro": false,
                        "fullname": "lijuanwang",
                        "user": "lijuanwang228",
                        "type": "user"
                    },
                    "name": "Lijuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:00:05.520Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4ae",
                    "name": "Ji Li",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4af",
                    "name": "Xiu Li",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4b0",
                    "user": {
                        "_id": "64c882f7527d7636555bbb2c",
                        "avatarUrl": "/avatars/578a118a945dd6fa62fd3be9d6e4e986.svg",
                        "isPro": false,
                        "fullname": "Zhouhui Lian",
                        "user": "lianzhouhui",
                        "type": "user"
                    },
                    "name": "Zhouhui Lian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:59:57.943Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4b1",
                    "name": "Gao Huang",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4b2",
                    "name": "Baining Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T16:57:04.000Z",
            "title": "ART: Anonymous Region Transformer for Variable Multi-Layer Transparent\n  Image Generation",
            "summary": "Multi-layer image generation is a fundamental task that enables users to\nisolate, select, and edit specific image layers, thereby revolutionizing\ninteractions with generative models. In this paper, we introduce the Anonymous\nRegion Transformer (ART), which facilitates the direct generation of variable\nmulti-layer transparent images based on a global text prompt and an anonymous\nregion layout. Inspired by Schema theory suggests that knowledge is organized\nin frameworks (schemas) that enable people to interpret and learn from new\ninformation by linking it to prior knowledge.}, this anonymous region layout\nallows the generative model to autonomously determine which set of visual\ntokens should align with which text tokens, which is in contrast to the\npreviously dominant semantic layout for the image generation task. In addition,\nthe layer-wise region crop mechanism, which only selects the visual tokens\nbelonging to each anonymous region, significantly reduces attention computation\ncosts and enables the efficient generation of images with numerous distinct\nlayers (e.g., 50+). When compared to the full attention approach, our method is\nover 12 times faster and exhibits fewer layer conflicts. Furthermore, we\npropose a high-quality multi-layer transparent image autoencoder that supports\nthe direct encoding and decoding of the transparency of variable multi-layer\nimages in a joint manner. By enabling precise control and scalable layer\ngeneration, ART establishes a new paradigm for interactive content creation.",
            "upvotes": 20,
            "discussionId": "67be81464084d82ee69ad576"
        },
        "translation_title": "ART: 가변 다층 투명 이미지 생성을 위한 Anonymous Region Transformer",
        "purpose": "사용자가 특정 이미지 레이어를 선택하고 편집할 수 있는 다층 이미지 생성의 효율성을 높이기 위한 연구",
        "method": [
            "Anonymous Region Transformer(ART)를 도입해 전역 텍스트 프롬프트와 익명 영역 레이아웃을 기반으로 다층 투명 이미지를 생성함(In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variable multi-layer transparent images based on a global text prompt and an anonymous region layout.)",
            "익명 영역 레이아웃을 통해 생성 모델이 텍스트 토큰과 시각 토큰을 자동으로 정렬할 수 있도록 함(This anonymous region layout allows the generative model to autonomously determine which set of visual tokens should align with which text tokens.)",
            "레이어별 지역 크롭 메커니즘을 이용해 주의 계산 비용을 크게 줄임(In addition, the layer-wise region crop mechanism, which only selects the visual tokens belonging to each anonymous region, significantly reduces attention computation costs.)"
        ],
        "conclusion": "ART는 12배 이상 빠르며, 조작성이 뛰어난 다층 투명 이미지 생성의 새로운 패러다임을 제시함.",
        "keywords": [
            "Image Generation",
            "Computer Vision",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2502.18449",
            "authors": [
                {
                    "_id": "67be845a8a5a80542314579f",
                    "user": {
                        "_id": "632a176259950c1d279d5ea7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632a176259950c1d279d5ea7/xsSGhBXalt9RaKzSKY8uk.jpeg",
                        "isPro": false,
                        "fullname": "Yuxiang Wei",
                        "user": "yuxiang630",
                        "type": "user"
                    },
                    "name": "Yuxiang Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:50:44.837Z",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a0",
                    "name": "Olivier Duchenne",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a1",
                    "user": {
                        "_id": "6481e0ac50b759c75d5fdad0",
                        "avatarUrl": "/avatars/49f08d989ca505ae01bce5578a94f6fe.svg",
                        "isPro": false,
                        "fullname": "Jade Copet",
                        "user": "JadeCopet",
                        "type": "user"
                    },
                    "name": "Jade Copet",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:50:58.290Z",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a2",
                    "name": "Quentin Carbonneaux",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a3",
                    "user": {
                        "_id": "656f473c14fa8cfccd14559e",
                        "avatarUrl": "/avatars/8f4fef3d835a7a11c2ab66dbf04f3424.svg",
                        "isPro": false,
                        "fullname": "Lingming Zhang",
                        "user": "lingming",
                        "type": "user"
                    },
                    "name": "Lingming Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:51:10.640Z",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a4",
                    "name": "Daniel Fried",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a5",
                    "user": {
                        "_id": "630eac7931970d1cd4fbacf2",
                        "avatarUrl": "/avatars/b7ccbddfa745db854dc342be1327cd53.svg",
                        "isPro": false,
                        "fullname": "Gabriel Synnaeve",
                        "user": "gsynnaeve",
                        "type": "user"
                    },
                    "name": "Gabriel Synnaeve",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:51:21.641Z",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a6",
                    "user": {
                        "_id": "6597e5a6420dcc68501a69e9",
                        "avatarUrl": "/avatars/da48b13e07c367ecd5c891abfd6c3ded.svg",
                        "isPro": false,
                        "fullname": "Rishabh Singh",
                        "user": "RishabhSingh021",
                        "type": "user"
                    },
                    "name": "Rishabh Singh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:51:28.321Z",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a7",
                    "name": "Sida I. Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T18:45:04.000Z",
            "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open\n  Software Evolution",
            "summary": "The recent DeepSeek-R1 release has demonstrated the immense potential of\nreinforcement learning (RL) in enhancing the general reasoning capabilities of\nlarge language models (LLMs). While DeepSeek-R1 and other follow-up work\nprimarily focus on applying RL to competitive coding and math problems, this\npaper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for\nreal-world software engineering. Leveraging a lightweight rule-based reward\n(e.g., the similarity score between ground-truth and LLM-generated solutions),\nSWE-RL enables LLMs to autonomously recover a developer's reasoning processes\nand solutions by learning from extensive open-source software evolution data --\nthe record of a software's entire lifecycle, including its code snapshots, code\nchanges, and events such as issues and pull requests. Trained on top of Llama\n3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve\nrate on SWE-bench Verified -- a human-verified collection of real-world GitHub\nissues. To our knowledge, this is the best performance reported for\nmedium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs\nlike GPT-4o. Surprisingly, despite performing RL solely on software evolution\ndata, Llama3-SWE-RL has even emerged with generalized reasoning skills. For\nexample, it shows improved results on five out-of-domain tasks, namely,\nfunction coding, library use, code reasoning, mathematics, and general language\nunderstanding, whereas a supervised-finetuning baseline even leads to\nperformance degradation on average. Overall, SWE-RL opens up a new direction to\nimprove the reasoning capabilities of LLMs through reinforcement learning on\nmassive software engineering data.",
            "upvotes": 18,
            "discussionId": "67be845b8a5a8054231457d6"
        },
        "translation_title": "SWE-RL: 오픈 소프트웨어 진화를 통한 LLM 추론 향상",
        "purpose": "대규모 언어 모델(LLM)의 추론 능력을 향상시키기 위한 RL 기반 접근법 개발",
        "method": [
            "가벼운 규칙 기반 보상 시스템을 활용하여 LLM이 개발자의 추론 과정과 솔루션을 자율적으로 복구할 수 있도록 함(Leveraging a lightweight rule-based reward, SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions.)",
            "방대한 오픈 소스 소프트웨어 진화 데이터를 활용하여 교육을 진행함(learning from extensive open-source software evolution data)",
            "Llama 3 기반으로 훈련된 모델이 SWE-bench Verified에서 41.0%의 해결률을 달성함(we train Llama3-SWE-RL-70B, achieving a 41.0% solve rate on SWE-bench Verified)."
        ],
        "conclusion": "SWE-RL은 소프트웨어 공학 데이터를 통해 LLM의 추론 능력을 개선할 새로운 방향을 제시하며, 소프트웨어 진화 데이터에서만 RL을 수행하였음에도 불구하고 일반화된 추론 능력을 보여주었음.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Reinforcement Learning"
        ]
    }
]