[
    {
        "paper": {
            "id": "2502.13923",
            "authors": [
                {
                    "_id": "67b6b0688b56622e70b9e83e",
                    "name": "Shuai Bai",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e83f",
                    "name": "Keqin Chen",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e840",
                    "name": "Xuejing Liu",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e841",
                    "name": "Jialin Wang",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e842",
                    "name": "Wenbin Ge",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e843",
                    "name": "Sibo Song",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e844",
                    "name": "Kai Dang",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e845",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e846",
                    "name": "Shijie Wang",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e847",
                    "name": "Jun Tang",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e848",
                    "name": "Humen Zhong",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e849",
                    "name": "Yuanzhi Zhu",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e84a",
                    "user": {
                        "_id": "6417fa211f1f3b0fa811edc0",
                        "avatarUrl": "/avatars/fa9e1ef1472a736c2ceebe12b77d6c89.svg",
                        "isPro": false,
                        "fullname": "Mingkun Yang",
                        "user": "ayumiymk",
                        "type": "user"
                    },
                    "name": "Mingkun Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-20T09:35:44.878Z",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e84b",
                    "name": "Zhaohai Li",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e84c",
                    "name": "Jianqiang Wan",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e84d",
                    "name": "Pengfei Wang",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e84e",
                    "name": "Wei Ding",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e84f",
                    "user": {
                        "_id": "63ee22e75f1300034ddaaf54",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676550873969-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Zheren Fu",
                        "user": "darkpromise",
                        "type": "user"
                    },
                    "name": "Zheren Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-20T10:49:47.484Z",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e850",
                    "name": "Yiheng Xu",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e851",
                    "name": "Jiabo Ye",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e852",
                    "name": "Xi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e853",
                    "name": "Tianbao Xie",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e854",
                    "name": "Zesen Cheng",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e855",
                    "name": "Hang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e856",
                    "name": "Zhibo Yang",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e857",
                    "user": {
                        "_id": "645b10e80c73ea27d13f7aca",
                        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
                        "isPro": false,
                        "fullname": "xuhaiyang",
                        "user": "xhyandwyy",
                        "type": "user"
                    },
                    "name": "Haiyang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-20T09:35:42.372Z",
                    "hidden": false
                },
                {
                    "_id": "67b6b0688b56622e70b9e858",
                    "name": "Junyang Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-19T18:00:14.000Z",
            "title": "Qwen2.5-VL Technical Report",
            "summary": "We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language\nseries, which demonstrates significant advancements in both foundational\ncapabilities and innovative functionalities. Qwen2.5-VL achieves a major leap\nforward in understanding and interacting with the world through enhanced visual\nrecognition, precise object localization, robust document parsing, and\nlong-video comprehension. A standout feature of Qwen2.5-VL is its ability to\nlocalize objects using bounding boxes or points accurately. It provides robust\nstructured data extraction from invoices, forms, and tables, as well as\ndetailed analysis of charts, diagrams, and layouts. To handle complex inputs,\nQwen2.5-VL introduces dynamic resolution processing and absolute time encoding,\nenabling it to process images of varying sizes and videos of extended durations\n(up to hours) with second-level event localization. This allows the model to\nnatively perceive spatial scales and temporal dynamics without relying on\ntraditional normalization techniques. By training a native dynamic-resolution\nVision Transformer (ViT) from scratch and incorporating Window Attention, we\nreduce computational overhead while maintaining native resolution. As a result,\nQwen2.5-VL excels not only in static image and document understanding but also\nas an interactive visual agent capable of reasoning, tool usage, and task\nexecution in real-world scenarios such as operating computers and mobile\ndevices. Qwen2.5-VL is available in three sizes, addressing diverse use cases\nfrom edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model\nmatches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly\nexcelling in document and diagram understanding. Additionally, Qwen2.5-VL\nmaintains robust linguistic performance, preserving the core language\ncompetencies of the Qwen2.5 LLM.",
            "upvotes": 71,
            "discussionId": "67b6b0688b56622e70b9e875"
        },
        "translation_title": "Qwen2.5-VL 기술 보고서",
        "purpose": "새로운 Qwen 비전-언어 모델인 Qwen2.5-VL의 기본 기능과 혁신 기능을 개선하려는 목표",
        "method": [
            "Qwen2.5-VL이 객체를 정확하게 지역화할 수 있는 기능을 통해 향상된 시각 인식과 정밀한 객체 위치 معينة를 제공함.(Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization.)",
            "다양한 크기의 이미지와 긴 영상(최대 수 시간)을 처리할 수 있는 동적 해상도 처리 및 절대 시간 인코딩을 도입함.(This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques.)",
            "기본 동적 해상도 Vision Transformer(ViT)를 처음부터 훈련하여 계산 오버헤드를 줄이고, 원래 해상도를 유지함.(By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution.)"
        ],
        "conclusion": "Qwen2.5-VL은 정적 이미지 및 문서 이해뿐만 아니라 실제 시나리오에서 추론, 도구 사용 및 작업 수행이 가능한 인터랙티브 비주얼 에이전트로서 우수한 성능을 발휘함.",
        "keywords": [
            "Vision-Language Models",
            "Document Parsing",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2502.13144",
            "authors": [
                {
                    "_id": "67b55c7fba22c1ddbb8d5746",
                    "user": {
                        "_id": "6536187bd34e9f02b9df1c3b",
                        "avatarUrl": "/avatars/0b34d62868b93053b0a05062a018b5bd.svg",
                        "isPro": false,
                        "fullname": "Hao Gao",
                        "user": "Hao605",
                        "type": "user"
                    },
                    "name": "Hao Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-19T09:00:48.944Z",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d5747",
                    "name": "Shaoyu Chen",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d5748",
                    "name": "Bo Jiang",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d5749",
                    "name": "Bencheng Liao",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d574a",
                    "name": "Yiang Shi",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d574b",
                    "name": "Xiaoyang Guo",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d574c",
                    "name": "Yuechuan Pu",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d574d",
                    "name": "Haoran Yin",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d574e",
                    "name": "Xiangyu Li",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d574f",
                    "name": "Xinbang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d5750",
                    "name": "Ying Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d5751",
                    "name": "Wenyu Liu",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d5752",
                    "name": "Qian Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b55c7fba22c1ddbb8d5753",
                    "name": "Xinggang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-18T18:59:21.000Z",
            "title": "RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based\n  Reinforcement Learning",
            "summary": "Existing end-to-end autonomous driving (AD) algorithms typically follow the\nImitation Learning (IL) paradigm, which faces challenges such as causal\nconfusion and the open-loop gap. In this work, we establish a 3DGS-based\nclosed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS\ntechniques, we construct a photorealistic digital replica of the real physical\nworld, enabling the AD policy to extensively explore the state space and learn\nto handle out-of-distribution scenarios through large-scale trial and error. To\nenhance safety, we design specialized rewards that guide the policy to\neffectively respond to safety-critical events and understand real-world causal\nrelationships. For better alignment with human driving behavior, IL is\nincorporated into RL training as a regularization term. We introduce a\nclosed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS\nenvironments. Compared to IL-based methods, RAD achieves stronger performance\nin most closed-loop metrics, especially 3x lower collision rate. Abundant\nclosed-loop results are presented at https://hgao-cv.github.io/RAD.",
            "upvotes": 27,
            "discussionId": "67b55c80ba22c1ddbb8d579c"
        },
        "translation_title": "RAD: 대규모 3DGS 기반 강화 학습을 통한 종단 간 주행 정책 훈련",
        "purpose": "종단 간 자율 주행 알고리즘의 성능을 향상시키기 위한 새로운 훈련 패러다임 개발",
        "method": [
            "3DGS 기술을 활용하여 물리적 세계의 포토리얼리스틱 디지털 복제를 구축함(we construct a photorealistic digital replica of the real physical world by leveraging 3DGS techniques.)",
            "안전성을 높이기 위해 안전-critical 이벤트에 효과적으로 대응하도록 정책을 안내하는 보상을 설계함(we design specialized rewards that guide the policy to effectively respond to safety-critical events.)",
            "RL 훈련에 IL을 정규화 항으로 통합하여 인간 운전 행동과의 일치를 개선함(IL is incorporated into RL training as a regularization term for better alignment with human driving behavior.)"
        ],
        "conclusion": "RAD는 대부분의 클로즈드 루프 메트릭에서 IL 기반 방법보다 더 나은 성능을 보여주며, 특히 충돌률이 3배 낮아짐을 확인함.",
        "keywords": [
            "Robotics",
            "Reinforcement Learning",
            "3D Vision"
        ]
    },
    {
        "paper": {
            "id": "2502.13128",
            "authors": [
                {
                    "_id": "67b6c696e9b901edeaf320d5",
                    "name": "Zihan Liu",
                    "hidden": false
                },
                {
                    "_id": "67b6c696e9b901edeaf320d6",
                    "name": "Shuangrui Ding",
                    "hidden": false
                },
                {
                    "_id": "67b6c696e9b901edeaf320d7",
                    "name": "Zhixiong Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b6c696e9b901edeaf320d8",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "67b6c696e9b901edeaf320d9",
                    "name": "Pan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b6c696e9b901edeaf320da",
                    "name": "Yuhang Zang",
                    "hidden": false
                },
                {
                    "_id": "67b6c696e9b901edeaf320db",
                    "name": "Yuhang Cao",
                    "hidden": false
                },
                {
                    "_id": "67b6c696e9b901edeaf320dc",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "67b6c696e9b901edeaf320dd",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-18T18:52:21.000Z",
            "title": "SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song\n  Generation",
            "summary": "Text-to-song generation, the task of creating vocals and accompaniment from\ntextual inputs, poses significant challenges due to domain complexity and data\nscarcity. Existing approaches often employ multi-stage generation procedures,\nresulting in cumbersome training and inference pipelines. In this paper, we\npropose SongGen, a fully open-source, single-stage auto-regressive transformer\ndesigned for controllable song generation. The proposed model facilitates\nfine-grained control over diverse musical attributes, including lyrics and\ntextual descriptions of instrumentation, genre, mood, and timbre, while also\noffering an optional three-second reference clip for voice cloning. Within a\nunified auto-regressive framework, SongGen supports two output modes: mixed\nmode, which generates a mixture of vocals and accompaniment directly, and\ndual-track mode, which synthesizes them separately for greater flexibility in\ndownstream applications. We explore diverse token pattern strategies for each\nmode, leading to notable improvements and valuable insights. Furthermore, we\ndesign an automated data preprocessing pipeline with effective quality control.\nTo foster community engagement and future research, we will release our model\nweights, training code, annotated data, and preprocessing pipeline. The\ngenerated samples are showcased on our project page at\nhttps://liuzh-19.github.io/SongGen/ , and the code will be available at\nhttps://github.com/LiuZH-19/SongGen .",
            "upvotes": 24,
            "discussionId": "67b6c698e9b901edeaf321a7"
        },
        "translation_title": "SongGen: 텍스트를 송으로 생성하기 위한 단일 단계 자기 회귀 변환기",
        "purpose": "텍스트를 기반으로 한 송 생성의 복잡성과 데이터 부족 문제를 해결하기 위해 단일 단계 모델 개발",
        "method": [
            "SongGen이라는 오픈 소스 자기 회귀 변환기를 제안하여 컨트롤 가능한 송 생성을 지원함(We propose SongGen, a fully open-source, single-stage auto-regressive transformer designed for controllable song generation.)",
            "모델이 노래의 다양한 속성을 세밀하게 조정할 수 있도록 설계함(The proposed model facilitates fine-grained control over diverse musical attributes, including lyrics and textual descriptions of instrumentation, genre, mood, and timbre, while also offering an optional three-second reference clip for voice cloning.)",
            "통합된 자기 회귀 프레임워크를 통해 혼합 모드 및 듀얼 트랙 모드를 지원함(SongGen supports two output modes: mixed mode, which generates a mixture of vocals and accompaniment directly, and dual-track mode, which synthesizes them separately for greater flexibility in downstream applications.)",
            "자동 데이터 전처리 파이프라인을 설계하여 품질 관리를 효과적으로 수행함(We design an automated data preprocessing pipeline with effective quality control.)"
        ],
        "conclusion": "SongGen은 다양한 음악적 속성을 조정할 수 있으며, 커뮤니티 참여와 향후 연구를 위한 모델 가중치 및 데이터가 공개될 예정임.",
        "keywords": [
            "Music Generation",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2502.13685",
            "authors": [
                {
                    "_id": "67b6dc1ba7567156c6547880",
                    "name": "Jusen Du",
                    "hidden": false
                },
                {
                    "_id": "67b6dc1ba7567156c6547881",
                    "user": {
                        "_id": "6246bb33da617c00b48e4d92",
                        "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
                        "isPro": false,
                        "fullname": "Weigao Sun",
                        "user": "weigao266",
                        "type": "user"
                    },
                    "name": "Weigao Sun",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-20T07:39:08.547Z",
                    "hidden": false
                },
                {
                    "_id": "67b6dc1ba7567156c6547882",
                    "name": "Disen Lan",
                    "hidden": false
                },
                {
                    "_id": "67b6dc1ba7567156c6547883",
                    "name": "Jiaxi Hu",
                    "hidden": false
                },
                {
                    "_id": "67b6dc1ba7567156c6547884",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-19T12:53:55.000Z",
            "title": "MoM: Linear Sequence Modeling with Mixture-of-Memories",
            "summary": "Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE.",
            "upvotes": 19,
            "discussionId": "67b6dc1ca7567156c65478b8"
        },
        "translation_title": "MoM: Mixture-of-Memories를 통한 선형 시퀀스 모델링",
        "purpose": "기억 용량을 극대화하면서 기억 간섭을 최소화하여 recall-intensive 작업에서의 성능 향상을 목표로 함.",
        "method": [
            "Mixture-of-Memories(MoM) 구조를 도입해 여러 독립적인 기억 상태를 활용함.",
            "라우터 네트워크를 사용해 입력 토큰을 특정 기억 상태로 유도함.",
            "각 기억 상태의 계산을 선형으로 유지하며 효율성을 보장함."
        ],
        "conclusion": "MoM은 recall-intensive 언어 작업에서 기존 선형 시퀀스 모델을 초월하는 성능을 보여줬으며, Transformer 모델과 비교할 만한 성능을 달성함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Memory Interference"
        ]
    },
    {
        "paper": {
            "id": "2502.13347",
            "authors": [
                {
                    "_id": "67b6a7e83ef3656c48f149b9",
                    "user": {
                        "_id": "6135eeeb5bc6ecdf86b60f0d",
                        "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
                        "isPro": false,
                        "fullname": "Shi Yu",
                        "user": "yushi",
                        "type": "user"
                    },
                    "name": "Shi Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-20T09:35:47.487Z",
                    "hidden": false
                },
                {
                    "_id": "67b6a7e83ef3656c48f149ba",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "67b6a7e83ef3656c48f149bb",
                    "name": "Chenyan Xiong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-19T00:31:43.000Z",
            "title": "Craw4LLM: Efficient Web Crawling for LLM Pretraining",
            "summary": "Web crawl is a main source of large language models' (LLMs) pretraining data,\nbut the majority of crawled web pages are discarded in pretraining due to low\ndata quality. This paper presents Crawl4LLM, an efficient web crawling method\nthat explores the web graph based on the preference of LLM pretraining.\nSpecifically, it leverages the influence of a webpage in LLM pretraining as the\npriority score of the web crawler's scheduler, replacing the standard graph\nconnectivity based priority. Our experiments on a web graph containing 900\nmillion webpages from a commercial search engine's index demonstrate the\nefficiency of Crawl4LLM in obtaining high-quality pretraining data. With just\n21% URLs crawled, LLMs pretrained on Crawl4LLM data reach the same downstream\nperformances of previous crawls, significantly reducing the crawling waste and\nalleviating the burdens on websites. Our code is publicly available at\nhttps://github.com/cxcscmu/Crawl4LLM.",
            "upvotes": 18,
            "discussionId": "67b6a7e93ef3656c48f149f1"
        },
        "translation_title": "Craw4LLM: LLM 사전 학습을 위한 효율적인 웹 크롤링",
        "purpose": "LLM 사전 학습 데이터의 품질을 높이기 위한 효율적인 웹 크롤링 방법 연구",
        "method": [
            "웹 그래프를 기반으로 LLM 사전 학습의 선호도에 맞춰 웹 크롤러의 스케줄러를 우선 순위로 설정함(We present Crawl4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining.)",
            "웹 페이지의 영향을 우선 순위 점수로 활용하여 기존의 그래프 연결 기반 우선 순위를 대체함(It leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority.)",
            "실험을 통해 900백만 개의 웹 페이지가 포함된 웹 그래프에서 Crawl4LLM의 효율성을 입증함(Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Crawl4LLM in obtaining high-quality pretraining data.)"
        ],
        "conclusion": "Crawl4LLM 데이터로 사전 학습한 LLM은 21%의 URL만 크롤링하여도 이전 크롤링 데이터와 동일한 다운스트림 성능을 달성할 수 있어 크롤링 낭비를 크게 줄이고 웹사이트에 대한 부담을 완화함.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Web Crawling"
        ]
    }
]