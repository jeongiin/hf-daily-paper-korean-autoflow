[
    {
        "paper": {
            "id": "2509.17567",
            "authors": [
                {
                    "_id": "68d2047d1ca7156988a8eca6",
                    "user": {
                        "_id": "6002c316698168af3bb9f4a6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6002c316698168af3bb9f4a6/M2J2QFCRc5RhYRoz7OuZN.png",
                        "isPro": false,
                        "fullname": "yangxiao",
                        "user": "YangXiao-nlp",
                        "type": "user"
                    },
                    "name": "Yang Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:07:07.291Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8eca7",
                    "user": {
                        "_id": "66d01e4401f2a6b4cd93ad87",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png",
                        "isPro": false,
                        "fullname": "Mohan Jiang",
                        "user": "mhjiang0408",
                        "type": "user"
                    },
                    "name": "Mohan Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T02:42:46.715Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8eca8",
                    "name": "Jie Sun",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8eca9",
                    "user": {
                        "_id": "668e476520e499a0786ea56e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668e476520e499a0786ea56e/lnvd1_UWW9o9ddrR6ehwR.png",
                        "isPro": false,
                        "fullname": "Keyu Li",
                        "user": "weizhihao1",
                        "type": "user"
                    },
                    "name": "Keyu Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:07:10.827Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecaa",
                    "user": {
                        "_id": "66fa544c54f87b607fbffd2e",
                        "avatarUrl": "/avatars/94195dcda0eb68e8fd20d80718744697.svg",
                        "isPro": false,
                        "fullname": "Jifan Lin",
                        "user": "evanlin2570",
                        "type": "user"
                    },
                    "name": "Jifan Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:07:05.060Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecab",
                    "user": {
                        "_id": "6651f8441b1ce9f4a61a09ee",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YM8m6_FLN_eF5PmqD7BfA.png",
                        "isPro": false,
                        "fullname": "Zhuang Yumin",
                        "user": "happyZYM",
                        "type": "user"
                    },
                    "name": "Yumin Zhuang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:23:43.458Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecac",
                    "name": "Ji Zeng",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecad",
                    "user": {
                        "_id": "65900d4ff5a209eeac08b463",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65900d4ff5a209eeac08b463/PJNNBRJIk1qR24oaRLTex.jpeg",
                        "isPro": false,
                        "fullname": "shijie xia",
                        "user": "seven-cat",
                        "type": "user"
                    },
                    "name": "Shijie Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:23:09.499Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecae",
                    "name": "Qishuo Hua",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecaf",
                    "user": {
                        "_id": "67638cc0d63e4b348e8a5fa3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67638cc0d63e4b348e8a5fa3/BZNlw1uTGUcumCrXKkerx.png",
                        "isPro": false,
                        "fullname": "Xuefeng Li",
                        "user": "drxuefeng",
                        "type": "user"
                    },
                    "name": "Xuefeng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:22:55.873Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb0",
                    "user": {
                        "_id": "6734008e534ff3213926e239",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/L91SOmXRLz8rbDRHz4Hxs.png",
                        "isPro": false,
                        "fullname": "Xiaojie Cai",
                        "user": "jerrycxj",
                        "type": "user"
                    },
                    "name": "Xiaojie Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:22:16.516Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb1",
                    "user": {
                        "_id": "666292594676a4e4e4d8743f",
                        "avatarUrl": "/avatars/69154b27e67fd4adae651f2e0cffb5b7.svg",
                        "isPro": false,
                        "fullname": "Tongyu Wang",
                        "user": "Sprzwty",
                        "type": "user"
                    },
                    "name": "Tongyu Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:22:46.864Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb2",
                    "name": "Yue Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb3",
                    "name": "Liming Liu",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb4",
                    "name": "Xia Wu",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb5",
                    "user": {
                        "_id": "68aed07e662f89a73a744613",
                        "avatarUrl": "/avatars/2827a3703ea44c58c461c87e6bd48953.svg",
                        "isPro": false,
                        "fullname": "jinlong hou",
                        "user": "wenzi001",
                        "type": "user"
                    },
                    "name": "Jinlong Hou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:22:06.411Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb6",
                    "user": {
                        "_id": "64358b811860001f14416fc1",
                        "avatarUrl": "/avatars/64a50495b116931124141d3e12fd6d30.svg",
                        "isPro": false,
                        "fullname": "Cheng Yuan",
                        "user": "chengyuan",
                        "type": "user"
                    },
                    "name": "Yuan Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:24:02.413Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb7",
                    "name": "Wenjie Li",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb8",
                    "name": "Xiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecb9",
                    "user": {
                        "_id": "61ad24836da53246bd6ac410",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61ad24836da53246bd6ac410/o-FL-C6B77iB94wyAtTuO.png",
                        "isPro": false,
                        "fullname": "Dequan Wang",
                        "user": "dqwang",
                        "type": "user"
                    },
                    "name": "Dequan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T11:21:41.249Z",
                    "hidden": false
                },
                {
                    "_id": "68d2047d1ca7156988a8ecba",
                    "name": "Pengfei Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6002c316698168af3bb9f4a6/bxJ1rPnNWVVybfIHpLjfo.png"
            ],
            "publishedAt": "2025-09-22T10:59:32.000Z",
            "submittedOnDailyAt": "2025-09-23T02:56:05.460Z",
            "title": "LIMI: Less is More for Agency",
            "submittedOnDailyBy": {
                "_id": "6002c316698168af3bb9f4a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6002c316698168af3bb9f4a6/M2J2QFCRc5RhYRoz7OuZN.png",
                "isPro": false,
                "fullname": "yangxiao",
                "user": "YangXiao-nlp",
                "type": "user"
            },
            "summary": "We define Agency as the emergent capacity of AI systems to function as\nautonomous agents actively discovering problems, formulating hypotheses, and\nexecuting solutions through self-directed engagement with environments and\ntools. This fundamental capability marks the dawn of the Age of AI Agency,\ndriven by a critical industry shift: the urgent need for AI systems that don't\njust think, but work. While current AI excels at reasoning and generating\nresponses, industries demand autonomous agents that can execute tasks, operate\ntools, and drive real-world outcomes. As agentic intelligence becomes the\ndefining characteristic separating cognitive systems from productive workers,\nefficiently cultivating machine autonomy becomes paramount. Current approaches\nassume that more data yields better agency, following traditional scaling laws\nfrom language modeling. We fundamentally challenge this paradigm. LIMI (Less Is\nMore for Intelligent Agency) demonstrates that agency follows radically\ndifferent development principles. Through strategic focus on collaborative\nsoftware development and scientific research workflows, we show that\nsophisticated agentic intelligence can emerge from minimal but strategically\ncurated demonstrations of autonomous behavior. Using only 78 carefully designed\ntraining samples, LIMI achieves 73.5% on comprehensive agency benchmarks,\ndramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),\nDeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).\nMost strikingly, LIMI demonstrates 53.7% improvement over models trained on\n10,000 samples-achieving superior agentic intelligence with 128 times fewer\nsamples. Our findings establish the Agency Efficiency Principle: machine\nautonomy emerges not from data abundance but from strategic curation of\nhigh-quality agentic demonstrations.",
            "upvotes": 55,
            "discussionId": "68d2047e1ca7156988a8ecbb",
            "projectPage": "https://github.com/GAIR-NLP/LIMI",
            "githubRepo": "https://github.com/GAIR-NLP/LIMI",
            "ai_summary": "LIMI demonstrates that sophisticated agentic intelligence can emerge from minimal, strategically curated demonstrations, outperforming data-intensive models on agency benchmarks.",
            "ai_keywords": [
                "Agency",
                "autonomous agents",
                "self-directed engagement",
                "agentic intelligence",
                "LIMI",
                "Agency Efficiency Principle"
            ],
            "githubStars": 16
        },
        "translation_title": "LIMI: 에이전시를 위한 적은 것이 더 많다",
        "purpose": "AI 시스템이 자율적인 에이전트로서 문제를 발견하고 해결하는 능력을 갖추도록 연구",
        "method": [
            "자율 행동의 전략적으로 선별된 시연에 중점을 두어 개발함(Through strategic focus on collaborative software development and scientific research workflows, we show that sophisticated agentic intelligence can emerge from minimal but strategically curated demonstrations of autonomous behavior.)",
            "78개의 정교하게 설계된 훈련 샘플만을 사용하여 에이전시 기준에서 73.5%를 달성함(Using only 78 carefully designed training samples, LIMI achieves 73.5% on comprehensive agency benchmarks.)",
            "기존의 최신 모델들과 비교하여 성능이 크게 향상됨(Most strikingly, LIMI demonstrates 53.7% improvement over models trained on 10,000 samples-achieving superior agentic intelligence with 128 times fewer samples.)"
        ],
        "conclusion": "에이전시 효율성 원칙을 확립하였으며, 기계 자율성은 데이터의 양보다 고품질 에이전트 행동의 전략적 선별에서 비롯됨.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2509.17627",
            "authors": [
                {
                    "_id": "68d20ce31ca7156988a8ed2a",
                    "user": {
                        "_id": "6678045bc3f824dde8e217ff",
                        "avatarUrl": "/avatars/068cc0e16a034cc35b7f058bcba87f4a.svg",
                        "isPro": false,
                        "fullname": "jinshu chen",
                        "user": "JinshuChen",
                        "type": "user"
                    },
                    "name": "Jinshu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:10:58.669Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed2b",
                    "user": {
                        "_id": "6752cd83ffaeeb979db974ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
                        "isPro": false,
                        "fullname": "Xinghui Li",
                        "user": "Crayon-Shinchan",
                        "type": "user"
                    },
                    "name": "Xinghui Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:10:50.287Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed2c",
                    "name": "Xu Bai",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed2d",
                    "name": "Tianxiang Ma",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed2e",
                    "name": "Pengze Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed2f",
                    "user": {
                        "_id": "6304e2dabad6ce7fc0287d57",
                        "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
                        "isPro": false,
                        "fullname": "Zhuowei_Chen",
                        "user": "ZhuoweiChen",
                        "type": "user"
                    },
                    "name": "Zhuowei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:11:09.773Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed30",
                    "name": "Gen Li",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed31",
                    "name": "Lijie Liu",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed32",
                    "name": "Songtao Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed33",
                    "user": {
                        "_id": "63b415037af2e415f2599c18",
                        "avatarUrl": "/avatars/4afbe7d6d05a702f1beeed9c53e78153.svg",
                        "isPro": false,
                        "fullname": "Bingchuan Li",
                        "user": "lbc402",
                        "type": "user"
                    },
                    "name": "Bingchuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T14:11:24.038Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ce31ca7156988a8ed34",
                    "name": "Qian He",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6752cd83ffaeeb979db974ae/sVfS9qyD1e1SqO9753UK_.mp4"
            ],
            "publishedAt": "2025-09-22T11:35:55.000Z",
            "submittedOnDailyAt": "2025-09-23T01:35:03.582Z",
            "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion\n  Transformer Models",
            "submittedOnDailyBy": {
                "_id": "6752cd83ffaeeb979db974ae",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
                "isPro": false,
                "fullname": "Xinghui Li",
                "user": "Crayon-Shinchan",
                "type": "user"
            },
            "summary": "Recent advances in video insertion based on diffusion models are impressive.\nHowever, existing methods rely on complex control signals but struggle with\nsubject consistency, limiting their practical applicability. In this paper, we\nfocus on the task of Mask-free Video Insertion and aim to resolve three key\nchallenges: data scarcity, subject-scene equilibrium, and insertion\nharmonization. To address the data scarcity, we propose a new data pipeline\nInsertPipe, constructing diverse cross-pair data automatically. Building upon\nour data pipeline, we develop OmniInsert, a novel unified framework for\nmask-free video insertion from both single and multiple subject references.\nSpecifically, to maintain subject-scene equilibrium, we introduce a simple yet\neffective Condition-Specific Feature Injection mechanism to distinctly inject\nmulti-source conditions and propose a novel Progressive Training strategy that\nenables the model to balance feature injection from subjects and source video.\nMeanwhile, we design the Subject-Focused Loss to improve the detailed\nappearance of the subjects. To further enhance insertion harmonization, we\npropose an Insertive Preference Optimization methodology to optimize the model\nby simulating human preferences, and incorporate a Context-Aware Rephraser\nmodule during reference to seamlessly integrate the subject into the original\nscenes. To address the lack of a benchmark for the field, we introduce\nInsertBench, a comprehensive benchmark comprising diverse scenes with\nmeticulously selected subjects. Evaluation on InsertBench indicates OmniInsert\noutperforms state-of-the-art closed-source commercial solutions. The code will\nbe released.",
            "upvotes": 40,
            "discussionId": "68d20ce41ca7156988a8ed35",
            "ai_summary": "OmniInsert addresses challenges in mask-free video insertion using a novel data pipeline, feature injection, progressive training, and context-aware rephrasing, outperforming commercial solutions.",
            "ai_keywords": [
                "diffusion models",
                "Mask-free Video Insertion",
                "data scarcity",
                "Condition-Specific Feature Injection",
                "Progressive Training",
                "Subject-Focused Loss",
                "Insertive Preference Optimization",
                "Context-Aware Rephraser",
                "InsertBench"
            ]
        },
        "translation_title": "OmniInsert: 확산 변환기 모델을 통한 마스크 없이 어떤 참조라도 비디오 삽입",
        "purpose": "마스크 없이 비디오를 삽입하는 문제에서 주제 일관성 문제를 해결하고, 데이터 부족, 주제-장면 균형 및 삽입 조화라는 세 가지 주요 도전에 대처하기 위한 연구.",
        "method": [
            "InsertPipe라는 새로운 데이터 파이프라인을 제안하여 다양한 교차 쌍 데이터를 자동으로 생성함(Data scarcity).",
            "Condition-Specific Feature Injection 메커니즘을 도입하여 주제와 장면 간의 균형을 유지함(Maintain subject-scene equilibrium).",
            "Subject-Focused Loss를 설계하여 주제의 세부적인 모습을 개선함."
        ],
        "conclusion": "OmniInsert는 최신 기술보다 우수한 성능을 보이며, 비디오 삽입 분야의 기준을 설정하는 InsertBench를 도입함.",
        "keywords": [
            "Video Generation",
            "Natural Language Processing",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2509.17765",
            "authors": [
                {
                    "_id": "68d20ca51ca7156988a8ed02",
                    "name": "Jin Xu",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed03",
                    "user": {
                        "_id": "661e577cbac5d981f883b743",
                        "avatarUrl": "/avatars/95e55e9707a6b55594c264081202d7f4.svg",
                        "isPro": false,
                        "fullname": "GuoZhifang",
                        "user": "ZhifangGuo",
                        "type": "user"
                    },
                    "name": "Zhifang Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:10:28.512Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed04",
                    "name": "Hangrui Hu",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed05",
                    "user": {
                        "_id": "62c6a751a71b40cf26f359a8",
                        "avatarUrl": "/avatars/49abd2e71946035452c316d703baaac6.svg",
                        "isPro": false,
                        "fullname": "Yunfei Chu",
                        "user": "faychu",
                        "type": "user"
                    },
                    "name": "Yunfei Chu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:10:44.908Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed06",
                    "name": "Xiong Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed07",
                    "user": {
                        "_id": "6594f06ac04427eb38444bce",
                        "avatarUrl": "/avatars/b13fbf589b25eff038deb3fa12d95871.svg",
                        "isPro": false,
                        "fullname": "Jinzheng He",
                        "user": "jinzheng-he",
                        "type": "user"
                    },
                    "name": "Jinzheng He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:10:52.346Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed08",
                    "name": "Yuxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed09",
                    "name": "Xian Shi",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed0a",
                    "name": "Ting He",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed0b",
                    "name": "Xinfa Zhu",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed0c",
                    "name": "Yuanjun Lv",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed0d",
                    "user": {
                        "_id": "62cfafa91f70d6b8c8ad0722",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cfafa91f70d6b8c8ad0722/WNbwMB2MQ_4GLzDnHPMwx.png",
                        "isPro": false,
                        "fullname": "Yongqi Wang",
                        "user": "Cyanbox",
                        "type": "user"
                    },
                    "name": "Yongqi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:11:22.199Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed0e",
                    "name": "Dake Guo",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed0f",
                    "name": "He Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed10",
                    "user": {
                        "_id": "659a5c786d20ab21b05be596",
                        "avatarUrl": "/avatars/3d0449e9e50751a338e063c50ee244e9.svg",
                        "isPro": false,
                        "fullname": "Linhan Ma",
                        "user": "Lhma-aslp",
                        "type": "user"
                    },
                    "name": "Linhan Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:11:37.816Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed11",
                    "name": "Pei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed12",
                    "name": "Xinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed13",
                    "user": {
                        "_id": "666fd382b955b0e655165768",
                        "avatarUrl": "/avatars/66476925471bda2dc9b57f091f245dd9.svg",
                        "isPro": false,
                        "fullname": "hongkun hao",
                        "user": "hongkunhao",
                        "type": "user"
                    },
                    "name": "Hongkun Hao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:11:50.894Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed14",
                    "name": "Zishan Guo",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed15",
                    "name": "Baosong Yang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed16",
                    "name": "Bin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed17",
                    "user": {
                        "_id": "630388b0d14428368d1616c5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630388b0d14428368d1616c5/Z8O82fDVlB5qkM8Jmq65_.jpeg",
                        "isPro": false,
                        "fullname": "Ziyang Ma",
                        "user": "BoJack",
                        "type": "user"
                    },
                    "name": "Ziyang Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:12:04.210Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed18",
                    "name": "Xipin Wei",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed19",
                    "name": "Shuai Bai",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed1a",
                    "user": {
                        "_id": "6461d675681b2e19b6acb5a5",
                        "avatarUrl": "/avatars/0d95d65d30f6672ec09dc92155324d7f.svg",
                        "isPro": false,
                        "fullname": "Keqin Chen",
                        "user": "chenkq",
                        "type": "user"
                    },
                    "name": "Keqin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:12:38.284Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed1b",
                    "name": "Xuejing Liu",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed1c",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed1d",
                    "user": {
                        "_id": "6417fa211f1f3b0fa811edc0",
                        "avatarUrl": "/avatars/fa9e1ef1472a736c2ceebe12b77d6c89.svg",
                        "isPro": false,
                        "fullname": "Mingkun Yang",
                        "user": "ayumiymk",
                        "type": "user"
                    },
                    "name": "Mingkun Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:12:49.920Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed1e",
                    "user": {
                        "_id": "6434d4989bd5a84b5dd0b0f5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
                        "isPro": false,
                        "fullname": "Dayiheng Liu",
                        "user": "Losin94",
                        "type": "user"
                    },
                    "name": "Dayiheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:12:57.409Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed1f",
                    "name": "Xingzhang Ren",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed20",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed21",
                    "name": "Rui Men",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed22",
                    "name": "Fan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed23",
                    "name": "Bowen Yu",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed24",
                    "user": {
                        "_id": "67e2432875313f8a046946ea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Syqlq19KVPKYD4t0xsKvM.png",
                        "isPro": false,
                        "fullname": "jianxin Yang",
                        "user": "yangjianxin",
                        "type": "user"
                    },
                    "name": "Jianxin Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:10:19.273Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed25",
                    "name": "Le Yu",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed26",
                    "user": {
                        "_id": "602f88f5e8149a962412a667",
                        "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Jingren",
                        "type": "user"
                    },
                    "name": "Jingren Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:10:12.277Z",
                    "hidden": false
                },
                {
                    "_id": "68d20ca51ca7156988a8ed27",
                    "user": {
                        "_id": "620760a26e3b7210c2ff1943",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
                        "isPro": false,
                        "fullname": "Junyang Lin",
                        "user": "JustinLin610",
                        "type": "user"
                    },
                    "name": "Junyang Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-23T10:09:59.230Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T13:26:24.000Z",
            "submittedOnDailyAt": "2025-09-23T01:27:51.983Z",
            "title": "Qwen3-Omni Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.",
            "upvotes": 30,
            "discussionId": "68d20ca51ca7156988a8ed28",
            "githubRepo": "https://github.com/QwenLM/Qwen3-Omni",
            "ai_summary": "Qwen3-Omni, a multimodal model, achieves state-of-the-art performance across text, image, audio, and video, using a Thinker-Talker MoE architecture and a lightweight causal ConvNet for efficient streaming synthesis.",
            "ai_keywords": [
                "multimodal model",
                "Thinker-Talker MoE architecture",
                "audio tasks",
                "audio-visual benchmarks",
                "open-source SOTA",
                "closed-source models",
                "Gemini-2.5-Pro",
                "Seed-ASR",
                "GPT-4o-Transcribe",
                "fluent text",
                "natural real-time speech",
                "text interaction",
                "speech understanding",
                "speech generation",
                "first-packet latency",
                "Talker",
                "discrete speech codecs",
                "multi-codebook scheme",
                "block-wise diffusion",
                "causal ConvNet",
                "cold-start settings",
                "multimodal reasoning",
                "Thinking model",
                "audio captioning model",
                "Qwen3-Omni-30B-A3B",
                "Qwen3-Omni-30B-A3B-Thinking",
                "Qwen3-Omni-30B-A3B-Captioner"
            ],
            "githubStars": 1357
        },
        "translation_title": "Qwen3-Omni 기술 보고서",
        "purpose": "텍스트, 이미지, 오디오, 비디오에서 최첨단 성능을 유지하는 단일 다중 모달 모델 개발",
        "method": [
            "Qwen3-Omni는 Thinker-Talker MoE 아키텍처를 채택하여 텍스트, 이미지, 오디오 및 비디오의 인식과 생성 통합.(Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video.)",
            "36개의 오디오 및 오디오-비주얼 벤치마크에서 32개의 오픈 소스 SOTA를 달성.(Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22.)",
            "모달리티 간의 멀티모달 추론을 강화하기 위해 Thinking 모델을 도입.(To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality.)"
        ],
        "conclusion": "Qwen3-Omni는 다양한 모달에서 우수한 성능을 보여주며, 공개된 모델인 Qwen3-Omni-30B-A3B와 그 변형들을 제공함.",
        "keywords": [
            "Multimodal Learning",
            "Audio Understanding",
            "Video Generation"
        ]
    },
    {
        "paper": {
            "id": "2509.18091",
            "authors": [
                {
                    "_id": "68d20b8a1ca7156988a8ece7",
                    "user": {
                        "_id": "64db88993725f8d9a908c077",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
                        "isPro": false,
                        "fullname": "Sunhao Dai",
                        "user": "KID-22",
                        "type": "user"
                    },
                    "name": "Sunhao Dai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:06:54.519Z",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ece8",
                    "user": {
                        "_id": "65acfb3a14e6582c30b4ce76",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65acfb3a14e6582c30b4ce76/RhEhePggBtyM0RIIqXQen.jpeg",
                        "isPro": false,
                        "fullname": "TangJiakai",
                        "user": "TangJiakai5704",
                        "type": "user"
                    },
                    "name": "Jiakai Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:06:57.620Z",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ece9",
                    "name": "Jiahua Wu",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecea",
                    "user": {
                        "_id": "67e21ce15d02fcdf8d7abd39",
                        "avatarUrl": "/avatars/01f63e591d327f3d54f51460a599dc80.svg",
                        "isPro": false,
                        "fullname": "kun",
                        "user": "vicowang",
                        "type": "user"
                    },
                    "name": "Kun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:41:11.576Z",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8eceb",
                    "user": {
                        "_id": "641f18a673cfc036ddbaeccf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RtOK_qV71NM87A67XFJCN.png",
                        "isPro": false,
                        "fullname": "Yuxuan Zhu",
                        "user": "Ethan7",
                        "type": "user"
                    },
                    "name": "Yuxuan Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:07:00.003Z",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecec",
                    "name": "Bingjun Chen",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8eced",
                    "name": "Bangyang Hong",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecee",
                    "name": "Yu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecef",
                    "name": "Cong Fu",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf0",
                    "name": "Kangle Wu",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf1",
                    "name": "Yabo Ni",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf2",
                    "name": "Anxiang Zeng",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf3",
                    "name": "Wenjie Wang",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf4",
                    "name": "Xu Chen",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf5",
                    "name": "Jun Xu",
                    "hidden": false
                },
                {
                    "_id": "68d20b8a1ca7156988a8ecf6",
                    "name": "See-Kiong Ng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T17:59:07.000Z",
            "submittedOnDailyAt": "2025-09-23T01:26:16.690Z",
            "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System",
            "submittedOnDailyBy": {
                "_id": "64db88993725f8d9a908c077",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
                "isPro": false,
                "fullname": "Sunhao Dai",
                "user": "KID-22",
                "type": "user"
            },
            "summary": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over +2% GMV/UU and a +2.90% increase in advertising\nrevenue.",
            "upvotes": 26,
            "discussionId": "68d20b8b1ca7156988a8ecf7",
            "ai_summary": "OnePiece integrates LLM-style context engineering and reasoning into industrial search and recommendation systems, achieving significant improvements in key business metrics.",
            "ai_keywords": [
                "Transformer architectures",
                "Deep Learning Recommendation Models (DLRMs)",
                "context engineering",
                "multi-step reasoning",
                "structured context engineering",
                "block-wise latent reasoning",
                "progressive multi-task training"
            ]
        },
        "translation_title": "OnePiece: 산업적인 Cascade Ranking 시스템에 Context Engineering과 Reasoning을 적용하기",
        "purpose": "산업 검색 및 추천 시스템에서 LLMs의 성공을 재현하기 위한 개선 방안 연구",
        "method": [
            "OnePiece라는 통합 프레임워크를 제안하여 LLM 스타일의 context engineering과 reasoning을 산업의 검색 및 랭킹 모델에 통합함.(we propose OnePiece, a unified framework that seamlessly integrates LLM-style context engineering and reasoning into both retrieval and ranking models of industrial cascaded pipelines.)",
            "구조화된 context engineering, 블록 단위 잠재적 reasoning, 그리고 점진적 멀티 태스크 훈련의 세 가지 주요 혁신을 도입함.(OnePiece is built on a pure Transformer backbone and further introduces three key innovations: structured context engineering, block-wise latent reasoning, and progressive multi-task training.)",
            "사용자 피드백 체인을 활용해 reasoning 단계를 효과적으로 감독하면서 훈련을 수행함.(which leverages user feedback chains to effectively supervise reasoning steps during training.)"
        ],
        "conclusion": "OnePiece는 Shopee의 개인화된 검색 시나리오에 배포되었으며, GMV/UU를 +2% 증가시키고 광고 수익을 +2.90% 향상시키는 등의 온라인 성과를 달성함.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2509.18056",
            "authors": [
                {
                    "_id": "68d23a2c1ca7156988a8ee4a",
                    "user": {
                        "_id": "67485bfd768f8d6a509d5cd7",
                        "avatarUrl": "/avatars/ad01e707a2066ef673ac7317ccdbb902.svg",
                        "isPro": false,
                        "fullname": "Yunheng Li",
                        "user": "lyhisme",
                        "type": "user"
                    },
                    "name": "Yunheng Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-23T10:05:02.046Z",
                    "hidden": false
                },
                {
                    "_id": "68d23a2c1ca7156988a8ee4b",
                    "name": "Jing Cheng",
                    "hidden": false
                },
                {
                    "_id": "68d23a2c1ca7156988a8ee4c",
                    "name": "Shaoyong Jia",
                    "hidden": false
                },
                {
                    "_id": "68d23a2c1ca7156988a8ee4d",
                    "name": "Hangyi Kuang",
                    "hidden": false
                },
                {
                    "_id": "68d23a2c1ca7156988a8ee4e",
                    "name": "Shaohui Jiao",
                    "hidden": false
                },
                {
                    "_id": "68d23a2c1ca7156988a8ee4f",
                    "name": "Qibin Hou",
                    "hidden": false
                },
                {
                    "_id": "68d23a2c1ca7156988a8ee50",
                    "name": "Ming-Ming Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-22T17:30:15.000Z",
            "submittedOnDailyAt": "2025-09-23T04:45:19.836Z",
            "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs",
            "submittedOnDailyBy": {
                "_id": "67485bfd768f8d6a509d5cd7",
                "avatarUrl": "/avatars/ad01e707a2066ef673ac7317ccdbb902.svg",
                "isPro": false,
                "fullname": "Yunheng Li",
                "user": "lyhisme",
                "type": "user"
            },
            "summary": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1",
            "upvotes": 23,
            "discussionId": "68d23a2c1ca7156988a8ee51",
            "ai_summary": "TempSamp-R1, a reinforcement fine-tuning framework, enhances multimodal large language models for video temporal grounding by using off-policy supervision and a hybrid Chain-of-Thought training paradigm, achieving state-of-the-art performance on benchmark datasets.",
            "ai_keywords": [
                "reinforcement fine-tuning",
                "multimodal large language models",
                "video temporal grounding",
                "Group Relative Policy Optimization",
                "on-policy sampling",
                "off-policy supervision",
                "non-linear soft advantage computation",
                "Chain-of-Thought training",
                "few-shot generalization"
            ]
        },
        "translation_title": "TempSamp-R1: 비디오 LLM을 위한 효과적인 시간 샘플링 및 강화 세밀 조정",
        "purpose": "다중 모달 대형 언어 모델(MLLM)을 비디오 시간 기초 작업에 적합하게 조정하기 위해 효과적인 강화 세밀 조정 프레임워크 개발",
        "method": [
            "기존의 강화 학습 방법인 Group Relative Policy Optimization(GRPO)가 정책 업데이트를 위해 on-policy 샘플링에 의존하는 것을 깨달음(We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates.)",
            "TempSamp-R1은 ground-truth 주석을 off-policy 감독으로 활용하여 시간적으로 정확한 지침을 제공함(To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance.)",
            "보상을 기반으로 한 업데이트의 변동성을 줄이고 훈련을 안정화하기 위해 비선형 소프트 이점 계산 방법을 제공함(TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation.)"
        ],
        "conclusion": "TempSamp-R1은 GRPO 기반 기준을 능가하며, Charades-STA, ActivityNet Captions, QVHighlights와 같은 벤치마크 데이터 세트에서 새로운 최첨단 성능을 확립함.",
        "keywords": [
            "Video Generation",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    }
]