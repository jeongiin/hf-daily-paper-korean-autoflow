[
    {
        "paper": {
            "id": "2601.17058",
            "authors": [
                {
                    "_id": "69782c96026bdf0473116e06",
                    "user": {
                        "_id": "68216c63856b96f869d1d116",
                        "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg",
                        "isPro": false,
                        "fullname": "Wei Zhou",
                        "user": "weizhoudb",
                        "type": "user"
                    },
                    "name": "Wei Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T13:59:49.701Z",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e07",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e08",
                    "name": "Haoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e09",
                    "name": "Zhenghao Li",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e0a",
                    "name": "Qikang He",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e0b",
                    "name": "Shaokun Han",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e0c",
                    "name": "Guoliang Li",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e0d",
                    "user": {
                        "_id": "64ef522242da8d2a897d62da",
                        "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg",
                        "isPro": false,
                        "fullname": "xuanhe zhou",
                        "user": "zhouxh19",
                        "type": "user"
                    },
                    "name": "Xuanhe Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T13:58:19.930Z",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e0e",
                    "user": {
                        "_id": "674fa2f067c963c50a066594",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg",
                        "isPro": false,
                        "fullname": "yeye he",
                        "user": "yeyehe",
                        "type": "user"
                    },
                    "name": "Yeye He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T13:58:27.638Z",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e0f",
                    "name": "Chunwei Liu",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e10",
                    "user": {
                        "_id": "66724ce47e7ff5d8bd069c7c",
                        "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg",
                        "isPro": false,
                        "fullname": "Zirui Tang",
                        "user": "TerryTang",
                        "type": "user"
                    },
                    "name": "Zirui Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T13:58:49.525Z",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e11",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e12",
                    "user": {
                        "_id": "695612aabf3c8959a3a05f9c",
                        "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg",
                        "isPro": false,
                        "fullname": "ShenTang990",
                        "user": "shentang",
                        "type": "user"
                    },
                    "name": "Shen Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T13:58:56.579Z",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e13",
                    "name": "Kai Zuo",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e14",
                    "user": {
                        "_id": "67efa8a2ed790a2e999dc216",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png",
                        "isPro": false,
                        "fullname": "Yuyu Luo",
                        "user": "luoyuyu",
                        "type": "user"
                    },
                    "name": "Yuyu Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T13:59:02.233Z",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e15",
                    "name": "Zhenzhe Zheng",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e16",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T13:57:14.525Z",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e17",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "69782c96026bdf0473116e18",
                    "name": "Fan Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-22T12:02:45.000Z",
            "submittedOnDailyAt": "2026-01-27T00:42:38.464Z",
            "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs",
            "submittedOnDailyBy": {
                "_id": "68216c63856b96f869d1d116",
                "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg",
                "isPro": false,
                "fullname": "Wei Zhou",
                "user": "weizhoudb",
                "type": "user"
            },
            "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.",
            "upvotes": 117,
            "discussionId": "69782c97026bdf0473116e19",
            "projectPage": "https://github.com/weAIDB/awesome-data-llm",
            "githubRepo": "https://github.com/weAIDB/awesome-data-llm",
            "githubRepoAddedBy": "user",
            "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.",
            "ai_keywords": [
                "data preparation",
                "large language models",
                "prompt-driven workflows",
                "agentic workflows",
                "data cleaning",
                "data integration",
                "data enrichment",
                "entity matching",
                "schema matching",
                "data annotation",
                "data profiling"
            ],
            "githubStars": 642,
            "organization": {
                "_id": "63e5ef7bf2e9a8f22c515654",
                "name": "SJTU",
                "fullname": "Shanghai Jiao Tong University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
            }
        },
        "translation_title": "LLM으로 데이터 준비의 혼란을 정리할 수 있을까? LLM을 활용한 응용 준비 데이터 준비에 대한 조사",
        "purpose": "다양한 데이터 중심 응용 프로그램을 위한 데이터 준비 절차를 LLM 기술을 통해 개선하기 위한 체계적인 검토",
        "method": [
            "수백 개의 최근 문헌을 조사하여 LLM 기술을 활용한 데이터 준비의 최신 흐름을 정리함( By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks.)",
            "데이터 준비 작업을 데이터 청소, 데이터 통합, 데이터 강화라는 세 가지 주요 작업으로 분류하여 각각의 기술과 강점, 한계를 강조함(Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning, data integration, and data enrichment.)",
            "일반적으로 사용되는 데이터셋과 평가 지표를 분석하여 LLM 기반 데이터 준비의 현재 문제점을 논의함(Moreover, we analyze commonly used datasets and evaluation metrics.)"
        ],
        "conclusion": "이 논문은 LLM을 활용한 데이터 준비가 데이터 처리의 혁신적인 패러다임이 될 수 있음을 보여주며, 신뢰할 수 있는 작업 흐름 디자인과 강력한 평가 프로토콜의 필요성을 강조함.",
        "keywords": [
            "Large Language Models",
            "Data Preparation",
            "Data Cleaning"
        ]
    },
    {
        "paper": {
            "id": "2601.18418",
            "authors": [
                {
                    "_id": "69785315026bdf0473116f6a",
                    "user": {
                        "_id": "62dce08bb2c60f29c3d0a5da",
                        "avatarUrl": "/avatars/87ce03e61c4c6eb686c9491ef4fda225.svg",
                        "isPro": false,
                        "fullname": "Ji Zeng",
                        "user": "stargazerzj",
                        "type": "user"
                    },
                    "name": "Ji Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-27T08:31:51.245Z",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f6b",
                    "name": "Dayuan Fu",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f6c",
                    "name": "Tiantian Mi",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f6d",
                    "name": "Yumin Zhuang",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f6e",
                    "user": {
                        "_id": "6865e6b362fc5689c5e67733",
                        "avatarUrl": "/avatars/186f3d248791d961b0a810d5225167cc.svg",
                        "isPro": false,
                        "fullname": "Yaxing Huang",
                        "user": "Rookie-Noob-Newbie",
                        "type": "user"
                    },
                    "name": "Yaxing Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T14:00:30.220Z",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f6f",
                    "user": {
                        "_id": "67638cc0d63e4b348e8a5fa3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67638cc0d63e4b348e8a5fa3/BZNlw1uTGUcumCrXKkerx.png",
                        "isPro": false,
                        "fullname": "Xuefeng Li",
                        "user": "drxuefeng",
                        "type": "user"
                    },
                    "name": "Xuefeng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T14:00:37.248Z",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f70",
                    "name": "Lyumanshan Ye",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f71",
                    "name": "Muhang Xie",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f72",
                    "name": "Qishuo Hua",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f73",
                    "name": "Zhen Huang",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f74",
                    "user": {
                        "_id": "66d01e4401f2a6b4cd93ad87",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png",
                        "isPro": false,
                        "fullname": "Mohan Jiang (SII)",
                        "user": "mhjiang0408",
                        "type": "user"
                    },
                    "name": "Mohan Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T14:00:23.438Z",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f75",
                    "name": "Hanning Wang",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f76",
                    "user": {
                        "_id": "66fa544c54f87b607fbffd2e",
                        "avatarUrl": "/avatars/94195dcda0eb68e8fd20d80718744697.svg",
                        "isPro": false,
                        "fullname": "Jifan Lin",
                        "user": "evanlin2570",
                        "type": "user"
                    },
                    "name": "Jifan Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T14:00:57.029Z",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f77",
                    "name": "Yang Xiao",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f78",
                    "name": "Jie Sun",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f79",
                    "user": {
                        "_id": "684faf712acd915b5afc055f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/684faf712acd915b5afc055f/K7icmL08HxniWDgdph73i.jpeg",
                        "isPro": false,
                        "fullname": "Yunze Wu",
                        "user": "wyzmike",
                        "type": "user"
                    },
                    "name": "Yunze Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T14:01:06.063Z",
                    "hidden": false
                },
                {
                    "_id": "69785315026bdf0473116f7a",
                    "name": "Pengfei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-26T12:20:18.000Z",
            "submittedOnDailyAt": "2026-01-27T03:34:37.777Z",
            "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering",
            "submittedOnDailyBy": {
                "_id": "66d01e4401f2a6b4cd93ad87",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png",
                "isPro": false,
                "fullname": "Mohan Jiang (SII)",
                "user": "mhjiang0408",
                "type": "user"
            },
            "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...",
            "upvotes": 99,
            "discussionId": "69785315026bdf0473116f7b",
            "githubRepo": "https://github.com/GAIR-NLP/daVinci-Dev",
            "githubRepoAddedBy": "user",
            "ai_summary": "Agentic mid-training enables large language models to develop autonomous software engineering capabilities through specialized data synthesis techniques that bridge the gap between static training data and dynamic development environments.",
            "ai_keywords": [
                "Large Language Model",
                "agentic software engineering",
                "mid-training",
                "distribution mismatch",
                "agent-native data",
                "contextually-native trajectories",
                "environmentally-native trajectories",
                "SWE-Bench Verified",
                "Kimi-Dev",
                "resolution rates"
            ],
            "githubStars": 22,
            "organization": {
                "_id": "630bc2d186b8b9904c33ce1b",
                "name": "GAIR",
                "fullname": "SII - GAIR",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"
            }
        },
        "translation_title": "daVinci-Dev: 소프트웨어 공학을 위한 에이전트 네이티브 중간 훈련",
        "purpose": "소프트웨어 공학의 에이전트 기능 향상을 위한 효율적 중간 훈련 방법을 탐구",
        "method": [
            "정적 훈련 데이터와 실시간 개발 환경 간의 분포 불일치를 해결하기 위해 체계적인 에이전트 중간 훈련 연구를 수행함(To address this, we present a systematic study of agentic mid-training...).",
            "에이전트 개발에 효과적인 데이터 합성 원칙과 훈련 방법론을 확립함(Central to our approach is agent-native data...).",
            "두 가지 보완적인 유형의 궤적, 즉 전체 정보 흐름을 보존하는 contextually-native trajectories와 실제 도구 호출 및 테스트 실행에서 수집된 environmentally-native trajectories를 사용함."
        ],
        "conclusion": "개발한 모델은 에이전트 기능이 우수하게 검증되었으며, 기존의 방법보다 더 효율적인 중간 훈련을 통해 향상된 성능을 달성함.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2601.17737",
            "authors": [
                {
                    "_id": "6978310b026bdf0473116e44",
                    "user": {
                        "_id": "64545c77a7ce0a8fde809912",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VDaMEM77Xv09dP6B5v3sK.jpeg",
                        "isPro": false,
                        "fullname": "ChenYuMu",
                        "user": "ChenYuMu",
                        "type": "user"
                    },
                    "name": "Chenyu Mu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T14:01:21.462Z",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e45",
                    "user": {
                        "_id": "6527a2df1eb78901534b0cc6",
                        "avatarUrl": "/avatars/f811d8c108930b41e2612c609d35e2eb.svg",
                        "isPro": false,
                        "fullname": "Xin He",
                        "user": "Kleinhe",
                        "type": "user"
                    },
                    "name": "Xin He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-27T09:03:20.414Z",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e46",
                    "user": {
                        "_id": "64300415b009240418dac70c",
                        "avatarUrl": "/avatars/5175cdbc7683b0b52d5c742e93d3be83.svg",
                        "isPro": false,
                        "fullname": "Qu Yang",
                        "user": "quyang22",
                        "type": "user"
                    },
                    "name": "Qu Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-27T09:03:22.475Z",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e47",
                    "name": "Wanshun Chen",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e48",
                    "name": "Jiadi Yao",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e49",
                    "name": "Huang Liu",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e4a",
                    "name": "Zihao Yi",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e4b",
                    "name": "Bo Zhao",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e4c",
                    "name": "Xingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e4d",
                    "name": "Ruotian Ma",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e4e",
                    "name": "Fanghua Ye",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e4f",
                    "name": "Erkun Yang",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e50",
                    "name": "Cheng Deng",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e51",
                    "name": "Zhaopeng Tu",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e52",
                    "name": "Xiaolong Li",
                    "hidden": false
                },
                {
                    "_id": "6978310b026bdf0473116e53",
                    "name": "Linus",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-25T08:10:28.000Z",
            "submittedOnDailyAt": "2026-01-27T01:05:46.612Z",
            "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation",
            "submittedOnDailyBy": {
                "_id": "67485743561b1e6f9579389f",
                "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
                "isPro": false,
                "fullname": "Zhaopeng Tu",
                "user": "zptu",
                "type": "user"
            },
            "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.",
            "upvotes": 45,
            "discussionId": "6978310b026bdf0473116e54",
            "projectPage": "https://xd-mu.github.io/ScriptIsAllYouNeed/",
            "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent",
            "githubRepoAddedBy": "user",
            "ai_summary": "A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.",
            "ai_keywords": [
                "video generation",
                "dialogue-to-cinematic-video",
                "ScripterAgent",
                "DirectorAgent",
                "cross-scene continuous generation",
                "ScriptBench",
                "Visual-Script Alignment",
                "CriticAgent"
            ],
            "githubStars": 214,
            "organization": {
                "_id": "6645f953c39288df638dbdd5",
                "name": "Tencent-Hunyuan",
                "fullname": "Tencent Hunyuan",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
            }
        },
        "translation_title": "모든 것이 스크립트에 달려있다: 장기 대화 기반 영화 비디오 생성을 위한 에이전틱 프레임워크",
        "purpose": "대화에서 영화적 비디오로의 긴 형식의 일관된 내러티브 생성을 개선하기 위한 에이전틱 프레임워크 개발",
        "method": [
            "ScripterAgent라는 모델을 도입하여 간단한 대화를 세부사항이 있는 실행 가능한 영화 스크립트로 변환함(we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation.)",
            "ScriptBench라는 대규모 벤치마크를 구축하여 멀티모달 문맥을 포함한 데이터셋을 전문가의 도움으로 주석 처리함(To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline.)",
            "생성된 스크립트에서 DirectorAgent를 사용하여 비디오 모델을 조정하고 장기적 일관성을 보장함(The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence.)"
        ],
        "conclusion": "우리의 프레임워크는 스크립트의 충실성과 시간적 정확성을 크게 개선하며, 자동 영화 제작의 미래에 대한 중요한 통찰력을 제공함.",
        "keywords": [
            "Video Generation",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2601.17027",
            "authors": [
                {
                    "_id": "69782c6d026bdf0473116dfa",
                    "user": {
                        "_id": "640d99628512ec51d7ef71c7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg",
                        "isPro": false,
                        "fullname": "Honglin Lin",
                        "user": "LHL3341",
                        "type": "user"
                    },
                    "name": "Honglin Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-27T13:56:57.457Z",
                    "hidden": false
                },
                {
                    "_id": "69782c6d026bdf0473116dfb",
                    "user": {
                        "_id": "67b30bb2c2e25cfcdeda4a2f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b30bb2c2e25cfcdeda4a2f/K5ePD5uWNkwlpkI_43Oe1.jpeg",
                        "isPro": false,
                        "fullname": "Qin, Chonghan",
                        "user": "J017athan",
                        "type": "user"
                    },
                    "name": "Chonghan Qin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-27T09:42:44.133Z",
                    "hidden": false
                },
                {
                    "_id": "69782c6d026bdf0473116dfc",
                    "user": {
                        "_id": "6625ef13605f46d05c1d0031",
                        "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
                        "isPro": false,
                        "fullname": "Zheng Liu",
                        "user": "starriver030515",
                        "type": "user"
                    },
                    "name": "Zheng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-27T09:42:39.432Z",
                    "hidden": false
                },
                {
                    "_id": "69782c6d026bdf0473116dfd",
                    "name": "Qizhi Pei",
                    "hidden": false
                },
                {
                    "_id": "69782c6d026bdf0473116dfe",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "69782c6d026bdf0473116dff",
                    "user": {
                        "_id": "6875f5b55096cad81398a5af",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6875f5b55096cad81398a5af/CwXl53Jdp3LsBuRudT_CM.jpeg",
                        "isPro": false,
                        "fullname": "Zhanping Zhong",
                        "user": "ChampionZhong",
                        "type": "user"
                    },
                    "name": "Zhanping Zhong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-27T09:42:41.958Z",
                    "hidden": false
                },
                {
                    "_id": "69782c6d026bdf0473116e00",
                    "name": "Xin Gao",
                    "hidden": false
                },
                {
                    "_id": "69782c6d026bdf0473116e01",
                    "name": "Yanfeng Wang",
                    "hidden": false
                },
                {
                    "_id": "69782c6d026bdf0473116e02",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "69782c6d026bdf0473116e03",
                    "name": "Lijun Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-17T14:18:36.000Z",
            "submittedOnDailyAt": "2026-01-27T00:43:13.802Z",
            "title": "Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility",
            "submittedOnDailyBy": {
                "_id": "640d99628512ec51d7ef71c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg",
                "isPro": false,
                "fullname": "Honglin Lin",
                "user": "LHL3341",
                "type": "user"
            },
            "summary": "While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit \"understand - plan - code\" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.",
            "upvotes": 32,
            "discussionId": "69782c6e026bdf0473116e04",
            "projectPage": "https://scigenbench.github.io/",
            "githubRepo": "https://github.com/SciGenBench/SciGenBench",
            "githubRepoAddedBy": "user",
            "ai_summary": "Scientific image synthesis using logic-driven frameworks like ImgCoder improves multimodal reasoning by addressing visual-logic divergence through structured generation and evaluation benchmarks.",
            "ai_keywords": [
                "Text-to-Image models",
                "Large Multimodal Models",
                "scientific correctness",
                "visual-logic divergence",
                "ImgCoder",
                "SciGenBench",
                "pixel-based generation",
                "programmatic synthesis",
                "logical validity",
                "information utility"
            ],
            "githubStars": 9,
            "organization": {
                "_id": "63e5ef7bf2e9a8f22c515654",
                "name": "SJTU",
                "fullname": "Shanghai Jiao Tong University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
            }
        },
        "translation_title": "과학적 이미지 합성: 벤치마킹, 방법론 및 하위 유틸리티",
        "purpose": "과학적으로 타당한 이미지를 합성하여 멀티모달 추론의 성능을 개선하기 위한 연구",
        "method": [
            "다양한 생성 패러다임과 평가, 그리고 하위 사용을 통한 과학적 이미지 합성에 대한 체계적인 연구를 수행함(We conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use.)",
            "ImgCoder라는 로직 기반 프레임워크를 제안하여 구조적 정확성을 개선함(we propose ImgCoder, a logic-driven framework that follows an explicit 'understand - plan - code' workflow to improve structural precision.)",
            "SciGenBench라는 평가 지표를 도입하여 생성된 이미지의 정보 유용성과 논리적 타당성을 기반으로 과학적 정확성을 평가함(we introduce SciGenBench, which evaluates generated images based on information utility and logical validity.)"
        ],
        "conclusion": "정확히 검증된 합성 과학 이미지로 대규모 멀티모달 모델을 미세 조정하면 일관된 추론 성능 향상을 이루어내며, 이는 텍스트 도메인에 유사한 확장 추세를 나타냄.",
        "keywords": [
            "Image Generation",
            "Multimodal Learning",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2601.17367",
            "authors": [
                {
                    "_id": "6978295b026bdf0473116db5",
                    "user": {
                        "_id": "64096ef79e9f790c905b846d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
                        "isPro": false,
                        "fullname": "Zecheng Tang",
                        "user": "ZetangForward",
                        "type": "user"
                    },
                    "name": "Zecheng Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-27T09:42:48.253Z",
                    "hidden": false
                },
                {
                    "_id": "6978295b026bdf0473116db6",
                    "user": {
                        "_id": "6732fb1d24b316be87acaafe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732fb1d24b316be87acaafe/BzD8HL4vhh3mfeSF3rm_1.jpeg",
                        "isPro": false,
                        "fullname": "Quantong Qiu",
                        "user": "QQTang1223",
                        "type": "user"
                    },
                    "name": "Quantong Qiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T14:27:08.590Z",
                    "hidden": false
                },
                {
                    "_id": "6978295b026bdf0473116db7",
                    "name": "Yi Yang",
                    "hidden": false
                },
                {
                    "_id": "6978295b026bdf0473116db8",
                    "name": "Zhiyi Hong",
                    "hidden": false
                },
                {
                    "_id": "6978295b026bdf0473116db9",
                    "name": "Haiya Xiang",
                    "hidden": false
                },
                {
                    "_id": "6978295b026bdf0473116dba",
                    "user": {
                        "_id": "6875e95998c692330d72dc9a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wY7yjIix_IptruwlcG5Q4.png",
                        "isPro": false,
                        "fullname": "Kebin Liu",
                        "user": "KebinLiu",
                        "type": "user"
                    },
                    "name": "Kebin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T14:27:45.073Z",
                    "hidden": false
                },
                {
                    "_id": "6978295b026bdf0473116dbb",
                    "user": {
                        "_id": "68cce9276e4618473d590342",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UqBWbWZ58EPwJ5iRUsJsB.png",
                        "isPro": false,
                        "fullname": "Qingqing Dang",
                        "user": "DaisyGrace",
                        "type": "user"
                    },
                    "name": "Qingqing Dang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T14:27:50.734Z",
                    "hidden": false
                },
                {
                    "_id": "6978295b026bdf0473116dbc",
                    "user": {
                        "_id": "6670e285b0c03c4e9d6e0985",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6670e285b0c03c4e9d6e0985/j9Zr-lOtrRmpFz5f4x420.jpeg",
                        "isPro": false,
                        "fullname": "Juntao Li",
                        "user": "douvleplus",
                        "type": "user"
                    },
                    "name": "Juntao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-27T14:27:56.908Z",
                    "hidden": false
                },
                {
                    "_id": "6978295b026bdf0473116dbd",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-24T08:22:07.000Z",
            "submittedOnDailyAt": "2026-01-27T00:51:37.565Z",
            "title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers",
            "submittedOnDailyBy": {
                "_id": "64096ef79e9f790c905b846d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
                "isPro": false,
                "fullname": "Zecheng Tang",
                "user": "ZetangForward",
                "type": "user"
            },
            "summary": "The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.",
            "upvotes": 25,
            "discussionId": "6978295b026bdf0473116dbe",
            "projectPage": "https://github.com/LCM-Lab/Elastic-Attention",
            "githubRepo": "https://github.com/LCM-Lab/Elastic-Attention",
            "githubRepoAddedBy": "user",
            "ai_summary": "Elastic Attention enables dynamic adjustment of attention sparsity during inference by integrating a lightweight Attention Router into pretrained models, achieving efficient long-context processing.",
            "ai_keywords": [
                "standard attention mechanisms",
                "sparse attention",
                "full attention",
                "hybrid attention strategies",
                "attention router",
                "attention heads",
                "long-context scenarios",
                "pretrained models",
                "computational efficiency"
            ],
            "githubStars": 11,
            "organization": {
                "_id": "61f8e653129c9ff1b911293d",
                "name": "SUDA",
                "fullname": "Soochow University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"
            }
        },
        "translation_title": "Elastic Attention: 효율적인 트랜스포머를 위한 테스트 시 적응형 희소성 비율",
        "purpose": "대규모 언어 모델의 긴 문맥 처리에서 효율성을 높이기 위해 동적으로 희소성 비율을 조정하는 방법 제안",
        "method": [
            "기존에 학습된 모델에 경량의 Attention Router를 통합하여 입력에 따라 희소성을 동적으로 조정함(To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input.)",
            "12시간의 훈련을 통해 강력한 성능과 효율적 추론을 가능하게 함(Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference.)",
            "세 가지 긴 문맥 벤치마크에서 실험을 수행하여 우리 방법의 우수성을 입증함(Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.)"
        ],
        "conclusion": "우리의 방법은 긴 문맥을 처리하는 데 있어 우수한 성능을 발휘하며 효율적인 추론을 가능하게 함.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    }
]