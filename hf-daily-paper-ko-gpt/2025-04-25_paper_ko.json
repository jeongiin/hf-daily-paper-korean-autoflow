[
    {
        "paper": {
            "id": "2504.17761",
            "authors": [
                {
                    "_id": "680af2df3b93130c9b2b90a7",
                    "name": "Shiyu Liu",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90a8",
                    "name": "Yucheng Han",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90a9",
                    "user": {
                        "_id": "649f7b742bf2e21d955c1067",
                        "avatarUrl": "/avatars/270026405d5640886a86f961a001b057.svg",
                        "isPro": false,
                        "fullname": "xing",
                        "user": "xingpng",
                        "type": "user"
                    },
                    "name": "Peng Xing",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T12:27:18.944Z",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90aa",
                    "name": "Fukun Yin",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90ab",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90ac",
                    "user": {
                        "_id": "64b914c8ace99c0723ad83a9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
                        "isPro": false,
                        "fullname": "Wei Cheng",
                        "user": "wchengad",
                        "type": "user"
                    },
                    "name": "Wei Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:36.757Z",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90ad",
                    "name": "Jiaqi Liao",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90ae",
                    "name": "Yingming Wang",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90af",
                    "name": "Honghao Fu",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b0",
                    "name": "Chunrui Han",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b1",
                    "name": "Guopeng Li",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b2",
                    "name": "Yuang Peng",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b3",
                    "name": "Quan Sun",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b4",
                    "name": "Jingwei Wu",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b5",
                    "name": "Yan Cai",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b6",
                    "name": "Zheng Ge",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b7",
                    "name": "Ranchen Ming",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b8",
                    "name": "Lei Xia",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b9",
                    "name": "Xianfang Zeng",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90ba",
                    "name": "Yibo Zhu",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90bb",
                    "name": "Binxing Jiao",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90bc",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90bd",
                    "user": {
                        "_id": "63417332c5565a4b8d43a0d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
                        "isPro": false,
                        "fullname": "Gang Yu",
                        "user": "skicy",
                        "type": "user"
                    },
                    "name": "Gang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:34.650Z",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90be",
                    "name": "Daxin Jiang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
            ],
            "publishedAt": "2025-04-24T17:25:12.000Z",
            "submittedOnDailyAt": "2025-04-25T01:12:44.269Z",
            "title": "Step1X-Edit: A Practical Framework for General Image Editing",
            "submittedOnDailyBy": {
                "_id": "64b914c8ace99c0723ad83a9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
                "isPro": false,
                "fullname": "Wei Cheng",
                "user": "wchengad",
                "type": "user"
            },
            "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
            "upvotes": 49,
            "discussionId": "680af2e13b93130c9b2b9132",
            "githubRepo": "https://github.com/stepfun-ai/Step1X-Edit",
            "ai_keywords": [
                "Multimodal LLM",
                "latent embedding",
                "diffusion image decoder",
                "data generation pipeline",
                "GEdit-Bench",
                "real-world user instructions"
            ]
        },
        "translation_title": "Step1X-Edit: 일반 이미지 편집을 위한 실용적인 프레임워크",
        "purpose": "최신 이미지 편집 모델인 Step1X-Edit를 공개하여 기존의 폐쇄형 모델과 비교할 수 있는 성능을 제공하기 위함",
        "method": [
            "Multimodal LLM을 사용해 참조 이미지와 사용자의 편집 지시를 처리함(In this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against closed-source models like GPT-4o and Gemini2 Flash.)",
            "고품질 데이터셋을 생성하기 위한 데이터 생성 파이프라인을 구축함(To train the model, we build a data generation pipeline to produce a high-quality dataset.)",
            "사용자 지침을 기반으로 한 새로운 벤치마크인 GEdit-Bench를 개발하여 평가를 수행함(For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions.)"
        ],
        "conclusion": "Step1X-Edit는 기존의 공개 소스 기준을 크게 초과하며, 주요 상용 모델의 성능에 근접함으로써 이미지 편집 분야에 중요한 기여를 함.",
        "keywords": [
            "Image Editing",
            "Multimodal Learning",
            "Image Generation"
        ]
    },
    {
        "paper": {
            "id": "2504.17502",
            "authors": [
                {
                    "_id": "680b44fb426b7d5bc2018c75",
                    "user": {
                        "_id": "631da07f6d6a5870f3d2c375",
                        "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
                        "isPro": false,
                        "fullname": "Aviv Slobodkin",
                        "user": "lovodkin93",
                        "type": "user"
                    },
                    "name": "Aviv Slobodkin",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-25T08:17:03.471Z",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c76",
                    "name": "Hagai Taitelbaum",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c77",
                    "name": "Yonatan Bitton",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c78",
                    "name": "Brian Gordon",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c79",
                    "name": "Michal Sokolik",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c7a",
                    "name": "Nitzan Bitton Guetta",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c7b",
                    "name": "Almog Gueta",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c7c",
                    "name": "Royi Rassin",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c7d",
                    "name": "Itay Laish",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c7e",
                    "name": "Dani Lischinski",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c7f",
                    "name": "Idan Szpektor",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T12:44:51.000Z",
            "submittedOnDailyAt": "2025-04-25T06:50:21.552Z",
            "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image\n  Generation",
            "submittedOnDailyBy": {
                "_id": "631da07f6d6a5870f3d2c375",
                "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
                "isPro": false,
                "fullname": "Aviv Slobodkin",
                "user": "lovodkin93",
                "type": "user"
            },
            "summary": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\nAnimal, Object), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.",
            "upvotes": 44,
            "discussionId": "680b44ff426b7d5bc2018d85",
            "ai_keywords": [
                "RefVNLI",
                "video-reasoning benchmarks",
                "image perturbations"
            ]
        },
        "translation_title": "RefVNLI: 주체 기반 Text-to-Image 생성의 확장 가능한 평가를 향하여",
        "purpose": "주체 기반 Text-to-Image 생성의 텍스트 정렬성과 주체 보존을 동시에 평가하기 위한 신뢰할 수 있는 자동 평가 방법 개발",
        "method": [
            "RefVNLI라는 비용 효율적인 메트릭을 도입하여 텍스트 정렬성과 주체 보존을 단일 예측으로 평가함(To address this, we introduce RefVNLI, a cost-effective metric that evaluates both textual alignment and subject preservation in a single prediction.)",
            "비디오 추론 벤치마크와 이미지 변형에서 파생된 대규모 데이터셋을 기반으로 RefVNLI를 훈련함(Trained on a large-scale dataset derived from video-reasoning benchmarks and image perturbations.)",
            "다양한 벤치마크와 주체 범주에서 기존 기준을 초과하거나 일치하는 성과를 기록함(RefVNLI outperforms or matches existing baselines across multiple benchmarks and subject categories.)"
        ],
        "conclusion": "RefVNLI는 텍스트 정렬에서 최대 6.4점, 주체 일관성에서 최대 8.5점 향상을 달성하며, 잘 알려지지 않은 개념에서도 인간 선호와 87% 이상의 정확도로 일치함.",
        "keywords": [
            "Image Generation",
            "Video Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2504.17192",
            "authors": [
                {
                    "_id": "680aee7bcf67477f2c00ca53",
                    "user": {
                        "_id": "64f7bf0c7565a69eb693ad1f",
                        "avatarUrl": "/avatars/aba6910aa39a3437a7f0df3f5cd49e6d.svg",
                        "isPro": false,
                        "fullname": "minju",
                        "user": "iaminju",
                        "type": "user"
                    },
                    "name": "Minju Seo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:41.304Z",
                    "hidden": false
                },
                {
                    "_id": "680aee7bcf67477f2c00ca54",
                    "user": {
                        "_id": "63036b6c5c70c21d0ea79d48",
                        "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
                        "isPro": false,
                        "fullname": "Jinheon Baek",
                        "user": "jinheon",
                        "type": "user"
                    },
                    "name": "Jinheon Baek",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:38.982Z",
                    "hidden": false
                },
                {
                    "_id": "680aee7bcf67477f2c00ca55",
                    "name": "Seongyun Lee",
                    "hidden": false
                },
                {
                    "_id": "680aee7bcf67477f2c00ca56",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T01:57:01.000Z",
            "submittedOnDailyAt": "2025-04-25T04:17:48.790Z",
            "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
            "submittedOnDailyBy": {
                "_id": "6550c4f27bbfce1878f5f280",
                "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
                "isPro": false,
                "fullname": "seongyun_lee",
                "user": "Seongyun",
                "type": "user"
            },
            "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.",
            "upvotes": 40,
            "discussionId": "680aee7dcf67477f2c00ca96",
            "githubRepo": "https://github.com/going-doer/Paper2Code"
        },
        "translation_title": "Paper2Code: 머신러닝 논문으로부터 코드 생성을 자동화하기",
        "purpose": "머신러닝 연구 결과를 재현하고 발전시키기 위한 코드 구현을 손쉽게 만들기",
        "method": [
            "PaperCoder라는 다중 에이전트 LLM 프레임워크를 도입하여 머신러닝 논문을 기능적 코드 저장소로 변환함(Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories.)",
            "PaperCoder는 계획, 분석, 생성의 세 단계로 진행되며, 각 단계는 전문 에이전트들이 효과적으로 협력하도록 설계됨(PaperCoder operates in three stages: planning, analysis, and generation, with each phase instantiated through a set of specialized agents designed to collaborate effectively across the pipeline.)",
            "머신러닝 논문에서 코드 구현을 생성하는 PaperCoder의 성능을 원작자와의 비교 등을 통해 평가함(We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors.)"
        ],
        "conclusion": "PaperCoder는 고품질의 신뢰성 있는 코드 생성을 성공적으로 이루었으며, 최근의 PaperBench 벤치마크에서도 강력한 기준을 초과하여 성능을 입증함.",
        "keywords": [
            "Natural Language Processing",
            "Code Generation",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2504.17432",
            "authors": [
                {
                    "_id": "680adfbe464a44cea0b843c1",
                    "name": "Tiancheng Gu",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c2",
                    "user": {
                        "_id": "63e202f352b7578dba448ab5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
                        "isPro": false,
                        "fullname": "Yang",
                        "user": "Kaichengalex",
                        "type": "user"
                    },
                    "name": "Kaicheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:46.935Z",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c3",
                    "name": "Ziyong Feng",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c4",
                    "name": "Xingjun Wang",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c5",
                    "name": "Yanzhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c6",
                    "name": "Dingkun Long",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c7",
                    "name": "Yingda Chen",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c8",
                    "name": "Weidong Cai",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c9",
                    "name": "Jiankang Deng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T10:51:52.000Z",
            "submittedOnDailyAt": "2025-04-25T01:11:53.967Z",
            "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
            "submittedOnDailyBy": {
                "_id": "63e202f352b7578dba448ab5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
                "isPro": false,
                "fullname": "Yang",
                "user": "Kaichengalex",
                "type": "user"
            },
            "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.",
            "upvotes": 26,
            "discussionId": "680adfbf464a44cea0b8440f",
            "projectPage": "https://garygutc.github.io/UniME/",
            "githubRepo": "https://github.com/deepglint/UniME",
            "ai_keywords": [
                "Contrastive Language-Image Pre-training (CLIP)",
                "Multimodal Large Language Models (MLLMs)",
                "Generalized vision-language understanding",
                "UniME (Universal Multimodal Embedding)",
                "Discriminative representations",
                "Textual discriminative knowledge distillation",
                "LLM-based teacher model",
                "Hard negative enhanced instruction tuning",
                "False negative contamination",
                "Challenging samples",
                "Discriminative power",
                "Instruction-following ability",
                "MMEB benchmark",
                "Short caption retrieval",
                "Long caption retrieval",
                "Compositional retrieval"
            ]
        },
        "translation_title": "모달리티 장벽을 허물다: 다중 모달 LLM을 통한 보편적 임베딩 학습",
        "purpose": "다양한 다운스트림 작업을 위한 차별화된 임베딩 학습 방안 제시 및 성능 향상",
        "method": [
            "MLLM 기반 교사 모델로부터 텍스트 지식 증류를 수행하여 MLLM의 언어 구성 요소의 임베딩 능력을 향상시킴(In the first stage, we perform textual discriminative knowledge distillation from a powerful LLM-based teacher model to enhance the embedding capability of the MLLM's language component.)",
            "하드 네거티브 강화 지시 튜닝을 도입하여 차별화된 표현 학습을 추가로 발전시킴(In the second stage, we introduce hard negative enhanced instruction tuning to further advance discriminative representation learning.)",
            "샘플 간의 하드 네거티브를 추출하여 모델이 도전적인 샘플에 집중하도록 유도함(This approach not only improves discriminative power but also enhances instruction-following ability in downstream tasks.)"
        ],
        "conclusion": "UniME는 모든 작업에서 일관된 성능 향상을 이루어내며, 차별화 및 조합 능력이 우수함을 입증함.",
        "keywords": [
            "Multimodal Learning",
            "Large Language Models",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2504.17207",
            "authors": [
                {
                    "_id": "680af2bf2fa10fbf21684bde",
                    "name": "Phillip Y. Lee",
                    "hidden": false
                },
                {
                    "_id": "680af2bf2fa10fbf21684bdf",
                    "name": "Jihyeon Je",
                    "hidden": false
                },
                {
                    "_id": "680af2bf2fa10fbf21684be0",
                    "name": "Chanho Park",
                    "hidden": false
                },
                {
                    "_id": "680af2bf2fa10fbf21684be1",
                    "name": "Mikaela Angelina Uy",
                    "hidden": false
                },
                {
                    "_id": "680af2bf2fa10fbf21684be2",
                    "name": "Leonidas Guibas",
                    "hidden": false
                },
                {
                    "_id": "680af2bf2fa10fbf21684be3",
                    "name": "Minhyuk Sung",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T02:41:34.000Z",
            "submittedOnDailyAt": "2025-04-25T00:59:29.327Z",
            "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation",
            "submittedOnDailyBy": {
                "_id": "6342796a0875f2c99cfd313b",
                "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
                "isPro": false,
                "fullname": "Yuseung \"Phillip\" Lee",
                "user": "phillipinseoul",
                "type": "user"
            },
            "summary": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.",
            "upvotes": 17,
            "discussionId": "680af2c02fa10fbf21684c1f",
            "ai_keywords": [
                "vision-language models (VLMs)",
                "mental imagery simulation",
                "perspective-taking",
                "visual understanding",
                "environmental interaction",
                "autonomous agents",
                "spatial reasoning",
                "perspective-aware reasoning capabilities",
                "egocentric interpretations",
                "mental imagery",
                "scene abstractions",
                "perspective transformations",
                "object detection",
                "segmentation",
                "orientation estimation",
                "synthetic benchmarks",
                "real-image benchmarks",
                "fine-tuned spatial reasoning models",
                "novel-view-synthesis-based approaches"
            ]
        },
        "translation_title": "정신 이미지 시뮬레이션을 통한 비전-언어 모델에서의 시각적 관점 인식 논리",
        "purpose": "비전-언어 모델이 인간 수준의 시각적 이해를 할 수 있도록 시각적 관점 인식 능력을 향상시키기 위한 프레임워크 제안",
        "method": [
            "정신 이미지를 활용해 다양한 관점에서 환경을 인식할 수 있도록 하는 Abstract Perspective Change (APC) 프레임워크를 제안함(we propose a framework for perspective-aware reasoning, named Abstract Perspective Change (APC), that effectively leverages vision foundation models, such as object detection, segmentation, and orientation estimation, to construct scene abstractions and enable perspective transformations.)",
            "APC를 통해 씬 추상화를 구성하고 관점 변환을 가능하게 함(we focus on the role of mental imagery, where humans perceive the world through abstracted representations that facilitate perspective shifts.)",
            "다양한 비전-언어 모델과 비교하여 다양한 벤치마크에서 APC의 성능을 테스트하여 시각적 관점 인식 향상을 실현함(Our experiments on synthetic and real-image benchmarks, compared with various VLMs, demonstrate significant improvements in perspective-aware reasoning with our framework.)"
        ],
        "conclusion": "APC 프레임워크는 비전-언어 모델의 시각적 관점 인식 능력을 크게 향상시키고, 공간적 추리 모델 및 새로운 뷰 합성 접근 방식보다 우수한 성능을 보임.",
        "keywords": [
            "Vision-Language Models",
            "Image Segmentation",
            "3D Vision"
        ]
    }
]