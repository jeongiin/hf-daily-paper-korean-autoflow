[
    {
        "paper": {
            "id": "2502.08910",
            "authors": [
                {
                    "_id": "67aebd48225614bbe7f6f271",
                    "user": {
                        "_id": "62e622d08e0b2dc6707f8794",
                        "avatarUrl": "/avatars/8c47b5c862f82d4258ba707c932f7f87.svg",
                        "isPro": false,
                        "fullname": "Heejun Lee",
                        "user": "gmlwns5176",
                        "type": "user"
                    },
                    "name": "Heejun Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:15.423Z",
                    "hidden": false
                },
                {
                    "_id": "67aebd48225614bbe7f6f272",
                    "user": {
                        "_id": "646cae3093badbc8c2e891c7",
                        "avatarUrl": "/avatars/4aae2aca70ea9dc58dd6f9f9b2be15e1.svg",
                        "isPro": false,
                        "fullname": "Geon Park",
                        "user": "geonp",
                        "type": "user"
                    },
                    "name": "Geon Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:12.988Z",
                    "hidden": false
                },
                {
                    "_id": "67aebd48225614bbe7f6f273",
                    "user": {
                        "_id": "657ffd9ba2ef32167533f04a",
                        "avatarUrl": "/avatars/e180e063c810c15d02b494727e962b84.svg",
                        "isPro": false,
                        "fullname": "Jaduk Suh",
                        "user": "Losif63",
                        "type": "user"
                    },
                    "name": "Jaduk Suh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:00:46.888Z",
                    "hidden": false
                },
                {
                    "_id": "67aebd48225614bbe7f6f274",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T02:52:01.000Z",
            "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU",
            "summary": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.",
            "upvotes": 50,
            "discussionId": "67aebd4a225614bbe7f6f2d6"
        },
        "translation_title": "InfiniteHiP: 단일 GPU에서 300만 토큰까지 언어 모델 컨텍스트 확장",
        "purpose": "매우 긴 텍스트에서 효과적이고 실용적인 컨텍스트 활용을 가능하게 하기 위함.",
        "method": [
            "모듈화된 계층적 토큰 프루닝 알고리즘을 통해 불필요한 컨텍스트 토큰을 동적으로 제거하여 처리 속도를 가속화함(InfiniteHiP, a novel, and practical LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens through a modular hierarchical token pruning algorithm.)",
            "LLM 내부의 주의 패턴에 따라 다양한 RoPE 조정 방법을 선택적으로 적용하여 긴 시퀀스에 대한 일반화를 가능하게 함(This method also allows generalization to longer sequences by selectively applying various RoPE adjustment methods according to the internal attention patterns within LLMs.)",
            "추론 중 키-값 캐시를 호스트 메모리로 오프로드하여 GPU 메모리 압력을 크게 줄임(We offload the key-value cache to host memory during inference, significantly reducing GPU memory pressure.)"
        ],
        "conclusion": "InfiniteHiP는 단일 L40s 48GB GPU에서 300만 토큰 처리를 가능하게 하여 주의 디코딩 속도를 18.95배 향상시키며, 추가적인 훈련 없이도 유용성과 실용성을 향상시킴.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2502.08690",
            "authors": [
                {
                    "_id": "67aec0a203bf3301ec29ac39",
                    "user": {
                        "_id": "633e6f07309a99325095dd42",
                        "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
                        "isPro": false,
                        "fullname": "Hoigi Seo",
                        "user": "Agorium",
                        "type": "user"
                    },
                    "name": "Hoigi Seo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:10.420Z",
                    "hidden": false
                },
                {
                    "_id": "67aec0a203bf3301ec29ac3a",
                    "name": "Wongi Jeong",
                    "hidden": false
                },
                {
                    "_id": "67aec0a203bf3301ec29ac3b",
                    "name": "Jae-sun Seo",
                    "hidden": false
                },
                {
                    "_id": "67aec0a203bf3301ec29ac3c",
                    "name": "Se Young Chun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T15:03:26.000Z",
            "title": "Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient\n  Text-to-Image Generation",
            "summary": "Large-scale text encoders in text-to-image (T2I) diffusion models have\ndemonstrated exceptional performance in generating high-quality images from\ntextual prompts. Unlike denoising modules that rely on multiple iterative\nsteps, text encoders require only a single forward pass to produce text\nembeddings. However, despite their minimal contribution to total inference time\nand floating-point operations (FLOPs), text encoders demand significantly\nhigher memory usage, up to eight times more than denoising modules. To address\nthis inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet\neffective pruning strategy specifically designed for text encoders in T2I\ndiffusion models. Skrr exploits the inherent redundancy in transformer blocks\nby selectively skipping or reusing certain layers in a manner tailored for T2I\ntasks, thereby reducing memory consumption without compromising performance.\nExtensive experiments demonstrate that Skrr maintains image quality comparable\nto the original model even under high sparsity levels, outperforming existing\nblockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory\nefficiency while preserving performance across multiple evaluation metrics,\nincluding the FID, CLIP, DreamSim, and GenEval scores.",
            "upvotes": 28,
            "discussionId": "67aec0a903bf3301ec29adf3"
        },
        "translation_title": "Skrr: 메모리 효율적인 Text-to-Image 생성을 위한 텍스트 인코더 레이어 스킵 및 재사용",
        "purpose": "Text-to-Image 생성 모델의 메모리 효율성을 개선하기 위한 방법 연구",
        "method": [
            "기존 텍스트 인코더의 비효율성을 해결하기 위해 Skip and Re-use layers(Skrr)라는 프루닝 전략을 제안함(To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models.)",
            "Skrr은 transformer 블록의 중복성을 활용하여 특정 레이어를 선택적으로 스킵하거나 재사용함(Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks.)",
            "광범위한 실험을 통해 Skrr이 원래 모델과 유사한 이미지 품질을 유지함을 입증함(Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels.)"
        ],
        "conclusion": "Skrr은 최신 메모리 효율성을 달성하면서 여러 평가 지표에서 성능을 보존함.",
        "keywords": [
            "Image Generation",
            "Large Language Models",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2502.09604",
            "authors": [
                {
                    "_id": "67aeac4f2d48d9bf7728334e",
                    "user": {
                        "_id": "5df84571da6d0311fd3d5407",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650651305661-5df84571da6d0311fd3d5407.png",
                        "isPro": false,
                        "fullname": "Yung-Sung Chuang",
                        "user": "voidism",
                        "type": "user"
                    },
                    "name": "Yung-Sung Chuang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-14T02:37:32.909Z",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf7728334f",
                    "user": {
                        "_id": "639aaf82a4c528850bba2bfe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639aaf82a4c528850bba2bfe/nn23r8bsNiOJzVUxAPfo7.png",
                        "isPro": false,
                        "fullname": "Benjamin Cohen-Wang",
                        "user": "bencw",
                        "type": "user"
                    },
                    "name": "Benjamin Cohen-Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:17.696Z",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283350",
                    "name": "Shannon Zejiang Shen",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283351",
                    "user": {
                        "_id": "6351712b40dffad651f128c7",
                        "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
                        "isPro": false,
                        "fullname": "Zhaofeng Wu",
                        "user": "ZhaofengWu",
                        "type": "user"
                    },
                    "name": "Zhaofeng Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:19.691Z",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283352",
                    "name": "Hu Xu",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283353",
                    "name": "Xi Victoria Lin",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283354",
                    "name": "James Glass",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283355",
                    "name": "Shang-Wen Li",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283356",
                    "name": "Wen-tau Yih",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T18:55:13.000Z",
            "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models",
            "summary": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.",
            "upvotes": 20,
            "discussionId": "67aeac502d48d9bf77283380"
        },
        "translation_title": "SelfCite: 큰 언어 모델에서 문맥 귀속을 위한 자기 지도 정렬",
        "purpose": "문맥에 따라 세부적인 문장 수준 인용을 생성하기 위한 자기 지도 접근법 연구",
        "method": [
            "SelfCite는 LLM이 자체적으로 제공하는 보상 신호를 활용하여 문맥 절제(context ablation)를 수행함(Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation.)",
            "문헌이 필요할 경우 관련 텍스트를 삭제하면 동일한 응답을 방지하도록 개선함(If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response.)",
            "LLM의 보상을 활용해 최고의 샘플링 전략을 개선하고, 직접적으로 인용 품질 향상을 위해 모델을 미세 조정함(This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations.)"
        ],
        "conclusion": "SelfCite는 LongBench-Cite 벤치마크에서 인용 F1 점수를 5.3 포인트까지 향상시키는 데 효과적이다.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2502.09619",
            "authors": [
                {
                    "_id": "67aef6212c36e4d8bd23740e",
                    "user": {
                        "_id": "6465fd33dac127ac80f0b334",
                        "avatarUrl": "/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg",
                        "isPro": false,
                        "fullname": "Jonathan Kahana",
                        "user": "jonkahana",
                        "type": "user"
                    },
                    "name": "Jonathan Kahana",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:41:31.020Z",
                    "hidden": false
                },
                {
                    "_id": "67aef6212c36e4d8bd23740f",
                    "name": "Or Nathan",
                    "hidden": false
                },
                {
                    "_id": "67aef6212c36e4d8bd237410",
                    "user": {
                        "_id": "630dd4218df86f1e5beb2ed7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
                        "isPro": false,
                        "fullname": "Eliahu Horwitz",
                        "user": "Eliahu",
                        "type": "user"
                    },
                    "name": "Eliahu Horwitz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T11:10:52.689Z",
                    "hidden": false
                },
                {
                    "_id": "67aef6212c36e4d8bd237411",
                    "user": {
                        "_id": "646cfc3b4220471ca0c56b20",
                        "avatarUrl": "/avatars/19d6ab141ec2cd25c1c3b45fd8f69910.svg",
                        "isPro": false,
                        "fullname": "Yedid Hoshen",
                        "user": "yedid",
                        "type": "user"
                    },
                    "name": "Yedid Hoshen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:41:43.672Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T18:59:44.000Z",
            "title": "Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights",
            "summary": "With the increasing numbers of publicly available models, there are probably\npretrained, online models for most tasks users require. However, current model\nsearch methods are rudimentary, essentially a text-based search in the\ndocumentation, thus users cannot find the relevant models. This paper presents\nProbeLog, a method for retrieving classification models that can recognize a\ntarget concept, such as \"Dog\", without access to model metadata or training\ndata. Differently from previous probing methods, ProbeLog computes a descriptor\nfor each output dimension (logit) of each model, by observing its responses on\na fixed set of inputs (probes). Our method supports both logit-based retrieval\n(\"find more logits like this\") and zero-shot, text-based retrieval (\"find all\nlogits corresponding to dogs\"). As probing-based representations require\nmultiple costly feedforward passes through the model, we develop a method,\nbased on collaborative filtering, that reduces the cost of encoding\nrepositories by 3x. We demonstrate that ProbeLog achieves high retrieval\naccuracy, both in real-world and fine-grained search tasks and is scalable to\nfull-size repositories.",
            "upvotes": 19,
            "discussionId": "67aef6222c36e4d8bd237472"
        },
        "translation_title": "이 모델은 개도 인식할 수 있을까? 가중치로부터의 제로샷 모델 검색",
        "purpose": "사용자가 필요한 작업에 대한 적절한 모델을 찾기 위해 모델 검색 방법을 개선하는 것",
        "method": [
            "ProbeLog 방법을 제시하여 대상 개념(예: '개')을 인식하는 분류 모델을 검색함(ProbeLog, a method for retrieving classification models that can recognize a target concept, such as 'Dog', without access to model metadata or training data.)",
            "각 모델의 출력 차원에 대한 설명자를 계산하여 고정된 입력 집합에 대한 반응을 관찰함(Differently from previous probing methods, ProbeLog computes a descriptor for each output dimension (logit) of each model, by observing its responses on a fixed set of inputs (probes).)",
            "협업 필터링에 기반한 방법을 개발하여 리포지토리 인코딩 비용을 3배 줄임(we develop a method, based on collaborative filtering, that reduces the cost of encoding repositories by 3x.)"
        ],
        "conclusion": "ProbeLog는 실제 및 세분화된 검색 작업 모두에서 높은 검색 정확도를 달성하며 전체 리포지토리에 대한 확장성이 있음.",
        "keywords": [
            "Computer Vision",
            "Image Classification",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2502.09056",
            "authors": [
                {
                    "_id": "67aea8d7926b659c7e959bbc",
                    "user": {
                        "_id": "62d192c2d50433c35eb1b48e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d192c2d50433c35eb1b48e/VjmDu8GOIuLuQNBQdQLLS.png",
                        "isPro": true,
                        "fullname": "Kunat Pipatanakul",
                        "user": "kunato",
                        "type": "user"
                    },
                    "name": "Kunat Pipatanakul",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:04:29.437Z",
                    "hidden": false
                },
                {
                    "_id": "67aea8d7926b659c7e959bbd",
                    "user": {
                        "_id": "615313b0793ef66b3324da1f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg",
                        "isPro": false,
                        "fullname": "Pittawat Taveekitworachai",
                        "user": "pittawat",
                        "type": "user"
                    },
                    "name": "Pittawat Taveekitworachai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:21.838Z",
                    "hidden": false
                },
                {
                    "_id": "67aea8d7926b659c7e959bbe",
                    "user": {
                        "_id": "63f6a050b4c9a104f4b95755",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f6a050b4c9a104f4b95755/eJQyJkenSz536j-EGcpkH.jpeg",
                        "isPro": false,
                        "fullname": "Potsawee Manakul",
                        "user": "potsawee",
                        "type": "user"
                    },
                    "name": "Potsawee Manakul",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:04:35.434Z",
                    "hidden": false
                },
                {
                    "_id": "67aea8d7926b659c7e959bbf",
                    "name": "Kasima Tharnpipitchai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T08:10:45.000Z",
            "title": "An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in\n  One Day via Model Merging",
            "summary": "This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks.",
            "upvotes": 19,
            "discussionId": "67aea8d8926b659c7e959bee"
        },
        "translation_title": "오픈 레시피: 모델 병합을 통한 언어 특정 LLM을 하루 만에 추론 모델로 조정하기",
        "purpose": "언어 특정 LLM의 추론 능력을 향상시키면서도 해당 언어의 능력을 유지하는 것",
        "method": [
            "DeepSeek R1의 우수한 추론 능력을 언어 특정 LLM에 통합하기 위한 데이터 선택 및 모델 병합 방식을 조사함(This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs).)",
            "언어 특정 LLM의 추론 능력을 향상시키기 위해 공개 데이터셋과 $120의 예산만으로 DeepSeek R1의 수준에 맞출 수 있음을 입증함(We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1.)"
        ],
        "conclusion": "언어 특정 LLM의 성능을 저하시키지 않으면서도 추론 능력을 크게 향상시킬 수 있음을 보여줌.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    }
]