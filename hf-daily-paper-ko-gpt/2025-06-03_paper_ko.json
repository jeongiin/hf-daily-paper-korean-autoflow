[
    {
        "paper": {
            "id": "2506.01939",
            "authors": [
                {
                    "_id": "683e7a6d97fd742a8edee1ba",
                    "user": {
                        "_id": "6486dde1f74857df3f1a5828",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
                        "isPro": false,
                        "fullname": "Shenzhi Wang",
                        "user": "shenzhi-wang",
                        "type": "user"
                    },
                    "name": "Shenzhi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:41:16.481Z",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1bb",
                    "name": "Le Yu",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1bc",
                    "name": "Chang Gao",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1bd",
                    "user": {
                        "_id": "610b70452719facd4ea85e28",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                        "isPro": false,
                        "fullname": "Chujie Zheng",
                        "user": "chujiezheng",
                        "type": "user"
                    },
                    "name": "Chujie Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:41:13.605Z",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1be",
                    "name": "Shixuan Liu",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1bf",
                    "name": "Rui Lu",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c0",
                    "name": "Kai Dang",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c1",
                    "user": {
                        "_id": "63f30b870a16587ea970edfe",
                        "avatarUrl": "/avatars/059491b33fecec69032e6d481229ee31.svg",
                        "isPro": false,
                        "fullname": "Xiong-Hui Chen",
                        "user": "xionghuichen",
                        "type": "user"
                    },
                    "name": "Xionghui Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:41:10.358Z",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c2",
                    "name": "Jianxin Yang",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c3",
                    "user": {
                        "_id": "64704e973601bb7b06643e98",
                        "avatarUrl": "/avatars/52e51f4d1be6769e4397b8be2799cf32.svg",
                        "isPro": false,
                        "fullname": "Zhenru Zhang",
                        "user": "Zhenru",
                        "type": "user"
                    },
                    "name": "Zhenru Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:40:21.027Z",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c4",
                    "user": {
                        "_id": "666aacfb918ba11c7c598194",
                        "avatarUrl": "/avatars/45bee8f1fdbdd256ee47d25e4bf01a7a.svg",
                        "isPro": false,
                        "fullname": "Yuqiong Liu",
                        "user": "lyq333",
                        "type": "user"
                    },
                    "name": "Yuqiong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:40:30.240Z",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c5",
                    "name": "An Yang",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c6",
                    "name": "Andrew Zhao",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c7",
                    "name": "Yang Yue",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c8",
                    "name": "Shiji Song",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c9",
                    "user": {
                        "_id": "63d9d68c1cae35c27bf7a6a7",
                        "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
                        "isPro": false,
                        "fullname": "Bowen Yu",
                        "user": "Tigerph",
                        "type": "user"
                    },
                    "name": "Bowen Yu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-03T04:30:38.648Z",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1ca",
                    "name": "Gao Huang",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1cb",
                    "user": {
                        "_id": "620760a26e3b7210c2ff1943",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
                        "isPro": false,
                        "fullname": "Junyang Lin",
                        "user": "JustinLin610",
                        "type": "user"
                    },
                    "name": "Junyang Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:39:54.278Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T17:54:39.000Z",
            "submittedOnDailyAt": "2025-06-03T03:09:30.655Z",
            "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "6486dde1f74857df3f1a5828",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
                "isPro": false,
                "fullname": "Shenzhi Wang",
                "user": "shenzhi-wang",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful approach to enhancing the reasoning capabilities of Large Language\nModels (LLMs), while its mechanisms are not yet well understood. In this work,\nwe undertake a pioneering exploration of RLVR through the novel perspective of\ntoken entropy patterns, comprehensively analyzing how different tokens\ninfluence reasoning performance. By examining token entropy patterns in\nChain-of-Thought (CoT) reasoning, we observe that only a small fraction of\ntokens exhibit high entropy, and these tokens act as critical forks that steer\nthe model toward diverse reasoning pathways. Furthermore, studying how entropy\npatterns evolve during RLVR training reveals that RLVR largely adheres to the\nbase model's entropy patterns, primarily adjusting the entropy of high-entropy\ntokens. These findings highlight the significance of high-entropy tokens (i.e.,\nforking tokens) to RLVR. We ultimately improve RLVR by restricting policy\ngradient updates to forking tokens and uncover a finding even beyond the 80/20\nrule: utilizing only 20% of the tokens while maintaining performance comparable\nto full-gradient updates on the Qwen3-8B base model and significantly\nsurpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71\non AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,\nhighlighting a strong scaling trend. In contrast, training exclusively on the\n80% lowest-entropy tokens leads to a marked decline in performance. These\nfindings indicate that the efficacy of RLVR primarily arises from optimizing\nthe high-entropy tokens that decide reasoning directions. Collectively, our\nresults highlight the potential to understand RLVR through a token-entropy\nperspective and optimize RLVR by leveraging high-entropy minority tokens to\nfurther improve LLM reasoning.",
            "upvotes": 78,
            "discussionId": "683e7a6e97fd742a8edee227",
            "projectPage": "https://shenzhi-wang.github.io/high-entropy-minority-tokens-rlvr/",
            "ai_summary": "Token entropy patterns are crucial in RLVR, with high-entropy tokens significantly impacting reasoning performance and model optimization.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "RLVR",
                "Large Language Models",
                "LLMs",
                "token entropy patterns",
                "Chain-of-Thought",
                "CoT reasoning",
                "high-entropy tokens",
                "policy gradient updates",
                "Qwen3-8B",
                "Qwen3-32B",
                "Qwen3-14B",
                "AIME"
            ]
        },
        "translation_title": "80/20 법칙을 넘어: 높은 엔트로피 소수 토큰이 LLM 추론을 위한 강화 학습을 이끌다",
        "purpose": "토큰 엔트로피 패턴을 통해 RLVR의 이론을 개선하고 LLM의 추론 능력을 향상시키기 위한 연구",
        "method": [
            "RLVR의 효과를 높이기 위해 토큰 엔트로피 패턴을 분석하고 고-엔트로피 토큰의 중요성을 연구함(we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance.)",
            "Chain-of-Thought 추론에서 고-엔트로피 토큰이 모델의 추론 경로를 조정하는 중요한 역할을 함을 관찰함(By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways.)",
            "정책 기울기 업데이트를 고-엔트로피 소수 토큰에 제한하여 RLVR을 개선함(We ultimately improve RLVR by restricting policy gradient updates to forking tokens.)"
        ],
        "conclusion": "고-엔트로피 소수 토큰을 최적화함으로써 LLM의 추론 성능을 크게 향상시킬 수 있음을 밝혀냄.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Reinforcement Learning"
        ]
    },
    {
        "paper": {
            "id": "2505.24760",
            "authors": [
                {
                    "_id": "683e6af92139ea008faa74ba",
                    "user": {
                        "_id": "65144e46004a986ccc9d21d6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
                        "isPro": false,
                        "fullname": "Zafir Stojanovski",
                        "user": "zafstojano",
                        "type": "user"
                    },
                    "name": "Zafir Stojanovski",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-03T06:28:46.378Z",
                    "hidden": false
                },
                {
                    "_id": "683e6af92139ea008faa74bb",
                    "user": {
                        "_id": "6303f5f37b50dd9d0a371b28",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6303f5f37b50dd9d0a371b28/H25eCzAYVwBtpSpD8tnUV.jpeg",
                        "isPro": false,
                        "fullname": "Oliver Stanley",
                        "user": "OllieStanley",
                        "type": "user"
                    },
                    "name": "Oliver Stanley",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:45:56.277Z",
                    "hidden": false
                },
                {
                    "_id": "683e6af92139ea008faa74bc",
                    "name": "Joe Sharratt",
                    "hidden": false
                },
                {
                    "_id": "683e6af92139ea008faa74bd",
                    "name": "Richard Jones",
                    "hidden": false
                },
                {
                    "_id": "683e6af92139ea008faa74be",
                    "name": "Abdulhakeem Adefioye",
                    "hidden": false
                },
                {
                    "_id": "683e6af92139ea008faa74bf",
                    "user": {
                        "_id": "6304061c0547362a22a76a17",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661339692442-6304061c0547362a22a76a17.jpeg",
                        "isPro": false,
                        "fullname": "Jean Kaddour",
                        "user": "JeanKaddour",
                        "type": "user"
                    },
                    "name": "Jean Kaddour",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T09:13:54.739Z",
                    "hidden": false
                },
                {
                    "_id": "683e6af92139ea008faa74c0",
                    "name": "Andreas Köpf",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T16:20:18.000Z",
            "submittedOnDailyAt": "2025-06-03T02:02:10.153Z",
            "title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards",
            "submittedOnDailyBy": {
                "_id": "65144e46004a986ccc9d21d6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
                "isPro": false,
                "fullname": "Zafir Stojanovski",
                "user": "zafstojano",
                "type": "user"
            },
            "summary": "We introduce Reasoning Gym (RG), a library of reasoning environments for\nreinforcement learning with verifiable rewards. It provides over 100 data\ngenerators and verifiers spanning multiple domains including algebra,\narithmetic, computation, cognition, geometry, graph theory, logic, and various\ncommon games. Its key innovation is the ability to generate virtually infinite\ntraining data with adjustable complexity, unlike most previous reasoning\ndatasets, which are typically fixed. This procedural generation approach allows\nfor continuous evaluation across varying difficulty levels. Our experimental\nresults demonstrate the efficacy of RG in both evaluating and reinforcement\nlearning of reasoning models.",
            "upvotes": 34,
            "discussionId": "683e6afa2139ea008faa7531",
            "githubRepo": "https://github.com/open-thought/reasoning-gym",
            "ai_summary": "Reasoning Gym provides a library of reasoning environments with verifiable rewards and procedural data generation for reinforcement learning, enabling the evaluation and training of reasoning models at varying difficulty levels.",
            "ai_keywords": [
                "reinforcement learning",
                "verifiable rewards",
                "data generators",
                "verifiers",
                "procedural generation",
                "reasoning models"
            ]
        },
        "translation_title": "REASONING GYM: 검증 가능한 보상을 위한 강화 학습 추론 환경",
        "purpose": "검증 가능한 보상을 가진 강화 학습을 위한 추론 환경을 제공하여 모델의 평가와 학습을 지원하기 위함.",
        "method": [
            "100개 이상의 데이터 생성기와 검증기를 포함한 Reasoning Gym 라이브러리를 개발함(We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards.)",
            "고정된 이전 추론 데이터셋들과 달리 조정 가능한 복잡성으로 사실상 무한한 훈련 데이터를 생성할 수 있는 방식을 채택함(Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed.)",
            "다양한 난이도를 통해 지속적으로 평가할 수 있도록 프로시저 생성 접근 방식을 활용함(This procedural generation approach allows for continuous evaluation across varying difficulty levels.)"
        ],
        "conclusion": "RG는 추론 모델의 평가와 강화 학습에서 효과적임을 실험적으로 입증함.",
        "keywords": [
            "Reinforcement Learning",
            "Verification",
            "Reasoning"
        ]
    },
    {
        "paper": {
            "id": "2505.23590",
            "authors": [
                {
                    "_id": "683e6b2d97fd742a8edb8a8e",
                    "user": {
                        "_id": "64f5c7cb65a4b1acb20ffc15",
                        "avatarUrl": "/avatars/67c42a6e26bbeddc4bf706eb8f1a75b1.svg",
                        "isPro": false,
                        "fullname": "Zifu Wang",
                        "user": "wangzifu",
                        "type": "user"
                    },
                    "name": "Zifu Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:41:52.137Z",
                    "hidden": false
                },
                {
                    "_id": "683e6b2d97fd742a8edb8a8f",
                    "user": {
                        "_id": "6477b2038ab7e732b6d8a9b5",
                        "avatarUrl": "/avatars/c859a35b8965a904f103bbc34f36ab2a.svg",
                        "isPro": false,
                        "fullname": "Junyi Zhu",
                        "user": "RyanZhu",
                        "type": "user"
                    },
                    "name": "Junyi Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:42:00.703Z",
                    "hidden": false
                },
                {
                    "_id": "683e6b2d97fd742a8edb8a90",
                    "name": "Bo Tang",
                    "hidden": false
                },
                {
                    "_id": "683e6b2d97fd742a8edb8a91",
                    "name": "Zhiyu Li",
                    "hidden": false
                },
                {
                    "_id": "683e6b2d97fd742a8edb8a92",
                    "name": "Feiyu Xiong",
                    "hidden": false
                },
                {
                    "_id": "683e6b2d97fd742a8edb8a93",
                    "name": "Jiaqian Yu",
                    "hidden": false
                },
                {
                    "_id": "683e6b2d97fd742a8edb8a94",
                    "name": "Matthew B. Blaschko",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T16:01:22.000Z",
            "submittedOnDailyAt": "2025-06-03T01:56:17.990Z",
            "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with\n  Jigsaw Puzzles",
            "submittedOnDailyBy": {
                "_id": "64f5c7cb65a4b1acb20ffc15",
                "avatarUrl": "/avatars/67c42a6e26bbeddc4bf706eb8f1a75b1.svg",
                "isPro": false,
                "fullname": "Zifu Wang",
                "user": "wangzifu",
                "type": "user"
            },
            "summary": "The application of rule-based reinforcement learning (RL) to multimodal large\nlanguage models (MLLMs) introduces unique challenges and potential deviations\nfrom findings in text-only domains, particularly for perception-heavy tasks.\nThis paper provides a comprehensive study of rule-based visual RL, using jigsaw\npuzzles as a structured experimental framework. Jigsaw puzzles offer inherent\nground truth, adjustable difficulty, and demand complex decision-making, making\nthem ideal for this study. Our research reveals several key findings:\nFirstly, we find that MLLMs, initially performing near to random\nguessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and\ngeneralize to complex, unseen configurations through fine-tuning.\nSecondly, training on jigsaw puzzles can induce generalization to\nother visual tasks, with effectiveness tied to specific task configurations.\nThirdly, MLLMs can learn and generalize with or without explicit\nreasoning, though open-source models often favor direct answering.\nConsequently, even when trained for step-by-step reasoning, they can ignore the\nthinking process in deriving the final answer. Fourthly, we observe\nthat complex reasoning patterns appear to be pre-existing rather than emergent,\nwith their frequency increasing alongside training and task difficulty.\nFinally, our results demonstrate that RL exhibits more effective\ngeneralization than Supervised Fine-Tuning (SFT), and an initial SFT cold start\nphase can hinder subsequent RL optimization. Although these observations are\nbased on jigsaw puzzles and may vary across other visual tasks, this research\ncontributes a valuable piece of jigsaw to the larger puzzle of collective\nunderstanding rule-based visual RL and its potential in multimodal learning.\nThe code is available at: https://github.com/zifuwanggg/Jigsaw-R1.",
            "upvotes": 24,
            "discussionId": "683e6b2e97fd742a8edb8ac1",
            "githubRepo": "https://github.com/zifuwanggg/Jigsaw-R1",
            "ai_summary": "Rule-based reinforcement learning applied to multimodal large language models demonstrates effective generalization in visual tasks, particularly using jigsaw puzzles, outperforming supervised fine-tuning.",
            "ai_keywords": [
                "rule-based reinforcement learning",
                "multimodal large language models",
                "visual RL",
                "jigsaw puzzles",
                "fine-tuning",
                "supervised fine-tuning",
                "complex decision-making",
                "visual tasks",
                "step-by-step reasoning",
                "generalization"
            ]
        },
        "translation_title": "Jigsaw-R1: 규칙 기반 시각 강화 학습에 관한 연구 - 조각 맞추기 퍼즐을 이용하여",
        "purpose": "규칙 기반 시각 강화 학습의 효과성과 가능성을 조각 맞추기 퍼즐을 통해 탐구하고자 함.",
        "method": [
            "조각 맞추기 퍼즐을 실험 프레임워크로 사용하여 연구를 진행함(we use jigsaw puzzles as a structured experimental framework).",
            "MLLMs가 조각 맞추기 퍼즐에서 초기에는 거의 랜덤 추측에 가까운 성능을 보이지만, 파인튜닝을 통해 높은 정확도를 달성함(we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning).",
            "조각 맞추기 교육이 다른 시각 작업으로의 일반화를 유도할 수 있음을 규명함(training on jigsaw puzzles can induce generalization to other visual tasks).",
            "RL의 일반화 성능이 Supervised Fine-Tuning(SFT)보다 뛰어난 결과를 보여줌(our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning)."
        ],
        "conclusion": "이 연구는 규칙 기반 시각 강화 학습의 이해를 확장하고 MLLMs의 잠재력을 밝혀내는 중요한 기여를 함.",
        "keywords": [
            "Multimodal Learning",
            "Computer Vision",
            "Reinforcement Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.00996",
            "authors": [
                {
                    "_id": "683e7cbf402acb186580d5ec",
                    "user": {
                        "_id": "64797735a68454566356b708",
                        "avatarUrl": "/avatars/3424d022dd8ad29b56eb41814c5c3dee.svg",
                        "isPro": false,
                        "fullname": "Kinam Kim",
                        "user": "kinam0252",
                        "type": "user"
                    },
                    "name": "Kinam Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:32:41.547Z",
                    "hidden": false
                },
                {
                    "_id": "683e7cbf402acb186580d5ed",
                    "user": {
                        "_id": "6331ed964db0a767bbee4706",
                        "avatarUrl": "/avatars/2ef3fa6083ca5208f4b87c91345267d9.svg",
                        "isPro": false,
                        "fullname": "Junha Hyung",
                        "user": "JunhaH",
                        "type": "user"
                    },
                    "name": "Junha Hyung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:32:50.417Z",
                    "hidden": false
                },
                {
                    "_id": "683e7cbf402acb186580d5ee",
                    "user": {
                        "_id": "64be3127805e5b64572da65c",
                        "avatarUrl": "/avatars/edd3e94ba9e375827cc75b164602bcac.svg",
                        "isPro": false,
                        "fullname": "Jaegul Choo",
                        "user": "joyfull78",
                        "type": "user"
                    },
                    "name": "Jaegul Choo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:33:03.788Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64797735a68454566356b708/a-8jl8yq3KdDLnuZEaB3K.mp4"
            ],
            "publishedAt": "2025-06-01T12:57:43.000Z",
            "submittedOnDailyAt": "2025-06-03T03:25:10.796Z",
            "title": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion\n  Models",
            "submittedOnDailyBy": {
                "_id": "64797735a68454566356b708",
                "avatarUrl": "/avatars/3424d022dd8ad29b56eb41814c5c3dee.svg",
                "isPro": false,
                "fullname": "Kinam Kim",
                "user": "kinam0252",
                "type": "user"
            },
            "summary": "Recent advances in text-to-video diffusion models have enabled high-quality\nvideo synthesis, but controllable generation remains challenging, particularly\nunder limited data and compute. Existing fine-tuning methods for conditional\ngeneration often rely on external encoders or architectural modifications,\nwhich demand large datasets and are typically restricted to spatially aligned\nconditioning, limiting flexibility and scalability. In this work, we introduce\nTemporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach\nfor adapting pretrained video diffusion models to diverse conditional\ngeneration tasks. Our key idea is to concatenate condition and target frames\nalong the temporal axis and insert intermediate buffer frames with\nprogressively increasing noise levels. These buffer frames enable smooth\ntransitions, aligning the fine-tuning process with the pretrained model's\ntemporal dynamics. TIC-FT requires no architectural changes and achieves strong\nperformance with as few as 10-30 training samples. We validate our method\nacross a range of tasks, including image-to-video and video-to-video\ngeneration, using large-scale base models such as CogVideoX-5B and Wan-14B.\nExtensive experiments show that TIC-FT outperforms existing baselines in both\ncondition fidelity and visual quality, while remaining highly efficient in both\ntraining and inference. For additional results, visit\nhttps://kinam0252.github.io/TIC-FT/",
            "upvotes": 22,
            "discussionId": "683e7cc1402acb186580d663",
            "ai_summary": "Temporal In-Context Fine-Tuning (TIC-FT) enhances pretrained video diffusion models for diverse conditional generation tasks with minimal data and without architectural changes.",
            "ai_keywords": [
                "text-to-video diffusion models",
                "fine-tuning",
                "external encoders",
                "architectural modifications",
                "Temporal In-Context Fine-Tuning",
                "condition and target frames",
                "buffer frames",
                "noise levels",
                "smooth transitions",
                "pretrained video diffusion models",
                "image-to-video generation",
                "video-to-video generation",
                "CogVideoX-5B",
                "Wan-14B"
            ]
        },
        "translation_title": "비디오 확산 모델의 다양한 제어를 위한 시간적 인-컨텍스트 미세 조정",
        "purpose": "제한된 데이터와 계산 환경에서도 고품질의 조절 가능한 비디오 생성을 위한 접근법 개선",
        "method": [
            "Temporal In-Context Fine-Tuning(TIC-FT)이라는 효율적이고 다용도의 접근법을 소개함(we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks.)",
            "조건 및 목표 프레임을 시간 축에 따라 결합하고 점진적으로 증가하는 노이즈 수준의 중간 버퍼 프레임을 삽입함(Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels.)",
            "TIC-FT는 구조적 변화 없이도 몇 개의 샘플로 강력한 성능을 발휘함(TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples.)"
        ],
        "conclusion": "TIC-FT는 이미지-비디오 및 비디오-비디오 생성 작업에서 기존 방법보다 조건 충실도와 시각 품질 모두에서 우수한 성능을 보여주며, 훈련과 추론 과정에서도 높은 효율성을 유지함.",
        "keywords": [
            "Video Generation",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    }
]