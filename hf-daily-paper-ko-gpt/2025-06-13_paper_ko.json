[
    {
        "paper": {
            "id": "2506.09513",
            "authors": [
                {
                    "_id": "684b8dbd3b733ba33368701b",
                    "user": {
                        "_id": "6723079ad1306fe9c76a1d29",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
                        "isPro": false,
                        "fullname": "Yu Sun",
                        "user": "YuSun-AI",
                        "type": "user"
                    },
                    "name": "Yu Sun",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-13T02:32:30.652Z",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba33368701c",
                    "name": "Xingyu Qian",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba33368701d",
                    "name": "Weiwen Xu",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba33368701e",
                    "user": {
                        "_id": "64b7cd74ff6d81ae297feded",
                        "avatarUrl": "/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg",
                        "isPro": false,
                        "fullname": "ZHANG HAO",
                        "user": "26hzhang",
                        "type": "user"
                    },
                    "name": "Hao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:43.056Z",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba33368701f",
                    "name": "Chenghao Xiao",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba333687020",
                    "name": "Long Li",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba333687021",
                    "user": {
                        "_id": "642eecbf9b2484d7d8526781",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642eecbf9b2484d7d8526781/4IvGbd66s49Wx5pZyZGHA.png",
                        "isPro": false,
                        "fullname": "Yu Rong",
                        "user": "Swrooy",
                        "type": "user"
                    },
                    "name": "Yu Rong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:40.908Z",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba333687022",
                    "name": "Wenbing Huang",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba333687023",
                    "name": "Qifeng Bai",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba333687024",
                    "name": "Tingyang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T08:36:55.000Z",
            "submittedOnDailyAt": "2025-06-13T01:06:46.741Z",
            "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "6723079ad1306fe9c76a1d29",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
                "isPro": false,
                "fullname": "Yu Sun",
                "user": "YuSun-AI",
                "type": "user"
            },
            "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a multi-agent\nverification and refinement process, where we design an Error Refiner\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
            "upvotes": 55,
            "discussionId": "684b8dbe3b733ba333687025",
            "githubRepo": "https://github.com/YuSun-Work/ReasonMed",
            "ai_summary": "ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.",
            "ai_keywords": [
                "reasoning-based large language models",
                "LLMs",
                "medical question answering",
                "ReasonMed",
                "multi-agent verification",
                "Error Refiner",
                "Chain-of-Thought",
                "CoT reasoning",
                "ReasonMed-7B",
                "PubMedQA"
            ]
        },
        "translation_title": "ReasonMed: 의료 추론 발전을 위한 37만 다중 에이전트 생성 데이터셋",
        "purpose": "지식 집약적인 의료 질문 응답 능력을 개선하기 위한 데이터셋과 최적의 모델 훈련 전략 연구",
        "method": [
            "다양한 LLM에서 생성된 170만 초기 추론 경로에서 37만 고품질 예제를 지닌 ReasonMed 데이터셋을 구축함(we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs.)",
            "다중 에이전트 검증 및 정제 과정을 통해 오류를 식별하고 수정하는 Error Refiner를 설계함(ReasonMed is constructed through a multi-agent verification and refinement process, where we design an Error Refiner to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier.)",
            "자세한 Chain-of-Thought (CoT) 추론과 간결한 답변 요약을 결합하여 가장 효과적인 파인튜닝 전략을 발견함(we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy.)"
        ],
        "conclusion": "ReasonMed-7B를 훈련시켜 서브 10B 모델의 새로운 기준을 설정하였으며, 기존 최고 성능을 4.17% 초과하고 PubMedQA에서 LLaMA3.1-70B를 4.60% 초과함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.10954",
            "authors": [
                {
                    "_id": "684b7ea83b733ba333686f8a",
                    "name": "Lianghong Guo",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f8b",
                    "name": "Yanlin Wang",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f8c",
                    "name": "Caihua Li",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f8d",
                    "name": "Pengyu Yang",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f8e",
                    "name": "Jiachi Chen",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f8f",
                    "user": {
                        "_id": "6355473d525beaee688b7ba1",
                        "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
                        "isPro": false,
                        "fullname": "Wei Tao",
                        "user": "itaowe",
                        "type": "user"
                    },
                    "name": "Wei Tao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:58.091Z",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f90",
                    "name": "Yingtian Zou",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f91",
                    "name": "Duyu Tang",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f92",
                    "name": "Zibin Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T17:54:17.000Z",
            "submittedOnDailyAt": "2025-06-13T00:07:20.052Z",
            "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks",
            "submittedOnDailyBy": {
                "_id": "6355473d525beaee688b7ba1",
                "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
                "isPro": false,
                "fullname": "Wei Tao",
                "user": "itaowe",
                "type": "user"
            },
            "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of 0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.",
            "upvotes": 35,
            "discussionId": "684b7ea83b733ba333686f93",
            "githubRepo": "https://github.com/DeepSoftwareAnalytics/swe-factory",
            "ai_summary": "A pipeline named SWE-Factory automates the creation and validation of GitHub issue resolution datasets for training and evaluating Large Language Models, using SWE-Builder for environment setup, exit-code-based grading, and automated fail2pass validation.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "SWE-Factory",
                "SWE-Builder",
                "multi-agent system",
                "environment memory pool",
                "exit-code-based grading",
                "automated fail2pass validation",
                "GPT-4.1-mini",
                "Gemini-2.5-flash",
                "precision",
                "recall"
            ]
        },
        "translation_title": "SWE-Factory: 문제 해결 훈련 데이터 및 평가 벤치마크를 위한 자동화 공장",
        "purpose": "소프트웨어 공학 능력을 평가하기 위한 대규모 GitHub 문제 해결 데이터 세트의 자동 생성 및 관리 개선",
        "method": [
            "SWE-Builder라는 다중 에이전트 시스템을 도입하여 평가 환경 구축 자동화(First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction.)",
            "표준화된 exit-code 기반 채점 방법을 사용하여 수동 파서 작성을 제거함(Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers.)",
            "신뢰할 수 있는 exit 코드 신호를 사용해 fail2pass 검증 프로세스를 자동화함(Finally, we automate the fail2pass validation process using these reliable exit code signals.)"
        ],
        "conclusion": "SWE-Factory는 유효한 작업 인스턴스를 효과적으로 생성하고, 채점 정확도를 100%로 달성하며, 검증에서도 높은 성과를 보임.",
        "keywords": [
            "Large Language Models",
            "Document Parsing",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2506.09993",
            "authors": [
                {
                    "_id": "684ae204dbd21a9cc27b0fba",
                    "user": {
                        "_id": "66012e9c9e1cf5eb41ee0c4c",
                        "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
                        "isPro": false,
                        "fullname": "Jaewon Min",
                        "user": "Min-Jaewon",
                        "type": "user"
                    },
                    "name": "Jaewon Min",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:41:25.024Z",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fbb",
                    "user": {
                        "_id": "65ec3449a69aaabb431db0da",
                        "avatarUrl": "/avatars/d7b507be0175a61a8fc21176eea45001.svg",
                        "isPro": false,
                        "fullname": "Jin Hyeon Kim",
                        "user": "jinlovespho",
                        "type": "user"
                    },
                    "name": "Jin Hyeon Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:41:22.776Z",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fbc",
                    "user": {
                        "_id": "6752b6315281c3cae4b0783f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xmcyVEl2xBhk3G5_7dmpz.png",
                        "isPro": false,
                        "fullname": "Paul Hyunbin Cho",
                        "user": "paulcho98",
                        "type": "user"
                    },
                    "name": "Paul Hyunbin Cho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:41:20.327Z",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fbd",
                    "user": {
                        "_id": "644be3e922d211df644416e9",
                        "avatarUrl": "/avatars/6bebbda7a7c16992eba64dc489eaeca5.svg",
                        "isPro": false,
                        "fullname": "Jaeeun Lee",
                        "user": "alicia10",
                        "type": "user"
                    },
                    "name": "Jaeeun Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T14:59:38.559Z",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fbe",
                    "name": "Jihye Park",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fbf",
                    "name": "Minkyu Park",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fc0",
                    "name": "Sangpil Kim",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fc1",
                    "name": "Hyunhee Park",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fc2",
                    "name": "Seungryong Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:59:46.000Z",
            "submittedOnDailyAt": "2025-06-13T00:32:01.285Z",
            "title": "Text-Aware Image Restoration with Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "66012e9c9e1cf5eb41ee0c4c",
                "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
                "isPro": false,
                "fullname": "Jaewon Min",
                "user": "Min-Jaewon",
                "type": "user"
            },
            "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
            "upvotes": 30,
            "discussionId": "684ae204dbd21a9cc27b0fc5",
            "projectPage": "https://cvlab-kaist.github.io/TAIR/",
            "githubRepo": "https://github.com/cvlab-kaist/TAIR",
            "ai_summary": "The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.",
            "ai_keywords": [
                "diffusion-based restoration",
                "text-image hallucination",
                "Text-Aware Image Restoration (TAIR)",
                "SA-Text",
                "multi-task diffusion framework",
                "TeReDiff",
                "text-spotting module",
                "text recognition accuracy"
            ]
        },
        "translation_title": "텍스트 인식에 기반한 이미지 복원: 확산 모델을 활용한 접근",
        "purpose": "이미지 복원에서 시각적 내용과 텍스트의 신뢰성을 동시에 회복하기 위한 새로운 접근법 연구",
        "method": [
            "100K개의 고품질 씬 이미지로 구성된 SA-Text라는 대규모 벤치마크를 제공함(Furthermore, we propose SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances.)",
            "TeReDiff라는 다중 작업 확산 프레임워크를 제안하며, 이를 통해 내부 특징을 텍스트 탐지 모듈과 통합하여 공동 학습의 장점을 활용함(Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training.)",
            "복원 과정에서 추출한 텍스트 표현을 후속 노이즈 제거 단계의 프롬프트로 활용함(This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps.)"
        ],
        "conclusion": "본 접근법은 기존 복원 방법보다 일관되게 우수한 성능을 보여주었고, 텍스트 인식 정확도에서 큰 향상을 이루어냈음.",
        "keywords": [
            "Image Restoration",
            "Multimodal Learning",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2506.10857",
            "authors": [
                {
                    "_id": "684b817e3b733ba333686f95",
                    "user": {
                        "_id": "64b89a14cf14c2fabe96664c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
                        "isPro": false,
                        "fullname": "Jiashuo Yu",
                        "user": "awojustin",
                        "type": "user"
                    },
                    "name": "Jiashuo Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:55.618Z",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f96",
                    "name": "Yue Wu",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f97",
                    "name": "Meng Chu",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f98",
                    "name": "Zhifei Ren",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f99",
                    "name": "Zizheng Huang",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f9a",
                    "user": {
                        "_id": "64c9beb2904317f42de06dd8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c9beb2904317f42de06dd8/he3rxfyzfwEd1vLuK6_o2.jpeg",
                        "isPro": false,
                        "fullname": "Pei Chu",
                        "user": "chupei",
                        "type": "user"
                    },
                    "name": "Pei Chu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:51.884Z",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f9b",
                    "name": "Ruijie Zhang",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f9c",
                    "name": "Yinan He",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f9d",
                    "name": "Qirui Li",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f9e",
                    "user": {
                        "_id": "64acbbd51aee69ece03c6c0c",
                        "avatarUrl": "/avatars/604df1cabc5faeda55022ae4c1997e56.svg",
                        "isPro": false,
                        "fullname": "Songze Li",
                        "user": "LarryLee",
                        "type": "user"
                    },
                    "name": "Songze Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:53.776Z",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f9f",
                    "name": "Zhenxiang Li",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686fa0",
                    "name": "Zhongying Tu",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686fa1",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686fa2",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686fa3",
                    "name": "Yali Wang",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686fa4",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686fa5",
                    "name": "Limin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T16:17:17.000Z",
            "submittedOnDailyAt": "2025-06-13T00:10:47.082Z",
            "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
            "submittedOnDailyBy": {
                "_id": "64b89a14cf14c2fabe96664c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
                "isPro": false,
                "fullname": "Jiashuo Yu",
                "user": "awojustin",
                "type": "user"
            },
            "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.",
            "upvotes": 27,
            "discussionId": "684b817e3b733ba333686fa6",
            "projectPage": "https://vrbench.github.io/",
            "githubRepo": "https://github.com/OpenGVLab/VRBench",
            "ai_summary": "VRBench is a long narrative video benchmark designed to evaluate models' multi-step reasoning and procedural validity through human-labeled question-answering pairs and a human-AI collaborative framework with a multi-phase evaluation pipeline.",
            "ai_keywords": [
                "VRBench",
                "multi-step reasoning",
                "temporal reasoning",
                "procedural validity",
                "long videos",
                "human-labeled",
                "multi-step question-answering",
                "expert inter-rater reviewing",
                "coherent reasoning chains",
                "event attribution",
                "implicit inference",
                "multi-phase evaluation",
                "progress-level LLM-guided scoring metric",
                "LLMs",
                "VLMs"
            ]
        },
        "translation_title": "VRBench: 긴 서사 영상을 위한 다단계 추론 벤치마크",
        "purpose": "대형 모델의 다단계 추론 능력을 평가하기 위한 긴 서사 영상 벤치마크 구축",
        "method": [
            "1,010개의 긴 영상과 9,468개의 인간 레이블링 다단계 질문-답변 쌍을 포함(It comprises 1,010 long videos along with 9,468 human-labeled multi-step question-answering pairs.)",
            "영상을 전문가의 리뷰를 통해 필터링하여 플롯 일관성을 우선시함(These videos are curated via a multi-stage filtering process including expert inter-rater reviewing to prioritize plot coherence.)",
            "다양한 추론 유형을 요구하는 인간-AI 협업 프레임워크를 개발함(We develop a human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps.)",
            "최종 결과를 위한 MCQ 외에도 다차원적으로 추론 체인의 질을 평가하기 위한 LLM 기반 점수 산출 지표를 제안함(Apart from the MCQs for the final results, we propose a progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively.)"
        ],
        "conclusion": "VRBench를 활용한 12개의 LLM과 16개의 VLM에 대한 포괄적인 평가를 통해 다단계 추론 분야의 발전에 기여할 수 있는 귀중한 통찰을 제공함.",
        "keywords": [
            "Video Understanding",
            "Natural Language Processing",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2506.10540",
            "authors": [
                {
                    "_id": "684bad683b733ba3336870b6",
                    "user": {
                        "_id": "652fb8bcc9dd2692a25ef2e3",
                        "avatarUrl": "/avatars/461e6cc1c3441cde18192b080b0b8576.svg",
                        "isPro": false,
                        "fullname": "Haoyuan Shi",
                        "user": "MrSunshy",
                        "type": "user"
                    },
                    "name": "Haoyuan Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:38:52.713Z",
                    "hidden": false
                },
                {
                    "_id": "684bad683b733ba3336870b7",
                    "user": {
                        "_id": "62fdb01bc1588e1d4c6c1a7c",
                        "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
                        "isPro": false,
                        "fullname": "Yunxin Li",
                        "user": "YunxinLi",
                        "type": "user"
                    },
                    "name": "Yunxin Li",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-13T04:47:39.539Z",
                    "hidden": false
                },
                {
                    "_id": "684bad683b733ba3336870b8",
                    "name": "Xinyu Chen",
                    "hidden": false
                },
                {
                    "_id": "684bad683b733ba3336870b9",
                    "name": "Longyue Wang",
                    "hidden": false
                },
                {
                    "_id": "684bad683b733ba3336870ba",
                    "name": "Baotian Hu",
                    "hidden": false
                },
                {
                    "_id": "684bad683b733ba3336870bb",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T10:06:21.000Z",
            "submittedOnDailyAt": "2025-06-13T03:26:17.710Z",
            "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
            "submittedOnDailyBy": {
                "_id": "62fdb01bc1588e1d4c6c1a7c",
                "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
                "isPro": false,
                "fullname": "Yunxin Li",
                "user": "YunxinLi",
                "type": "user"
            },
            "summary": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.",
            "upvotes": 25,
            "discussionId": "684bad683b733ba3336870bc",
            "ai_summary": "AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.",
            "ai_keywords": [
                "multi-agent framework",
                "Director Agent",
                "Photography Agent",
                "Reviewer Agent",
                "Post-Production Agent",
                "Monte Carlo Tree Search (MCTS)",
                "AniEval",
                "VBench",
                "action completion",
                "story-level consistency",
                "animation-specific features"
            ]
        },
        "translation_title": "AniMaker: MCTS 기반 자동화 다중 에이전트 애니메이티드 스토리텔링",
        "purpose": "일관된 스토리텔링 비디오를 생성하고 다중 장면 및 캐릭터를 효과적으로 활용하기 위한 시스템 개발",
        "method": [
            "여러 에이전트를 활용해 스토리 보드 생성, 비디오 클립 생성, 평가 및 편집 등 다양한 단계를 담당하는 AniMaker 프레임워크를 구축함(We introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection.)",
            "MCTS-Gen이라는 상반된 생성 전략을 적용해 고품질 클립을 효율적으로 생성함(Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy.).",
            "AniEval를 통해 각 클립의 중요 요소를 평가하며, 스토리 작성의 일관성을 확인함(AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, assesses critical aspects such as story-level consistency.)"
        ],
        "conclusion": "AniMaker는 비디오 생성의 품질을 현저히 향상시키며, AI 기반 스토리텔링 애니메이션을 생산 기준에 가깝게 발전시킴.",
        "keywords": [
            "Video Generation",
            "Image Generation",
            "Natural Language Processing"
        ]
    }
]