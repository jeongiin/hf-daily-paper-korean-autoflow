[
    {
        "paper": {
            "id": "2510.26697",
            "authors": [
                {
                    "_id": "690466b52c556835fa67f16e",
                    "user": {
                        "_id": "6645e248b03e00c6a0f61b88",
                        "avatarUrl": "/avatars/97af419819044ff90e1937916d6377b1.svg",
                        "isPro": false,
                        "fullname": "zhichaowang",
                        "user": "Jadeislaw",
                        "type": "user"
                    },
                    "name": "Zhichao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-31T14:26:18.863Z",
                    "hidden": false
                },
                {
                    "_id": "690466b52c556835fa67f16f",
                    "user": {
                        "_id": "6734a0fe3ed65dd196e40cfa",
                        "avatarUrl": "/avatars/dfa0f7e8d53ff611d3af51d3c10adac6.svg",
                        "isPro": false,
                        "fullname": "madongyang",
                        "user": "ldsjmdy",
                        "type": "user"
                    },
                    "name": "Dongyang Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-31T14:26:15.874Z",
                    "hidden": false
                },
                {
                    "_id": "690466b52c556835fa67f170",
                    "name": "Xinting Huang",
                    "hidden": false
                },
                {
                    "_id": "690466b52c556835fa67f171",
                    "name": "Deng Cai",
                    "hidden": false
                },
                {
                    "_id": "690466b52c556835fa67f172",
                    "user": {
                        "_id": "627ca439a09edfe62239c671",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1652335660508-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Tian Lan",
                        "user": "GMFTBY",
                        "type": "user"
                    },
                    "name": "Tian Lan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-31T14:26:21.582Z",
                    "hidden": false
                },
                {
                    "_id": "690466b52c556835fa67f173",
                    "name": "Jiahao Xu",
                    "hidden": false
                },
                {
                    "_id": "690466b52c556835fa67f174",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "690466b52c556835fa67f175",
                    "name": "Xiaoying Tang",
                    "hidden": false
                },
                {
                    "_id": "690466b52c556835fa67f176",
                    "name": "Yan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-30T17:01:43.000Z",
            "submittedOnDailyAt": "2025-10-31T11:19:13.159Z",
            "title": "The End of Manual Decoding: Towards Truly End-to-End Language Models",
            "submittedOnDailyBy": {
                "_id": "627ca439a09edfe62239c671",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1652335660508-noauth.jpeg",
                "isPro": false,
                "fullname": "Tian Lan",
                "user": "GMFTBY",
                "type": "user"
            },
            "summary": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.",
            "upvotes": 55,
            "discussionId": "690466b52c556835fa67f177",
            "githubRepo": "https://github.com/Zacks917/AutoDeco",
            "githubStars": 12,
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "translation_title": "수동 디코딩의 종말: 진정한 엔드투엔드 언어 모델을 향하여",
        "purpose": "LLMs의 디코딩 과정을 자동화하고 엔드투엔드 생성 방식의 진정한 구현을 목표로 함.",
        "method": [
            "AutoDeco라는 새로운 아키텍처를 소개하여 디코딩 전략을 스스로 제어하도록 학습하게 함(This paper introduces AutoDeco, a novel architecture that enables truly 'end-to-end' generation by learning to control its own decoding strategy.)",
            "표준 transformer에 경량 헤드를 추가하여 각 단계에서 상황에 맞는 temperature와 top-p 값을 예측하도록 함(We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits.)",
            "AutoDeco를 사용하여 디코딩을 매개변수화된 토큰 수준의 과정으로 변환하여 단일 전방 패스를 통해 샘플링 전략을 자기 조정할 수 있도록 함(This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.)"
        ],
        "conclusion": "AutoDeco는 기존의 디코딩 전략보다 성능이 뛰어나며, 새로운 지시 기반 디코딩 제어 능력을 발견하여 자연어 명령을 해석하고 온도와 top-p를 조정하는 새로운 패러다임을 제시함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2510.26583",
            "authors": [
                {
                    "_id": "690414cc2c556835fa67efb5",
                    "user": {
                        "_id": "648683de623b5f050213f2be",
                        "avatarUrl": "/avatars/83ecbcf4a21f68d2893de79f0444d6e3.svg",
                        "isPro": false,
                        "fullname": "Yufeng Cui",
                        "user": "YufengCui",
                        "type": "user"
                    },
                    "name": "Yufeng Cui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-31T14:27:32.859Z",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efb6",
                    "name": "Honghao Chen",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efb7",
                    "name": "Haoge Deng",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efb8",
                    "name": "Xu Huang",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efb9",
                    "name": "Xinghang Li",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efba",
                    "name": "Jirong Liu",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efbb",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efbc",
                    "name": "Zhuoyan Luo",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efbd",
                    "name": "Jinsheng Wang",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efbe",
                    "user": {
                        "_id": "656428b5462e5ebcbf537d4e",
                        "avatarUrl": "/avatars/cbedecc9c6f2afee2ca6b72efb593561.svg",
                        "isPro": false,
                        "fullname": "Wenxuan Wang",
                        "user": "Rookielion",
                        "type": "user"
                    },
                    "name": "Wenxuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-31T14:27:30.411Z",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efbf",
                    "name": "Yueze Wang",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efc0",
                    "name": "Chengyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efc1",
                    "name": "Fan Zhang",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efc2",
                    "name": "Yingli Zhao",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efc3",
                    "user": {
                        "_id": "6565bc5ee5aac326bfc98e39",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vIfHy9Y1yAK6A96UCHNBH.jpeg",
                        "isPro": false,
                        "fullname": "Ting Pan",
                        "user": "PhyscalX",
                        "type": "user"
                    },
                    "name": "Ting Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-31T14:27:37.508Z",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efc4",
                    "name": "Xianduo Li",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efc5",
                    "name": "Zecheng Hao",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efc6",
                    "name": "Wenxuan Ma",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efc7",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efc8",
                    "name": "Yulong Ao",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efc9",
                    "name": "Tiejun Huang",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efca",
                    "name": "Zhongyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "690414cc2c556835fa67efcb",
                    "user": {
                        "_id": "63ca558304c979828311c5a5",
                        "avatarUrl": "/avatars/2a439d79fba2f987cabe780d10c94d25.svg",
                        "isPro": false,
                        "fullname": "Xinlong Wang",
                        "user": "xinlongwang",
                        "type": "user"
                    },
                    "name": "Xinlong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-31T14:27:35.127Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-30T15:11:16.000Z",
            "submittedOnDailyAt": "2025-10-31T00:20:30.779Z",
            "title": "Emu3.5: Native Multimodal Models are World Learners",
            "submittedOnDailyBy": {
                "_id": "63ca558304c979828311c5a5",
                "avatarUrl": "/avatars/2a439d79fba2f987cabe780d10c94d25.svg",
                "isPro": false,
                "fullname": "Xinlong Wang",
                "user": "xinlongwang",
                "type": "user"
            },
            "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.",
            "upvotes": 53,
            "discussionId": "690414cc2c556835fa67efcc",
            "projectPage": "https://emu.world/",
            "githubRepo": "https://github.com/baaivision/Emu3.5",
            "githubStars": 697,
            "organization": {
                "_id": "61be9739d2f9358e24ca0a4f",
                "name": "BAAI",
                "fullname": "Beijing Academy of Artificial Intelligence",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
            }
        },
        "translation_title": "Emu3.5: 네이티브 멀티모달 모델들이 세계 학습자입니다.",
        "purpose": "비전과 언어를 아우르는 다음 상태 예측을 위한 대규모 멀티모달 모델 개발",
        "method": [
            "Emu3.5를 비전-언어가 섞인 데이터로 다음 토큰 예측의 통합 목표로 end-to-end로 사전 훈련함(Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data...).",
            "모델에 대한 대규모 강화 학습 후 훈련을 진행하여 멀티모달 추론 및 생성을 향상함(Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation.)",
            "Discrete Diffusion Adaptation (DiDA) 기법을 사용하여 예측 효율성을 개선하고 이미지 당 추론 속도를 20배 가속함(To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA)...)."
        ],
        "conclusion": "Emu3.5는 다양한 시나리오와 작업에서의 일관된 세계 탐색 및 조작이 가능한 강력한 멀티모달 능력을 보이며, 커뮤니티 연구를 지원하기 위해 오픈소스로 제공됨.",
        "keywords": [
            "Multimodal Learning",
            "Image Generation",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2510.15510",
            "authors": [
                {
                    "_id": "68feef866cdff8b857f471d1",
                    "user": {
                        "_id": "66f989359ebb4cf074a1f9f7",
                        "avatarUrl": "/avatars/d0e899f579e0897a849e79688bf19c8a.svg",
                        "isPro": false,
                        "fullname": "Heeseong Shin",
                        "user": "hsshin98",
                        "type": "user"
                    },
                    "name": "Heeseong Shin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-28T15:38:00.777Z",
                    "hidden": false
                },
                {
                    "_id": "68feef866cdff8b857f471d2",
                    "user": {
                        "_id": "64b9feed96676e40d0fa89a7",
                        "avatarUrl": "/avatars/2154a6ceb87677ad2c9d9620de5b18ec.svg",
                        "isPro": false,
                        "fullname": "Byeongho Heo",
                        "user": "bhheo",
                        "type": "user"
                    },
                    "name": "Byeongho Heo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-31T14:28:34.237Z",
                    "hidden": false
                },
                {
                    "_id": "68feef866cdff8b857f471d3",
                    "name": "Dongyoon Han",
                    "hidden": false
                },
                {
                    "_id": "68feef866cdff8b857f471d4",
                    "name": "Seungryong Kim",
                    "hidden": false
                },
                {
                    "_id": "68feef866cdff8b857f471d5",
                    "user": {
                        "_id": "67c6a1e75e2443d7d5f85cb3",
                        "avatarUrl": "/avatars/0569b368520411ab828d46725bc3896a.svg",
                        "isPro": false,
                        "fullname": "Taekyung Kim",
                        "user": "taekyung-k",
                        "type": "user"
                    },
                    "name": "Taekyung Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-27T10:25:08.822Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T10:24:14.000Z",
            "submittedOnDailyAt": "2025-10-31T04:40:23.632Z",
            "title": "Exploring Conditions for Diffusion models in Robotic Control",
            "submittedOnDailyBy": {
                "_id": "66f989359ebb4cf074a1f9f7",
                "avatarUrl": "/avatars/d0e899f579e0897a849e79688bf19c8a.svg",
                "isPro": false,
                "fullname": "Heeseong Shin",
                "user": "hsshin98",
                "type": "user"
            },
            "summary": "While pre-trained visual representations have significantly advanced\nimitation learning, they are often task-agnostic as they remain frozen during\npolicy learning. In this work, we explore leveraging pre-trained text-to-image\ndiffusion models to obtain task-adaptive visual representations for robotic\ncontrol, without fine-tuning the model itself. However, we find that naively\napplying textual conditions - a successful strategy in other vision domains -\nyields minimal or even negative gains in control tasks. We attribute this to\nthe domain gap between the diffusion model's training data and robotic control\nenvironments, leading us to argue for conditions that consider the specific,\ndynamic visual information required for control. To this end, we propose ORCA,\nwhich introduces learnable task prompts that adapt to the control environment\nand visual prompts that capture fine-grained, frame-specific details. Through\nfacilitating task-adaptive representations with our newly devised conditions,\nour approach achieves state-of-the-art performance on various robotic control\nbenchmarks, significantly surpassing prior methods.",
            "upvotes": 35,
            "discussionId": "68feef866cdff8b857f471d6",
            "projectPage": "https://orca-rc.github.io/",
            "ai_summary": "ORCA uses learnable task and visual prompts to adapt pre-trained text-to-image diffusion models for robotic control, achieving state-of-the-art performance on benchmarks.",
            "ai_keywords": [
                "text-to-image diffusion models",
                "task-adaptive visual representations",
                "robotic control",
                "domain gap",
                "learnable task prompts",
                "visual prompts",
                "frame-specific details"
            ],
            "organization": {
                "_id": "64ffe603efd273eec7768bde",
                "name": "naver-ai",
                "fullname": "NAVER AI Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ff1755b75685dd7a46e146/Zj2bxgq31oSqwVrw16IE_.png"
            }
        },
        "translation_title": "로봇 제어를 위한 확산 모델 조건 탐색",
        "purpose": "로봇 제어에서 작업에 적합한 시각적 표현을 도출하기 위한 연구",
        "method": [
            "사전 훈련된 text-to-image 확산 모델을 활용하여 정책 학습 중에 미세 조정 없이 작업 적응형 시각적 표현을 얻는 방법을 탐색함(We explore leveraging pre-trained text-to-image diffusion models to obtain task-adaptive visual representations for robotic control, without fine-tuning the model itself.)",
            "기존의 비전 분야에서 성공적으로 사용된 텍스트 조건을 단순히 적용하기보다는 제어를 위한 특정 동적 시각 정보를 고려한 조건을 주장함(We attribute this to the domain gap between the diffusion model's training data and robotic control environments, leading us to argue for conditions that consider the specific, dynamic visual information required for control.)",
            "ORCA라는 방법을 제안하여 제어 환경에 적응하는 학습 가능한 작업 프롬프트와 세부적인 프레임 특정 정보를 캡처하는 시각적 프롬프트를 도입함(To this end, we propose ORCA, which introduces learnable task prompts that adapt to the control environment and visual prompts that capture fine-grained, frame-specific details.)"
        ],
        "conclusion": "제안된 조건을 통해 작업 적응형 표현을 용이하게 하여 다양한 로봇 제어 벤치마크에서 최첨단 성능을 달성하고, 이전 방법들을 크게 초월함.",
        "keywords": [
            "Robotics",
            "Image Understanding",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2510.26692",
            "authors": [
                {
                    "_id": "690414082c556835fa67ef77",
                    "name": "Kimi Team",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef78",
                    "name": "Yu Zhang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef79",
                    "name": "Zongyu Lin",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef7a",
                    "name": "Xingcheng Yao",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef7b",
                    "name": "Jiaxi Hu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef7c",
                    "name": "Fanqing Meng",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef7d",
                    "name": "Chengyin Liu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef7e",
                    "name": "Xin Men",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef7f",
                    "name": "Songlin Yang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef80",
                    "user": {
                        "_id": "65e98b74b8dbf436e6585106",
                        "avatarUrl": "/avatars/6e694cf5d11e437a80b3c4fffe1b959e.svg",
                        "isPro": false,
                        "fullname": "LiZhiyuan",
                        "user": "zhiyuan8",
                        "type": "user"
                    },
                    "name": "Zhiyuan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-31T14:27:40.234Z",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef81",
                    "name": "Wentao Li",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef82",
                    "name": "Enzhe Lu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef83",
                    "name": "Weizhou Liu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef84",
                    "name": "Yanru Chen",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef85",
                    "name": "Weixin Xu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef86",
                    "name": "Longhui Yu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef87",
                    "name": "Yejie Wang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef88",
                    "name": "Yu Fan",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef89",
                    "name": "Longguang Zhong",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef8a",
                    "name": "Enming Yuan",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef8b",
                    "name": "Dehao Zhang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef8c",
                    "name": "Yizhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef8d",
                    "name": "T. Y. Liu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef8e",
                    "name": "Haiming Wang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef8f",
                    "name": "Shengjun Fang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef90",
                    "name": "Weiran He",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef91",
                    "name": "Shaowei Liu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef92",
                    "name": "Yiwei Li",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef93",
                    "name": "Jianlin Su",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef94",
                    "name": "Jiezhong Qiu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef95",
                    "name": "Bo Pang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef96",
                    "name": "Junjie Yan",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef97",
                    "name": "Zhejun Jiang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef98",
                    "name": "Weixiao Huang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef99",
                    "name": "Bohong Yin",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef9a",
                    "name": "Jiacheng You",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef9b",
                    "name": "Chu Wei",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef9c",
                    "name": "Zhengtao Wang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef9d",
                    "name": "Chao Hong",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef9e",
                    "name": "Yutian Chen",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67ef9f",
                    "name": "Guanduo Chen",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efa0",
                    "name": "Yucheng Wang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efa1",
                    "name": "Huabin Zheng",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efa2",
                    "name": "Feng Wang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efa3",
                    "name": "Yibo Liu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efa4",
                    "name": "Mengnan Dong",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efa5",
                    "name": "Zheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efa6",
                    "name": "Siyuan Pan",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efa7",
                    "name": "Wenhao Wu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efa8",
                    "name": "Yuhao Wu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efa9",
                    "name": "Longyu Guan",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efaa",
                    "name": "Jiawen Tao",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efab",
                    "name": "Guohong Fu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efac",
                    "name": "Xinran Xu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efad",
                    "name": "Yuzhi Wang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efae",
                    "name": "Guokun Lai",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efaf",
                    "name": "Yuxin Wu",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efb0",
                    "name": "Xinyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efb1",
                    "name": "Zhilin Yang",
                    "hidden": false
                },
                {
                    "_id": "690414082c556835fa67efb2",
                    "name": "Yulun Du",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-30T16:59:43.000Z",
            "submittedOnDailyAt": "2025-10-31T00:12:50.143Z",
            "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
            "upvotes": 29,
            "discussionId": "690414092c556835fa67efb3",
            "githubRepo": "https://github.com/MoonshotAI/Kimi-Linear",
            "githubStars": 492,
            "organization": {
                "_id": "6425a114812813f8f4a9b02c",
                "name": "moonshotai",
                "fullname": "Moonshot AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"
            }
        },
        "translation_title": "Kimi Linear: 표현력 있고 효율적인 어텐션 아키텍처",
        "purpose": "Kimi Linear를 통해 다양한 시나리오에서 기존의 Full Attention을 능가하는 새로운 어텐션 아키텍처 개발",
        "method": [
            "Kimi Delta Attention(KDA)를 도입하여 Gated DeltaNet의 세분화된 게이팅 메커니즘을 구현함(At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism.)",
            "특수 목적의 청크 방식 알고리즘을 통해 높은 하드웨어 효율성을 달성함(Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices.)",
            "Kimi Linear 모델을 사전 훈련시키고, 모든 평가 작업에서 Full MLA를 능가함(Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks.)"
        ],
        "conclusion": "Kimi Linear는 뛰어난 성능과 효율성을 갖춘 어텐션 아키텍처의 대체재로서, 긴 입력 및 출력 길이가 필요한 작업에도 적합함.",
        "keywords": [
            "Natural Language Processing",
            "Multimodal Learning",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2510.26802",
            "authors": [
                {
                    "_id": "690417142c556835fa67f021",
                    "name": "Ziyu Guo",
                    "hidden": false
                },
                {
                    "_id": "690417142c556835fa67f022",
                    "name": "Xinyan Chen",
                    "hidden": false
                },
                {
                    "_id": "690417142c556835fa67f023",
                    "name": "Renrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "690417142c556835fa67f024",
                    "name": "Ruichuan An",
                    "hidden": false
                },
                {
                    "_id": "690417142c556835fa67f025",
                    "user": {
                        "_id": "661de604f8dcbd5a207c9012",
                        "avatarUrl": "/avatars/58f5689237dc33972703971642c8c8b1.svg",
                        "isPro": false,
                        "fullname": "yu",
                        "user": "yqi19",
                        "type": "user"
                    },
                    "name": "Yu Qi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-31T14:27:15.920Z",
                    "hidden": false
                },
                {
                    "_id": "690417142c556835fa67f026",
                    "user": {
                        "_id": "66e3fd2e25f8e85f9b3f2df2",
                        "avatarUrl": "/avatars/cf1ee71fad1dcf5fe872174936cdaec8.svg",
                        "isPro": false,
                        "fullname": "Liang YZ",
                        "user": "exptest2",
                        "type": "user"
                    },
                    "name": "Dongzhi Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-31T14:27:13.758Z",
                    "hidden": false
                },
                {
                    "_id": "690417142c556835fa67f027",
                    "name": "Xiangtai Li",
                    "hidden": false
                },
                {
                    "_id": "690417142c556835fa67f028",
                    "name": "Manyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "690417142c556835fa67f029",
                    "name": "Hongsheng Li",
                    "hidden": false
                },
                {
                    "_id": "690417142c556835fa67f02a",
                    "name": "Pheng-Ann Heng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-30T17:59:55.000Z",
            "submittedOnDailyAt": "2025-10-31T00:26:10.415Z",
            "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark",
            "submittedOnDailyBy": {
                "_id": "645b8b2687c79b6ec0bb3b7a",
                "avatarUrl": "/avatars/00a9db32a42dc950112bf2593bb109cb.svg",
                "isPro": false,
                "fullname": "Renrui",
                "user": "ZrrSkywalker",
                "type": "user"
            },
            "summary": "Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io",
            "upvotes": 27,
            "discussionId": "690417152c556835fa67f02b",
            "projectPage": "https://video-cof.github.io/",
            "githubRepo": "https://github.com/ZiyuGuo99/MME-CoF",
            "githubStars": 14,
            "organization": {
                "_id": "6390c6fdd00f25601f445cd4",
                "name": "CUHK-CSE",
                "fullname": "The Chinese University of Hong Kong",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/621f2eb36e152b56a7cf0248/o8RRAczRjfNEzq70GzUwQ.png"
            }
        },
        "translation_title": "비디오 모델은 제로샷 추론자 역할을 할 준비가 되었는가? MME-CoF 벤치마크를 통한 실증 연구",
        "purpose": "비디오 모델이 어려운 시각적 추론 시나리오에서 제로샷 추론자로서 준비가 되었는지를 평가하기 위한 연구",
        "method": [
            "주요 비디오 모델인 Veo-3의 추론 행동을 12개의 차원에서 평가함(We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic.)",
            "평가 데이터를 MME-CoF라는 컴팩트한 벤치마크로 구성하여 체계적인 평가를 가능하게 함(we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning.)",
            "현재 비디오 모델의 강점과 실패 양상을 체계적으로 분석함(we systematically characterizing both its strengths and failure modes.)"
        ],
        "conclusion": "현재 비디오 모델은 짧은 기간의 공간적 일관성, 세밀한 구체성 및 국소적 역학에서는 유망한 추론 패턴을 보이지만, 장기 인과 추론, 엄격한 기하학적 제약 및 추상적 논리에서는 한계가 있어 개별적으로 신뢰할 수는 없으나, 전용 추론 모델과 함께 보완적인 시각 엔진으로써 유망한 신호를 보임.",
        "keywords": [
            "Video Generation",
            "Video Understanding",
            "Multimodal Learning"
        ]
    }
]