[
    {
        "paper": {
            "id": "2508.02324",
            "authors": [
                {
                    "_id": "68919a57f01a094725f83598",
                    "name": "Chenfei Wu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f83599",
                    "name": "Jiahao Li",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f8359a",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f8359b",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f8359c",
                    "name": "Kaiyuan Gao",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f8359d",
                    "name": "Kun Yan",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f8359e",
                    "name": "Sheng-ming Yin",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f8359f",
                    "name": "Shuai Bai",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a0",
                    "name": "Xiao Xu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a1",
                    "name": "Yilei Chen",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a2",
                    "name": "Yuxiang Chen",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a3",
                    "name": "Zecheng Tang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a4",
                    "name": "Zekai Zhang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a5",
                    "name": "Zhengyi Wang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a6",
                    "name": "An Yang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a7",
                    "name": "Bowen Yu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a8",
                    "name": "Chen Cheng",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a9",
                    "name": "Dayiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835aa",
                    "name": "Deqing Li",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835ab",
                    "name": "Hang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835ac",
                    "name": "Hao Meng",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835ad",
                    "name": "Hu Wei",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835ae",
                    "name": "Jingyuan Ni",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835af",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b0",
                    "name": "Kuan Cao",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b1",
                    "name": "Liang Peng",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b2",
                    "name": "Lin Qu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b3",
                    "name": "Minggang Wu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b4",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b5",
                    "name": "Shuting Yu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b6",
                    "name": "Tingkun Wen",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b7",
                    "name": "Wensen Feng",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b8",
                    "name": "Xiaoxiao Xu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b9",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835ba",
                    "name": "Yichang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835bb",
                    "name": "Yongqiang Zhu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835bc",
                    "name": "Yujia Wu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835bd",
                    "name": "Yuxuan Cai",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835be",
                    "name": "Zenan Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/616fb788e2ad27af26561b1a/IGGtaa2qHZfwfhjAJfV-Q.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/616fb788e2ad27af26561b1a/wBudoV6xlkMnSdkPjkC4C.png"
            ],
            "publishedAt": "2025-08-04T11:49:20.000Z",
            "submittedOnDailyAt": "2025-08-05T04:39:37.929Z",
            "title": "Qwen-Image Technical Report",
            "submittedOnDailyBy": {
                "_id": "616fb788e2ad27af26561b1a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675485317568-616fb788e2ad27af26561b1a.jpeg",
                "isPro": false,
                "fullname": "Xiao Xu",
                "user": "LooperXX",
                "type": "user"
            },
            "summary": "We present Qwen-Image, an image generation foundation model in the Qwen\nseries that achieves significant advances in complex text rendering and precise\nimage editing. To address the challenges of complex text rendering, we design a\ncomprehensive data pipeline that includes large-scale data collection,\nfiltering, annotation, synthesis, and balancing. Moreover, we adopt a\nprogressive training strategy that starts with non-text-to-text rendering,\nevolves from simple to complex textual inputs, and gradually scales up to\nparagraph-level descriptions. This curriculum learning approach substantially\nenhances the model's native text rendering capabilities. As a result,\nQwen-Image not only performs exceptionally well in alphabetic languages such as\nEnglish, but also achieves remarkable progress on more challenging logographic\nlanguages like Chinese. To enhance image editing consistency, we introduce an\nimproved multi-task training paradigm that incorporates not only traditional\ntext-to-image (T2I) and text-image-to-image (TI2I) tasks but also\nimage-to-image (I2I) reconstruction, effectively aligning the latent\nrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed\nthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic and\nreconstructive representations, respectively. This dual-encoding mechanism\nenables the editing module to strike a balance between preserving semantic\nconsistency and maintaining visual fidelity. Qwen-Image achieves\nstate-of-the-art performance, demonstrating its strong capabilities in both\nimage generation and editing across multiple benchmarks.",
            "upvotes": 63,
            "discussionId": "68919a57f01a094725f835bf",
            "githubRepo": "https://github.com/QwenLM/Qwen-Image",
            "ai_summary": "Qwen-Image, an image generation model, advances text rendering and image editing through a comprehensive data pipeline, progressive training, and dual-encoding mechanism.",
            "ai_keywords": [
                "data pipeline",
                "progressive training",
                "curriculum learning",
                "text-to-image",
                "text-image-to-image",
                "image-to-image",
                "latent representations",
                "dual-encoding mechanism",
                "semantic consistency",
                "visual fidelity"
            ],
            "githubStars": 1373
        },
        "translation_title": "Qwen-Image 기술 보고서",
        "purpose": "복잡한 텍스트 렌더링과 정밀한 이미지 편집에서 significativa한 발전을 이루기 위한 이미지 생성 기초 모델 연구",
        "method": [
            "대규모 데이터 수집, 필터링, 주석, 합성 및 균형을 포함하는 종합적인 데이터 파이프라인 설계(We design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing.)",
            "점진적 훈련 전략을 채택하여 간단한 텍스트부터 복잡한 텍스트 입력으로 발전시키도록 함(Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions.)",
            "기본 이미지 및 VAE 인코더에 원본 이미지를 별도로 입력하여 의미적 및 재구성 표현을 얻음(Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively.)"
        ],
        "conclusion": "Qwen-Image는 이미지 생성 및 편집에서 최첨단 성능을 달성하며 다양한 기준에서 뛰어난 능력을 입증함.",
        "keywords": [
            "Image Generation",
            "Image Editing",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2508.01959",
            "authors": [
                {
                    "_id": "68919172f01a094725f8355d",
                    "name": "Junjie Wu",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f8355e",
                    "name": "Jiangnan Li",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f8355f",
                    "name": "Yuqing Li",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f83560",
                    "name": "Lemao Liu",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f83561",
                    "name": "Liyan Xu",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f83562",
                    "name": "Jiwei Li",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f83563",
                    "name": "Dit-Yan Yeung",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f83564",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f83565",
                    "name": "Mo Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-03T23:59:31.000Z",
            "submittedOnDailyAt": "2025-08-05T03:38:04.443Z",
            "title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic\n  Association and Long Story Comprehension",
            "submittedOnDailyBy": {
                "_id": "64d36ca3036ae0b3756588e3",
                "avatarUrl": "/avatars/0d7dfbd681b1157a38d0f0a86f19b702.svg",
                "isPro": false,
                "fullname": "Junjie Wu",
                "user": "junjiewu",
                "type": "user"
            },
            "summary": "Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications.",
            "upvotes": 34,
            "discussionId": "68919173f01a094725f83566",
            "projectPage": "https://huggingface.co/SituatedEmbedding",
            "ai_summary": "A new training paradigm and situated embedding models (SitEmb) enhance retrieval performance by conditioning short text chunks on broader context windows, outperforming state-of-the-art models with fewer parameters.",
            "ai_keywords": [
                "Retrieval-augmented generation (RAG)",
                "context window",
                "embedding models",
                "situated context",
                "situated embedding models (SitEmb)",
                "BGE-M3",
                "downstream applications"
            ]
        },
        "translation_title": "SitEmb-v1.5: 의미적 연관성과 긴 이야기 이해를 위한 개선된 맥락 인식 밀집 검색",
        "purpose": "긴 문서에서 효과적인 검색 성능을 향상시키는 방법을 연구",
        "method": [
            "짧은 조각을 더 넓은 맥락 창에 맞추어 표현하여 검색 성능을 높임 (We propose an alternative approach to this challenge by representing short chunks in a way that is conditioned on a broader context window to enhance retrieval performance.)",
            "새로운 훈련 패러다임을 도입해 SitEmb라는 맥락 밀집 임베딩 모델을 개발함 (we introduce a new training paradigm and develop the situated embedding models (SitEmb).)",
            "특정 검색 능력을 평가하기 위해 책 줄거리 검색 데이터 세트를 수집함 (To evaluate our method, we curate a book-plot retrieval dataset specifically designed to assess situated retrieval capabilities.)"
        ],
        "conclusion": "SitEmb-v1 모델은 기존의 강력한 임베딩 모델들을 능가하며, SitEmb-v1.5 모델은 성능을 10% 이상 향상시켜 다양한 언어와 여러 하위 응용 프로그램에서 좋은 결과를 보임.",
        "keywords": [
            "Natural Language Processing",
            "Retrieval-Augmented Generation",
            "Long Story Comprehension"
        ]
    },
    {
        "paper": {
            "id": "2508.02276",
            "authors": [
                {
                    "_id": "68917696f01a094725f834b1",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b2",
                    "name": "Zhuoyun Yu",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b3",
                    "name": "Jiapeng Chen",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b4",
                    "name": "Yan Cui",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b5",
                    "name": "Daniel Shao",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b6",
                    "name": "Weixu Wang",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b7",
                    "name": "Fang Wu",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b8",
                    "name": "Yuchen Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b9",
                    "name": "Wenqi Shi",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834ba",
                    "name": "Zhi Huang",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834bb",
                    "name": "Arman Cohan",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834bc",
                    "name": "Xihong Lin",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834bd",
                    "name": "Fabian Theis",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834be",
                    "name": "Smita Krishnaswamy",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834bf",
                    "name": "Mark Gerstein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T10:43:31.000Z",
            "submittedOnDailyAt": "2025-08-05T01:42:34.345Z",
            "title": "CellForge: Agentic Design of Virtual Cell Models",
            "submittedOnDailyBy": {
                "_id": "63357c608adfa81faf2ac180",
                "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
                "isPro": false,
                "fullname": "Xiangru Tang",
                "user": "RTT1",
                "type": "user"
            },
            "summary": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.",
            "upvotes": 24,
            "discussionId": "68917697f01a094725f834c2",
            "ai_summary": "CellForge, an agentic system using a multi-agent framework, transforms raw single-cell multi-omics data into optimized computational models for virtual cells, outperforming state-of-the-art methods in single-cell perturbation prediction.",
            "ai_keywords": [
                "multi-agent framework",
                "single-cell multi-omics data",
                "task analysis",
                "method design",
                "experiment execution",
                "LLM agents",
                "gene knockouts",
                "drug treatments",
                "cytokine stimulations"
            ]
        },
        "translation_title": "CellForge: 가상 세포 모델의 능동적 설계",
        "purpose": "가상 세포 모델을 위한 최적화된 계산 모델을 자동으로 구축하기 위해 다양한 데이터와 연구 목표를 변환하는 시스템 개발",
        "method": [
            "CellForge는 입력으로 단일 세포 다중 오믹스 데이터와 작업 설명을 사용하여 최적화된 모델 아키텍처와 훈련을 위한 실행 가능한 코드를 생성함(Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells.)",
            "세 가지 핵심 모듈(Task Analysis, Method Design, Experiment Execution)로 구성되어 있으며 족보, 최적의 모델링 전략을 개발, 그리고 코드 자동 생성을 수행함(The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code.)",
            "Design 모듈의 에이전트들은 서로 다른 관점을 가진 전문가들로 나뉘어 협력하여 합의된 해결책을 찾음(The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus.)"
        ],
        "conclusion": "CellForge는 단일 세포 자극 예측에서 뛰어난 성능을 보이며, 다양한 데이터셋에 대해 최첨단 방법보다 일관되게 우수한 결과를 나타냄.",
        "keywords": [
            "Large Language Models",
            "Image Generation",
            "3D Vision"
        ]
    },
    {
        "paper": {
            "id": "2508.02150",
            "authors": [
                {
                    "_id": "6891804cf01a094725f8350e",
                    "name": "Qingyu Ren",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f8350f",
                    "name": "Qianyu He",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83510",
                    "name": "Bowei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83511",
                    "name": "Jie Zeng",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83512",
                    "name": "Jiaqing Liang",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83513",
                    "name": "Yanghua Xiao",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83514",
                    "name": "Weikang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83515",
                    "name": "Zeye Sun",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83516",
                    "name": "Fei Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T07:48:59.000Z",
            "submittedOnDailyAt": "2025-08-05T04:44:57.940Z",
            "title": "Beyond the Trade-off: Self-Supervised Reinforcement Learning for\n  Reasoning Models' Instruction Following",
            "submittedOnDailyBy": {
                "_id": "636b36351340f879a2ec2bb1",
                "avatarUrl": "/avatars/260a1c15f9c14c967125469072020946.svg",
                "isPro": false,
                "fullname": "QianyuHe",
                "user": "Abbey4799",
                "type": "user"
            },
            "summary": "Reasoning models excel in complex problem solving but exhibit a concerning\ntrade off between reasoning capabilities and instruction following abilities.\nExisting approaches for improving instruction following rely on stronger\nexternal models, creating methodological bottlenecks and practical limitations\nincluding increased costs and accessibility constraints. We propose a\nself-supervised RL framework that leverages reasoning models' own internal\nsignals to improve instruction following capabilities without external\nsupervision. Extensive experiments demonstrate that our framework significantly\nimproves instruction following capabilities while maintaining reasoning\nperformance, offering a scalable and cost-effective approach to enhance\ninstruction following in reasoning models. The data and code are publicly\navailable at https://github.com/Rainier-rq/verl-if.",
            "upvotes": 18,
            "discussionId": "6891804df01a094725f83517",
            "githubRepo": "https://github.com/Rainier-rq/verl-if",
            "ai_summary": "A self-supervised RL framework enhances instruction following in reasoning models without external supervision, maintaining reasoning performance and offering scalability and cost-effectiveness.",
            "ai_keywords": [
                "self-supervised RL",
                "reasoning models",
                "instruction following",
                "internal signals",
                "scalability",
                "cost-effectiveness"
            ],
            "githubStars": 5
        },
        "translation_title": "교훈 따르기 향상을 위한 자기 지도 강화 학습의 새로운 접근법",
        "purpose": "Reasoning 모델의 교훈 따르기 능력을 향상시키기 위한 방법론을 제안하고자 함",
        "method": [
            "자체적으로 강화 학습(RL) 프레임워크를 제안하여 Reasoning 모델의 내부 신호를 활용함(We propose a self-supervised RL framework that leverages reasoning models' own internal signals to improve instruction following capabilities without external supervision.)",
            "풍부한 실험을 통해 제안한 프레임워크가 교훈 따르기 능력을 유의미하게 향상시키면서도 Reasoning 성능을 유지함을 입증함(Extensive experiments demonstrate that our framework significantly improves instruction following capabilities while maintaining reasoning performance.)"
        ],
        "conclusion": "제안한 방법은 Reasoning 모델의 교훈 따르기 능력을 향상시키는 효과적이고 비용 효율적인 접근 방식임.",
        "keywords": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.01059",
            "authors": [
                {
                    "_id": "68916c3af01a094725f83460",
                    "name": "Sajana Weerawardhena",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83461",
                    "name": "Paul Kassianik",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83462",
                    "name": "Blaine Nelson",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83463",
                    "name": "Baturay Saglam",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83464",
                    "name": "Anu Vellore",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83465",
                    "name": "Aman Priyanshu",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83466",
                    "name": "Supriti Vijay",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83467",
                    "name": "Massimo Aufiero",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83468",
                    "name": "Arthur Goldblatt",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83469",
                    "name": "Fraser Burch",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f8346a",
                    "name": "Ed Li",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f8346b",
                    "name": "Jianliang He",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f8346c",
                    "name": "Dhruv Kedia",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f8346d",
                    "name": "Kojin Oshiba",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f8346e",
                    "name": "Zhouran Yang",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f8346f",
                    "name": "Yaron Singer",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83470",
                    "name": "Amin Karbasi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T20:25:57.000Z",
            "submittedOnDailyAt": "2025-08-05T01:01:25.435Z",
            "title": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report",
            "submittedOnDailyBy": {
                "_id": "620042b28c2eb991da50d34e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620042b28c2eb991da50d34e/Q5cj1GIq3XnKj-K64Mtyd.jpeg",
                "isPro": true,
                "fullname": "Aman Priyanshu",
                "user": "AmanPriyanshu",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have shown remarkable success across many\ndomains, yet their integration into cybersecurity applications remains limited\ndue to a lack of general-purpose cybersecurity data, representational\ncomplexity, and safety and regulatory concerns. To address this gap, we\npreviously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable\nfor fine-tuning on downstream tasks. That model, however, was not designed for\nchat-style interactions or instruction-following. In this report, we release\nFoundation-Sec-8B-Instruct: a model specifically trained for general-purpose\ncybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific\nknowledge with instruction-following, conversational capabilities, and\nalignment with human preferences to produce high-quality, relevant responses.\nComprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms\nLlama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its\ninstruction-following performance. It is also competitive with GPT-4o-mini on\ncyber threat intelligence and instruction-following tasks. We envision\nFoundation-Sec-8B-Instruct becoming an indispensable assistant in the daily\nworkflows of cybersecurity professionals. We release the model publicly at\nhttps://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.",
            "upvotes": 16,
            "discussionId": "68916c3af01a094725f83471",
            "ai_summary": "Foundation-Sec-8B-Instruct is a cybersecurity-focused LLM designed for chat-style interactions and instruction-following, outperforming other models in cybersecurity tasks while matching their instruction-following capabilities.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "cybersecurity",
                "Foundation-Sec-8B",
                "fine-tuning",
                "instruction-following",
                "conversational capabilities",
                "human preferences",
                "cybersecurity dialogue",
                "Llama 3.1-8B-Instruct",
                "GPT-4o-mini",
                "cyber threat intelligence"
            ]
        },
        "translation_title": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct 기술 보고서",
        "purpose": "사이버 보안 분야에서의 일반 목적 언어 모델의 적용 확대",
        "method": [
            "Foundation-Sec-8B를 기반으로, 일반 목적의 사이버 보안 대화에 특화된 모델인 Foundation-Sec-8B-Instruct를 학습시킴(In this report, we release Foundation-Sec-8B-Instruct: a model specifically trained for general-purpose cybersecurity dialogue.)",
            "도메인 전문 지식과 지시 수행, 대화 기능, 인간 선호에 맞춘 정렬 기능을 결합하여 고품질의 관련 응답을 생성함(Built on Foundation-Sec-8B, it combines domain-specific knowledge with instruction-following, conversational capabilities, and alignment with human preferences to produce high-quality, relevant responses.)"
        ],
        "conclusion": "Foundation-Sec-8B-Instruct는 여러 사이버 보안 작업에서 Llama 3.1-8B-Instruct를 능가하며, 사이버 보안 전문가의 일상 업무에서 필수적인 도우미가 되기를 기대함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Cybersecurity"
        ]
    }
]