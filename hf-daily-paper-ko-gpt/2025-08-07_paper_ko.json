[
    {
        "paper": {
            "id": "2508.04026",
            "authors": [
                {
                    "_id": "68941e5d741a16f544fbceed",
                    "name": "Shunyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbceee",
                    "user": {
                        "_id": "6417d9ea8f689506e7148417",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
                        "isPro": false,
                        "fullname": "minghao",
                        "user": "Liam-Liu",
                        "type": "user"
                    },
                    "name": "Minghao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:35.426Z",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbceef",
                    "name": "Huichi Zhou",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef0",
                    "name": "Zhenyu Cui",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef1",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef2",
                    "name": "Yuhao Zhou",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef3",
                    "name": "Wendong Fan",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef4",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef5",
                    "name": "Jiajun Shi",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef6",
                    "user": {
                        "_id": "65b8909c89eb3dfbe8d26780",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b8909c89eb3dfbe8d26780/aJExXsCN9oQOdd9bsQOEO.jpeg",
                        "isPro": false,
                        "fullname": "Weihao XUAN",
                        "user": "weihao1115",
                        "type": "user"
                    },
                    "name": "Weihao Xuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:31.782Z",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef7",
                    "name": "Jiaxing Huang",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef8",
                    "name": "Shuang Luo",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef9",
                    "name": "Fang Wu",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcefa",
                    "name": "Heli Qi",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcefb",
                    "name": "Qingcheng Zeng",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcefc",
                    "name": "Ziqi Ren",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcefd",
                    "name": "Jialiang Gao",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcefe",
                    "name": "Jindi Lv",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbceff",
                    "name": "Junjie Wang",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf00",
                    "name": "Aosong Feng",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf01",
                    "name": "Heng Zhou",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf02",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf03",
                    "name": "Zhenfei Yin",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf04",
                    "name": "Wenlong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf05",
                    "name": "Guohao Li",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf06",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf07",
                    "name": "Irene Li",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf08",
                    "name": "Lei Ma",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf09",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf0a",
                    "name": "Qunshu Lin",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf0b",
                    "name": "Mingli Song",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf0c",
                    "name": "Dacheng Tao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T02:38:18.000Z",
            "submittedOnDailyAt": "2025-08-07T02:06:19.798Z",
            "title": "VeriGUI: Verifiable Long-Chain GUI Dataset",
            "submittedOnDailyBy": {
                "_id": "6713afea187a20dc579e121b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6713afea187a20dc579e121b/ELxVQLVF9ifuT-TCWPK22.jpeg",
                "isPro": false,
                "fullname": "Shunyu Liu",
                "user": "liushunyu",
                "type": "user"
            },
            "summary": "Recent studies have delved into constructing autonomous agents capable of\nperforming complex Graphical User Interface (GUI)-based computer tasks, with\nthe potential to revolutionize human-computer interaction. Despite encouraging\nresults, existing efforts mainly focus on short-term interactions and rely on\noutcome-only verification, thereby limiting their scalability in real-world GUI\napplications that demand long-horizon task decomposition and execution. In this\nwork, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed\nto facilitate the development and evaluation of generalist GUI agents operating\nin realistic computer environments. Our dataset emphasizes two critical\ndimensions: (1) long-chain complexity, with tasks decomposed into a sequence of\ninterdependent subtasks spanning hundreds of steps, explicitly designed to\nallow any subtask to serve as a valid starting point; and (2) subtask-level\nverifiability, which enables diverse exploration strategies within each\nsubtask, while ensuring that each subtask-level goal remains verifiable and\nconsistent. The dataset consists of GUI task trajectories across both desktop\nand web, annotated by human experts. Extensive experiments on VeriGUI using\nvarious agents with different foundation models reveal significant performance\ngaps in handling long-horizon tasks, highlighting the need for more robust\nplanning and decision-making capabilities in GUI agents.",
            "upvotes": 79,
            "discussionId": "68941e5e741a16f544fbcf0d",
            "githubRepo": "https://github.com/VeriGUI-Team/VeriGUI",
            "ai_summary": "VeriGUI is a novel dataset for evaluating GUI agents in long-horizon tasks, emphasizing long-chain complexity and subtask-level verifiability.",
            "ai_keywords": [
                "Graphical User Interface (GUI)",
                "GUI agents",
                "long-chain complexity",
                "subtask-level verifiability",
                "GUI task trajectories",
                "desktop",
                "web",
                "human experts",
                "long-horizon tasks",
                "robust planning",
                "decision-making capabilities"
            ],
            "githubStars": 55
        },
        "translation_title": "VeriGUI: 검증 가능한 장기 체인 GUI 데이터셋",
        "purpose": "현실적인 컴퓨터 환경에서 작동하는 일반화된 GUI 에이전트의 개발 및 평가를 돕기 위한 장기 과제로 구성된 검증 가능한 데이터셋 연구",
        "method": [
            "GUI 기반 작업을 복잡한 서브 작업으로 나누어 수백 단계로 구성된 장기 체인 과제를 설정함(our dataset emphasizes two critical dimensions: long-chain complexity, with tasks decomposed into a sequence of interdependent subtasks spanning hundreds of steps)",
            "각 서브 작업을 검증 가능하고 일관성 있는 목표로 설정하여 다양한 탐색 전략을 가능하게 함(subtask-level verifiability, which enables diverse exploration strategies within each subtask, while ensuring that each subtask-level goal remains verifiable and consistent)",
            "데이터셋은 전문가에 의해 주석이 달린 데스크탑 및 웹의 GUI 작업 궤적으로 구성됨."
        ],
        "conclusion": "VeriGUI의 실험을 통해 기존 에이전트들이 장기 과제를 처리하는 데에서 성능 차이가 나타났으며, 이를 통해 GUI 에이전트의 더 강력한 계획 및 의사결정 능력의 필요성이 강조됨.",
        "keywords": [
            "Robotics",
            "Multimodal Learning",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2508.01191",
            "authors": [
                {
                    "_id": "689414b0741a16f544fbcec8",
                    "user": {
                        "_id": "65b2fae679954e21ac426aec",
                        "avatarUrl": "/avatars/c495c4ee9afb140a87ebc04602aa8c3e.svg",
                        "isPro": false,
                        "fullname": "Chengshuai Zhao",
                        "user": "chengshuaizhao",
                        "type": "user"
                    },
                    "name": "Chengshuai Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:44.551Z",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbcec9",
                    "name": "Zhen Tan",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbceca",
                    "user": {
                        "_id": "68942d7d73b80b8311a584fa",
                        "avatarUrl": "/avatars/80114f28c5166ba79e4418a738960268.svg",
                        "isPro": false,
                        "fullname": "PingchuanMa",
                        "user": "ympc08",
                        "type": "user"
                    },
                    "name": "Pingchuan Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:47.428Z",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbcecb",
                    "user": {
                        "_id": "6474e1afb68461d5cf7c41cc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
                        "isPro": false,
                        "fullname": "Dawei Li",
                        "user": "wjldw",
                        "type": "user"
                    },
                    "name": "Dawei Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:54.238Z",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbcecc",
                    "user": {
                        "_id": "61f087a0a57920a251ec1a6f",
                        "avatarUrl": "/avatars/4402b7986152bb37e02f1305c6bcce2e.svg",
                        "isPro": false,
                        "fullname": "Bohan Jiang",
                        "user": "Bohan",
                        "type": "user"
                    },
                    "name": "Bohan Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:50.992Z",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbcecd",
                    "name": "Yancheng Wang",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbcece",
                    "name": "Yingzhen Yang",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbcecf",
                    "name": "Huan Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-02T04:37:28.000Z",
            "submittedOnDailyAt": "2025-08-07T01:29:05.289Z",
            "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
            "submittedOnDailyBy": {
                "_id": "65b2fae679954e21ac426aec",
                "avatarUrl": "/avatars/c495c4ee9afb140a87ebc04602aa8c3e.svg",
                "isPro": false,
                "fullname": "Chengshuai Zhao",
                "user": "chengshuaizhao",
                "type": "user"
            },
            "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning.",
            "upvotes": 49,
            "discussionId": "689414b0741a16f544fbced0",
            "githubRepo": "https://github.com/ChengshuaiZhao0/DataAlchemy",
            "ai_summary": "CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.",
            "ai_keywords": [
                "Chain-of-Thought",
                "Large Language Model",
                "CoT reasoning",
                "inductive bias",
                "DataAlchemy",
                "distribution discrepancy",
                "reasoning paths",
                "generalizable reasoning"
            ],
            "githubStars": 12
        },
        "translation_title": "LLM의 Chain-of-Thought 추론은 환상인가? 데이터 분포 관점에서의 고찰",
        "purpose": "LLM의 Chain-of-Thought 추론이 표면적일 수 있음을 규명하고, 그 효과성이 데이터 분포의 차이에 의해 제한된다는 것을 연구하기 위함",
        "method": [
            "데이터 분포 관점에서 CoT 추론을 분석하고, CoT가 훈련 데이터로부터 학습한 구조적 유도 편향을 반영하는지를 조사함(we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data)",
            "DataAlchemy라는 통제된 환경을 설계하여 LLM을 처음부터 훈련하고 다양한 분포 조건에서 체계적으로 조사함(we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions)",
            "세 가지 차원(작업, 길이, 형식)에서 CoT 추론을 분석함(we dissect CoT reasoning via three dimensions: task, length, and format)"
        ],
        "conclusion": "CoT 추론은 훈련 데이터의 분포를 넘는 경우 쉽게 붕괴되는 경향이 있으며, 이 연구는 진정하고 일반화 가능한 추론을 달성하는 지속적인 도전의 중요성을 강조함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.02694",
            "authors": [
                {
                    "_id": "689385f0741a16f544fbcd8c",
                    "name": "Ningning Wang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd8d",
                    "name": "Xavier Hu",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd8e",
                    "name": "Pai Liu",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd8f",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd90",
                    "name": "Yue Hou",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd91",
                    "name": "Heyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd92",
                    "name": "Shengyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd93",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd94",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd95",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd96",
                    "name": "Changwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd97",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd98",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd99",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T17:56:51.000Z",
            "submittedOnDailyAt": "2025-08-07T03:39:00.617Z",
            "title": "Efficient Agents: Building Effective Agents While Reducing Cost",
            "submittedOnDailyBy": {
                "_id": "6892b4459f604ad07412f117",
                "avatarUrl": "/avatars/fb9e54e1689b4add084e32d7e5dd2f16.svg",
                "isPro": false,
                "fullname": "Xavier Hu",
                "user": "xavier-hu",
                "type": "user"
            },
            "summary": "The remarkable capabilities of Large Language Model (LLM)-driven agents have\nenabled sophisticated systems to tackle complex, multi-step tasks, but their\nescalating costs threaten scalability and accessibility. This work presents the\nfirst systematic study of the efficiency-effectiveness trade-off in modern\nagent systems, addressing the critical need for cost-effective designs without\nsacrificing performance. We investigate three key questions: (1) How much\ncomplexity do agentic tasks inherently require? (2) When do additional modules\nyield diminishing returns? (3) How much efficiency can be gained through the\ndesign of efficient agent frameworks? Through an empirical analysis on the GAIA\nbenchmark, we evaluate the impact of LLM backbone selection, agent framework\ndesigns, and test-time scaling strategies. Using the cost-of-pass metric, we\nquantify the efficiency-performance trade-off across these dimensions. Our\nfindings inform the development of Efficient Agents , a novel agent framework\nthat has an optimal complexity to task requirements. Efficient Agents retains\n96.7% of the performance of OWL, one leading open-source agent framework, while\nreducing operational costs from 0.398 to 0.228, resulting in a 28.4%\nimprovement in cost-of-pass. Our work provides actionable insights for\ndesigning efficient, high-performing agent systems, advancing the accessibility\nand sustainability of AI-driven solutions.",
            "upvotes": 39,
            "discussionId": "689385f0741a16f544fbcd9a",
            "githubRepo": "https://github.com/OPPO-PersonalAI/OAgents",
            "ai_summary": "A study on the efficiency-effectiveness trade-off in LLM-driven agent systems identifies optimal agent framework design to reduce costs while maintaining performance.",
            "ai_keywords": [
                "Large Language Model",
                "LLM",
                "agent systems",
                "efficiency-effectiveness trade-off",
                "agentic tasks",
                "agent framework",
                "GAIA benchmark",
                "LLM backbone",
                "test-time scaling strategies",
                "cost-of-pass",
                "Efficient Agents",
                "OWL"
            ],
            "githubStars": 120
        },
        "translation_title": "효율적인 에이전트: 비용을 줄이면서 효과적인 에이전트 구축하기",
        "purpose": "AI 드리븐 솔루션의 접근성과 지속 가능성을 높이기 위해 비용 효율적인 에이전트 시스템 설계 필요성 연구",
        "method": [
            "LLM 구동 에이전트에서 효율성과 효과의 균형에 대한 체계적인 연구 수행(we present the first systematic study of the efficiency-effectiveness trade-off in modern agent systems.)",
            "GAIA 벤치마크를 사용하여 LLM 백본 선택, 에이전트 프레임워크 설계, 테스트 시간 스케일링 전략의 영향을 평가함(Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies.)",
            "비용-효율 메트릭을 활용하여 다양한 측면에서 효율성과 성능 간의 균형을 정량화함(Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions.)"
        ],
        "conclusion": "Efficient Agents는 OWL 프레임워크의 96.7% 성능을 유지하면서 운영 비용을 줄여, 경제성을 28.4% 향상시키는 효과적인 에이전트 시스템의 개발에 기여함.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2508.04700",
            "authors": [
                {
                    "_id": "68941f54741a16f544fbcf0f",
                    "user": {
                        "_id": "63fda3fced9eead590ff6918",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Zeyi Sun",
                        "user": "Zery",
                        "type": "user"
                    },
                    "name": "Zeyi Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:28.544Z",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf10",
                    "name": "Ziyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf11",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:25.677Z",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf12",
                    "name": "Yuhang Cao",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf13",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf14",
                    "name": "Tong Wu",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf15",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf16",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T17:58:46.000Z",
            "submittedOnDailyAt": "2025-08-07T02:09:08.225Z",
            "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience",
            "submittedOnDailyBy": {
                "_id": "63fda3fced9eead590ff6918",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
                "isPro": false,
                "fullname": "Zeyi Sun",
                "user": "Zery",
                "type": "user"
            },
            "summary": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.",
            "upvotes": 33,
            "discussionId": "68941f54741a16f544fbcf17",
            "projectPage": "https://github.com/SunzeY/SEAgent",
            "githubRepo": "https://github.com/SunzeY/SEAgent",
            "ai_summary": "SEAgent, an agentic self-evolving framework, enables computer-use agents to autonomously master novel software through experiential learning and a curriculum of tasks, achieving superior performance compared to existing methods.",
            "ai_keywords": [
                "vision-language models",
                "computer use agents",
                "SEAgent",
                "experiential learning",
                "World State Model",
                "Curriculum Generator",
                "adversarial imitation",
                "Group Relative Policy Optimization",
                "specialist-to-generalist training",
                "OS-World",
                "UI-TARS"
            ],
            "githubStars": 35
        },
        "translation_title": "SEAgent: 경험으로부터 자율 학습하는 자기 진화형 컴퓨터 사용 에이전트",
        "purpose": "새로운 소프트웨어 환경에서 자율적으로 학습하고 진화하는 컴퓨터 사용 에이전트를 개발하기 위함",
        "method": [
            "SEAgent를 통해 컴퓨터 사용 에이전트가 새로운 소프트웨어 환경을 자율적으로 배울 수 있도록 하는 경험 학습 프레임워크를 제안함(To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software.)",
            "World State Model과 Curriculum Generator를 설계하여 점진적인 경험 축적과 점점 더 도전적인 문제를 생성함(To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks.)",
            "경험 학습을 통해 정책을 업데이트하고, 전문 에이전트로부터의 통찰력을 통합하여 강력한 일반화된 CUA를 개발함(Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution.)"
        ],
        "conclusion": "SEAgent는 다섯 개의 새로운 소프트웨어 환경에서 23.2%의 성공률 향상을 달성하여 개별 전문 에이전트 집합보다 뛰어난 성능을 입증함.",
        "keywords": [
            "Computer Vision",
            "Natural Language Processing",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2508.04280",
            "authors": [
                {
                    "_id": "68947269741a16f544fbd068",
                    "user": {
                        "_id": "6348202122bc15d2636ccf87",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673510811337-6348202122bc15d2636ccf87.jpeg",
                        "isPro": true,
                        "fullname": "Natyren",
                        "user": "GeorgeBredis",
                        "type": "user"
                    },
                    "name": "George Bredis",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:37:32.805Z",
                    "hidden": false
                },
                {
                    "_id": "68947269741a16f544fbd069",
                    "name": "Stanislav Dereka",
                    "hidden": false
                },
                {
                    "_id": "68947269741a16f544fbd06a",
                    "name": "Viacheslav Sinii",
                    "hidden": false
                },
                {
                    "_id": "68947269741a16f544fbd06b",
                    "name": "Ruslan Rakhimov",
                    "hidden": false
                },
                {
                    "_id": "68947269741a16f544fbd06c",
                    "user": {
                        "_id": "62a9c8edc19f92ae443ab37f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
                        "isPro": false,
                        "fullname": "Daniil Gavrilov",
                        "user": "kefirski",
                        "type": "user"
                    },
                    "name": "Daniil Gavrilov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:37:30.344Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T10:08:48.000Z",
            "submittedOnDailyAt": "2025-08-07T08:11:29.844Z",
            "title": "Enhancing Vision-Language Model Training with Reinforcement Learning in\n  Synthetic Worlds for Real-World Success",
            "submittedOnDailyBy": {
                "_id": "62a9c8edc19f92ae443ab37f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
                "isPro": false,
                "fullname": "Daniil Gavrilov",
                "user": "kefirski",
                "type": "user"
            },
            "summary": "Interactive multimodal agents must convert raw visual observations into\ncoherent sequences of language-conditioned actions -- a capability that current\nvision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)\nefforts could, in principle, endow VLMs with such skills, but they have seldom\ntested whether the learned behaviours generalize beyond their training\nsimulators, and they depend either on brittle hyperparameter tuning or on\ndense-reward environments with low state variability. We introduce\nVision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,\nhyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens\nwhile learning value only at the environment-step level: an arrangement, to our\nknowledge, not previously explored for large VLMs or LLMs. This simple\ndecoupling removes unstable weighting terms and yields faster, more reliable\nconvergence. Training a single VLM with VL-DAC in one inexpensive simulator at\na time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies\nthat generalize widely: +50\\% relative on BALROG (game-centric agentic\ncontrol), +5\\% relative on the hardest part of VSI-Bench (spatial planning),\nand +2\\% on VisualWebBench (web navigation), all without degrading general\nimage understanding accuracy. These results provide the first evidence that a\nsimple RL algorithm can train VLMs entirely in cheap synthetic worlds while\ndelivering measurable gains on real-image agentic, spatial-reasoning, and\nweb-navigation benchmarks.",
            "upvotes": 25,
            "discussionId": "6894726a741a16f544fbd06d",
            "githubRepo": "https://github.com/corl-team/VL-DAC",
            "ai_summary": "A lightweight, hyperparameter-free RL algorithm, VL-DAC, enables VLMs to learn generalized policies from inexpensive simulators, improving performance on real-world benchmarks without sacrificing image understanding accuracy.",
            "ai_keywords": [
                "Vision-Language Decoupled Actor-Critic (VL-DAC)",
                "PPO updates",
                "action tokens",
                "value learning",
                "MiniWorld",
                "Gym-Cards",
                "ALFWorld",
                "WebShop",
                "BALROG",
                "VSI-Bench",
                "VisualWebBench",
                "general image understanding accuracy"
            ],
            "githubStars": 0
        },
        "translation_title": "합성 세계에서 실제 성공을 위한 비전-언어 모델 훈련 강화하기",
        "purpose": "비전-언어 모델(VLM) 훈련 시 강화 학습으로 상호작용하는 다중 모달 에이전트의 성능 향상 목표",
        "method": [
            "VL-DAC라는 경량의 하이퍼파라미터 없는 강화 학습 알고리즘을 소개함(We introduce Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight, hyperparameter-free RL algorithm.)",
            "행동 토큰에 PPO 업데이트를 적용하며 환경 단계 수준에서만 가치를 학습하도록 구성함(VL-DAC applies PPO updates to action tokens while learning value only at the environment-step level: an arrangement, to our knowledge, not previously explored for large VLMs or LLMs.)",
            "하나의 저렴한 시뮬레이터에서 VLM을 훈련하여 광범위하게 일반화된 정책을 생성함(Training a single VLM with VL-DAC in one inexpensive simulator at a time already produces policies that generalize widely.)"
        ],
        "conclusion": "VL-DAC를 사용해 저렴한 합성 세계에서 훈련된 VLM이 실제 이미지 작업에서 성과를 낼 수 있다는 첫 번째 증거를 제공함.",
        "keywords": [
            "Vision-Language Models",
            "Reinforcement Learning",
            "Multimodal Learning"
        ]
    }
]