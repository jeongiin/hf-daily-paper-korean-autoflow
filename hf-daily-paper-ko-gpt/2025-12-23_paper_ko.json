[
    {
        "paper": {
            "id": "2512.16676",
            "authors": [
                {
                    "_id": "6949026334f46eaf46cbb3d1",
                    "name": "Hao Liang",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d2",
                    "name": "Xiaochen Ma",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d3",
                    "name": "Zhou Liu",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d4",
                    "name": "Zhen Hao Wong",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d5",
                    "name": "Zhengyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d6",
                    "name": "Zimo Meng",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d7",
                    "name": "Runming He",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d8",
                    "name": "Chengyu Shen",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d9",
                    "name": "Qifeng Cai",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3da",
                    "name": "Zhaoyang Han",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3db",
                    "name": "Meiyi Qiang",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3dc",
                    "name": "Yalin Feng",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3dd",
                    "name": "Tianyi Bai",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3de",
                    "name": "Zewei Pan",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3df",
                    "name": "Ziyi Guo",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e0",
                    "name": "Yizhen Jiang",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e1",
                    "name": "Jingwen Deng",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e2",
                    "name": "Qijie You",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e3",
                    "name": "Peichao Lai",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e4",
                    "name": "Tianyu Guo",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e5",
                    "name": "Chi Hsu Tsai",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e6",
                    "name": "Hengyi Feng",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e7",
                    "name": "Rui Hu",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e8",
                    "name": "Wenkai Yu",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e9",
                    "name": "Junbo Niu",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3ea",
                    "name": "Bohan Zeng",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3eb",
                    "name": "Ruichuan An",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3ec",
                    "name": "Lu Ma",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3ed",
                    "name": "Jihao Huang",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3ee",
                    "name": "Yaowei Zheng",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3ef",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3f0",
                    "name": "Linpeng Tang",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3f1",
                    "name": "Bin Cui",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3f2",
                    "name": "Weinan E",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3f3",
                    "name": "Wentao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-18T15:46:15.000Z",
            "submittedOnDailyAt": "2025-12-23T01:07:14.287Z",
            "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
            "submittedOnDailyBy": {
                "_id": "6671214c92412fd4640714eb",
                "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
                "isPro": false,
                "fullname": "bohan zeng",
                "user": "zbhpku",
                "type": "user"
            },
            "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
            "upvotes": 152,
            "discussionId": "6949026334f46eaf46cbb3f4",
            "projectPage": "https://github.com/OpenDCAI/DataFlow",
            "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.",
            "ai_keywords": [
                "DataFlow",
                "Large Language Models (LLMs)",
                "data preparation pipelines",
                "system-level abstractions",
                "PyTorch-style pipeline construction API",
                "reusable operators",
                "domain-general pipelines",
                "Text-to-SQL",
                "agentic RAG",
                "large-scale knowledge extraction",
                "DataFlow-Agent",
                "operator synthesis",
                "pipeline planning",
                "iterative verification"
            ],
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "translation_title": "DataFlow: 데이터 중심 AI 시대를 위한 통합 데이터 준비 및 워크플로우 자동화의 LLM 기반 프레임워크",
        "purpose": "고품질 데이터 요구에 부응하기 위해 신뢰할 수 있고 의미가 풍부한 데이터 준비 파이프라인 구축",
        "method": [
            "DataFlow라는 통합 LLM 기반 데이터 준비 프레임워크를 제안함(To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework.)",
            "모듈화되고 재사용 가능하며 조합할 수 있는 데이터 변환을 가능하게 하는 시스템 수준의 추상화를 설계함(DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations.)",
            "자연어 사양을 실행 가능한 파이프라인으로 자동 변환하는 DataFlow-Agent를 도입함(To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines...)"
        ],
        "conclusion": "DataFlow는 결과적으로 LLM 성능을 일관되게 향상시키며, 신뢰할 수 있고 재현 가능한 데이터 준비를 위한 고성능 기반을 제공함.",
        "keywords": [
            "Large Language Models",
            "Data Preparation",
            "Workflow Automation"
        ]
    },
    {
        "paper": {
            "id": "2512.19693",
            "authors": [
                {
                    "_id": "694a0ffa335742716e93227d",
                    "name": "Weichen Fan",
                    "hidden": false
                },
                {
                    "_id": "694a0ffa335742716e93227e",
                    "name": "Haiwen Diao",
                    "hidden": false
                },
                {
                    "_id": "694a0ffa335742716e93227f",
                    "name": "Quan Wang",
                    "hidden": false
                },
                {
                    "_id": "694a0ffa335742716e932280",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "694a0ffa335742716e932281",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T18:59:57.000Z",
            "submittedOnDailyAt": "2025-12-23T01:15:14.379Z",
            "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.",
            "upvotes": 51,
            "discussionId": "694a0ffa335742716e932282",
            "githubRepo": "https://github.com/WeichenFan/UAE",
            "githubRepoAddedBy": "user",
            "ai_summary": "Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.",
            "ai_keywords": [
                "spectral characteristics",
                "semantic encoders",
                "pixel encoders",
                "feature spectrum",
                "low-frequency components",
                "high-frequency information",
                "Prism Hypothesis",
                "Unified Autoencoding",
                "frequency-band modulator",
                "ImageNet",
                "MS-COCO",
                "latent space"
            ],
            "githubStars": 47
        },
        "translation_title": "프리즘 가설: 통합 오토인코딩을 통한 의미적 및 픽셀 표현의 조화",
        "purpose": "의미적 구조와 픽셀 세부 정보를 통합하여 조화롭게 처리하는 새로운 모델 개발",
        "method": [
            "다양한 의미적 및 픽셀 인코더의 스펙트럼 특성을 체계적으로 분석함(In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders.)",
            "의미적 인코더는 저주파 구성 요소를 캡처하고 픽셀 인코더는 고주파 정보를 보존한다는 발견을 통해 프리즘 가설을 정의함(This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure.)",
            "의미적 구조와 픽셀 세부 정보를 조화시키기 위해 주파수 대역 변조기를 통해 Unified Autoencoding (UAE) 모델을 제안함(Building on this insight, we propose Unified Autoencoding (UAE)...)"
        ],
        "conclusion": "UAE 모델은 의미적 추상화와 픽셀 수준의 충실도를 효과적으로 통합하여 뛰어난 성능을 발휘함.",
        "keywords": [
            "Image Generation",
            "Image Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.17650",
            "authors": [
                {
                    "_id": "6948c8f334f46eaf46cbb325",
                    "name": "Zhongwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6948c8f334f46eaf46cbb326",
                    "name": "Fuchen Long",
                    "hidden": false
                },
                {
                    "_id": "6948c8f334f46eaf46cbb327",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "6948c8f334f46eaf46cbb328",
                    "name": "Zhaofan Qiu",
                    "hidden": false
                },
                {
                    "_id": "6948c8f334f46eaf46cbb329",
                    "name": "Wu Liu",
                    "hidden": false
                },
                {
                    "_id": "6948c8f334f46eaf46cbb32a",
                    "name": "Ting Yao",
                    "hidden": false
                },
                {
                    "_id": "6948c8f334f46eaf46cbb32b",
                    "name": "Tao Mei",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6496f5754a3c31df8e3139f6/3S3unvdbINRHQFW85psrn.mp4"
            ],
            "publishedAt": "2025-12-19T14:49:30.000Z",
            "submittedOnDailyAt": "2025-12-23T01:38:37.820Z",
            "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
            "submittedOnDailyBy": {
                "_id": "6496f5754a3c31df8e3139f6",
                "avatarUrl": "/avatars/cf789d1986f976373c82b2976df4542a.svg",
                "isPro": false,
                "fullname": "Zhongwei Zhang",
                "user": "zzwustc",
                "type": "user"
            },
            "summary": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.",
            "upvotes": 37,
            "discussionId": "6948c8f334f46eaf46cbb32c",
            "projectPage": "https://zhw-zhang.github.io/ReCo-page/",
            "githubRepo": "https://github.com/HiDream-ai/ReCo",
            "githubRepoAddedBy": "user",
            "ai_summary": "ReCo is a novel instructional video editing paradigm that enhances accuracy and reduces token interference by incorporating constraint modeling and regularization techniques during in-context generation.",
            "ai_keywords": [
                "in-context generation",
                "instructional video editing",
                "denoising",
                "ReCo",
                "constraint modeling",
                "latent regularization",
                "attention regularization",
                "backward denoised latents",
                "attention maps",
                "ReCo-Data",
                "video diffusion learning",
                "instruction-video pairs",
                "video editing tasks"
            ],
            "githubStars": 25,
            "organization": {
                "_id": "61d8000084231b832e5bbd99",
                "name": "ustc",
                "fullname": "university of science and technology  of china",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"
            }
        },
        "translation_title": "Instructional Video 편집을 위한 Region-Constraint In-Context Generation",
        "purpose": "Instructional video 편집에서 비효율적인 수정을 방지하고 편집 품질을 높이기 위한 새로운 패러다임 제안",
        "method": [
            "ReCo라는 새로운 instructional video 편집 패러다임을 도입하여 편집 영역과 비편집 영역의 제약 모델링을 탐구함.(we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation.)",
            "ReCo는 소스 및 타겟 비디오를 폭 방향으로 연결하여 공동으로 노이즈 제거를 수행함.(ReCo width-wise concatenates source and target video for joint denoising.)",
            "편집 지역의 변화를 강조하고 비편집 영역의 불필요한 콘텐츠 생성을 완화하기 위해 두 가지 정규화 항, 즉 latent 및 attention 정규화를 사용하는 방법을 제안함.(ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively.)"
        ],
        "conclusion": "ReCo는 instructional video 편집의 네 가지 주요 작업에서 무궁무진한 성능을 입증하며, 고품질의 ReCo-Data 데이터셋을 제공하여 모델 훈련에 기여함.",
        "keywords": [
            "Computer Vision",
            "Video Generation",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2512.17040",
            "authors": [
                {
                    "_id": "6948b33d34f46eaf46cbb293",
                    "user": {
                        "_id": "6449db44df4e6cb7eaef912a",
                        "avatarUrl": "/avatars/777cea252e06933863bda10dc3543f59.svg",
                        "isPro": false,
                        "fullname": "Min-Jung Kim",
                        "user": "emjay73",
                        "type": "user"
                    },
                    "name": "Min-Jung Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-22T10:58:39.498Z",
                    "hidden": false
                },
                {
                    "_id": "6948b33d34f46eaf46cbb294",
                    "name": "Jeongho Kim",
                    "hidden": false
                },
                {
                    "_id": "6948b33d34f46eaf46cbb295",
                    "name": "Hoiyeong Jin",
                    "hidden": false
                },
                {
                    "_id": "6948b33d34f46eaf46cbb296",
                    "name": "Junha Hyung",
                    "hidden": false
                },
                {
                    "_id": "6948b33d34f46eaf46cbb297",
                    "name": "Jaegul Choo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-18T20:03:05.000Z",
            "submittedOnDailyAt": "2025-12-23T03:45:12.735Z",
            "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation",
            "submittedOnDailyBy": {
                "_id": "6449db44df4e6cb7eaef912a",
                "avatarUrl": "/avatars/777cea252e06933863bda10dc3543f59.svg",
                "isPro": false,
                "fullname": "Min-Jung Kim",
                "user": "emjay73",
                "type": "user"
            },
            "summary": "Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/",
            "upvotes": 25,
            "discussionId": "6948b33d34f46eaf46cbb298",
            "projectPage": "https://emjay73.github.io/InfCam/",
            "githubRepo": "https://github.com/emjay73/InfCam",
            "githubRepoAddedBy": "user",
            "ai_summary": "InfCam generates high-fidelity videos with accurate camera poses by using infinite homography warping and augmenting synthetic datasets with diverse trajectories.",
            "ai_keywords": [
                "video diffusion models",
                "camera-controlled video generation",
                "trajectory-conditioned video generation",
                "depth-free",
                "infinite homography warping",
                "3D camera rotations",
                "2D latent space",
                "residual parallax term",
                "data augmentation pipeline",
                "multiview datasets",
                "focal lengths"
            ],
            "githubStars": 6,
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "translation_title": "무한 호모그래피를 통한 카메라 제어 비디오 생성의 강력한 조건부 처리",
        "purpose": "동적 장면에서 카메라 제어를 통한 높은 품질의 비디오 생성을 위한 방법 개선",
        "method": [
            "InfCam이라는 깊이 없는 비디오-비디오 생성 프레임워크를 제안함(we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity.)",
            "무한 호모그래피 왜곡을 사용하여 3D 카메라 회전을 비디오 확산 모델의 2D 잠재 공간 내에서 직접 인코딩함(This framework integrates two key components: infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model.)",
            "데이터 증강 파이프라인을 통해 기존의 합성 다중 뷰 데이터셋을 다양한 카메라 경로와 초점 길이를 가진 시퀀스로 변환함(a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths.)"
        ],
        "conclusion": "InfCam은 카메라 포즈 정확도와 시각적 품질에서 기존 방법들을 초월하며, 합성 데이터에서 실제 데이터로 잘 일반화됨.",
        "keywords": [
            "Video Generation",
            "3D Vision",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.19134",
            "authors": [
                {
                    "_id": "694a1765335742716e9322b7",
                    "name": "Dehai Min",
                    "hidden": false
                },
                {
                    "_id": "694a1765335742716e9322b8",
                    "name": "Kailin Zhang",
                    "hidden": false
                },
                {
                    "_id": "694a1765335742716e9322b9",
                    "name": "Tongtong Wu",
                    "hidden": false
                },
                {
                    "_id": "694a1765335742716e9322ba",
                    "name": "Lu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T08:28:05.000Z",
            "submittedOnDailyAt": "2025-12-23T01:46:57.477Z",
            "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
            "submittedOnDailyBy": {
                "_id": "629c6ee73a3221bb210afc2d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg",
                "isPro": false,
                "fullname": "Dehai Min",
                "user": "ZhishanQ",
                "type": "user"
            },
            "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.",
            "upvotes": 23,
            "discussionId": "694a1765335742716e9322bb",
            "githubRepo": "https://github.com/ZhishanQ/QuCo-RAG",
            "githubRepoAddedBy": "user",
            "ai_summary": "QuCo-RAG uses objective corpus statistics to mitigate hallucinations in large language models during generation, improving accuracy across various benchmarks.",
            "ai_keywords": [
                "dynamic retrieval-augmented generation",
                "large language models",
                "hallucinations",
                "model-internal signals",
                "logits",
                "entropy",
                "pre-training data",
                "uncertainty quantification",
                "low-frequency entities",
                "entity co-occurrence",
                "Infini-gram",
                "multi-hop QA",
                "EM gains",
                "OLMo-2",
                "Llama",
                "Qwen",
                "GPT",
                "biomedical QA",
                "domain generalization",
                "corpus-grounded verification"
            ],
            "githubStars": 6
        },
        "translation_title": "QuCo-RAG: 동적 검색 증강 생성을 위한 사전 훈련 데이터의 불확실성 정량화",
        "purpose": "동적 검색 증강 생성에서 잘못된 출력을 줄이기 위한 신뢰할 수 있는 불확실성 정량화 방법 개발",
        "method": [
            "기존 방법이 내부 신호에 의존하는 대신, 사전 훈련 데이터에서 계산된 객관적인 통계를 사용함(e.g., logits, entropy)",
            "생성 전에는 긴 꼬리 지식 격차를 나타내는 저주파수 엔티티를 식별함(we identify low-frequency entities indicating long-tail knowledge gaps)",
            "생성 중에는 사전 훈련 데이터에서 엔티티의 동시 발생을 검증하여, 동시 발생이 없을 경우 환각 위험 신호가 됨(we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk)",
            "4조 개 토큰에 대한 밀리초 지연 쿼리를 통해 Infini-gram을 활용하여 불확실성이 높을 때 검색을 촉발함(Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high)"
        ],
        "conclusion": "QuCo-RAG는 동적 RAG의 원칙적이고 사실상 모델 독립적인 패러다임으로 확립되었으며, 다양한 모델에 대해 EM 개선이 입증됨.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    }
]