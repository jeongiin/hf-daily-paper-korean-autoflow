[
    {
        "paper": {
            "id": "2508.21104",
            "authors": [
                {
                    "_id": "68b50650851c6e7b001eca07",
                    "name": "Wenfeng Feng",
                    "hidden": false
                },
                {
                    "_id": "68b50650851c6e7b001eca08",
                    "name": "Penghong Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b50650851c6e7b001eca09",
                    "user": {
                        "_id": "644a1dbb9c340e5e1e713153",
                        "avatarUrl": "/avatars/21cb93ad067a798a39829ef7e67c70b8.svg",
                        "isPro": false,
                        "fullname": "JGC",
                        "user": "Nothing2Say",
                        "type": "user"
                    },
                    "name": "Guochao Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T07:50:45.446Z",
                    "hidden": false
                },
                {
                    "_id": "68b50650851c6e7b001eca0a",
                    "user": {
                        "_id": "64ae631b58bd9e9cc2f5a749",
                        "avatarUrl": "/avatars/ce6426ec3bdb618a9e449297e7f147e0.svg",
                        "isPro": false,
                        "fullname": "Chuzhan HAO",
                        "user": "Chuzhan",
                        "type": "user"
                    },
                    "name": "Chuzhan Hao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-02T07:57:08.870Z",
                    "hidden": false
                },
                {
                    "_id": "68b50650851c6e7b001eca0b",
                    "name": "Yuewei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b50650851c6e7b001eca0c",
                    "name": "Hao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-28T09:18:26.000Z",
            "submittedOnDailyAt": "2025-09-02T00:25:08.396Z",
            "title": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "644a1dbb9c340e5e1e713153",
                "avatarUrl": "/avatars/21cb93ad067a798a39829ef7e67c70b8.svg",
                "isPro": false,
                "fullname": "JGC",
                "user": "Nothing2Say",
                "type": "user"
            },
            "summary": "Critic-free reinforcement learning methods, particularly group policies, have\nattracted considerable attention for their efficiency in complex tasks.\nHowever, these methods rely heavily on multiple sampling and comparisons within\nthe policy to estimate advantage, which may cause the policy to fall into local\noptimum and increase computational cost. To address these issues, we propose\nPVPO, an efficient reinforcement learning method enhanced by an advantage\nreference anchor and data pre-sampling. Specifically, we use the reference\nmodel to rollout in advance and employ the calculated reward score as a\nreference anchor. Our approach effectively corrects the cumulative bias\nintroduced by intra-group comparisons and significantly reduces reliance on the\nnumber of rollouts. Meanwhile, the reference model can assess sample difficulty\nduring data pre-sampling, enabling effective selection of high-gain data to\nimprove training efficiency. Experiments conducted on nine datasets across two\ndomains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our\napproach not only demonstrates robust generalization across multiple tasks, but\nalso exhibits scalable performance across models of varying scales.",
            "upvotes": 17,
            "discussionId": "68b50651851c6e7b001eca0d",
            "ai_summary": "PVPO, an enhanced reinforcement learning method using a reference anchor and data pre-sampling, achieves state-of-the-art performance with reduced computational cost and improved generalization.",
            "ai_keywords": [
                "critic-free reinforcement learning",
                "group policies",
                "advantage estimation",
                "local optimum",
                "computational cost",
                "advantage reference anchor",
                "data pre-sampling",
                "reference model",
                "rollout",
                "reward score",
                "cumulative bias",
                "intra-group comparisons",
                "sample difficulty",
                "high-gain data",
                "training efficiency",
                "state-of-the-art performance",
                "robust generalization",
                "scalable performance"
            ]
        },
        "translation_title": "PVPO: 에이전트 폴리시 최적화를 위한 사전 추정 값 기반 정책 최적화",
        "purpose": "복잡한 작업에서 효과적인 강화 학습을 통해 성능 개선을 위한 새로운 방법론 연구",
        "method": [
            "효율적인 강화 학습 방법인 PVPO를 제안하고, Advantage reference anchor와 데이터 사전 샘플링을 활용함(To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling.)",
            "사전 모델을 사용해 미리 롤아웃을 진행하고, 계산된 보상 점수를 기준 앵커로 사용함(Specifically, we use the reference model to rollout in advance and employ the calculated reward score as a reference anchor.)",
            "이 방법은 그룹 내 비교로 인해 발생하는 누적 편향을 효과적으로 수정하고 롤아웃 수 의존도를 크게 줄임.Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts.)"
        ],
        "conclusion": "PVPO는 뛰어난 성능을 발휘하며 다양한 작업에서 강력한 일반화 능력을 보여줌.",
        "keywords": [
            "Reinforcement Learning",
            "Policy Optimization",
            "Generalization"
        ]
    },
    {
        "paper": {
            "id": "2508.19813",
            "authors": [
                {
                    "_id": "68b1484bb19c540001484a18",
                    "name": "Jie Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a19",
                    "name": "Changzai Pan",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a1a",
                    "name": "Kaiwen Wei",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a1b",
                    "name": "Sishi Xiong",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a1c",
                    "name": "Yu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a1d",
                    "name": "Xiangyu Li",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a1e",
                    "name": "Jiaxin Peng",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a1f",
                    "name": "Xiaoyan Gu",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a20",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a21",
                    "name": "Wenhan Chang",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a22",
                    "name": "Zhenhe Wu",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a23",
                    "name": "Jiang Zhong",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a24",
                    "name": "Shuangyong Song",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a25",
                    "name": "Yongxiang Li",
                    "hidden": false
                },
                {
                    "_id": "68b1484bb19c540001484a26",
                    "name": "Xuelong Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T11:55:40.000Z",
            "submittedOnDailyAt": "2025-09-02T07:08:06.784Z",
            "title": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real\n  World Industrial Tables",
            "submittedOnDailyBy": {
                "_id": "64ccb9bfead94891d12aef42",
                "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg",
                "isPro": false,
                "fullname": "Yang Jian",
                "user": "CSJianYang",
                "type": "user"
            },
            "summary": "Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench. Source code and data will be available after acceptance.",
            "upvotes": 7,
            "discussionId": "68b1484bb19c540001484a27",
            "ai_summary": "A bilingual benchmark named T2R-bench is proposed to evaluate the performance of large language models in generating reports from tables, highlighting the need for improvement in this task.",
            "ai_keywords": [
                "large language models",
                "table reasoning",
                "table-to-report task",
                "T2R-bench",
                "industrial tables",
                "report generation",
                "evaluation criteria",
                "Deepseek-R1"
            ]
        },
        "translation_title": "T2R-bench: 실제 산업 테이블에서 기사 수준 보고서를 생성하기 위한 벤치마크",
        "purpose": "산업 테이블 정보를 보고서로 변환하는 과제의 평가 및 개선을 위한 벤치마크 구축",
        "method": [
            "table-to-report 작업을 제안하고, 이를 위한 이중 언어 벤치마크인 T2R-bench를 구축함(we propose the table-to-report task and construct a bilingual benchmark named T2R-bench).",
            "T2R-bench는 실제 시나리오에서 파생된 457개의 산업 테이블로 구성되며, 19개 산업 도메인과 4종의 산업 테이블을 포함함(the benchmark comprises 457 industrial tables, all derived from real-world scenarios and encompassing 19 industry domains as well as 4 types of industrial tables).",
            "보고서 생성 품질을 공정하게 측정하는 평가 기준을 제안함(Furthermore, we propose an evaluation criteria to fairly measure the quality of report generation)."
        ],
        "conclusion": "현재 최첨단 모델들도 보고서 생성에서 62.71의 점수만을 달성하여, LLMs는 T2R-bench에서 개선의 여지가 많음을 보여줌.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Document Parsing"
        ]
    },
    {
        "paper": {
            "id": "2508.19060",
            "authors": [
                {
                    "_id": "68b2a781851c6e7b001ec685",
                    "user": {
                        "_id": "644abca2bef23513f3e6eb55",
                        "avatarUrl": "/avatars/1e5a40a90a6425b297118f3790ca7c79.svg",
                        "isPro": false,
                        "fullname": "Blaz Rolih",
                        "user": "blaz-r",
                        "type": "user"
                    },
                    "name": "Blaž Rolih",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T07:52:54.791Z",
                    "hidden": false
                },
                {
                    "_id": "68b2a781851c6e7b001ec686",
                    "user": {
                        "_id": "63636d302bff406cb0575289",
                        "avatarUrl": "/avatars/cebc02551b7f0418f5d33466959e0796.svg",
                        "isPro": false,
                        "fullname": "Matic Fučka",
                        "user": "MaticFuc",
                        "type": "user"
                    },
                    "name": "Matic Fučka",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T13:51:37.034Z",
                    "hidden": false
                },
                {
                    "_id": "68b2a781851c6e7b001ec687",
                    "name": "Danijel Skočaj",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-26T14:20:21.000Z",
            "submittedOnDailyAt": "2025-09-02T01:45:44.492Z",
            "title": "No Label Left Behind: A Unified Surface Defect Detection Model for all\n  Supervision Regimes",
            "submittedOnDailyBy": {
                "_id": "644abca2bef23513f3e6eb55",
                "avatarUrl": "/avatars/1e5a40a90a6425b297118f3790ca7c79.svg",
                "isPro": false,
                "fullname": "Blaz Rolih",
                "user": "blaz-r",
                "type": "user"
            },
            "summary": "Surface defect detection is a critical task across numerous industries, aimed\nat efficiently identifying and localising imperfections or irregularities on\nmanufactured components. While numerous methods have been proposed, many fail\nto meet industrial demands for high performance, efficiency, and adaptability.\nExisting approaches are often constrained to specific supervision scenarios and\nstruggle to adapt to the diverse data annotations encountered in real-world\nmanufacturing processes, such as unsupervised, weakly supervised, mixed\nsupervision, and fully supervised settings. To address these challenges, we\npropose SuperSimpleNet, a highly efficient and adaptable discriminative model\nbuilt on the foundation of SimpleNet. SuperSimpleNet incorporates a novel\nsynthetic anomaly generation process, an enhanced classification head, and an\nimproved learning procedure, enabling efficient training in all four\nsupervision scenarios, making it the first model capable of fully leveraging\nall available data annotations. SuperSimpleNet sets a new standard for\nperformance across all scenarios, as demonstrated by its results on four\nchallenging benchmark datasets. Beyond accuracy, it is very fast, achieving an\ninference time below 10 ms. With its ability to unify diverse supervision\nparadigms while maintaining outstanding speed and reliability, SuperSimpleNet\nrepresents a promising step forward in addressing real-world manufacturing\nchallenges and bridging the gap between academic research and industrial\napplications. Code: https://github.com/blaz-r/SuperSimpleNet",
            "upvotes": 4,
            "discussionId": "68b2a781851c6e7b001ec688",
            "githubRepo": "https://github.com/blaz-r/SuperSimplenet",
            "ai_summary": "SuperSimpleNet, an efficient and adaptable model based on SimpleNet, addresses diverse supervision scenarios in surface defect detection with high performance and low inference time.",
            "ai_keywords": [
                "SimpleNet",
                "synthetic anomaly generation",
                "classification head",
                "learning procedure",
                "unsupervised",
                "weakly supervised",
                "mixed supervision",
                "fully supervised",
                "benchmark datasets",
                "inference time"
            ],
            "githubStars": 78
        },
        "translation_title": "라벨을 남기지 않다: 모든 감독 체계를 위한 통합된 표면 결함 탐지 모델",
        "purpose": "모든 감독 시나리오에서 효율적이고 적응 가능한 표면 결함 탐지 모델을 개발하여 실제 제조 환경에서의 정확성과 효율성을 높이고자 함",
        "method": [
            "SuperSimpleNet이라는 효율적이고 적응 가능한 모델을 제안함(We propose SuperSimpleNet, a highly efficient and adaptable discriminative model).",
            "합성 이상 생성 과정 및 개선된 분류 헤드를 사용하여 모든 네 가지 감독 관련 시나리오에서 효율적인 훈련이 가능하도록 설계함(SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure).",
            "네 가지 도전적인 벤치마크 데이터셋에서 성능을 입증함(SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets)."
        ],
        "conclusion": "SuperSimpleNet은 다양한 감독 패러다임을 통합하면서도 뛰어난 속도와 신뢰성을 유지하여 실제 제조 도전 과제를 해결하는 데 유망한 진전을 보여줌.",
        "keywords": [
            "Image Understanding",
            "Robotics",
            "Document Parsing"
        ]
    },
    {
        "paper": {
            "id": "2508.17378",
            "authors": [
                {
                    "_id": "68b69c0626d3ff3f69d5fdc8",
                    "user": {
                        "_id": "628f7a71dd993507cfcbe587",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
                        "isPro": true,
                        "fullname": "Omartificial Intelligence Space",
                        "user": "Omartificial-Intelligence-Space",
                        "type": "user"
                    },
                    "name": "Omer Nacar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-02T07:54:04.897Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-24T14:32:15.000Z",
            "submittedOnDailyAt": "2025-09-02T05:56:22.215Z",
            "title": "UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via\n  HUMAIN Chat",
            "submittedOnDailyBy": {
                "_id": "628f7a71dd993507cfcbe587",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
                "isPro": true,
                "fullname": "Omartificial Intelligence Space",
                "user": "Omartificial-Intelligence-Space",
                "type": "user"
            },
            "summary": "Large language models (LLMs) trained primarily on English corpora often\nstruggle to capture the linguistic and cultural nuances of Arabic. To address\nthis gap, the Saudi Data and AI Authority (SDAIA) introduced the ALLaM family\nof Arabic-focused models. The most capable of these available to the public,\nALLaM-34B, was subsequently adopted by HUMAIN, who developed and deployed\nHUMAIN Chat, a closed conversational web service built on this model. This\npaper presents an expanded and refined UI-level evaluation of ALLaM-34B.\nUsing a prompt pack spanning modern standard Arabic, five regional dialects,\ncode-switching, factual knowledge, arithmetic and temporal reasoning, creative\ngeneration, and adversarial safety, we collected 115 outputs (23 prompts times\n5 runs) and scored each with three frontier LLM judges (GPT-5, Gemini 2.5 Pro,\nClaude Sonnet-4). We compute category-level means with 95\\% confidence\nintervals, analyze score distributions, and visualize dialect-wise metric heat\nmaps. The updated analysis reveals consistently high performance on generation\nand code-switching tasks (both averaging 4.92/5), alongside strong results in\nMSA handling (4.74/5), solid reasoning ability (4.64/5), and improved dialect\nfidelity (4.21/5). Safety-related prompts show stable, reliable performance of\n(4.54/5). Taken together, these results position ALLaM-34B as a robust and\nculturally grounded Arabic LLM, demonstrating both technical strength and\npractical readiness for real-world deployment.",
            "upvotes": 3,
            "discussionId": "68b69c0726d3ff3f69d5fdc9",
            "ai_summary": "The evaluation of ALLaM-34B, an Arabic-focused LLM, demonstrates high performance across various tasks including generation, code-switching, MSA handling, reasoning, dialect fidelity, and safety, positioning it as a robust and culturally grounded model.",
            "ai_keywords": [
                "LLMs",
                "Arabic-focused models",
                "ALLaM-34B",
                "HUMAIN Chat",
                "UI-level evaluation",
                "prompt pack",
                "regional dialects",
                "code-switching",
                "factual knowledge",
                "arithmetic",
                "temporal reasoning",
                "creative generation",
                "adversarial safety",
                "LLM judges",
                "category-level means",
                "confidence intervals",
                "score distributions",
                "dialect-wise metric heat maps",
                "MSA handling",
                "reasoning ability",
                "dialect fidelity",
                "safety-related prompts"
            ]
        },
        "translation_title": "UI 수준의 ALLaM 34B 평가: HUMAIN Chat을 통한 아랍 중심 LLM 측정",
        "purpose": "Arabic 언어와 문화의 미세한 차이를 포착하기 위한 LLM의 성능 평가",
        "method": [
            "ALLaM-34B 모델을 기반으로 한 HUMAIN Chat 웹 서비스를 개발하고 배포함(This paper presents an expanded and refined UI-level evaluation of ALLaM-34B.)",
            "다양한 주제를 포함하는 프롬프트 팩을 통해 115개의 출력을 수집하고 세 가지 프론티어 LLM 자문단이 평가함(Using a prompt pack spanning modern standard Arabic, five regional dialects, code-switching, factual knowledge, arithmetic and temporal reasoning, creative generation, and adversarial safety, we collected 115 outputs and scored each with three frontier LLM judges.)",
            "95% 신뢰 구간으로 카테고리별 평균 점수를 계산하고 주제를 시각화하여 분석함(We compute category-level means with 95% confidence intervals, analyze score distributions, and visualize dialect-wise metric heat maps.)"
        ],
        "conclusion": "ALLaM-34B는 생성 작업에서 높은 성능을 나타내며, 아랍어 LLM로서의 기술적 강점과 실용 준비성을 입증함.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.17198",
            "authors": [
                {
                    "_id": "68b6af405aa065c9126126e0",
                    "name": "Shouwei Ruan",
                    "hidden": false
                },
                {
                    "_id": "68b6af405aa065c9126126e1",
                    "name": "Liyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68b6af405aa065c9126126e2",
                    "name": "Caixin Kang",
                    "hidden": false
                },
                {
                    "_id": "68b6af405aa065c9126126e3",
                    "name": "Qihui Zhu",
                    "hidden": false
                },
                {
                    "_id": "68b6af405aa065c9126126e4",
                    "name": "Songming Liu",
                    "hidden": false
                },
                {
                    "_id": "68b6af405aa065c9126126e5",
                    "name": "Xingxing Wei",
                    "hidden": false
                },
                {
                    "_id": "68b6af405aa065c9126126e6",
                    "name": "Hang Su",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63fc4751a3c067e62899a3a1/7XKx2xnUY_Oe3xavF_BJE.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/63fc4751a3c067e62899a3a1/j0meF2fFxPdKfqbS6G7CZ.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/63fc4751a3c067e62899a3a1/idiTLb2Is6FDJ0cpqxF6M.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/63fc4751a3c067e62899a3a1/mBK1C4ljGc25BQ7X2xjOv.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/63fc4751a3c067e62899a3a1/Mar8ha7u6DTeMh29uy71_.gif"
            ],
            "publishedAt": "2025-08-24T03:20:48.000Z",
            "submittedOnDailyAt": "2025-09-02T07:28:23.027Z",
            "title": "From reactive to cognitive: brain-inspired spatial intelligence for\n  embodied agents",
            "submittedOnDailyBy": {
                "_id": "63fc4751a3c067e62899a3a1",
                "avatarUrl": "/avatars/d851f7d623eb433e5669bbd9d9b2c354.svg",
                "isPro": false,
                "fullname": "RSW",
                "user": "RSW233",
                "type": "user"
            },
            "summary": "Spatial cognition enables adaptive goal-directed behavior by constructing\ninternal models of space. Robust biological systems consolidate spatial\nknowledge into three interconnected forms: landmarks for salient cues,\nroute knowledge for movement trajectories, and survey\nknowledge for map-like representations. While recent advances in multi-modal\nlarge language models (MLLMs) have enabled visual-language reasoning in\nembodied agents, these efforts lack structured spatial memory and instead\noperate reactively, limiting their generalization and adaptability in complex\nreal-world environments. Here we present Brain-inspired Spatial Cognition for\nNavigation (BSC-Nav), a unified framework for constructing and leveraging\nstructured spatial memory in embodied agents. BSC-Nav builds allocentric\ncognitive maps from egocentric trajectories and contextual cues, and\ndynamically retrieves spatial knowledge aligned with semantic goals. Integrated\nwith powerful MLLMs, BSC-Nav achieves state-of-the-art efficacy and efficiency\nacross diverse navigation tasks, demonstrates strong zero-shot generalization,\nand supports versatile embodied behaviors in the real physical world, offering\na scalable and biologically grounded path toward general-purpose spatial\nintelligence.",
            "upvotes": 3,
            "discussionId": "68b6af405aa065c9126126e7",
            "githubRepo": "https://github.com/Heathcliff-saku/BSC-Nav",
            "ai_summary": "BSC-Nav constructs allocentric cognitive maps from egocentric trajectories and contextual cues, enabling embodied agents to perform diverse navigation tasks with zero-shot generalization and versatile behaviors.",
            "ai_keywords": [
                "allocentric cognitive maps",
                "egocentric trajectories",
                "contextual cues",
                "structured spatial memory",
                "zero-shot generalization",
                "embodied behaviors",
                "spatial intelligence"
            ],
            "githubStars": 18
        },
        "translation_title": "반응적에서 인지적: 체화된 에이전트를 위한 뇌에서 영감을 받은 공간 지능",
        "purpose": "체화된 에이전트를 위한 구조화된 공간 기억을 구축하고 활용하여 목표 지향 행동을 개선하기 위함",
        "method": [
            "BSC-Nav라는 통합 프레임워크를 제시하여 구조화된 공간 기억을 만들고 활용함(Here we present Brain-inspired Spatial Cognition for Navigation (BSC-Nav), a unified framework for constructing and leveraging structured spatial memory in embodied agents.)",
            "BSC-Nav는 이고센트릭 경로와 맥락 단서를 바탕으로 알로센트릭 인지지도를 구축함(BSC-Nav builds allocentric cognitive maps from egocentric trajectories and contextual cues.)",
            "BSC-Nav는 MLLM과 통합되어 다양한 탐색 작업에서 뛰어난 성능을 달성함(Integrated with powerful MLLMs, BSC-Nav achieves state-of-the-art efficacy and efficiency across diverse navigation tasks.)"
        ],
        "conclusion": "BSC-Nav는 다양한 상황에서 우수한 일반화 성능을 보여주고, 실제 물리적 세계에서 다재다능한 행동을 지원하는 효율적인 공간 지능을 제공함.",
        "keywords": [
            "Spatial Cognition",
            "Large Language Models",
            "Navigation"
        ]
    }
]