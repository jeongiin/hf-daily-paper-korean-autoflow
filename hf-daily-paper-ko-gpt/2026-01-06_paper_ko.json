[
    {
        "paper": {
            "id": "2601.01739",
            "authors": [
                {
                    "_id": "695c72346aa73bc11f0913bf",
                    "user": {
                        "_id": "6044fd39e6aa3e130cb92867",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6044fd39e6aa3e130cb92867/L5hb8vpHY6SKMEL-Xacma.jpeg",
                        "isPro": false,
                        "fullname": "Eunbi Choi",
                        "user": "unbiarirang",
                        "type": "user"
                    },
                    "name": "Eunbi Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:17.472Z",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c0",
                    "user": {
                        "_id": "64d31ca9465b6039259838df",
                        "avatarUrl": "/avatars/b3bde5067ed3fcd908d3d91c00680bfb.svg",
                        "isPro": false,
                        "fullname": "kibong choi",
                        "user": "bongchoi",
                        "type": "user"
                    },
                    "name": "Kibong Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:11.322Z",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c1",
                    "name": "Seokhee Hong",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c2",
                    "user": {
                        "_id": "63c50e590c24c8b53958f75e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673858632881-noauth.png",
                        "isPro": false,
                        "fullname": "Junwon Hwang",
                        "user": "nuxlear",
                        "type": "user"
                    },
                    "name": "Junwon Hwang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:09.333Z",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c3",
                    "name": "Hyojin Jeon",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c4",
                    "user": {
                        "_id": "66a9e066a203add977948988",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a9e066a203add977948988/mwVS-vt-8p-DFC5T9H9H3.jpeg",
                        "isPro": false,
                        "fullname": "hyunjik.jo",
                        "user": "switiz87",
                        "type": "user"
                    },
                    "name": "Hyunjik Jo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:13.551Z",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c5",
                    "name": "Joonkee Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c6",
                    "name": "Seonghwan Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c7",
                    "name": "Soyeon Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c8",
                    "name": "Sunkyoung Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c9",
                    "name": "Yireun Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ca",
                    "name": "Yongil Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913cb",
                    "name": "Haeju Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913cc",
                    "name": "Jinsik Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913cd",
                    "name": "Kyungmin Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ce",
                    "name": "Sangha Park",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913cf",
                    "name": "Heuiyeen Yeen",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d0",
                    "name": "Hwan Chang",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d1",
                    "name": "Stanley Jungkyu Choi",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d2",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d3",
                    "name": "Jiwon Ham",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d4",
                    "name": "Kijeong Jeon",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d5",
                    "name": "Geunyeong Jeong",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d6",
                    "name": "Gerrard Jeongwon Jo",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d7",
                    "name": "Yonghwan Jo",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d8",
                    "name": "Jiyeon Jung",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d9",
                    "name": "Naeun Kang",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913da",
                    "name": "Dohoon Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913db",
                    "name": "Euisoon Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913dc",
                    "name": "Hayeon Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913dd",
                    "name": "Hyosang Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913de",
                    "name": "Hyunseo Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913df",
                    "name": "Jieun Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e0",
                    "name": "Minu Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e1",
                    "name": "Myoungshin Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e2",
                    "name": "Unsol Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e3",
                    "name": "Youchul Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e4",
                    "name": "YoungJin Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e5",
                    "name": "Chaeeun Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e6",
                    "name": "Chaeyoon Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e7",
                    "user": {
                        "_id": "6399ab9e92e12136b99ef60e",
                        "avatarUrl": "/avatars/b76895c53f0f046586555c20292c78a1.svg",
                        "isPro": false,
                        "fullname": "Changhun Lee",
                        "user": "xvyaward",
                        "type": "user"
                    },
                    "name": "Changhun Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:15.420Z",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e8",
                    "name": "Dahm Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e9",
                    "name": "Edward Hwayoung Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ea",
                    "name": "Honglak Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913eb",
                    "name": "Jinsang Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ec",
                    "name": "Jiyoung Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ed",
                    "name": "Sangeun Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ee",
                    "name": "Seungwon Lim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ef",
                    "name": "Solji Lim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f0",
                    "name": "Woohyung Lim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f1",
                    "name": "Chanwoo Moon",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f2",
                    "name": "Jaewoo Park",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f3",
                    "name": "Jinho Park",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f4",
                    "name": "Yongmin Park",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f5",
                    "name": "Hyerin Seo",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f6",
                    "name": "Wooseok Seo",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f7",
                    "name": "Yongwoo Song",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f8",
                    "name": "Sejong Yang",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f9",
                    "name": "Sihoon Yang",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913fa",
                    "name": "Chang En Yea",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913fb",
                    "name": "Sihyuk Yi",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913fc",
                    "name": "Chansik Yoon",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913fd",
                    "name": "Dongkeun Yoon",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913fe",
                    "name": "Sangyeon Yoon",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ff",
                    "name": "Hyeongu Yun",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T02:30:59.000Z",
            "submittedOnDailyAt": "2026-01-06T01:03:14.011Z",
            "title": "K-EXAONE Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
            "upvotes": 41,
            "discussionId": "695c72356aa73bc11f091400",
            "githubRepo": "https://github.com/LG-AI-EXAONE/K-EXAONE",
            "githubRepoAddedBy": "user",
            "ai_summary": "K-EXAONE is a multilingual language model with a Mixture-of-Experts architecture that achieves competitive performance on various benchmarks while supporting multiple languages and long-context windows.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "256K-token context window",
                "multilingual language model",
                "parameter-efficient fine-tuning"
            ],
            "githubStars": 38,
            "organization": {
                "_id": "66a89bc1d96a5adbccbe85d4",
                "name": "LGAI-EXAONE",
                "fullname": "LG AI Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"
            }
        },
        "translation_title": "K-EXAONE 기술 보고서",
        "purpose": "산업 및 연구 응용 프로그램을 위한 강력한 AI 기반 모델 개발",
        "method": [
            "LG AI Research에 의해 개발된 K-EXAONE은 Mixture-of-Experts 아키텍처를 기반으로 하여 236B의 총 매개변수를 가진다.(K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters)",
            "추론 중 23B 매개변수만 활성화하여 256K-token의 컨텍스트 윈도우를 지원함.(activating 23B parameters during inference. It supports a 256K-token context window)",
            "여섯 개 언어(한국어, 영어, 스페인어, 독일어, 일본어, 베트남어)를 지원함.(covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese.)",
            "K-EXAONE을 포괄적인 벤치마크 평가로 평가함.(We evaluate K-EXAONE on a comprehensive benchmark suite)"
        ],
        "conclusion": "K-EXAONE은 유사한 크기의 오픈 웨이트 모델과 동등한 성능을 보여주며, AI 발전을 통해 더 나은 삶을 추구하는 모델로 자리 잡음.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2601.02204",
            "authors": [
                {
                    "_id": "695c7d0d6aa73bc11f091433",
                    "name": "Huichao Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091434",
                    "user": {
                        "_id": "64b796079ebb7e6c7ddcdabf",
                        "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg",
                        "isPro": false,
                        "fullname": "Liao Qu",
                        "user": "leo1117",
                        "type": "user"
                    },
                    "name": "Liao Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:57:57.686Z",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091435",
                    "name": "Yiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091436",
                    "name": "Hang Chen",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091437",
                    "name": "Yangyang Song",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091438",
                    "name": "Yongsheng Dong",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091439",
                    "name": "Shikun Sun",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09143a",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09143b",
                    "name": "Xu Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09143c",
                    "user": {
                        "_id": "6344dcb1cd37e44d9ed46508",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg",
                        "isPro": false,
                        "fullname": "Yi Jiang",
                        "user": "JiangYi",
                        "type": "user"
                    },
                    "name": "Yi Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:57:55.158Z",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09143d",
                    "name": "Hu Ye",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09143e",
                    "name": "Bo Chen",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09143f",
                    "name": "Yiming Gao",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091440",
                    "name": "Peng Liu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091441",
                    "name": "Akide Liu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091442",
                    "name": "Zhipeng Yang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091443",
                    "name": "Qili Deng",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091444",
                    "name": "Linjie Xing",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091445",
                    "name": "Jiyang Liu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091446",
                    "name": "Zhao Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091447",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091448",
                    "name": "Mingcong Liu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091449",
                    "name": "Yi Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09144a",
                    "name": "Qian He",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09144b",
                    "name": "Xiwei Hu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09144c",
                    "name": "Zhongqi Qi",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09144d",
                    "name": "Jie Shao",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09144e",
                    "name": "Zhiye Fu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09144f",
                    "name": "Shuai Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091450",
                    "name": "Fangmin Chen",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091451",
                    "name": "Xuezhi Chai",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091452",
                    "name": "Zhihua Wu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091453",
                    "name": "Yitong Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091454",
                    "name": "Zehuan Yuan",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091455",
                    "name": "Daniel K. Du",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091456",
                    "name": "Xinglong Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T15:27:04.000Z",
            "submittedOnDailyAt": "2026-01-06T00:52:35.953Z",
            "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
            "submittedOnDailyBy": {
                "_id": "64b796079ebb7e6c7ddcdabf",
                "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg",
                "isPro": false,
                "fullname": "Liao Qu",
                "user": "leo1117",
                "type": "user"
            },
            "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.",
            "upvotes": 37,
            "discussionId": "695c7d0d6aa73bc11f091457",
            "githubRepo": "https://github.com/ByteVisionLab/NextFlow",
            "githubRepoAddedBy": "user",
            "ai_summary": "NextFlow is a unified decoder-only autoregressive transformer that processes interleaved text-image tokens, enabling fast multimodal generation through novel next-token and next-scale prediction strategies.",
            "ai_keywords": [
                "decoder-only autoregressive transformer",
                "interleaved text-image discrete tokens",
                "unified vision representation",
                "multimodal understanding",
                "multimodal generation",
                "next-token prediction",
                "next-scale prediction",
                "raster-scan methods",
                "visual generation",
                "prefix-tuning strategy",
                "reinforcement learning",
                "diffusion baselines"
            ],
            "githubStars": 39,
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "translation_title": "NextFlow: 통합된 순차 모델링이 다중 모드 이해 및 생성을 활성화하다",
        "purpose": "다중 모드 이해와 생성 기능을 활성화하기 위한 통합된 오토리그레시브 트랜스포머 개발",
        "method": [
            "NextFlow는 6조 개의 텍스트-이미지 이산 토큰으로 훈련된 통합된 디코더 전용 오토리그레시브 트랜스포머임(We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens.)",
            "텍스트에 대해서는 다음 토큰 예측을, 시각 생성에 대해서는 다음 스케일 예측을 채택함(Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation.)",
            "전통적인 래스터 스캔 방식에서 벗어나 1024x1024 이미지를 5초 만에 생성하는 성능을 달성함(This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds.)"
        ],
        "conclusion": "NextFlow는 통합된 모델 중에서 최첨단 성능을 달성하고, 시각적 품질 면에서 전문화된 확산 기초 모델과 견줄 수 있는 결과를 보임.",
        "keywords": [
            "Multimodal Learning",
            "Image Generation",
            "Video Generation"
        ]
    },
    {
        "paper": {
            "id": "2601.01425",
            "authors": [
                {
                    "_id": "695c765d6aa73bc11f091402",
                    "name": "Xu Guo",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091403",
                    "name": "Fulong Ye",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091404",
                    "name": "Xinghui Li",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091405",
                    "name": "Pengqi Tu",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091406",
                    "name": "Pengze Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091407",
                    "user": {
                        "_id": "674566cb79d6f3a9da7be0de",
                        "avatarUrl": "/avatars/b6a5384820e150405039aa2b9badac29.svg",
                        "isPro": false,
                        "fullname": "Qichao Sun",
                        "user": "Simons212",
                        "type": "user"
                    },
                    "name": "Qichao Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:07.336Z",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091408",
                    "name": "Songtao Zhao",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091409",
                    "name": "Xiangwang Hou",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f09140a",
                    "name": "Qian He",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"
            ],
            "publishedAt": "2026-01-04T08:07:11.000Z",
            "submittedOnDailyAt": "2026-01-06T00:30:06.666Z",
            "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
            "submittedOnDailyBy": {
                "_id": "67d50738fed7787297d737d6",
                "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg",
                "isPro": false,
                "fullname": "xuguo",
                "user": "XuGuo699",
                "type": "user"
            },
            "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.",
            "upvotes": 33,
            "discussionId": "695c765d6aa73bc11f09140b",
            "projectPage": "https://guoxu1233.github.io/DreamID-V/",
            "githubRepo": "https://github.com/bytedance/DreamID-V",
            "githubRepoAddedBy": "user",
            "ai_summary": "A novel video face swapping framework combines image face swapping techniques with diffusion transformers and curriculum learning to achieve superior identity preservation and visual realism.",
            "ai_keywords": [
                "Video Face Swapping",
                "Image Face Swapping",
                "diffusion transformer",
                "Modality-Aware Conditioning",
                "Synthetic-to-Real Curriculum",
                "Identity-Coherence Reinforcement Learning",
                "IDBench-V",
                "Identity-Anchored Video Synthesizer",
                "bidirectional ID quadruplets",
                "multi-model conditions"
            ],
            "githubStars": 55,
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "translation_title": "DreamID-V: 고충실도 얼굴 스와핑을 위한 이미지-비디오 격차 해소",
        "purpose": "Video Face Swapping(VFS)에서 원본 정보와 특성을 보존하면서도 변별력 있게 얼굴 정체성을 이전하기 위한 새로운 방법을 제안.",
        "method": [
            "새로운 데이터 파이프라인 SyncID-Pipe를 소개하여 ID 고정 비디오 생성기를 사전 학습하고 IFS 모델과 결합해 명확한 감독을 위한 ID 쌍을 생성함.(We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision.)",
            "Diffusion Transformer 기반의 DreamID-V 프레임워크를 제안하고, 다중 모델 조건을 주입하기 위해 Modality-Aware Conditioning 모듈을 사용함.(We propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions.)",
            "Synthetic-to-Real Curriculum 메커니즘과 Identity-Coherence Reinforcement Learning 전략을 통해 시각적 사실성과 정체성 일관성을 향상시킴.(Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios.)"
        ],
        "conclusion": "DreamID-V는 기존 방법들보다 우수한 성능을 보여주며 다양한 스와핑 관련 작업에 쉽게 적용될 수 있는 뛰어난 versatility를 가짐.",
        "keywords": [
            "Image Generation",
            "Video Generation",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2601.02256",
            "authors": [
                {
                    "_id": "695c7d256aa73bc11f091459",
                    "name": "Shikun Sun",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f09145a",
                    "user": {
                        "_id": "64b796079ebb7e6c7ddcdabf",
                        "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg",
                        "isPro": false,
                        "fullname": "Liao Qu",
                        "user": "leo1117",
                        "type": "user"
                    },
                    "name": "Liao Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:57:53.171Z",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f09145b",
                    "name": "Huichao Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f09145c",
                    "name": "Yiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f09145d",
                    "name": "Yangyang Song",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f09145e",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f09145f",
                    "name": "Xu Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f091460",
                    "name": "Yi Jiang",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f091461",
                    "name": "Daniel K. Du",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f091462",
                    "name": "Xinglong Wu",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f091463",
                    "name": "Jia Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T16:36:40.000Z",
            "submittedOnDailyAt": "2026-01-06T00:51:20.686Z",
            "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
            "submittedOnDailyBy": {
                "_id": "64b796079ebb7e6c7ddcdabf",
                "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg",
                "isPro": false,
                "fullname": "Liao Qu",
                "user": "leo1117",
                "type": "user"
            },
            "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.",
            "upvotes": 26,
            "discussionId": "695c7d266aa73bc11f091464",
            "ai_summary": "Visual autoregressive models face training instability due to asynchronous policy conflicts, which are addressed through a novel framework enhancing group relative policy optimization with intermediate rewards, dynamic time-step reweighting, and mask propagation algorithms.",
            "ai_keywords": [
                "AutoRegressive",
                "diffusion",
                "Visual AutoRegressive",
                "reinforcement learning",
                "Group Relative Policy Optimization",
                "intermediate reward",
                "dynamic time-step reweighting",
                "mask propagation",
                "Reward Feedback Learning",
                "visual generation"
            ],
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "translation_title": "VAR RL을 올바르게 수행하기: 시각적 자기 회귀 생성에서 비동기 정책 충돌 해결하기",
        "purpose": "Visual AutoRegressive (VAR) 모델의 생성 과정에서 발생하는 비동기 정책 충돌 문제를 해결하여 훈련의 안정성과 정합성을 개선하려는 목표",
        "method": [
            "Group Relative Policy Optimization (GRPO)를 향상시키기 위한 새로운 프레임워크를 제안함(we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts.)",
            "초기 단계 생성을 유도하기 위한 안정화 중간 보상을 통합함(our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation;)",
            "정확한 신용 할당을 위한 동적 시간 단계 재조정 방식을 적용함; 3) 최적화 효과를 공간적 및 시간적으로 분리하기 위한 Reward Feedback Learning (ReFL) 원리에 기반한 새로운 마스크 전파 알고리즘을 설계함(3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally.)"
        ],
        "conclusion": "제안한 방법은 샘플 품질과 목표 정합성에서 기존 GRPO 기준 대비 크게 개선된 결과를 보여, VAR 모델을 위한 강력하고 효과적인 최적화를 가능케 함.",
        "keywords": [
            "Computer Vision",
            "Image Generation",
            "Reinforcement Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.24138",
            "authors": [
                {
                    "_id": "695c9b756aa73bc11f0914fb",
                    "name": "Haoran He",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f0914fc",
                    "name": "Yuxiao Ye",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f0914fd",
                    "name": "Jie Liu",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f0914fe",
                    "name": "Jiajun Liang",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f0914ff",
                    "name": "Zhiyong Wang",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f091500",
                    "name": "Ziyang Yuan",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f091501",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f091502",
                    "name": "Hangyu Mao",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f091503",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f091504",
                    "name": "Ling Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-30T10:55:45.000Z",
            "submittedOnDailyAt": "2026-01-06T02:53:45.651Z",
            "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
            "submittedOnDailyBy": {
                "_id": "6672937ceac0fb1b9e516595",
                "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
                "isPro": false,
                "fullname": "haoran he",
                "user": "haoranhe",
                "type": "user"
            },
            "summary": "Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.",
            "upvotes": 21,
            "discussionId": "695c9b766aa73bc11f091505",
            "projectPage": "https://tinnerhrhe.github.io/gardo_project/",
            "githubRepo": "https://github.com/tinnerhrhe/GARDO",
            "githubRepoAddedBy": "user",
            "ai_summary": "Online reinforcement learning for diffusion model fine-tuning suffers from reward hacking due to proxy reward mismatches, which GARDO addresses through selective regularization, adaptive reference updates, and diversity-aware reward amplification.",
            "ai_keywords": [
                "diffusion models",
                "reinforcement learning",
                "reward hacking",
                "proxy reward",
                "regularization",
                "online policy",
                "reference policy",
                "mode collapse",
                "diversity-aware optimization",
                "adaptive regularization",
                "sample efficiency",
                "exploration"
            ],
            "githubStars": 18
        },
        "translation_title": "GARDO: 보상 해킹 없이 확산 모델 강화하기",
        "purpose": "텍스트-이미지 정렬을 개선하기 위해 온라인 강화 학습을 통한 확산 모델의 미세 조정 연구",
        "method": [
            "GARDO라는 다목적 프레임워크를 제안하여 샘플 효율성, 효과적인 탐색, 보상 해킹 방지의 경쟁 요구사항을 충족함(To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO).)",
            "정확한 기준 모델을 사용하여 불확실성이 높은 샘플 집합에 대해서만 선택적으로 패널티를 부여함(our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty.)",
            "원하는 보상 목표를 보장하기 위해 reference 모델을 주기적으로 업데이트하여 현재 정책의 능력에 맞추는 적응형 정규화 메커니즘을 도입함(GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy)."
        ],
        "conclusion": "GARDO는 보상 해킹을 완화하고 생성 다양성을 증가시키면서도 샘플 효율성과 탐색을 희생하지 않는 효과와 강 robustness를 보여줌.",
        "keywords": [
            "Image Generation",
            "Video Generation",
            "Reinforcement Learning"
        ]
    }
]