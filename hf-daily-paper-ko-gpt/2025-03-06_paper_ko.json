[
    {
        "paper": {
            "id": "2503.00865",
            "authors": [
                {
                    "_id": "67c666245e2443d7d5e9b76a",
                    "user": {
                        "_id": "64802face9ff472e30dc1ceb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
                        "isPro": false,
                        "fullname": "Yiran Zhao",
                        "user": "Yiran0924",
                        "type": "user"
                    },
                    "name": "Yiran Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-04T08:51:21.231Z",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b76b",
                    "user": {
                        "_id": "61657b0b20606e5e73f611cc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61657b0b20606e5e73f611cc/6ZPne2GYlWkxrx35ND1P8.png",
                        "isPro": false,
                        "fullname": "CHAOQUN LIU",
                        "user": "lukecq",
                        "type": "user"
                    },
                    "name": "Chaoqun Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T09:27:33.956Z",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b76c",
                    "name": "Yue Deng",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b76d",
                    "user": {
                        "_id": "671609f7664f44a151f1f0e8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fEQLuH1kdW5Pd9Y_J64hN.png",
                        "isPro": false,
                        "fullname": "jiahao ying",
                        "user": "jhying",
                        "type": "user"
                    },
                    "name": "Jiahao Ying",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T09:35:39.926Z",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b76e",
                    "user": {
                        "_id": "6539c87ba318a98bf0d15dd8",
                        "avatarUrl": "/avatars/beb9ba6eeacb61addc5897836bd59f55.svg",
                        "isPro": false,
                        "fullname": "Mahani Aljunied",
                        "user": "maljunied",
                        "type": "user"
                    },
                    "name": "Mahani Aljunied",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T09:35:33.285Z",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b76f",
                    "name": "Zhaodonghui Li",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b770",
                    "user": {
                        "_id": "6454685a548f22be598414c4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg",
                        "isPro": false,
                        "fullname": "Lidong Bing",
                        "user": "LidongBing",
                        "type": "user"
                    },
                    "name": "Lidong Bing",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T09:35:19.611Z",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b771",
                    "user": {
                        "_id": "604f67ef0fe8ff3ec13d71ef",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
                        "isPro": false,
                        "fullname": "Hou Pong (Ken) Chan",
                        "user": "kenchan0226",
                        "type": "user"
                    },
                    "name": "Hou Pong Chan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T09:35:50.272Z",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b772",
                    "name": "Yu Rong",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b773",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b774",
                    "user": {
                        "_id": "60dff6ae19a362a8c27862aa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60dff6ae19a362a8c27862aa/LIYzLB3cdPh-B3XIBgBCC.jpeg",
                        "isPro": false,
                        "fullname": "Wenxuan Zhang",
                        "user": "isakzhang",
                        "type": "user"
                    },
                    "name": "Wenxuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T09:27:36.769Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-02T11:53:55.000Z",
            "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of\n  Global Speakers",
            "summary": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), yet open-source multilingual LLMs remain scarce, with existing models\noften limited in language coverage. Such models typically prioritize\nwell-resourced languages, while widely spoken but under-resourced languages are\noften overlooked. To address this disparity, we introduce Babel, an\nopen multilingual LLM that covers the top 25 languages by number of speakers,\nsupports over 90% of the global population, and includes many languages\nneglected by other open multilingual LLMs. Unlike traditional continue\npretraining approaches, Babel expands its parameter count through a layer\nextension technique that elevates Babel's performance ceiling. We introduce two\nvariants: Babel-9B, designed for efficient inference and\nfine-tuning, and Babel-83B, which sets a new standard for open\nmultilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its\nsuperior performance compared to open LLMs of comparable size. In addition,\nusing open-source supervised fine-tuning datasets, Babel achieves remarkable\nperformance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat\nsetting a new standard for multilingual tasks, reaching the same level of\ncommercial models.",
            "upvotes": 37,
            "discussionId": "67c666255e2443d7d5e9b7b3",
            "projectPage": "https://babel-llm.github.io/babel-llm/",
            "githubRepo": "https://github.com/babel-llm/babel-llm"
        },
        "translation_title": "Babel: 90% 이상의 전 세계 화자를 위한 열린 다국어 대형 언어 모델",
        "purpose": "다국어 처리에서 널리 사용되지 않는 언어를 포함한 다국어 대형 언어 모델을 구축하여 언어 간 불균형 문제 해결",
        "method": [
            "Babel이라는 개방형 다국어 LLM을 소개하여, 상위 25개 언어를 지원하고 90% 이상의 글로벌 인구를 포괄함(To address this disparity, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supports over 90% of the global population.)",
            "파라미터 수를 확장하는 레이어 확장 기술을 통해 Babel의 성능을 높임(Unlike traditional continue pretraining approaches, Babel expands its parameter count through a layer extension technique that elevates Babel's performance ceiling.)",
            "Babel의 두 가지 변형인 Babel-9B와 Babel-83B를 설계하여 효율적인 추론 및 세밀 조정과 새로운 표준을 설정함(We introduce two variants: Babel-9B, designed for efficient inference and fine-tuning, and Babel-83B, which sets a new standard for open multilingual LLMs.)"
        ],
        "conclusion": "Babel은 다국어 작업에서 뛰어난 성능을 발휘하며, 상용 모델과 동등한 수준을 달성함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2503.03746",
            "authors": [
                {
                    "_id": "67c991abe7dba9cdf9f41e67",
                    "name": "Shimao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c991abe7dba9cdf9f41e68",
                    "user": {
                        "_id": "63fb6e281b4b1bd4e7ffc5be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liu",
                        "user": "lx865712528",
                        "type": "user"
                    },
                    "name": "Xiao Liu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-06T12:14:52.016Z",
                    "hidden": false
                },
                {
                    "_id": "67c991abe7dba9cdf9f41e69",
                    "name": "Xin Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c991abe7dba9cdf9f41e6a",
                    "name": "Junxiao Liu",
                    "hidden": false
                },
                {
                    "_id": "67c991abe7dba9cdf9f41e6b",
                    "name": "Zheheng Luo",
                    "hidden": false
                },
                {
                    "_id": "67c991abe7dba9cdf9f41e6c",
                    "name": "Shujian Huang",
                    "hidden": false
                },
                {
                    "_id": "67c991abe7dba9cdf9f41e6d",
                    "name": "Yeyun Gong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-05T18:58:44.000Z",
            "title": "Process-based Self-Rewarding Language Models",
            "summary": "Large Language Models have demonstrated outstanding performance across\nvarious downstream tasks and have been widely applied in multiple scenarios.\nHuman-annotated preference data is used for training to further improve LLMs'\nperformance, which is constrained by the upper limit of human performance.\nTherefore, Self-Rewarding method has been proposed, where LLMs generate\ntraining data by rewarding their own outputs. However, the existing\nself-rewarding paradigm is not effective in mathematical reasoning scenarios\nand may even lead to a decline in performance. In this work, we propose the\nProcess-based Self-Rewarding pipeline for language models, which introduces\nlong-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference\noptimization within the self-rewarding paradigm. Our new paradigm successfully\nenhances the performance of LLMs on multiple mathematical reasoning benchmarks\nthrough iterative Process-based Self-Rewarding, demonstrating the immense\npotential of self-rewarding to achieve LLM reasoning that may surpass human\ncapabilities.",
            "upvotes": 14,
            "discussionId": "67c991abe7dba9cdf9f41e96"
        },
        "translation_title": "프로세스 기반 자기 보상 언어 모델",
        "purpose": "언어 모델의 성능을 개선하기 위한 새로운 자기 보상 방법 제안",
        "method": [
            "자기 보상 패러다임 내에서 긴 사고 과정을 도입함(we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning).",
            "단계별로 LLM을 심사자로 활용하고 단계별 선호 최적화를 수행함(introduces step-wise LLM-as-a-Judge, and step-wise preference optimization).",
            "프로세스 기반 자기 보상을 통해 언어 모델의 수학적 추론 성능을 향상시킴(Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding)."
        ],
        "conclusion": "새로운 자기 보상 방법이 LLM의 성능을 향상시켜 인간의 능력을 초월하는 추론을 달성할 가능성을 보여줌.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2503.00329",
            "authors": [
                {
                    "_id": "67c755f898a2e37274c62c96",
                    "name": "Benjamin Schneider",
                    "hidden": false
                },
                {
                    "_id": "67c755f898a2e37274c62c97",
                    "name": "Florian Kerschbaum",
                    "hidden": false
                },
                {
                    "_id": "67c755f898a2e37274c62c98",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T09:50:07.881Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-01T03:29:02.000Z",
            "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
            "summary": "Visual embedding models excel at zero-shot tasks like visual retrieval and\nclassification. However, these models cannot be used for tasks that contain\nambiguity or require user instruction. These tasks necessitate a multimodal\nembedding model, which outputs embeddings that combine visual and natural\nlanguage input. Existing CLIP-based approaches embed images and text\nindependently, and fuse the result. We find that this results in weak\ninteractions between modalities, and poor user control over the representation.\nWe introduce ABC, an open-source multimodal embedding model that uses a\nvision-language model backbone to deeply integrate image features with natural\nlanguage instructions. ABC achieves bestfor-size performance on MSCOCO\nimage-to-text retrieval and is the top performing model on classification and\nVQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly\nunified vision-language representation, ABC can use natural language to solve\nsubtle and potentially ambiguous visual retrieval problems. To evaluate this\ncapability, we design CtrlBench, a benchmark that requires interleaving textual\ninstructions with image content for correct retrieval. ABC advances the state\nof multimodal embeddings by offering high-quality representations and flexible\nnatural language control. Our model and datasets are available at our project\npage.",
            "upvotes": 12,
            "discussionId": "67c7560298a2e37274c6311d",
            "projectPage": "https://tiger-ai-lab.github.io/ABC/",
            "githubRepo": "https://github.com/TIGER-AI-Lab/ABC"
        },
        "translation_title": "ABC: VLM을 활용한 멀티모달 임베딩의 더 나은 제어 달성",
        "purpose": "모델이 시각적과 자연어 입력을 통합하여 사용자 제어를 개선하는 멀티모달 임베딩을 연구",
        "method": [
            "VLM 기반의 ABC 라는 오픈 소스 멀티모달 임베딩 모델을 도입하여 이미지 특성을 자연어 지침과 깊이 통합함(We introduce ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions.)",
            "ABC는 MS-COCO 이미지-텍스트 검색에서 최고의 성과를 내고 분류 및 VQA 작업에서 최상위 성능을 기록함(ABC achieves best-for-size performance on MSCOCO image-to-text retrieval and is the top performing model on classification and VQA tasks in the Massive Multimodal Embedding Benchmark.)",
            "CtrlBench라는 벤치마크를 설계하여 올바른 검색을 위해 이미지 콘텐츠와 텍스트 지침을 교차로 요구함(To evaluate this capability, we design CtrlBench, a benchmark that requires interleaving textual instructions with image content for correct retrieval.)"
        ],
        "conclusion": "ABC는 고품질의 멀티모달 표현과 유연한 자연어 제어를 제공함으로써 멀티모달 임베딩의 수준을 향상시킴.",
        "keywords": [
            "Multimodal Learning",
            "Vision-Language Models",
            "Image Classification"
        ]
    },
    {
        "paper": {
            "id": "2503.03751",
            "authors": [
                {
                    "_id": "67c912b1b5903dd437cc2370",
                    "user": {
                        "_id": "658529d61c461dfe88afe8e8",
                        "avatarUrl": "/avatars/a22c1b07d28c2662833c462c6537d835.svg",
                        "isPro": false,
                        "fullname": "Xuanchi Ren",
                        "user": "xrenaa",
                        "type": "user"
                    },
                    "name": "Xuanchi Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T09:55:04.321Z",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2371",
                    "name": "Tianchang Shen",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2372",
                    "name": "Jiahui Huang",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2373",
                    "name": "Huan Ling",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2374",
                    "name": "Yifan Lu",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2375",
                    "name": "Merlin Nimier-David",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2376",
                    "name": "Thomas Müller",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2377",
                    "name": "Alexander Keller",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2378",
                    "name": "Sanja Fidler",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2379",
                    "name": "Jun Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-05T18:59:50.000Z",
            "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
            "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
            "upvotes": 10,
            "discussionId": "67c912b9b5903dd437cc2505"
        },
        "translation_title": "GEN3C: 정밀 카메라 제어를 통한 3D 기반 일관된 비디오 생성",
        "purpose": "정확한 카메라 제어와 시간적 3D 일관성을 가진 비디오 생성 모델 개선",
        "method": [
            "3D 캐시를 통해 2D 렌더링을 사용하여 새로운 카메라 궤적에 따라 다음 프레임을 생성함(When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user.)",
            "3D 정보 예측을 통해 이전에 생성된 프레임이나 시드 이미지에서 점군(point clouds)을 획득함(point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames.)",
            "이 모델은 이전에 생성한 내용을 기억할 필요 없이 새로운 영역에 집중하며 다음 프레임으로 장면 상태를 진행함(The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame.)"
        ],
        "conclusion": "GEN3C는 이전 모델보다 더 정밀한 카메라 제어를 제공하며, 드라이빙 장면 및 단안 동적 비디오와 같은 어려운 설정에서도 최첨단 성능을 보임.",
        "keywords": [
            "Video Generation",
            "3D Vision",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2503.02951",
            "authors": [
                {
                    "_id": "67c907ea7568a12737ad4535",
                    "user": {
                        "_id": "653df1323479e9ebbe3eb6cc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
                        "isPro": true,
                        "fullname": "Zhangchen Xu",
                        "user": "flydust",
                        "type": "user"
                    },
                    "name": "Zhangchen Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T09:26:50.636Z",
                    "hidden": false
                },
                {
                    "_id": "67c907ea7568a12737ad4536",
                    "user": {
                        "_id": "637c88b6d55081513c5690d8",
                        "avatarUrl": "/avatars/6766e23ebf46b46d6c8b48351c571907.svg",
                        "isPro": false,
                        "fullname": "Yang Liu",
                        "user": "nlpyang",
                        "type": "user"
                    },
                    "name": "Yang Liu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-06T02:26:54.940Z",
                    "hidden": false
                },
                {
                    "_id": "67c907ea7568a12737ad4537",
                    "user": {
                        "_id": "605e8dfd5abeb13e714c4c18",
                        "avatarUrl": "/avatars/bc27a0ed17b2bd4311e89d3028fa327b.svg",
                        "isPro": false,
                        "fullname": "yueqin yin",
                        "user": "yyqoni",
                        "type": "user"
                    },
                    "name": "Yueqin Yin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T09:26:48.614Z",
                    "hidden": false
                },
                {
                    "_id": "67c907ea7568a12737ad4538",
                    "user": {
                        "_id": "653b2524b77b5e255f2d29d2",
                        "avatarUrl": "/avatars/f69aea8de84c435295e7638bad5bd82e.svg",
                        "isPro": false,
                        "fullname": "Mingyuan Zhou",
                        "user": "mingyuanzhou",
                        "type": "user"
                    },
                    "name": "Mingyuan Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T10:03:56.474Z",
                    "hidden": false
                },
                {
                    "_id": "67c907ea7568a12737ad4539",
                    "name": "Radha Poovendran",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T19:17:36.000Z",
            "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for\n  Coding",
            "summary": "We introduce KodCode, a synthetic dataset that addresses the persistent\nchallenge of acquiring high-quality, verifiable training data across diverse\ndifficulties and domains for training Large Language Models for coding.\nExisting code-focused resources typically fail to ensure either the breadth of\ncoverage (e.g., spanning simple coding tasks to advanced algorithmic problems)\nor verifiable correctness (e.g., unit tests). In contrast, KodCode comprises\nquestion-solution-test triplets that are systematically validated via a\nself-verification procedure. Our pipeline begins by synthesizing a broad range\nof coding questions, then generates solutions and test cases with additional\nattempts allocated to challenging problems. Finally, post-training data\nsynthesis is done by rewriting questions into diverse formats and generating\nresponses under a test-based reject sampling procedure from a reasoning model\n(DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding\ndataset. KodCode is suitable for supervised fine-tuning and the paired unit\ntests also provide great potential for RL tuning. Fine-tuning experiments on\ncoding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench)\ndemonstrate that KodCode-tuned models achieve state-of-the-art performance,\nsurpassing models like Qwen2.5-Coder-32B-Instruct and\nDeepSeek-R1-Distill-Llama-70B.",
            "upvotes": 10,
            "discussionId": "67c907ee7568a12737ad4633",
            "projectPage": "https://kodcode-ai.github.io/",
            "githubRepo": "https://github.com/KodCode-AI/kodcode"
        },
        "translation_title": "KodCode: 코딩을 위한 다양하고 도전적이며 검증 가능한 합성 데이터셋",
        "purpose": "다양한 난이도와 도메인에서 고품질의 검증 가능한 훈련 데이터를 확보하기 위한 합성 데이터셋 개발",
        "method": [
            "폭넓은 코딩 질문을 합성하여 시작하고, 그에 대한 솔루션과 테스트 케이스를 생성함(Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems.)",
            "질문을 다양한 형식으로 다시 작성하고, 추론 모델을 활용한 테스트 기반 거부 샘플링 절차 아래에서 응답을 생성함(Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1).)",
            "자가 검증 절차를 통해 질문-솔루션-테스트 삼중항을 체계적으로 검증함(In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure.)"
        ],
        "conclusion": "KodCode로 튜닝된 모델은 HumanEval(+), MBPP(+), BigCodeBench, LiveCodeBench와 같은 코딩 벤치마크에서 최첨단 성능을 달성하며, Qwen2.5-Coder-32B-Instruct 및 DeepSeek-R1-Distill-Llama-70B 모델을 초월함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Robotics"
        ]
    }
]