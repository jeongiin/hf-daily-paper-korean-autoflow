[
    {
        "paper": {
            "id": "2510.19338",
            "authors": [
                {
                    "_id": "68f98d4fb9b2e4ae046737dc",
                    "name": "Ling Team",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737dd",
                    "name": "Bin Han",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737de",
                    "name": "Caizhi Tang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737df",
                    "name": "Chen Liang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e0",
                    "name": "Donghao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e1",
                    "name": "Fan Yuan",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e2",
                    "name": "Feng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e3",
                    "name": "Jie Gao",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e4",
                    "user": {
                        "_id": "64993d522ca6f96c8b8988ca",
                        "avatarUrl": "/avatars/0a8087cbbbcf5772288fe60c2c47eadb.svg",
                        "isPro": false,
                        "fullname": "Jingyu, Hu",
                        "user": "hjyai94",
                        "type": "user"
                    },
                    "name": "Jingyu Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:49:07.705Z",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e5",
                    "user": {
                        "_id": "641c10cdbfe1bb9c4bce652f",
                        "avatarUrl": "/avatars/7ed954de541ac5bc80f58fb5db3866ea.svg",
                        "isPro": false,
                        "fullname": "longfei li",
                        "user": "long0x0",
                        "type": "user"
                    },
                    "name": "Longfei Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:49:12.772Z",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e6",
                    "name": "Meng Li",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e7",
                    "name": "Mingyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e8",
                    "name": "Peijie Jiang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737e9",
                    "name": "Peng Jiao",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737ea",
                    "user": {
                        "_id": "5fde26773930f07f74aaf912",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fde26773930f07f74aaf912/SD93SVyVCRcTJbogBcf9J.jpeg",
                        "isPro": false,
                        "fullname": "Qian Zhao",
                        "user": "im0qianqian",
                        "type": "user"
                    },
                    "name": "Qian Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:48:55.104Z",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737eb",
                    "name": "Qingyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737ec",
                    "name": "Wenbo Shen",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737ed",
                    "name": "Xinxing Yang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737ee",
                    "name": "Yalin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737ef",
                    "name": "Yankun Ren",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f0",
                    "name": "Yao Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f1",
                    "name": "Yibo Cao",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f2",
                    "name": "Yixuan Sun",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f3",
                    "user": {
                        "_id": "6430d8ad440a10209803d52f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6430d8ad440a10209803d52f/BDU_9ou3flPge_uwG_3b8.jpeg",
                        "isPro": false,
                        "fullname": "Yue Zhang",
                        "user": "York-Z",
                        "type": "user"
                    },
                    "name": "Yue Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:49:02.156Z",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f4",
                    "name": "Yuchen Fang",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f5",
                    "name": "Zibin Lin",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f6",
                    "name": "Zixuan Cheng",
                    "hidden": false
                },
                {
                    "_id": "68f98d4fb9b2e4ae046737f7",
                    "name": "Jun Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T07:59:38.000Z",
            "submittedOnDailyAt": "2025-10-23T00:35:16.243Z",
            "title": "Every Attention Matters: An Efficient Hybrid Architecture for\n  Long-Context Reasoning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "In this technical report, we present the Ring-linear model series,\nspecifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.\nRing-mini-linear-2.0 comprises 16B parameters and 957M activations, while\nRing-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both\nmodels adopt a hybrid architecture that effectively integrates linear attention\nand softmax attention, significantly reducing I/O and computational overhead in\nlong-context inference scenarios. Compared to a 32 billion parameter dense\nmodel, this series reduces inference cost to 1/10, and compared to the original\nRing series, the cost is also reduced by over 50%. Furthermore, through\nsystematic exploration of the ratio between different attention mechanisms in\nthe hybrid architecture, we have identified the currently optimal model\nstructure. Additionally, by leveraging our self-developed high-performance FP8\noperator library-linghe, overall training efficiency has been improved by 50%.\nBenefiting from the high alignment between the training and inference engine\noperators, the models can undergo long-term, stable, and highly efficient\noptimization during the reinforcement learning phase, consistently maintaining\nSOTA performance across multiple challenging complex reasoning benchmarks.",
            "upvotes": 57,
            "discussionId": "68f98d50b9b2e4ae046737f8",
            "ai_summary": "The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency.",
            "ai_keywords": [
                "linear attention",
                "softmax attention",
                "hybrid architecture",
                "FP8 operator library",
                "reinforcement learning",
                "SOTA performance"
            ],
            "organization": {
                "_id": "67c1d682826160b28f778510",
                "name": "antgroup",
                "fullname": "Ant Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
            }
        },
        "translation_title": "모든 Attention은 중요하다: 장기 맥락 추론을 위한 효율적인 하이브리드 아키텍처",
        "purpose": "장기 맥락에서의 추론 비용을 크게 줄이기 위한 효율적인 모델 구조 연구",
        "method": [
            "Ring-linear 모델 시리즈, 특히 Ring-mini-linear-2.0 (16B 파라미터, 957M 활성화)와 Ring-flash-linear-2.0 (104B 파라미터, 6.1B 활성화)을 제안함(In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.)",
            "하이브리드 아키텍처를 통해 linear attention과 softmax attention을 통합하여 장기 맥락 추론 시 I/O 및 계산 오버헤드를 크게 줄임(Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios.)",
            "다양한 attention 메커니즘 비율의 체계적 탐색을 통해 최적 모델 구조를 식별함(Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure.)",
            "FP8 연산자 라이브러리인 linghe를 활용해 전체 훈련 효율성을 50% 향상시킴(Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%.)"
        ],
        "conclusion": "이 모델들은 강화 학습 단계 동안 안정적이고 효율적인 최적화를 통해 다양한 복잡한 추론 벤치마크에서 SOTA 성능을 지속적으로 유지함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2510.18927",
            "authors": [
                {
                    "_id": "68f9a187b9b2e4ae04673870",
                    "name": "Zhiheng Xi",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673871",
                    "name": "Xin Guo",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673872",
                    "name": "Yang Nan",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673873",
                    "name": "Enyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673874",
                    "user": {
                        "_id": "68f9b5fa0313464f8c8132d1",
                        "avatarUrl": "/avatars/2bb01b4ef391140daa8299f6a55c19a3.svg",
                        "isPro": false,
                        "fullname": "Shen",
                        "user": "Vindicaters",
                        "type": "user"
                    },
                    "name": "Junrui Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:47:23.670Z",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673875",
                    "name": "Wenxiang Chen",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673876",
                    "name": "Jiaqi Liu",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673877",
                    "name": "Jixuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673878",
                    "name": "Zhihao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673879",
                    "name": "Honglin Guo",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae0467387a",
                    "name": "Xun Deng",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae0467387b",
                    "name": "Zhikai Lei",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae0467387c",
                    "name": "Miao Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae0467387d",
                    "name": "Guoteng Wang",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae0467387e",
                    "name": "Shuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae0467387f",
                    "name": "Peng Sun",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673880",
                    "name": "Rui Zheng",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673881",
                    "name": "Hang Yan",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673882",
                    "name": "Tao Gui",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673883",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f9a187b9b2e4ae04673884",
                    "name": "Xuanjing Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-21T12:55:04.000Z",
            "submittedOnDailyAt": "2025-10-23T02:07:16.010Z",
            "title": "BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via\n  Balanced Policy Optimization with Adaptive Clipping",
            "submittedOnDailyBy": {
                "_id": "653a6e5cae155b92bae77b74",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg",
                "isPro": false,
                "fullname": "Zhiheng Xi",
                "user": "WooooDyy",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has recently become the core paradigm for\naligning and strengthening large language models (LLMs). Yet, applying RL in\noff-policy settings--where stale data from past policies are used for\ntraining--improves sample efficiency, but remains challenging: policy entropy\ndeclines sharply, optimization often becomes unstable and may even collapse.\nThrough theoretical and empirical analysis, we identify two key insights: (i)\nan imbalance in optimization, where negative-advantage samples dominate the\npolicy gradient, suppressing useful behaviors and risking gradient explosions;\nand (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping\nmechanism in PPO-like objectives systematically blocks entropy-increasing\nupdates, thereby driving the policy toward over-exploitation at the expense of\nexploration. Building on these insights, we propose BAlanced Policy\nOptimization with Adaptive Clipping (BAPO), a simple yet effective method that\ndynamically adjusts clipping bounds to adaptively re-balance positive and\nnegative contributions, preserve entropy, and stabilize RL optimization. Across\ndiverse off-policy scenarios--including sample replay and partial rollout--BAPO\nachieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025\nbenchmarks, our 7B BAPO model surpasses open-source counterparts such as\nSkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art\nresults among models of the same scale but also outperforms leading proprietary\nsystems like o3-mini and Gemini-2.5-Flash-Thinking.",
            "upvotes": 57,
            "discussionId": "68f9a187b9b2e4ae04673885",
            "projectPage": "https://github.com/WooooDyy/BAPO",
            "githubRepo": "https://github.com/WooooDyy/BAPO",
            "ai_summary": "BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models.",
            "ai_keywords": [
                "reinforcement learning",
                "large language models",
                "off-policy settings",
                "policy entropy",
                "policy gradient",
                "Entropy-Clip Rule",
                "PPO-like objectives",
                "gradient explosions",
                "sample replay",
                "partial rollout",
                "AIME 2024",
                "AIME 2025",
                "SkyWork-OR1-7B",
                "o3-mini",
                "Gemini-2.5-Flash-Thinking"
            ],
            "githubStars": 48
        },
        "translation_title": "BAPO: 균형 잡힌 정책 최적화와 적응형 클리핑으로 LLM을 위한 오프정책 강화 학습 안정화",
        "purpose": "대규모 언어 모델(LLM)의 샘플 효율성을 높이기 위해 오프정책 강화 학습의 안정성 문제를 해결하려는 연구",
        "method": [
            "이론적 및 경험적 분석을 통해 최적화의 불균형 문제와 그로 인한 정책 손실을 파악함(Through theoretical and empirical analysis, we identify two key insights: a key insight regarding optimization imbalance.)",
            "Entropy-Clip Rule을 도출하여 기존 PPO 목표에서 엔트로피 증가 업데이트가 차단된다는 사실을 발견함(The derived Entropy-Clip Rule reveals that fixed clipping mechanism blocks entropy-increasing updates.)",
            "BAPO라는 방법을 제안하여 클리핑 범위를 동적으로 조정하여 긍정적 및 부정적 기여를 다시 균형 잡고, 엔트로피를 보존하며 RL 최적화를 안정화함(Building on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), a method that dynamically adjusts clipping bounds.)"
        ],
        "conclusion": "BAPO는 다양한 오프정책 시나리오에서 빠르고 안정적이며 데이터 효율적인 학습을 달성하며, AIME 2024 및 AIME 2025 벤치마크에서 높은 성능을 보임.",
        "keywords": [
            "Reinforcement Learning",
            "Large Language Models",
            "Policy Optimization"
        ]
    },
    {
        "paper": {
            "id": "2510.19363",
            "authors": [
                {
                    "_id": "68f9884db9b2e4ae0467374e",
                    "user": {
                        "_id": "6495b0b844bc2e9ce6cc849b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/j6aucl_tefMHwtD-bdUAw.jpeg",
                        "isPro": false,
                        "fullname": "Siyuan Wang",
                        "user": "OldKingMeister",
                        "type": "user"
                    },
                    "name": "Siyuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:51:12.876Z",
                    "hidden": false
                },
                {
                    "_id": "68f9884db9b2e4ae0467374f",
                    "name": "Gaokai Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f9884db9b2e4ae04673750",
                    "name": "Li Lyna Zhang",
                    "hidden": false
                },
                {
                    "_id": "68f9884db9b2e4ae04673751",
                    "name": "Ning Shang",
                    "hidden": false
                },
                {
                    "_id": "68f9884db9b2e4ae04673752",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "68f9884db9b2e4ae04673753",
                    "name": "Dongyao Chen",
                    "hidden": false
                },
                {
                    "_id": "68f9884db9b2e4ae04673754",
                    "name": "Mao Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T08:35:28.000Z",
            "submittedOnDailyAt": "2025-10-23T00:18:13.806Z",
            "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts",
            "submittedOnDailyBy": {
                "_id": "62b0009c72043b05d29492b2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
                "isPro": false,
                "fullname": "Li Lyna Zhang",
                "user": "lynazhang",
                "type": "user"
            },
            "summary": "Reasoning over long contexts is essential for large language models. While\nreinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\"\nmoments in chain-of-thought, the advanced thinking patterns required for\nlong-context reasoning remain largely unexplored, and high-difficulty RL data\nare scarce. In this paper, we introduce LoongRL, a data-driven RL method for\nadvanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis\napproach that transforms short multi-hop QA into high-difficulty long-context\ntasks by inserting UUID chains that hide the true question among large\ncollections of distracting documents. Solving these tasks requires the model to\ntrace the correct chain step-by-step, identify the true question, retrieve\nrelevant facts and reason over them to answer correctly. RL training on\nKeyChain data induces an emergent plan-retrieve-reason-recheck reasoning\npattern that generalizes far beyond training length. Models trained at 16K\neffectively solve 128K tasks without prohibitive full-length RL rollout costs.\nOn Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA\naccuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches\na score of 74.2, rivaling much larger frontier models such as o3-mini (74.5)\nand DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all\n128K needle-in-a-haystack stress tests, and preserves short-context reasoning\ncapabilities.",
            "upvotes": 38,
            "discussionId": "68f9884db9b2e4ae04673755",
            "ai_summary": "LoongRL, a data-driven reinforcement learning method, enhances long-context reasoning by transforming short multi-hop QA into high-difficulty tasks, improving accuracy and generalization in large language models.",
            "ai_keywords": [
                "reinforcement learning",
                "LoongRL",
                "KeyChain",
                "multi-hop QA",
                "long-context reasoning",
                "plan-retrieve-reason-recheck",
                "RL rollout",
                "Qwen2.5-7B",
                "Qwen2.5-14B",
                "o3-mini",
                "DeepSeek-R1",
                "needle-in-a-haystack stress tests"
            ],
            "organization": {
                "_id": "68151d0f51add3813f3f7d1b",
                "name": "MicrosoftResearch",
                "fullname": "Microsoft Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
            }
        },
        "translation_title": "LoongRL: 긴 문맥에 대한 고급 추론을 위한 강화 학습",
        "purpose": "긴 문맥에서의 복잡한 사고 패턴을 탐구하고 고난이도 강화 학습 데이터 부족 문제를 해결하기 위한 연구",
        "method": [
            "LoongRL이라는 데이터 기반 강화 학습 방법을 도입하여 긴 문맥에서의 고급 추론을 지원함(In this paper, we introduce LoongRL, a data-driven RL method for advanced long-context reasoning.)",
            "KeyChain 접근 방식을 통해 짧은 다중-hop QA를 고난이도 긴 문맥 작업으로 변환함(Central to LoongRL is KeyChain, a synthesis approach that transforms short multi-hop QA into high-difficulty long-context tasks by inserting UUID chains that hide the true question among large collections of distracting documents.)",
            "KeyChain 데이터를 기반으로 모델 훈련을 통해 계획-검색-추론-재확인 추론 패턴을 유도함(RL training on KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning pattern that generalizes far beyond training length.)"
        ],
        "conclusion": "LoongRL은 긴 문맥의 다중-hop QA 정확도를 대폭 향상시켰으며, 짧은 문맥의 추론 능력도 유지함.",
        "keywords": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2510.19430",
            "authors": [
                {
                    "_id": "68f99e80b9b2e4ae04673853",
                    "name": "GigaBrain Team",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673854",
                    "name": "Angen Ye",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673855",
                    "name": "Boyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673856",
                    "name": "Chaojun Ni",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673857",
                    "name": "Guan Huang",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673858",
                    "name": "Guosheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673859",
                    "name": "Haoyun Li",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467385a",
                    "user": {
                        "_id": "679342914fa1841697cafac9",
                        "avatarUrl": "/avatars/217e8bd4fad85f486c4af83e29e12193.svg",
                        "isPro": false,
                        "fullname": "lijie",
                        "user": "danielleetrade",
                        "type": "user"
                    },
                    "name": "Jie Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:47:33.050Z",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467385b",
                    "name": "Jiagang Zhu",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467385c",
                    "name": "Lv Feng",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467385d",
                    "name": "Peng Li",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467385e",
                    "name": "Qiuping Deng",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467385f",
                    "name": "Runqi Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673860",
                    "name": "Wenkang Qin",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673861",
                    "name": "Xinze Chen",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673862",
                    "user": {
                        "_id": "6426616ea5ec4a5cbc535634",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/hH6JsxnXeakH3mBTeNNmO.jpeg",
                        "isPro": false,
                        "fullname": "JeffWang",
                        "user": "Jeff-Wang",
                        "type": "user"
                    },
                    "name": "Xiaofeng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T14:47:40.222Z",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673863",
                    "name": "Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673864",
                    "name": "Yifan Li",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673865",
                    "name": "Yilong Li",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673866",
                    "name": "Yiran Ding",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673867",
                    "name": "Yuan Xu",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673868",
                    "name": "Yun Ye",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae04673869",
                    "name": "Yukun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467386a",
                    "name": "Zhehao Dong",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467386b",
                    "name": "Zhenan Wang",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467386c",
                    "name": "Zhichao Liu",
                    "hidden": false
                },
                {
                    "_id": "68f99e80b9b2e4ae0467386d",
                    "name": "Zheng Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-22T09:57:13.000Z",
            "submittedOnDailyAt": "2025-10-23T03:11:44.473Z",
            "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
            "submittedOnDailyBy": {
                "_id": "6426616ea5ec4a5cbc535634",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/hH6JsxnXeakH3mBTeNNmO.jpeg",
                "isPro": false,
                "fullname": "JeffWang",
                "user": "Jeff-Wang",
                "type": "user"
            },
            "summary": "Training Vision-Language-Action (VLA) models for generalist robots typically\nrequires large-scale real-world robot data, which is expensive and\ntime-consuming to collect. The inefficiency of physical data collection\nseverely limits the scalability, and generalization capacity of current VLA\nsystems. To address this challenge, we introduce GigaBrain-0, a novel VLA\nfoundation model empowered by world model-generated data (e.g., video\ngeneration, real2real transfer, human transfer, view transfer, sim2real\ntransfer data). By leveraging world models to generate diverse data at scale,\nGigaBrain-0 significantly reduces reliance on real robot data while improving\ncross-task generalization. Our approach further improves policy robustness\nthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,\nenabling the model to reason about spatial geometry, object states, and\nlong-horizon dependencies during task execution. This leads to substantial\ngains in real-world performance on dexterous, long-horizon, and mobile\nmanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves\nsuperior generalization across variations in appearances (e.g., textures,\ncolors), object placements, and camera viewpoints. Additionally, we present\nGigaBrain-0-Small, an optimized lightweight variant designed to run efficiently\non devices such as the NVIDIA Jetson AGX Orin.",
            "upvotes": 30,
            "discussionId": "68f99e81b9b2e4ae0467386e",
            "projectPage": "https://gigabrain0.github.io/",
            "githubRepo": "https://github.com/open-gigaai/giga-brain-0",
            "ai_summary": "GigaBrain-0, a VLA foundation model, uses world model-generated data to enhance cross-task generalization and policy robustness, improving real-world performance on complex manipulation tasks.",
            "ai_keywords": [
                "VLA models",
                "world model-generated data",
                "video generation",
                "real2real transfer",
                "human transfer",
                "view transfer",
                "sim2real transfer",
                "RGBD input modeling",
                "Chain-of-Thought supervision",
                "spatial geometry",
                "object states",
                "long-horizon dependencies",
                "dexterous manipulation",
                "mobile manipulation"
            ],
            "githubStars": 19,
            "organization": {
                "_id": "68d6587936e2de9610d9f5f0",
                "name": "open-gigaai",
                "fullname": "GigaAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d6394328e169473e90e4a6/zUK7FKr_8XqrN0aFUgsD-.png"
            }
        },
        "translation_title": "GigaBrain-0: 세계 모델로 구동되는 비전-언어-행동 모델",
        "purpose": "비용과 시간을 절약하여 일반적인 로봇을 위한 Vision-Language-Action 모델의 효율성을 높이기 위해",
        "method": [
            "세계 모델을 활용해 다양한 데이터 생성(e.g., video generation, human transfer 등)(By leveraging world models to generate diverse data at scale)",
            "로봇 데이터 의존성을 줄이고 cross-task generalization을 향상함(GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization)",
            "RGBD 입력 모델링과 Chain-of-Thought 초지를 통해 정책 강인성을 개선함(This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks)"
        ],
        "conclusion": "GigaBrain-0은 다양한 환경 변화에 대한 뛰어난 일반화를 달성하고, 실제 로봇 작업의 성능을 크게 향상시킴.",
        "keywords": [
            "Vision-Language Models",
            "Video Generation",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2510.15511",
            "authors": [
                {
                    "_id": "68f5fa388589920bf4d3225c",
                    "user": {
                        "_id": "66e56ad4c86016ecd972a9c5",
                        "avatarUrl": "/avatars/53ffcbe05c238d78ea227ed4e2b24b70.svg",
                        "isPro": false,
                        "fullname": "Giorgos Nikolaou",
                        "user": "giorgosnik02",
                        "type": "user"
                    },
                    "name": "Giorgos Nikolaou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T15:01:13.658Z",
                    "hidden": false
                },
                {
                    "_id": "68f5fa388589920bf4d3225d",
                    "name": "Tommaso Mencattini",
                    "hidden": false
                },
                {
                    "_id": "68f5fa388589920bf4d3225e",
                    "name": "Donato Crisostomi",
                    "hidden": false
                },
                {
                    "_id": "68f5fa388589920bf4d3225f",
                    "user": {
                        "_id": "5e8ef1f14957053f606489e6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
                        "isPro": false,
                        "fullname": "Andrea Santilli",
                        "user": "teelinsan",
                        "type": "user"
                    },
                    "name": "Andrea Santilli",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-23T15:01:19.541Z",
                    "hidden": false
                },
                {
                    "_id": "68f5fa388589920bf4d32260",
                    "name": "Yannis Panagakis",
                    "hidden": false
                },
                {
                    "_id": "68f5fa388589920bf4d32261",
                    "name": "Emanuele Rodola'",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-17T10:25:30.000Z",
            "submittedOnDailyAt": "2025-10-23T10:54:24.652Z",
            "title": "Language Models are Injective and Hence Invertible",
            "submittedOnDailyBy": {
                "_id": "63ab16a6d7ee953f604ecd52",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ab16a6d7ee953f604ecd52/ujylOpczHKxU6Kfr-jGVr.png",
                "isPro": false,
                "fullname": "Tommaso Mencattini",
                "user": "tmencatt",
                "type": "user"
            },
            "summary": "Transformer components such as non-linear activations and normalization are\ninherently non-injective, suggesting that different inputs could map to the\nsame output and prevent exact recovery of the input from a model's\nrepresentations. In this paper, we challenge this view. First, we prove\nmathematically that transformer language models mapping discrete input\nsequences to their corresponding sequence of continuous representations are\ninjective and therefore lossless, a property established at initialization and\npreserved during training. Second, we confirm this result empirically through\nbillions of collision tests on six state-of-the-art language models, and\nobserve no collisions. Third, we operationalize injectivity: we introduce\nSipIt, the first algorithm that provably and efficiently reconstructs the exact\ninput text from hidden activations, establishing linear-time guarantees and\ndemonstrating exact invertibility in practice. Overall, our work establishes\ninjectivity as a fundamental and exploitable property of language models, with\ndirect implications for transparency, interpretability, and safe deployment.",
            "upvotes": 22,
            "discussionId": "68f5fa388589920bf4d32262",
            "ai_summary": "Transformer language models are proven to be injective, allowing exact input reconstruction from hidden activations, which has implications for transparency and safety.",
            "ai_keywords": [
                "transformer components",
                "non-linear activations",
                "normalization",
                "injective",
                "language models",
                "continuous representations",
                "collision tests",
                "SipIt",
                "linear-time guarantees",
                "exact invertibility"
            ],
            "organization": {
                "_id": "6070204769a66931a0273ef7",
                "name": "gladia",
                "fullname": "Gladia Research Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1617961184086-5e8ef1f14957053f606489e6.png"
            }
        },
        "translation_title": "언어 모델은 단사적이며 따라서 가역적이다",
        "purpose": "Transformer 언어 모델이 입력의 정확한 복원이 가능하다는 것을 수학적으로 증명하고 이를 기반으로 새로운 알고리즘을 개발하는 것",
        "method": [
            "Transformer 언어 모델이 입력을 해당하는 연속 표현으로 매핑할 때 단사적이라는 것을 수학적으로 증명함(First, we prove mathematically that transformer language models mapping discrete input sequences to their corresponding sequence of continuous representations are injective and therefore lossless.)",
            "6개 최첨단 언어 모델에 대해 수십억 건의 충돌 테스트를 수행하여 충돌이 없음을 확인함(Second, we confirm this result empirically through billions of collision tests on six state-of-the-art language models, and observe no collisions.)",
            "SipIt이라는 알고리즘을 소개하여 숨겨진 활성화에서 정확한 입력 텍스트를 효율적으로 재구성할 수 있음을 입증함(Third, we operationalize injectivity: we introduce SipIt, the first algorithm that provably and efficiently reconstructs the exact input text from hidden activations.)"
        ],
        "conclusion": "본 연구는 언어 모델의 단사성을 기초로 한 알고리즘의 개발로 투명성과 해석 가능성, 안전한 배포에 대한 직접적인 함의를 제시함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    }
]