[
    {
        "paper": {
            "id": "2511.18538",
            "authors": [
                {
                    "_id": "692e667137312eaa83fd8832",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8833",
                    "name": "Xianglong Liu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8834",
                    "name": "Weifeng Lv",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8835",
                    "name": "Ken Deng",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8836",
                    "name": "Shawn Guo",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8837",
                    "name": "Lin Jing",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8838",
                    "name": "Yizhi Li",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8839",
                    "name": "Shark Liu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd883a",
                    "name": "Xianzhen Luo",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd883b",
                    "name": "Yuyu Luo",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd883c",
                    "name": "Changzai Pan",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd883d",
                    "name": "Ensheng Shi",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd883e",
                    "name": "Yingshui Tan",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd883f",
                    "name": "Renshuai Tao",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8840",
                    "user": {
                        "_id": "66a8e2538407031e388c501f",
                        "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg",
                        "isPro": false,
                        "fullname": "wjj",
                        "user": "wuyuverse",
                        "type": "user"
                    },
                    "name": "Jiajun Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-02T08:39:36.195Z",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8841",
                    "name": "Xianjie Wu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8842",
                    "name": "Zhenhe Wu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8843",
                    "name": "Daoguang Zan",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8844",
                    "name": "Chenchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8845",
                    "user": {
                        "_id": "672c9ba69380700b602c46c1",
                        "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg",
                        "isPro": false,
                        "fullname": "wei zhang",
                        "user": "zwpride",
                        "type": "user"
                    },
                    "name": "Wei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-02T08:39:37.970Z",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8846",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8847",
                    "name": "Terry Yue Zhuo",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8848",
                    "name": "Kerui Cao",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8849",
                    "name": "Xianfu Cheng",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd884a",
                    "name": "Jun Dong",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd884b",
                    "name": "Shengjie Fang",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd884c",
                    "name": "Zhiwei Fei",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd884d",
                    "name": "Xiangyuan Guan",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd884e",
                    "name": "Qipeng Guo",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd884f",
                    "name": "Zhiguang Han",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8850",
                    "name": "Joseph James",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8851",
                    "name": "Tianqi Luo",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8852",
                    "name": "Renyuan Li",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8853",
                    "name": "Yuhang Li",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8854",
                    "name": "Yiming Liang",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8855",
                    "name": "Congnan Liu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8856",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8857",
                    "name": "Qian Liu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8858",
                    "name": "Ruitong Liu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8859",
                    "name": "Tyler Loakman",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd885a",
                    "name": "Xiangxin Meng",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd885b",
                    "name": "Chuang Peng",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd885c",
                    "name": "Tianhao Peng",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd885d",
                    "name": "Jiajun Shi",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd885e",
                    "name": "Mingjie Tang",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd885f",
                    "name": "Boyang Wang",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8860",
                    "name": "Haowen Wang",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8861",
                    "name": "Yunli Wang",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8862",
                    "name": "Fanglin Xu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8863",
                    "name": "Zihan Xu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8864",
                    "name": "Fei Yuan",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8865",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-02T08:39:34.025Z",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8866",
                    "user": {
                        "_id": "65f40e83653c231cbaf7defe",
                        "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg",
                        "isPro": false,
                        "fullname": "Jiayi Zhang",
                        "user": "didiforhugface",
                        "type": "user"
                    },
                    "name": "Jiayi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-02T08:39:32.149Z",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8867",
                    "name": "Xinhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8868",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8869",
                    "name": "Hualei Zhu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd886a",
                    "name": "King Zhu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd886b",
                    "name": "Brown Dai",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd886c",
                    "name": "Aishan Liu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd886d",
                    "name": "Zhoujun Li",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd886e",
                    "name": "Chenghua Lin",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd886f",
                    "name": "Tianyu Liu",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8870",
                    "name": "Chao Peng",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8871",
                    "name": "Kai Shen",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8872",
                    "name": "Libo Qin",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8873",
                    "name": "Shuangyong Song",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8874",
                    "name": "Zizheng Zhan",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8875",
                    "name": "Jiajun Zhang",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8876",
                    "name": "Jie Zhang",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8877",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "692e667137312eaa83fd8878",
                    "name": "Bo Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-23T17:09:34.000Z",
            "submittedOnDailyAt": "2025-12-02T02:55:07.234Z",
            "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence",
            "submittedOnDailyBy": {
                "_id": "64ccb9bfead94891d12aef42",
                "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg",
                "isPro": false,
                "fullname": "Yang Jian",
                "user": "CSJianYang",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.",
            "upvotes": 168,
            "discussionId": "692e667237312eaa83fd8879",
            "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.",
            "ai_keywords": [
                "Transformer-based architectures",
                "HumanEval",
                "prompting paradigms",
                "code pre-training",
                "supervised fine-tuning",
                "reinforcement learning",
                "autonomous coding agents",
                "GPT-4",
                "Claude",
                "LLaMA",
                "StarCoder",
                "Code LLaMA",
                "DeepSeek-Coder",
                "QwenCoder",
                "code correctness",
                "security",
                "contextual awareness",
                "software-related code tasks",
                "scaling law",
                "framework selection",
                "hyperparameter sensitivity",
                "model architectures",
                "dataset comparisons"
            ],
            "organization": {
                "_id": "63ba7720fc454697637969f1",
                "name": "Beihang",
                "fullname": "Beihang University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"
            }
        },
        "translation_title": "코드 기초 모델에서 에이전트 및 애플리케이션으로: 코드 인텔리전스를 위한 실용 가이드",
        "purpose": "코드 LLM의 모델 생애 주기를 체계적으로 분석하고 실용적인 가이드를 제공하여 코드 자동화 성능을 향상시키는 것",
        "method": [
            "코드 LLM에 대한 분석 및 실험을 통해 모델 생애 주기를 다룸(we provide a comprehensive synthesis and practical guide about code LLMs, systematically examining the complete model life cycle from data curation to post-training).",
            "일반 LLM, 코드 전문 LLM을 비판적으로 검토하고 기술, 설계 결정, 트레이드오프 분석(We analyze the code capability of the general LLMs and code-specialized LLMs, critically examining the techniques, design decisions, and trade-offs).",
            "일반 연구와 실제 배포 간의 격차를 설명하고 실용적인 필요에 맞는 연구 방향을 제시함(Further, we articulate the research-practice gap between academic research and real-world deployment, and map promising research directions to practical needs)."
        ],
        "conclusion": "코드 LLM의 성능 향상과 실용적 적용을 위한 체계적인 접근법과 분석을 제공하며, 연구와 실제 간의 격차를 좁히는 방향을 모색함.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2511.20785",
            "authors": [
                {
                    "_id": "692d430f4397b1ec214f696e",
                    "user": {
                        "_id": "6524d665ab1416594149e07e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png",
                        "isPro": false,
                        "fullname": "Zuhao Yang",
                        "user": "mwxely",
                        "type": "user"
                    },
                    "name": "Zuhao Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T09:10:11.311Z",
                    "hidden": false
                },
                {
                    "_id": "692d430f4397b1ec214f696f",
                    "user": {
                        "_id": "6690f58e2f9f6f9c88e91031",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png",
                        "isPro": false,
                        "fullname": "Sudong Wang",
                        "user": "xiao4435",
                        "type": "user"
                    },
                    "name": "Sudong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T09:10:14.173Z",
                    "hidden": false
                },
                {
                    "_id": "692d430f4397b1ec214f6970",
                    "user": {
                        "_id": "64bb77e786e7fb5b8a317a43",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png",
                        "isPro": false,
                        "fullname": "kcz",
                        "user": "kcz358",
                        "type": "user"
                    },
                    "name": "Kaichen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T10:06:41.343Z",
                    "hidden": false
                },
                {
                    "_id": "692d430f4397b1ec214f6971",
                    "user": {
                        "_id": "66bf00ca5b4e241fe266059d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png",
                        "isPro": false,
                        "fullname": "Keming Wu",
                        "user": "wukeming11",
                        "type": "user"
                    },
                    "name": "Keming Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T09:10:09.461Z",
                    "hidden": false
                },
                {
                    "_id": "692d430f4397b1ec214f6972",
                    "name": "Sicong Leng",
                    "hidden": false
                },
                {
                    "_id": "692d430f4397b1ec214f6973",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "692d430f4397b1ec214f6974",
                    "name": "Chengwei Qin",
                    "hidden": false
                },
                {
                    "_id": "692d430f4397b1ec214f6975",
                    "name": "Shijian Lu",
                    "hidden": false
                },
                {
                    "_id": "692d430f4397b1ec214f6976",
                    "name": "Xingxuan Li",
                    "hidden": false
                },
                {
                    "_id": "692d430f4397b1ec214f6977",
                    "user": {
                        "_id": "6454685a548f22be598414c4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg",
                        "isPro": false,
                        "fullname": "Lidong Bing",
                        "user": "LidongBing",
                        "type": "user"
                    },
                    "name": "Lidong Bing",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-02T08:49:36.056Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"
            ],
            "publishedAt": "2025-11-25T19:22:48.000Z",
            "submittedOnDailyAt": "2025-12-02T00:35:56.511Z",
            "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling",
            "submittedOnDailyBy": {
                "_id": "6524d665ab1416594149e07e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png",
                "isPro": false,
                "fullname": "Zuhao Yang",
                "user": "mwxely",
                "type": "user"
            },
            "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .",
            "upvotes": 130,
            "discussionId": "692d430f4397b1ec214f6978",
            "projectPage": "https://evolvinglmms-lab.github.io/LongVT/",
            "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT",
            "ai_summary": "LongVT is an end-to-end framework that enhances video reasoning with textual Chain-of-Thought by using interleaved Multimodal Chain-of-Tool-Thought, enabling global-to-local reasoning and leveraging LMMs for video cropping and frame resampling.",
            "ai_keywords": [
                "LMMs",
                "video reasoning",
                "textual Chain-of-Thought",
                "hallucinations",
                "long-form videos",
                "temporal grounding",
                "Multimodal Chain-of-Tool-Thought",
                "end-to-end agentic framework",
                "global-to-local reasoning",
                "video cropping",
                "frame resampling",
                "VideoSIAH",
                "question-answering",
                "tool-integrated cold-start supervised fine-tuning",
                "agentic reinforcement learning",
                "agentic reinforcement fine-tuning",
                "long-video understanding",
                "reasoning benchmarks"
            ],
            "githubStars": 75,
            "organization": {
                "_id": "6583eb89bed3689928f5d845",
                "name": "lmms-lab",
                "fullname": "LMMs-Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"
            }
        },
        "translation_title": "LongVT: 네이티브 도구 호출을 통한 '긴 비디오로 생각하기' 장려",
        "purpose": "비디오 추론에서 Hallucination 문제를 해결하고 긴 비디오에 대한 이해를 향상시키기 위한 프레임워크 개발",
        "method": [
            "LMM의 시간적 기반 능력을 활용해 특정 비디오 클립을 확대하는 비디오 크롭 도구로 사용함(we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip)",
            "LongVT 프레임워크를 통해 '긴 비디오로 생각하기'를 가능하게 하는 Multimodal Chain-of-Tool-Thought를 도입함(in order to enable 'Thinking with Long Videos' via interleaved Multimodal Chain-of-Tool-Thought)",
            "훈련 데이터 세트를 247.9K 샘플과 1.6K, 15.4K 샘플로 구성하여 수집하고 평가용으로 1,280 QA 쌍을 신중하게 선별함(our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning)"
        ],
        "conclusion": "LongVT는 긴 비디오 이해 및 추론 벤치마크에서 기존 강력한 모델들을 일관되게 능가함.",
        "keywords": [
            "Video Understanding",
            "Multimodal Learning",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2512.01374",
            "authors": [
                {
                    "_id": "692e6bf937312eaa83fd8890",
                    "user": {
                        "_id": "610b70452719facd4ea85e28",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                        "isPro": false,
                        "fullname": "Chujie Zheng",
                        "user": "chujiezheng",
                        "type": "user"
                    },
                    "name": "Chujie Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-02T08:39:27.206Z",
                    "hidden": false
                },
                {
                    "_id": "692e6bf937312eaa83fd8891",
                    "name": "Kai Dang",
                    "hidden": false
                },
                {
                    "_id": "692e6bf937312eaa83fd8892",
                    "name": "Bowen Yu",
                    "hidden": false
                },
                {
                    "_id": "692e6bf937312eaa83fd8893",
                    "name": "Mingze Li",
                    "hidden": false
                },
                {
                    "_id": "692e6bf937312eaa83fd8894",
                    "name": "Huiqiang Jiang",
                    "hidden": false
                },
                {
                    "_id": "692e6bf937312eaa83fd8895",
                    "name": "Junrong Lin",
                    "hidden": false
                },
                {
                    "_id": "692e6bf937312eaa83fd8896",
                    "name": "Yuqiong Liu",
                    "hidden": false
                },
                {
                    "_id": "692e6bf937312eaa83fd8897",
                    "user": {
                        "_id": "62088594a5943c8a8fc94560",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png",
                        "isPro": false,
                        "fullname": "An Yang",
                        "user": "yangapku",
                        "type": "user"
                    },
                    "name": "An Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-02T08:39:25.208Z",
                    "hidden": false
                },
                {
                    "_id": "692e6bf937312eaa83fd8898",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "692e6bf937312eaa83fd8899",
                    "name": "Junyang Lin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"
            ],
            "publishedAt": "2025-12-01T07:45:39.000Z",
            "submittedOnDailyAt": "2025-12-02T02:47:49.367Z",
            "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices",
            "submittedOnDailyBy": {
                "_id": "63d9d68c1cae35c27bf7a6a7",
                "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
                "isPro": false,
                "fullname": "Bowen Yu",
                "user": "Tigerph",
                "type": "user"
            },
            "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.",
            "upvotes": 47,
            "discussionId": "692e6bfa37312eaa83fd889a",
            "ai_summary": "The paper provides a theoretical foundation for optimizing sequence-level rewards in reinforcement learning using token-level objectives, highlighting the importance of techniques like importance sampling correction, clipping, and Routing Replay for stabilizing training, especially with large language models.",
            "ai_keywords": [
                "reinforcement learning",
                "large language models",
                "sequence-level reward",
                "token-level objective",
                "policy gradient methods",
                "REINFORCE",
                "first-order approximation",
                "training-inference discrepancy",
                "policy staleness",
                "importance sampling correction",
                "clipping",
                "Routing Replay",
                "Mixture-of-Experts",
                "on-policy training",
                "off-policy updates"
            ],
            "organization": {
                "_id": "64c8b5837fe12ecd0a7e92eb",
                "name": "Qwen",
                "fullname": "Qwen",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
            }
        },
        "translation_title": "LLM을 통한 강화 학습 안정화: 공식화와 실제 적용",
        "purpose": "LLM을 활용해 강화 학습의 안정성을 높이는 방법을 제안하고, 보상 최적화 조건을 설명하기 위함",
        "method": [
            "정책 경량화 방법에서 토큰 레벨 목표를 통해 순차적 보상을 최적화하는 새로운 공식화 제안( This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE.)",
            "훈련과 추론의 불일치 및 정책의 노후화를 최소화할 때 대리 목표의 유효성이 증가함을 보여줌( Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized.)",
            "정교한 실험을 통해 중요도 샘플링 보정이 최상의 훈련 안정성을 제공함을 입증( Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability.)"
        ],
        "conclusion": "훈련이 안정화되면 초기 설정과 관계없이 최종 성능이 일관되게 나타나는 것을 보여줌",
        "keywords": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2512.01948",
            "authors": [
                {
                    "_id": "692e62d937312eaa83fd87e7",
                    "name": "Dingling Zhang",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87e8",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87e9",
                    "user": {
                        "_id": "6704ee27386892c420db1938",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg",
                        "isPro": false,
                        "fullname": "JinCheng Ren",
                        "user": "JinChengRen",
                        "type": "user"
                    },
                    "name": "Jincheng Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-02T08:39:42.538Z",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87ea",
                    "name": "Kangqi Song",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87eb",
                    "name": "Xinran Zhou",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87ec",
                    "name": "Boyu Feng",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87ed",
                    "name": "Shudong Liu",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87ee",
                    "name": "Jiabin Luo",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87ef",
                    "name": "Weihao Xie",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87f0",
                    "name": "Zhaohui Wang",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87f1",
                    "name": "Tianrui Qin",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87f2",
                    "name": "King Zhu",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87f3",
                    "name": "Yuqing Wang",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87f4",
                    "name": "Qianben Chen",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87f5",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87f6",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87f7",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "692e62d937312eaa83fd87f8",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-01T17:58:59.000Z",
            "submittedOnDailyAt": "2025-12-02T01:25:02.560Z",
            "title": "How Far Are We from Genuinely Useful Deep Research Agents?",
            "submittedOnDailyBy": {
                "_id": "65377c30e48353201e6fdda0",
                "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                "isPro": false,
                "fullname": "Jiaheng Liu",
                "user": "CheeryLJH",
                "type": "user"
            },
            "summary": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.",
            "upvotes": 41,
            "discussionId": "692e62d937312eaa83fd87f9",
            "githubRepo": "https://github.com/OPPO-PersonalAI/FINDER_DEFT",
            "ai_summary": "FINDER is a benchmark for deep research agents with standardized human-curated tasks and DEFT is a failure taxonomy revealing that DRAs struggle with evidence integration, verification, and reasoning-resilient planning.",
            "ai_keywords": [
                "Deep Research Agents",
                "Fine-grained DEepResearch bench",
                "structured checklist items",
                "Deep rEsearch Failure Taxonomy",
                "reasoning",
                "retrieval",
                "generation",
                "failure modes",
                "reasoning-resilient planning"
            ],
            "githubStars": 11
        },
        "translation_title": "진정으로 유용한 심층 연구 에이전트까지 얼마나 멀었는가?",
        "purpose": "심층 연구 에이전트(DRA)의 보고서 생성 성능을 평가하고 개선하기 위한 새로운 기준 마련",
        "method": [
            "Fine-grained DEepResearch bench (FINDER)를 제시하여 100개의 인간이 큐레이션한 연구 과제와 419개의 구조화된 체크리스트 항목으로 구성된 향상된 기준을 개발함(we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items).",
            "기존 DRA가 생성한 약 1,000개의 보고서를 기반으로 Deep rEsearch Failure Taxonomy (DEFT)를 제안함(we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents).",
            "DEFT는 추론, 검색, 생성 등에서의 14가지 세분화된 실패 모드를 포함하고, 인간-LLM 공동 주석 및 주석 간 신뢰성 검증을 바탕으로 구축됨(DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation)."
        ],
        "conclusion": "현재의 DRA는 작업 이해에는 어려움이 없지만, 증거 통합, 검증 및 추론 저항적 계획에서 어려움을 겪고 있다는 연구 결과를 도출함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Document Parsing"
        ]
    }
]