[
    {
        "paper": {
            "id": "2511.22699",
            "authors": [
                {
                    "_id": "692d06234397b1ec214f6788",
                    "name": "Z-Image Team",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f6789",
                    "user": {
                        "_id": "692d0e6bb14ceb758205d0dd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg",
                        "isPro": false,
                        "fullname": "Huanqia Cai",
                        "user": "Orion-Cai",
                        "type": "user"
                    },
                    "name": "Huanqia Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T09:13:26.669Z",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f678a",
                    "user": {
                        "_id": "67777b7a8376dfe003afa951",
                        "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg",
                        "isPro": false,
                        "fullname": "Sihan Cao",
                        "user": "Sihan-Cao",
                        "type": "user"
                    },
                    "name": "Sihan Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T09:13:33.191Z",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f678b",
                    "name": "Ruoyi Du",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f678c",
                    "name": "Peng Gao",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f678d",
                    "name": "Steven Hoi",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f678e",
                    "name": "Shijie Huang",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f678f",
                    "name": "Zhaohui Hou",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f6790",
                    "user": {
                        "_id": "662a0f2d4bab737c1a279843",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/K7r6sHF_tU3TjeiNgaMdS.png",
                        "isPro": false,
                        "fullname": "Dengyang Jiang",
                        "user": "DyJiang",
                        "type": "user"
                    },
                    "name": "Dengyang Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T10:07:15.555Z",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f6791",
                    "user": {
                        "_id": "6537e8eab01250d1d6efed3a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg",
                        "isPro": false,
                        "fullname": "Xin",
                        "user": "Srameo",
                        "type": "user"
                    },
                    "name": "Xin Jin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T09:11:15.288Z",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f6792",
                    "name": "Liangchen Li",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f6793",
                    "user": {
                        "_id": "6285a9133ab6642179158944",
                        "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
                        "isPro": false,
                        "fullname": "Zhen Li",
                        "user": "Paper99",
                        "type": "user"
                    },
                    "name": "Zhen Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T09:11:16.899Z",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f6794",
                    "user": {
                        "_id": "6740a5730bb4a675446a80ad",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg",
                        "isPro": false,
                        "fullname": "Zhong-Yu Li",
                        "user": "lzyhha",
                        "type": "user"
                    },
                    "name": "Zhong-Yu Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T10:07:08.972Z",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f6795",
                    "name": "David Liu",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f6796",
                    "name": "Dongyang Liu",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f6797",
                    "user": {
                        "_id": "66332475351231c428653b6b",
                        "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg",
                        "isPro": false,
                        "fullname": "Junhan Shi",
                        "user": "jshmsjh",
                        "type": "user"
                    },
                    "name": "Junhan Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T10:07:38.865Z",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f6798",
                    "user": {
                        "_id": "64379d79fac5ea753f1c10f3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png",
                        "isPro": false,
                        "fullname": "Jerry Wu",
                        "user": "QJerry",
                        "type": "user"
                    },
                    "name": "Qilong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T09:11:18.709Z",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f6799",
                    "name": "Feng Yu",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f679a",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f679b",
                    "name": "Shifeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "692d06234397b1ec214f679c",
                    "name": "Shilin Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-27T18:52:07.000Z",
            "submittedOnDailyAt": "2025-12-01T00:38:17.269Z",
            "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
            "submittedOnDailyBy": {
                "_id": "6285a9133ab6642179158944",
                "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
                "isPro": false,
                "fullname": "Zhen Li",
                "user": "Paper99",
                "type": "user"
            },
            "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
            "upvotes": 63,
            "discussionId": "692d06234397b1ec214f679d",
            "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/",
            "githubRepo": "https://github.com/Tongyi-MAI/Z-Image",
            "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.",
            "ai_keywords": [
                "Scalable Single-Stream Diffusion Transformer",
                "S3-DiT",
                "diffusion transformer",
                "omni-pre-training",
                "instruction-following capabilities",
                "photorealistic image generation",
                "bilingual text rendering",
                "distillation scheme",
                "reward post-training",
                "H800 GPU",
                "VRAM"
            ],
            "githubStars": 3229,
            "organization": {
                "_id": "6925b20fed452d1567c012d3",
                "name": "Tongyi-MAI",
                "fullname": "Tongyi-MAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"
            }
        },
        "translation_title": "Z-Image: 단일 스트림 확산 변환기를 활용한 효율적인 이미지 생성 기초 모델",
        "purpose": "고성능 이미지 생성 모델의 접근성과 효율성을 높이기 위한 연구",
        "method": [
            "20B에서 80B 파라미터를 가진 기존 모델들과 달리, 6B 파라미터의 효율적인 기초 생성 모델 Z-Image를 제안함(we propose Z-Image, an efficient 6B-parameter foundation generative model...).",
            "모델 생애주기를 체계적으로 최적화하여 전체 훈련 작업을 314K H800 GPU 시간으로 완료함(By systematically optimizing the entire model lifecycle... we complete the full training workflow in just 314K H800 GPU hours...).",
            "소수 단계 증류 스킴을 통해 Z-Image-Turbo를 생성하여 빠른 추론 지연 시간을 달성함(Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo...)."
        ],
        "conclusion": "Z-Image는 뛰어난 포토리얼리스틱 이미지 생성 능력과 이중 언어 텍스트 렌더링을 보여주며, 최첨단 모델과 경쟁할 수 있는 성과를 달성함.",
        "keywords": [
            "Image Generation",
            "Large Language Models",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2511.22625",
            "authors": [
                {
                    "_id": "692d118f4397b1ec214f683f",
                    "name": "Fukun Yin",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f6840",
                    "name": "Shiyu Liu",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f6841",
                    "name": "Yucheng Han",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f6842",
                    "name": "Zhibo Wang",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f6843",
                    "name": "Peng Xing",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f6844",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f6845",
                    "user": {
                        "_id": "64b914c8ace99c0723ad83a9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
                        "isPro": false,
                        "fullname": "Wei Cheng",
                        "user": "wchengad",
                        "type": "user"
                    },
                    "name": "Wei Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T09:10:49.645Z",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f6846",
                    "user": {
                        "_id": "66179d0a8d7b07669c8abb72",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66179d0a8d7b07669c8abb72/0Cl3_hpfBtL4DBKB5_ug5.png",
                        "isPro": false,
                        "fullname": "Yingming Wang",
                        "user": "ymwangv",
                        "type": "user"
                    },
                    "name": "Yingming Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T10:11:39.029Z",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f6847",
                    "name": "Aojie Li",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f6848",
                    "user": {
                        "_id": "6503ccaf13d750b4604649e4",
                        "avatarUrl": "/avatars/96bebff9284d61f37c83e7da6a7e9bac.svg",
                        "isPro": false,
                        "fullname": "Zixin Yin",
                        "user": "zachary-yin",
                        "type": "user"
                    },
                    "name": "Zixin Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T10:12:21.015Z",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f6849",
                    "user": {
                        "_id": "64c9bac33cfe45b07179568d",
                        "avatarUrl": "/avatars/4a8206cdb1770a8cdaae0d0a2b7b59f2.svg",
                        "isPro": false,
                        "fullname": "Pengtao Chen",
                        "user": "PengtaoChen",
                        "type": "user"
                    },
                    "name": "Pengtao Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T10:11:57.109Z",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f684a",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f684b",
                    "user": {
                        "_id": "60d4440fe648443279aaffd8",
                        "avatarUrl": "/avatars/bf7209c1f14ae120f5bfda5fda1301b7.svg",
                        "isPro": false,
                        "fullname": "Daxin Jiang",
                        "user": "djiang",
                        "type": "user"
                    },
                    "name": "Daxin Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T10:12:41.639Z",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f684c",
                    "name": "Xianfang Zeng",
                    "hidden": false
                },
                {
                    "_id": "692d118f4397b1ec214f684d",
                    "user": {
                        "_id": "63417332c5565a4b8d43a0d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
                        "isPro": false,
                        "fullname": "Gang Yu",
                        "user": "skicy",
                        "type": "user"
                    },
                    "name": "Gang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T09:10:47.476Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ePq9HeiskK7FFzwEgqppg.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/uyf0WIc1F6Zpdth1OHtkl.png"
            ],
            "publishedAt": "2025-11-27T17:02:48.000Z",
            "submittedOnDailyAt": "2025-12-01T01:25:40.597Z",
            "title": "REASONEDIT: Towards Reasoning-Enhanced Image Editing Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q).",
            "upvotes": 37,
            "discussionId": "692d118f4397b1ec214f684e",
            "githubRepo": "https://github.com/stepfun-ai/Step1X-Edit?tab=readme-ov-file#step1x-edit-v1p2-v12",
            "ai_summary": "Integrating reasoning mechanisms into image editing models enhances performance by improving instruction understanding and result correction.",
            "ai_keywords": [
                "multimodal large language model",
                "MLLM",
                "diffusion decoder",
                "Step1X-Edit",
                "Qwen-Image-Edit",
                "thinking mechanism",
                "reflection",
                "image editing",
                "instruction understanding",
                "editing accuracy",
                "world knowledge",
                "ImgEdit",
                "GEdit",
                "Kris",
                "DiT"
            ],
            "githubStars": 1777,
            "organization": {
                "_id": "66e43eae9d477f566f937935",
                "name": "stepfun-ai",
                "fullname": "StepFun",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
            }
        },
        "translation_title": "REASONEDIT: 추론 강화 이미지 편집 모델을 향하여",
        "purpose": "이미지 편집 모델의 성능을 향상시키기 위한 추론 기능 활용 연구",
        "method": [
            "MLLM(다중 모달 대형 언어 모델)와 디퓨전 디코더를 결합하여 이미지 편집을 위한 프레임워크를 개발함(we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models.)",
            "사고(thinking)와 반성(reflection) 두 가지 추론 메커니즘을 탐구하여 명령 이해 및 편집 정확도를 높임(we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy.)",
            "사고-편집-반성 루프를 통해 추론 기반 이미지 편집을 수행함(our proposed framework enables image editing in a thinking-editing-reflection loop.)"
        ],
        "conclusion": "추론 접근 방식을 통해 기존 이미지 편집 모델보다 성능이 크게 향상됨을 확인하였고, 다양한 모델에서 개선된 결과를 보여줌.",
        "keywords": [
            "Image Editing",
            "Multimodal Learning",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2511.23475",
            "authors": [
                {
                    "_id": "692d0d8f4397b1ec214f67e7",
                    "name": "Zhizhou Zhong",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67e8",
                    "name": "Yicheng Ji",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67e9",
                    "name": "Zhe Kong",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67ea",
                    "user": {
                        "_id": "667baf38d24677e9afb85871",
                        "avatarUrl": "/avatars/447e05b8a0700128f5f38e34ccfadb39.svg",
                        "isPro": false,
                        "fullname": "yiying liu",
                        "user": "yiying66",
                        "type": "user"
                    },
                    "name": "Yiying Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T10:09:18.266Z",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67eb",
                    "name": "Jiarui Wang",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67ec",
                    "name": "Jiasun Feng",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67ed",
                    "name": "Lupeng Liu",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67ee",
                    "name": "Xiangyi Wang",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67ef",
                    "name": "Yanjia Li",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67f0",
                    "name": "Yuqing She",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67f1",
                    "name": "Ying Qin",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67f2",
                    "name": "Huan Li",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67f3",
                    "user": {
                        "_id": "616446e50a2d89b7d3462615",
                        "avatarUrl": "/avatars/969de84cd239e298bec3c93f7ff8fb0d.svg",
                        "isPro": false,
                        "fullname": "shuiyang mao",
                        "user": "symao",
                        "type": "user"
                    },
                    "name": "Shuiyang Mao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T10:10:16.459Z",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67f4",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "692d0d8f4397b1ec214f67f5",
                    "user": {
                        "_id": "65f865cf806814aec87d45ce",
                        "avatarUrl": "/avatars/6b931c1efe20422a5985fce6a9e36aed.svg",
                        "isPro": false,
                        "fullname": "Wenhan Luo",
                        "user": "whluo",
                        "type": "user"
                    },
                    "name": "Wenhan Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T10:10:09.471Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-28T18:59:01.000Z",
            "submittedOnDailyAt": "2025-12-01T01:08:00.505Z",
            "title": "AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.",
            "upvotes": 32,
            "discussionId": "692d0d904397b1ec214f67f6",
            "projectPage": "https://hkust-c4g.github.io/AnyTalker-homepage/",
            "githubRepo": "https://github.com/HKUST-C4G/AnyTalker",
            "ai_summary": "The proposed AnyTalker framework generates high-quality multi-person talking videos by extending Diffusion Transformer with identity-aware attention, leveraging single-person videos for training, and using a specialized dataset for evaluation.",
            "ai_keywords": [
                "Diffusion Transformer",
                "identity-aware attention",
                "multi-person video generation",
                "audio-driven generation",
                "multi-stream processing",
                "lip synchronization",
                "visual quality",
                "interactivity",
                "dataset"
            ],
            "githubStars": 82
        },
        "translation_title": "AnyTalker: 상호작용 개선을 통한 다중 인물 대화 비디오 생성 확장",
        "purpose": "비용 문제와 상호작용의 일관성을 해결하여 다중 인물 대화 비디오 생성을 개선하기 위한 연구",
        "method": [
            "AnyTalker라는 다중 인물 생성 프레임워크를 제안하여 확장 가능한 다중 스트림 처리 아키텍처를 구현함(To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture.)",
            "Diffusion Transformer의 주목 메커니즘을 수정하여 신분 인식이 가능한 주목 메커니즘을 개발하고, 이를 통해 다양한 신분을 처리하도록 함(Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities.)",
            "단일 인물 비디오만을 사용해 다중 인물 말하기 패턴을 학습하고, 적은 수의 다중 인물 클립으로 상호작용을 개선하는 훈련 파이프라인을 제안함(Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos.)"
        ],
        "conclusion": "AnyTalker는 뛰어난 입술 동기화, 시각적 품질 및 자연스러운 상호작용을 실현하여 데이터 비용과 신분 확장성 사이의 균형을 잘 맞춤.",
        "keywords": [
            "Video Generation",
            "Image Understanding",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2511.23199",
            "authors": [
                {
                    "_id": "692d09fb4397b1ec214f67b2",
                    "user": {
                        "_id": "640ebdfefdeaae139086f4d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg",
                        "isPro": true,
                        "fullname": "Zhenxiong Tan",
                        "user": "Yuanshi",
                        "type": "user"
                    },
                    "name": "Zhenxiong Tan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T09:11:13.525Z",
                    "hidden": false
                },
                {
                    "_id": "692d09fb4397b1ec214f67b3",
                    "user": {
                        "_id": "668e740f1173ab43d9d9ed5e",
                        "avatarUrl": "/avatars/caa9b47c2a5f6d6d679759b8b234a0ab.svg",
                        "isPro": false,
                        "fullname": "Zeqing Wang",
                        "user": "INV-WZQ",
                        "type": "user"
                    },
                    "name": "Zeqing Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T10:06:43.036Z",
                    "hidden": false
                },
                {
                    "_id": "692d09fb4397b1ec214f67b4",
                    "user": {
                        "_id": "634cfebc350bcee9bed20a4d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
                        "isPro": false,
                        "fullname": "Xingyi Yang",
                        "user": "adamdad",
                        "type": "user"
                    },
                    "name": "Xingyi Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-01T09:11:11.586Z",
                    "hidden": false
                },
                {
                    "_id": "692d09fb4397b1ec214f67b5",
                    "name": "Songhua Liu",
                    "hidden": false
                },
                {
                    "_id": "692d09fb4397b1ec214f67b6",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/TtAxSiJXuWb3AASpinIdd.mp4"
            ],
            "publishedAt": "2025-11-28T14:03:39.000Z",
            "submittedOnDailyAt": "2025-12-01T02:09:07.640Z",
            "title": "Vision Bridge Transformer at Scale",
            "submittedOnDailyBy": {
                "_id": "634cfebc350bcee9bed20a4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
                "isPro": false,
                "fullname": "Xingyi Yang",
                "user": "adamdad",
                "type": "user"
            },
            "summary": "We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.",
            "upvotes": 27,
            "discussionId": "692d09fb4397b1ec214f67b7",
            "projectPage": "https://yuanshi9815.github.io/ViBT_homepage/",
            "githubRepo": "https://github.com/Yuanshi9815/ViBT",
            "ai_summary": "Bridge Models, instantiated as Vision Bridge Transformer (ViBT), efficiently translate data through direct modeling of input-to-output trajectories, achieving robust performance in image and video editing tasks at large scales.",
            "ai_keywords": [
                "Vision Bridge Transformer (ViBT)",
                "Brownian Bridge Models",
                "diffusion models",
                "data-to-data translation",
                "variance-stabilized velocity-matching objective"
            ],
            "githubStars": 29
        },
        "translation_title": "대규모 비전 브릿지 변환기(ViBT)",
        "purpose": "조건부 생성을 위한 새로운 모델을 통해 이미지 및 비디오 번역 작업의 효율성을 향상시키고자 함",
        "method": [
            "Brownian Bridge Models를 기반으로 하는 대규모 모델 ViBT를 소개함(We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation.)",
            "20B 및 1.3B 파라미터로 모델 크기를 확장하여 이미지와 비디오 번역 작업에서 효과성을 입증함(By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks.)",
            "Transformer 아키텍처를 채택하고, 강력한 훈련을 위한 분산 안정화 속도 일치 목표를 제안함(To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training.)"
        ],
        "conclusion": "브릿지 모델의 확장을 통해 이미지 편집 및 복잡한 비디오 번역 작업에서 강력한 성능을 확인한 결과를 기록함.",
        "keywords": [
            "Image Generation",
            "Video Generation",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2511.22663",
            "authors": [
                {
                    "_id": "692cff484397b1ec214f676e",
                    "name": "Dian Zheng",
                    "hidden": false
                },
                {
                    "_id": "692cff484397b1ec214f676f",
                    "name": "Manyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "692cff484397b1ec214f6770",
                    "name": "Hongyu Li",
                    "hidden": false
                },
                {
                    "_id": "692cff484397b1ec214f6771",
                    "name": "Kai Zou",
                    "hidden": false
                },
                {
                    "_id": "692cff484397b1ec214f6772",
                    "name": "Hongbo Liu",
                    "hidden": false
                },
                {
                    "_id": "692cff484397b1ec214f6773",
                    "name": "Ziyu Guo",
                    "hidden": false
                },
                {
                    "_id": "692cff484397b1ec214f6774",
                    "user": {
                        "_id": "67079840a9bcb7459b8d2a46",
                        "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
                        "isPro": false,
                        "fullname": "Kaituo Feng",
                        "user": "KaituoFeng",
                        "type": "user"
                    },
                    "name": "Kaituo Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T10:13:27.799Z",
                    "hidden": false
                },
                {
                    "_id": "692cff484397b1ec214f6775",
                    "user": {
                        "_id": "6497ff395b5d43c1c77d2a11",
                        "avatarUrl": "/avatars/e3704701ed3ea0c1c165001521f40086.svg",
                        "isPro": false,
                        "fullname": "Yexin Liu",
                        "user": "AI4VR",
                        "type": "user"
                    },
                    "name": "Yexin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-01T10:14:03.791Z",
                    "hidden": false
                },
                {
                    "_id": "692cff484397b1ec214f6776",
                    "name": "Ying Luo",
                    "hidden": false
                },
                {
                    "_id": "692cff484397b1ec214f6777",
                    "name": "Yan Feng",
                    "hidden": false
                },
                {
                    "_id": "692cff484397b1ec214f6778",
                    "name": "Peng Pei",
                    "hidden": false
                },
                {
                    "_id": "692cff484397b1ec214f6779",
                    "name": "Xunliang Cai",
                    "hidden": false
                },
                {
                    "_id": "692cff484397b1ec214f677a",
                    "name": "Hongsheng Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-27T17:55:25.000Z",
            "submittedOnDailyAt": "2025-12-01T00:57:45.124Z",
            "title": "Architecture Decoupling Is Not All You Need For Unified Multimodal Model",
            "submittedOnDailyBy": {
                "_id": "67079840a9bcb7459b8d2a46",
                "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
                "isPro": false,
                "fullname": "Kaituo Feng",
                "user": "KaituoFeng",
                "type": "user"
            },
            "summary": "Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.",
            "upvotes": 20,
            "discussionId": "692cff484397b1ec214f677b",
            "projectPage": "https://zhengdian1.github.io/AIA-project/",
            "githubRepo": "https://github.com/zhengdian1/AIA",
            "ai_summary": "The proposed Attention Interaction Alignment (AIA) loss improves cross-modal attention and performance in unified multimodal models for image generation and understanding without decoupling.",
            "ai_keywords": [
                "multimodal models",
                "image generation",
                "understanding",
                "task-specific multimodal interaction",
                "cross-modal attention",
                "Attention Interaction Alignment (AIA) loss",
                "SFT",
                "post-training stage",
                "Qwen-VL",
                "HunyuanImage",
                "Emu3",
                "Janus-Pro"
            ],
            "githubStars": 13
        },
        "translation_title": "아키텍처 분리는 통합 다중 모달 모델에 필요한 전부가 아니다",
        "purpose": "모델 분리 없이 작업 간의 충돌을 완화하고 성능을 높이는 방법 탐색",
        "method": [
            "모델 분리가 충돌을 완화하는 이유를 분석함(We analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models.)",
            "Attention Interaction Alignment (AIA) 손실을 제안하여 학습 과정에서 작업별 다중 모달 상호작용 패턴을 명시적으로 학습함(We propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training.)",
            "AIA 손실을 Emu3와 Janus-Pro에 적용하여 일반화를 입증함(We apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively.)"
        ],
        "conclusion": "AIA는 교차 모달 주의 패턴을 정제하고 생성 및 이해 성능을 모두 향상시킴.",
        "keywords": [
            "Multimodal Learning",
            "Image Generation",
            "Image Understanding"
        ]
    }
]