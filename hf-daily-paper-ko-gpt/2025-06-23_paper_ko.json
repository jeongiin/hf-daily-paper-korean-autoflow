[
    {
        "paper": {
            "id": "2506.16406",
            "authors": [
                {
                    "_id": "6858d099c0c8e29df8ea3ccb",
                    "name": "Zhiyuan Liang",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3ccc",
                    "name": "Dongwen Tang",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3ccd",
                    "name": "Yuhao Zhou",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3cce",
                    "name": "Xuanlei Zhao",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3ccf",
                    "name": "Mingjia Shi",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3cd0",
                    "name": "Wangbo Zhao",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3cd1",
                    "name": "Zekai Li",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3cd2",
                    "name": "Peihao Wang",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3cd3",
                    "name": "Konstantin Schürholt",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3cd4",
                    "name": "Damian Borth",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3cd5",
                    "name": "Michael M. Bronstein",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3cd6",
                    "name": "Yang You",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3cd7",
                    "name": "Zhangyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6858d099c0c8e29df8ea3cd8",
                    "user": {
                        "_id": "655452b8432af1b1116394d1",
                        "avatarUrl": "/avatars/85860fb3c2d09c9c23e7677d7129cca3.svg",
                        "isPro": false,
                        "fullname": "Kai Wang",
                        "user": "VictorKai1996NUS",
                        "type": "user"
                    },
                    "name": "Kai Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-23T08:14:35.657Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/655452b8432af1b1116394d1/ZdZe7m_v8V6alBd1NZa-m.mp4"
            ],
            "publishedAt": "2025-06-19T15:38:21.000Z",
            "submittedOnDailyAt": "2025-06-23T02:39:37.807Z",
            "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
            "submittedOnDailyBy": {
                "_id": "655452b8432af1b1116394d1",
                "avatarUrl": "/avatars/85860fb3c2d09c9c23e7677d7129cca3.svg",
                "isPro": false,
                "fullname": "Kai Wang",
                "user": "VictorKai1996NUS",
                "type": "user"
            },
            "summary": "Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank\nadaptation (LoRA) reduce the cost of customizing large language models (LLMs),\nyet still require a separate optimization run for every downstream dataset. We\nintroduce Drag-and-Drop LLMs (\\textit{DnD)}, a prompt-conditioned\nparameter generator that eliminates per-task training by mapping a handful of\nunlabeled task prompts directly to LoRA weight updates. A lightweight text\nencoder distills each prompt batch into condition embeddings, which are then\ntransformed by a cascaded hyper-convolutional decoder into the full set of LoRA\nmatrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD\nproduces task-specific parameters in seconds, yielding i) up to\n12,000times lower overhead than full fine-tuning, ii) average gains\nup to 30\\% in performance over the strongest training LoRAs on unseen\ncommon-sense reasoning, math, coding, and multimodal benchmarks, and iii)\nrobust cross-domain generalization despite never seeing the target data or\nlabels. Our results demonstrate that prompt-conditioned parameter generation is\na viable alternative to gradient-based adaptation for rapidly specializing\nLLMs. Our project is available at\nhttps://jerryliang24.github.io/DnD{https://jerryliang24.github.io/DnD}.",
            "upvotes": 65,
            "discussionId": "6858d099c0c8e29df8ea3cd9",
            "projectPage": "https://jerryliang24.github.io/DnD/",
            "ai_summary": "Drag-and-Drop LLMs generate task-specific parameters through prompt-conditioned parameter generation, achieving significant efficiency gains and cross-domain generalization without per-task training.",
            "ai_keywords": [
                "Parameter-Efficient Fine-Tuning",
                "PEFT",
                "low-rank adaptation",
                "LoRA",
                "large language models",
                "prompts",
                "condition embeddings",
                "hyper-convolutional decoder",
                "LoRA matrices",
                "common-sense reasoning",
                "math",
                "coding",
                "multimodal benchmarks"
            ]
        },
        "translation_title": "Drag-and-Drop LLMs: 제로 샷 프롬프트-투-웨이트",
        "purpose": "대규모 언어 모델(LLM)을 여러 데이터셋에 맞춤화할 때 필요한 비용과 시간을 줄이기 위한 방법 연구",
        "method": [
            "프롬프트 조건화된 매개변수 생성기인 Drag-and-Drop LLMs(DnD)를 도입함(We introduce Drag-and-Drop LLMs (DnD), a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates.)",
            "각 프롬프트 배치를 조건 임베딩으로 변환하는 경량 텍스트 인코더를 사용함(A lightweight text encoder distills each prompt batch into condition embeddings.)",
            "훈련된 임베딩을 Cascade 하이퍼 컨볼루션 디코더를 통해 전체 LoRA 행렬 집합으로 변환함, 이를 통해 작업별 매개변수를 빠르게 생성함(then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices.)"
        ],
        "conclusion": "DnD는 대규모 언어 모델을 빠르게 특화하는 효과적인 대안으로 확인되었으며, 전체 파인 튜닝보다 최대 12,000배 낮은 오버헤드로 성능을 향상시킴.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2506.16035",
            "authors": [
                {
                    "_id": "6858d76cc0c8e29df8ea3cdb",
                    "user": {
                        "_id": "638828121901766b88076aa1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638828121901766b88076aa1/rXlOO7eewmmaSN_hQIVz7.jpeg",
                        "isPro": false,
                        "fullname": "Vishesh Tripathi",
                        "user": "vishesh-t27",
                        "type": "user"
                    },
                    "name": "Vishesh Tripathi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-23T08:14:30.878Z",
                    "hidden": false
                },
                {
                    "_id": "6858d76cc0c8e29df8ea3cdc",
                    "name": "Tanmay Odapally",
                    "hidden": false
                },
                {
                    "_id": "6858d76cc0c8e29df8ea3cdd",
                    "name": "Indraneel Das",
                    "hidden": false
                },
                {
                    "_id": "6858d76cc0c8e29df8ea3cde",
                    "user": {
                        "_id": "64103f66928400b4164308f0",
                        "avatarUrl": "/avatars/6799d4a365776f83cecf7b9f468f3d4f.svg",
                        "isPro": false,
                        "fullname": "uday allu",
                        "user": "udayallu",
                        "type": "user"
                    },
                    "name": "Uday Allu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-23T08:14:33.310Z",
                    "hidden": false
                },
                {
                    "_id": "6858d76cc0c8e29df8ea3cdf",
                    "name": "Biddwan Ahmed",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-19T05:11:43.000Z",
            "submittedOnDailyAt": "2025-06-23T03:13:41.130Z",
            "title": "Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal\n  Document Understanding",
            "submittedOnDailyBy": {
                "_id": "638828121901766b88076aa1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638828121901766b88076aa1/rXlOO7eewmmaSN_hQIVz7.jpeg",
                "isPro": false,
                "fullname": "Vishesh Tripathi",
                "user": "vishesh-t27",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Generation (RAG) systems have revolutionized information\nretrieval and question answering, but traditional text-based chunking methods\nstruggle with complex document structures, multi-page tables, embedded figures,\nand contextual dependencies across page boundaries. We present a novel\nmultimodal document chunking approach that leverages Large Multimodal Models\n(LMMs) to process PDF documents in batches while maintaining semantic coherence\nand structural integrity. Our method processes documents in configurable page\nbatches with cross-batch context preservation, enabling accurate handling of\ntables spanning multiple pages, embedded visual elements, and procedural\ncontent. We evaluate our approach on a curated dataset of PDF documents with\nmanually crafted queries, demonstrating improvements in chunk quality and\ndownstream RAG performance. Our vision-guided approach achieves better accuracy\ncompared to traditional vanilla RAG systems, with qualitative analysis showing\nsuperior preservation of document structure and semantic coherence.",
            "upvotes": 45,
            "discussionId": "6858d76cc0c8e29df8ea3ce0",
            "ai_summary": "A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.",
            "ai_keywords": [
                "Retrieval-Augmented Generation (RAG)",
                "Large Multimodal Models (LMMs)",
                "document chunking",
                "semantic coherence",
                "structural integrity",
                "cross-batch context preservation",
                "vision-guided approach"
            ]
        },
        "translation_title": "비전 기반 청킹이 필요하다: 다중 모달 문서 이해로 RAG 향상",
        "purpose": "복잡한 문서 구조를 효과적으로 처리하기 위한 새로운 청킹 방법 연구",
        "method": [
            "대규모 다중 모달 모델(LMMs)을 활용하여 PDF 문서를 배치로 처리함(We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity.)",
            "컨텍스트 보존을 통해 다중 페이지에 걸쳐 있는 테이블과 시각적 요소를 정확하게 처리함(Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content.)",
            "수작업으로 작성된 쿼리를 사용하여 PDF 문서의 커리테드 데이터 세트에서 접근 방식을 평가함(We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance.)"
        ],
        "conclusion": "비전 기반 접근 방식이 전통적인 RAG 시스템보다 더 나은 정확성을 달성하고 문서 구조와 의미의 일관성을 더욱 잘 보존함을 확인함.",
        "keywords": [
            "Document Parsing",
            "Multimodal Learning",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2506.16054",
            "authors": [
                {
                    "_id": "6858e225c0c8e29df8ea3d0f",
                    "user": {
                        "_id": "6454568636821f6860fed410",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6454568636821f6860fed410/VsKn0GawOBZo1hmqHnmV1.png",
                        "isPro": false,
                        "fullname": "Tianchen Zhao",
                        "user": "A-suozhang",
                        "type": "user"
                    },
                    "name": "Tianchen Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-23T08:14:13.425Z",
                    "hidden": false
                },
                {
                    "_id": "6858e225c0c8e29df8ea3d10",
                    "name": "Ke Hong",
                    "hidden": false
                },
                {
                    "_id": "6858e225c0c8e29df8ea3d11",
                    "name": "Xinhao Yang",
                    "hidden": false
                },
                {
                    "_id": "6858e225c0c8e29df8ea3d12",
                    "name": "Xuefeng Xiao",
                    "hidden": false
                },
                {
                    "_id": "6858e225c0c8e29df8ea3d13",
                    "name": "Huixia Li",
                    "hidden": false
                },
                {
                    "_id": "6858e225c0c8e29df8ea3d14",
                    "name": "Feng Ling",
                    "hidden": false
                },
                {
                    "_id": "6858e225c0c8e29df8ea3d15",
                    "name": "Ruiqi Xie",
                    "hidden": false
                },
                {
                    "_id": "6858e225c0c8e29df8ea3d16",
                    "name": "Siqi Chen",
                    "hidden": false
                },
                {
                    "_id": "6858e225c0c8e29df8ea3d17",
                    "name": "Hongyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "6858e225c0c8e29df8ea3d18",
                    "name": "Yichong Zhang",
                    "hidden": false
                },
                {
                    "_id": "6858e225c0c8e29df8ea3d19",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-19T06:25:02.000Z",
            "submittedOnDailyAt": "2025-06-23T03:52:52.372Z",
            "title": "PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and\n  Quantized Attention in Visual Generation Models",
            "submittedOnDailyBy": {
                "_id": "6454568636821f6860fed410",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6454568636821f6860fed410/VsKn0GawOBZo1hmqHnmV1.png",
                "isPro": false,
                "fullname": "Tianchen Zhao",
                "user": "A-suozhang",
                "type": "user"
            },
            "summary": "In visual generation, the quadratic complexity of attention mechanisms\nresults in high memory and computational costs, especially for longer token\nsequences required in high-resolution image or multi-frame video generation. To\naddress this, prior research has explored techniques such as sparsification and\nquantization. However, these techniques face significant challenges under low\ndensity and reduced bitwidths. Through systematic analysis, we identify that\nthe core difficulty stems from the dispersed and irregular characteristics of\nvisual attention patterns. Therefore, instead of introducing specialized\nsparsification and quantization design to accommodate such patterns, we propose\nan alternative strategy: *reorganizing* the attention pattern to alleviate the\nchallenges. Inspired by the local aggregation nature of visual feature\nextraction, we design a novel **Pattern-Aware token ReOrdering (PARO)**\ntechnique, which unifies the diverse attention patterns into a\nhardware-friendly block-wise pattern. This unification substantially simplifies\nand enhances both sparsification and quantization. We evaluate the\nperformance-efficiency trade-offs of various design choices and finalize a\nmethodology tailored for the unified pattern. Our approach, **PAROAttention**,\nachieves video and image generation with lossless metrics, and nearly identical\nresults from full-precision (FP) baselines, while operating at notably lower\ndensity (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to\n**2.7x** end-to-end latency speedup.",
            "upvotes": 40,
            "discussionId": "6858e225c0c8e29df8ea3d1a",
            "projectPage": "https://a-suozhang.xyz/paroattn.github.io/",
            "ai_summary": "PAROAttention reorganizes visual attention patterns to enable efficient sparsification and quantization, reducing memory and computational costs with minimal impact on performance.",
            "ai_keywords": [
                "attention mechanisms",
                "sparsification",
                "quantization",
                "visual attention patterns",
                "Pattern-Aware token ReOrdering (PARO)",
                "local aggregation",
                "hardware-friendly block-wise pattern",
                "end-to-end latency speedup",
                "INT8/INT4",
                "PAROAttention"
            ]
        },
        "translation_title": "PAROAttention: 효율적인 희소 및 양자화된 주의력을 위한 패턴 인식 재배치",
        "purpose": "고해상도 이미지나 다중 프레임 비디오 생성에서의 메모리 및 계산 비용 문제 해결",
        "method": [
            "시각적 주의력 패턴의 분산 및 불규칙성을 분석하여 문제의 핵심 원인을 파악함(Through systematic analysis, we identify that the core difficulty stems from the dispersed and irregular characteristics of visual attention patterns.)",
            "주의력 패턴을 재조직하는 새로운 전략인 Pattern-Aware token ReOrdering (PARO) 기술을 제안함(Therefore, instead of introducing specialized sparsification and quantization design to accommodate such patterns, we propose an alternative strategy: reorganizing the attention pattern to alleviate the challenges.)",
            "PAROAttention 접근 방식을 통해 구현된 통합된 패턴을 위해 맞춤형 방법론을 확립하고 성능-효율성 간의 균형을 평가함(We evaluate the performance-efficiency trade-offs of various design choices and finalize a methodology tailored for the unified pattern.)"
        ],
        "conclusion": "PAROAttention은 손실 없는 지표로 비디오 및 이미지 생성을 달성하며, 전체 정밀도와 거의 동일한 결과를 유지하면서도 상당히 낮은 밀도와 비트폭에서 작동하여 엔드 투 엔드 대기 시간을 1.9배에서 2.7배 향상시킴.",
        "keywords": [
            "Image Generation",
            "Video Generation",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2506.09049",
            "authors": [
                {
                    "_id": "6858c341c0c8e29df8ea3c7f",
                    "user": {
                        "_id": "64eadcb03d76028d805a7818",
                        "avatarUrl": "/avatars/528e4fded4419caf08589b2ed40437bc.svg",
                        "isPro": false,
                        "fullname": "Li Kang",
                        "user": "FACEONG",
                        "type": "user"
                    },
                    "name": "Li Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-23T08:14:50.165Z",
                    "hidden": false
                },
                {
                    "_id": "6858c341c0c8e29df8ea3c80",
                    "name": "Xiufeng Song",
                    "hidden": false
                },
                {
                    "_id": "6858c341c0c8e29df8ea3c81",
                    "name": "Heng Zhou",
                    "hidden": false
                },
                {
                    "_id": "6858c341c0c8e29df8ea3c82",
                    "name": "Yiran Qin",
                    "hidden": false
                },
                {
                    "_id": "6858c341c0c8e29df8ea3c83",
                    "name": "Jie Yang",
                    "hidden": false
                },
                {
                    "_id": "6858c341c0c8e29df8ea3c84",
                    "name": "Xiaohong Liu",
                    "hidden": false
                },
                {
                    "_id": "6858c341c0c8e29df8ea3c85",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "6858c341c0c8e29df8ea3c86",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "6858c341c0c8e29df8ea3c87",
                    "name": "Zhenfei Yin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T17:59:44.000Z",
            "submittedOnDailyAt": "2025-06-23T01:31:50.738Z",
            "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "64eadcb03d76028d805a7818",
                "avatarUrl": "/avatars/528e4fded4419caf08589b2ed40437bc.svg",
                "isPro": false,
                "fullname": "Li Kang",
                "user": "FACEONG",
                "type": "user"
            },
            "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.",
            "upvotes": 27,
            "discussionId": "6858c341c0c8e29df8ea3c88",
            "projectPage": "https://faceong.github.io/VIKI-R/",
            "githubRepo": "https://github.com/MARS-EAI/VIKI-R",
            "ai_summary": "VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.",
            "ai_keywords": [
                "embodied agents",
                "VIKI-Bench",
                "multi-agent cooperation",
                "vision-language models",
                "VIKI-R",
                "Chain-of-Thought",
                "reinforcement learning",
                "compositional cooperation",
                "multi-level reward signals",
                "robot embodiments",
                "multi-view visual observations",
                "structured supervision signals"
            ]
        },
        "translation_title": "VIKI-R: 강화 학습을 통한 구체화된 다중 에이전트 협동 조정",
        "purpose": "다양한 환경에서 구체화된 다중 에이전트의 협동을 향상시키기 위한 구조적 벤치마크 및 방법론 구축",
        "method": [
            "VIKI-Bench라는 계층적 벤치마크를 소개하여 에이전트 활성화, 작업 계획 및 궤적 인식의 세 가지 구조적 수준을 설정함(we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception.)",
            "사전 학습된 비전-언어 모델(VLM)을 Chain-of-Thought 주석 시연으로 미세 조정하고 이후 다단계 보상 신호 아래 강화 학습을 수행하는 VIKI-R 프레임워크를 제안함(we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals.)",
            "광범위한 실험을 통해 VIKI-R이 모든 작업 수준에서 기준 방법보다 현저하게 성능이 우수함을 보여줌(Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels.)"
        ],
        "conclusion": "VIKI-Bench와 VIKI-R은 구체화된 AI 시스템에서 비주얼 기반 다중 에이전트 협동을 발전시키는 통합 테스트베드와 방법을 제공함.",
        "keywords": [
            "Reinforcement Learning",
            "Vision-Language Models",
            "Multi-Agent Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.17201",
            "authors": [
                {
                    "_id": "6858c46fc0c8e29df8ea3c8a",
                    "name": "Jiaqi Li",
                    "hidden": false
                },
                {
                    "_id": "6858c46fc0c8e29df8ea3c8b",
                    "name": "Junshu Tang",
                    "hidden": false
                },
                {
                    "_id": "6858c46fc0c8e29df8ea3c8c",
                    "name": "Zhiyong Xu",
                    "hidden": false
                },
                {
                    "_id": "6858c46fc0c8e29df8ea3c8d",
                    "name": "Longhuang Wu",
                    "hidden": false
                },
                {
                    "_id": "6858c46fc0c8e29df8ea3c8e",
                    "name": "Yuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "6858c46fc0c8e29df8ea3c8f",
                    "name": "Shuai Shao",
                    "hidden": false
                },
                {
                    "_id": "6858c46fc0c8e29df8ea3c90",
                    "name": "Tianbao Yu",
                    "hidden": false
                },
                {
                    "_id": "6858c46fc0c8e29df8ea3c91",
                    "name": "Zhiguo Cao",
                    "hidden": false
                },
                {
                    "_id": "6858c46fc0c8e29df8ea3c92",
                    "name": "Qinglin Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-20T17:50:37.000Z",
            "submittedOnDailyAt": "2025-06-23T01:35:50.876Z",
            "title": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Recent advances in diffusion-based and controllable video generation have\nenabled high-quality and temporally coherent video synthesis, laying the\ngroundwork for immersive interactive gaming experiences. However, current\nmethods face limitations in dynamics, generality, long-term consistency, and\nefficiency, which limit the ability to create various gameplay videos. To\naddress these gaps, we introduce Hunyuan-GameCraft, a novel framework for\nhigh-dynamic interactive video generation in game environments. To achieve\nfine-grained action control, we unify standard keyboard and mouse inputs into a\nshared camera representation space, facilitating smooth interpolation between\nvarious camera and movement operations. Then we propose a hybrid\nhistory-conditioned training strategy that extends video sequences\nautoregressively while preserving game scene information. Additionally, to\nenhance inference efficiency and playability, we achieve model distillation to\nreduce computational overhead while maintaining consistency across long\ntemporal sequences, making it suitable for real-time deployment in complex\ninteractive environments. The model is trained on a large-scale dataset\ncomprising over one million gameplay recordings across over 100 AAA games,\nensuring broad coverage and diversity, then fine-tuned on a carefully annotated\nsynthetic dataset to enhance precision and control. The curated game scene data\nsignificantly improves the visual fidelity, realism and action controllability.\nExtensive experiments demonstrate that Hunyuan-GameCraft significantly\noutperforms existing models, advancing the realism and playability of\ninteractive game video generation.",
            "upvotes": 18,
            "discussionId": "6858c46fc0c8e29df8ea3c93",
            "ai_summary": "Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.",
            "ai_keywords": [
                "diffusion-based",
                "controllable video generation",
                "temporally coherent video synthesis",
                "high-dynamic interactive video generation",
                "shared camera representation space",
                "hybrid history-conditioned training strategy",
                "model distillation",
                "real-time deployment",
                "large-scale dataset",
                "synthetic dataset",
                "game scene data"
            ]
        },
        "translation_title": "Hunyuan-GameCraft: 하이 다이나믹 인터랙티브 게임 비디오 생성기",
        "purpose": "다양한 게임플레이 비디오를 생성할 수 있는 능력을 향상시키기 위한 새로운 프레임워크 개발",
        "method": [
            "키보드와 마우스 입력을 통합하여 원활한 카메라와 동작 간 간섭이 가능하도록 함(To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations.)",
            "비디오 시퀀스를 자동 회귀 방식으로 확장하는 하이브리드 히스토리 조건 훈련 전략을 제안함(Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information.)",
            "모델 디스틸레이션을 통해 계산 부담을 줄이면서도 긴 시간 시퀀스에서의 일관성을 유지함(Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences.)"
        ],
        "conclusion": "Hunyuan-GameCraft는 기존 모델보다 뛰어난 성능을 보이며, 인터랙티브 게임 비디오 생성의 사실성과 플레이 가능성을 크게 향상시킴.",
        "keywords": [
            "Video Generation",
            "Computer Vision",
            "Robotics"
        ]
    }
]