[
    {
        "paper": {
            "id": "2505.09388",
            "authors": [
                {
                    "_id": "68299e3128752b51372d31ea",
                    "user": {
                        "_id": "62088594a5943c8a8fc94560",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png",
                        "isPro": false,
                        "fullname": "An Yang",
                        "user": "yangapku",
                        "type": "user"
                    },
                    "name": "An Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T06:43:00.733Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31eb",
                    "user": {
                        "_id": "6799128b9da39716ab1ebd95",
                        "avatarUrl": "/avatars/677d8ae2087137134c3f0e58f4cf769f.svg",
                        "isPro": false,
                        "fullname": "Anfeng Li",
                        "user": "laf070810",
                        "type": "user"
                    },
                    "name": "Anfeng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:15:44.771Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31ec",
                    "user": {
                        "_id": "64b0a77df12b47366663884c",
                        "avatarUrl": "/avatars/a212ea862abb5966060e439dd0e7656f.svg",
                        "isPro": false,
                        "fullname": "Baosong Yang",
                        "user": "Baosong",
                        "type": "user"
                    },
                    "name": "Baosong Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:15:37.853Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31ed",
                    "user": {
                        "_id": "64b93578ee257c3a4cfceed1",
                        "avatarUrl": "/avatars/e6188562254f75a09b4048b800860016.svg",
                        "isPro": false,
                        "fullname": "Beichen Zhang",
                        "user": "BeichenZhang",
                        "type": "user"
                    },
                    "name": "Beichen Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:16:13.672Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31ee",
                    "user": {
                        "_id": "61e4c4ca1ab24785ac11ba69",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg",
                        "isPro": false,
                        "fullname": "Binyuan Hui",
                        "user": "huybery",
                        "type": "user"
                    },
                    "name": "Binyuan Hui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:16:22.151Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31ef",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f0",
                    "user": {
                        "_id": "6583ab7983a9e1460c67d876",
                        "avatarUrl": "/avatars/74400bc448c3f07e23a4cd53d68a6af7.svg",
                        "isPro": false,
                        "fullname": "bowen",
                        "user": "bowenYu",
                        "type": "user"
                    },
                    "name": "Bowen Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:16:31.453Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f1",
                    "name": "Chang Gao",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f2",
                    "name": "Chengen Huang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f3",
                    "name": "Chenxu Lv",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f4",
                    "user": {
                        "_id": "610b70452719facd4ea85e28",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                        "isPro": false,
                        "fullname": "Chujie Zheng",
                        "user": "chujiezheng",
                        "type": "user"
                    },
                    "name": "Chujie Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T06:43:04.798Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f5",
                    "user": {
                        "_id": "6434d4989bd5a84b5dd0b0f5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
                        "isPro": false,
                        "fullname": "Dayiheng Liu",
                        "user": "Losin94",
                        "type": "user"
                    },
                    "name": "Dayiheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:17:32.677Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f6",
                    "name": "Fan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f7",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f8",
                    "name": "Feng Hu",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f9",
                    "name": "Hao Ge",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31fa",
                    "user": {
                        "_id": "6436618aeef1f55654a9f458",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436618aeef1f55654a9f458/OvxGtuDg2GAFG9As-2hzW.jpeg",
                        "isPro": false,
                        "fullname": "Haoran Wei",
                        "user": "HaoranWei",
                        "type": "user"
                    },
                    "name": "Haoran Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:17:56.110Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31fb",
                    "name": "Huan Lin",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31fc",
                    "user": {
                        "_id": "63281d05ac205d01918b5fc7",
                        "avatarUrl": "/avatars/fc3e0f7285bb2869a92670f764dfc535.svg",
                        "isPro": false,
                        "fullname": "Jialong Tang",
                        "user": "Jialong",
                        "type": "user"
                    },
                    "name": "Jialong Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:18:16.959Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31fd",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31fe",
                    "user": {
                        "_id": "654bead777401b47e6424f88",
                        "avatarUrl": "/avatars/7bcbdbb051c93b004f0dc3ad36c4a0ce.svg",
                        "isPro": false,
                        "fullname": "Jianhong Tu",
                        "user": "ToviTu",
                        "type": "user"
                    },
                    "name": "Jianhong Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:18:30.045Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31ff",
                    "name": "Jianwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3200",
                    "name": "Jianxin Yang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3201",
                    "name": "Jiaxi Yang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3202",
                    "name": "Jing Zhou",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3203",
                    "user": {
                        "_id": "602f88f5e8149a962412a667",
                        "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Jingren",
                        "type": "user"
                    },
                    "name": "Jingren Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:20:51.253Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3204",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3205",
                    "name": "Kai Dang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3206",
                    "name": "Keqin Bao",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3207",
                    "name": "Kexin Yang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3208",
                    "name": "Le Yu",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3209",
                    "name": "Lianghao Deng",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d320a",
                    "name": "Mei Li",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d320b",
                    "user": {
                        "_id": "5f8946925d083370c711f296",
                        "avatarUrl": "/avatars/14246aae3b1f8b7ad050f8ff2c8b260e.svg",
                        "isPro": false,
                        "fullname": "Mingfeng Xue",
                        "user": "mingfengxue",
                        "type": "user"
                    },
                    "name": "Mingfeng Xue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:21:56.048Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d320c",
                    "name": "Mingze Li",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d320d",
                    "name": "Pei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d320e",
                    "user": {
                        "_id": "62f220ccee7d7af44979efc7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f220ccee7d7af44979efc7/RImNglMumGCpAKB5gin6k.jpeg",
                        "isPro": false,
                        "fullname": "Peng Wang",
                        "user": "ZJUPeng",
                        "type": "user"
                    },
                    "name": "Peng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T06:43:02.813Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d320f",
                    "name": "Qin Zhu",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3210",
                    "name": "Rui Men",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3211",
                    "user": {
                        "_id": "6629ed94aabce1b25c3db90c",
                        "avatarUrl": "/avatars/cbc39db81c8e8f950d3bd2c2e03f71c8.svg",
                        "isPro": false,
                        "fullname": "Ruize Gao",
                        "user": "gaoruize",
                        "type": "user"
                    },
                    "name": "Ruize Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:21:46.295Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3212",
                    "name": "Shixuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3213",
                    "name": "Shuang Luo",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3214",
                    "name": "Tianhao Li",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3215",
                    "name": "Tianyi Tang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3216",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3217",
                    "name": "Xingzhang Ren",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3218",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3219",
                    "name": "Xinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d321a",
                    "name": "Xuancheng Ren",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d321b",
                    "name": "Yang Fan",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d321c",
                    "name": "Yang Su",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d321d",
                    "name": "Yichang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d321e",
                    "name": "Yinger Zhang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d321f",
                    "name": "Yu Wan",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3220",
                    "user": {
                        "_id": "666aacfb918ba11c7c598194",
                        "avatarUrl": "/avatars/45bee8f1fdbdd256ee47d25e4bf01a7a.svg",
                        "isPro": false,
                        "fullname": "Yuqiong Liu",
                        "user": "lyq333",
                        "type": "user"
                    },
                    "name": "Yuqiong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:20:06.363Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3221",
                    "name": "Zekun Wang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3222",
                    "user": {
                        "_id": "672c25ca8cfb61188128eb6f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FJWy9Tt7UQmu9KcTOx3Rt.png",
                        "isPro": false,
                        "fullname": "Zeyu Cui",
                        "user": "misakamage",
                        "type": "user"
                    },
                    "name": "Zeyu Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:19:43.843Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3223",
                    "name": "Zhenru Zhang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3224",
                    "name": "Zhipeng Zhou",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3225",
                    "user": {
                        "_id": "647ccbd6e07cf9bb2d485244",
                        "avatarUrl": "/avatars/e8915abaff04f6762247e196b7cf84df.svg",
                        "isPro": false,
                        "fullname": "Zihan Qiu",
                        "user": "QwQZh",
                        "type": "user"
                    },
                    "name": "Zihan Qiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:18:58.545Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-14T13:41:34.000Z",
            "submittedOnDailyAt": "2025-05-19T01:23:20.310Z",
            "title": "Qwen3 Technical Report",
            "submittedOnDailyBy": {
                "_id": "610b70452719facd4ea85e28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                "isPro": false,
                "fullname": "Chujie Zheng",
                "user": "chujiezheng",
                "type": "user"
            },
            "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
            "upvotes": 97,
            "discussionId": "68299e3228752b51372d325f",
            "projectPage": "https://qwenlm.github.io/blog/qwen3/",
            "githubRepo": "https://github.com/QwenLM/Qwen3",
            "ai_keywords": [
                "large language models (LLMs)",
                "Mixture-of-Expert (MoE) architectures",
                "thinking mode",
                "non-thinking mode",
                "chat-optimized models",
                "dedicated reasoning models",
                "thinking budget mechanism",
                "computational resources adaptively",
                "inference",
                "latency",
                "performance",
                "code generation",
                "mathematical reasoning",
                "agent tasks",
                "multilingual support",
                "cross-lingual understanding",
                "generation capabilities"
            ]
        },
        "translation_title": "Qwen3 기술 보고서",
        "purpose": "성능, 효율성 및 다국어 능력을 개선하기 위한 대규모 언어 모델 연구",
        "method": [
            "Qwen 모델 패밀리의 최신 버전인 Qwen3를 발표함(In this work, we present Qwen3, the latest version of the Qwen model family.)",
            "Qwen3는 밀집 아키텍처와 Mixture-of-Expert(MoE) 아키텍처를 가진 다양한 모델로 구성됨(The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures.)",
            "사고 모드와 비사고 모드를 통합한 통합 프레임워크를 채택하여 모델 전환의 필요성을 없앰(A key innovation in Qwen3 is the integration of thinking mode and non-thinking mode into a unified framework.)",
            "사용자가 추론 중 자원을 할당할 수 있는 사고 예산 메커니즘을 도입함(Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference.)"
        ],
        "conclusion": "Qwen3는 다양한 벤치마크에서 최첨단 결과를 달성하며, 다국어 지원을 29개 언어에서 119개로 확장하여 글로벌 접근성을 향상시킴.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2505.11049",
            "authors": [
                {
                    "_id": "682af4241286a7273c5bfd09",
                    "user": {
                        "_id": "6650c77a74664a42ddfb9187",
                        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
                        "isPro": false,
                        "fullname": "yueliu1999",
                        "user": "yueliu1999",
                        "type": "user"
                    },
                    "name": "Yue Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:18:09.117Z",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd0a",
                    "user": {
                        "_id": "6366429195204b4649c658b8",
                        "avatarUrl": "/avatars/5d80e9ebe0b57fd815f36796b9187248.svg",
                        "isPro": false,
                        "fullname": "Shengfang Zhai",
                        "user": "zsf",
                        "type": "user"
                    },
                    "name": "Shengfang Zhai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:18:16.514Z",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd0b",
                    "user": {
                        "_id": "61711f02e0b1ddb56eb9b526",
                        "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
                        "isPro": true,
                        "fullname": "Mingzhe Du",
                        "user": "Elfsong",
                        "type": "user"
                    },
                    "name": "Mingzhe Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:18:22.634Z",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd0c",
                    "user": {
                        "_id": "65efc25828426de60f977dfc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8ZcIoo58JPLdnjm-jZeo.png",
                        "isPro": false,
                        "fullname": "Yulin Chen",
                        "user": "CallMeChen",
                        "type": "user"
                    },
                    "name": "Yulin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:18:29.818Z",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd0d",
                    "name": "Tri Cao",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd0e",
                    "user": {
                        "_id": "62728f4f6253fe2068da1021",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
                        "isPro": false,
                        "fullname": "Hongcheng Gao",
                        "user": "HongchengGao",
                        "type": "user"
                    },
                    "name": "Hongcheng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:18:48.924Z",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd0f",
                    "name": "Cheng Wang",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd10",
                    "name": "Xinfeng Li",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd11",
                    "name": "Kun Wang",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd12",
                    "name": "Junfeng Fang",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd13",
                    "user": {
                        "_id": "669e19e5dac1eb34c0f5f505",
                        "avatarUrl": "/avatars/bec7d1d1dac2ad6570844d1f00e7df0a.svg",
                        "isPro": false,
                        "fullname": "Jiaheng Zhang",
                        "user": "jiaheng233",
                        "type": "user"
                    },
                    "name": "Jiaheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:19:28.414Z",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd14",
                    "user": {
                        "_id": "651d8032c50012d33e914f2f",
                        "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
                        "isPro": false,
                        "fullname": "Bryan Hooi",
                        "user": "bhooi",
                        "type": "user"
                    },
                    "name": "Bryan Hooi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:19:35.772Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T09:46:10.000Z",
            "submittedOnDailyAt": "2025-05-19T07:36:35.140Z",
            "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning",
            "submittedOnDailyBy": {
                "_id": "6650c77a74664a42ddfb9187",
                "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
                "isPro": false,
                "fullname": "yueliu1999",
                "user": "yueliu1999",
                "type": "user"
            },
            "summary": "To enhance the safety of VLMs, this paper introduces a novel reasoning-based\nVLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the\nguard model to deliberatively reason before making moderation decisions via\nonline RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with\n123K samples and 631K reasoning steps, spanning text, image, and text-image\ninputs. Then, based on it, we cold-start our model's reasoning ability via SFT.\nIn addition, we further enhance reasoning regarding moderation through online\nRL. Concretely, to enhance diversity and difficulty of samples, we conduct\nrejection sampling followed by data augmentation via the proposed safety-aware\ndata concatenation. Besides, we use a dynamic clipping parameter to encourage\nexploration in early stages and exploitation in later stages. To balance\nperformance and token efficiency, we design a length-aware safety reward that\nintegrates accuracy, format, and token cost. Extensive experiments demonstrate\nthe superiority of our model. Remarkably, it surpasses the runner-up by 19.27%\nF1 score on average. We release data, code, and models (3B/7B) of\nGuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/",
            "upvotes": 39,
            "discussionId": "682af42c1286a7273c5bfed9",
            "ai_keywords": [
                "GuardReasoner-VL",
                "online RL",
                "GuardReasoner-VLTrain",
                "reasoning corpus",
                "SFT",
                "rejection sampling",
                "data augmentation",
                "safety-aware data concatenation",
                "dynamic clipping parameter",
                "length-aware safety reward",
                "F1 score"
            ]
        },
        "translation_title": "GuardReasoner-VL: VLM을 강화된 추론으로 안전하게 보호하기",
        "purpose": "VLM의 안전성을 향상시키기 위한 새로운 추론 기반 VLM 가드 모델 개발",
        "method": [
            "GuardReasoner-VLTrain이라는 123K 샘플과 631K 추론 단계를 포함한 추론 코퍼스를 구축함(First, we construct GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps.)",
            "SFT를 통해 모델의 추론 능력을 초기화하고, 이후 온라인 RL을 통해 조정에 대한 추론을 더욱 향상시킴(we cold-start our model's reasoning ability via SFT. In addition, we further enhance reasoning regarding moderation through online RL.)",
            "수용과 샘플 다양성 및 난이도를 높이기 위해 데이터 증대와 동적 클리핑 파라미터를 사용함(Besides, we use a dynamic clipping parameter to encourage exploration in early stages and exploitation in later stages.)"
        ],
        "conclusion": "우리 모델은 19.27% F1 점수로 차기 모델을 초월하며, 데이터를 공개하고 여러 가지 모델을 제공함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2505.10610",
            "authors": [
                {
                    "_id": "682adaf581c740ab4aabc5a3",
                    "user": {
                        "_id": "62281c11236b7b2eefa7f198",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62281c11236b7b2eefa7f198/xuTCVMUahpg4Mfb7L62sm.jpeg",
                        "isPro": false,
                        "fullname": "Zhaowei Wang",
                        "user": "ZhaoweiWang",
                        "type": "user"
                    },
                    "name": "Zhaowei Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T13:09:12.290Z",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5a4",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5a5",
                    "name": "Xiyu Ren",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5a6",
                    "name": "Jipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5a7",
                    "user": {
                        "_id": "654a500d9b8bd6406d431c0d",
                        "avatarUrl": "/avatars/a6b76441bbc6f4b71d49c52e454c9ef7.svg",
                        "isPro": false,
                        "fullname": "Yu Zhao",
                        "user": "yuzhaouoe",
                        "type": "user"
                    },
                    "name": "Yu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T13:09:10.234Z",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5a8",
                    "user": {
                        "_id": "657ccbf2869d5bb0e53b482f",
                        "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
                        "isPro": false,
                        "fullname": "Rohit Saxena",
                        "user": "rohitsaxena",
                        "type": "user"
                    },
                    "name": "Rohit Saxena",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T13:09:08.362Z",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5a9",
                    "name": "Liang Cheng",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5aa",
                    "name": "Ginny Wong",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5ab",
                    "name": "Simon See",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5ac",
                    "name": "Pasquale Minervini",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5ad",
                    "name": "Yangqiu Song",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5ae",
                    "name": "Mark Steedman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T17:52:54.000Z",
            "submittedOnDailyAt": "2025-05-19T08:37:50.522Z",
            "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models\n  Effectively and Thoroughly",
            "submittedOnDailyBy": {
                "_id": "657ccbf2869d5bb0e53b482f",
                "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
                "isPro": false,
                "fullname": "Rohit Saxena",
                "user": "rohitsaxena",
                "type": "user"
            },
            "summary": "The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs.",
            "upvotes": 33,
            "discussionId": "682adaf681c740ab4aabc5e2",
            "projectPage": "https://zhaowei-wang-nlp.github.io/MMLongBench-page/",
            "githubRepo": "https://github.com/EdinburghNLP/MMLongBench",
            "ai_keywords": [
                "long-context vision-language models (LCVLMs)",
                "MMLongBench",
                "Visual RAG",
                "Many-Shot ICL",
                "vision patches",
                "cross-modal tokenization scheme",
                "long-context vision-language tasks",
                "reasoning ability"
            ]
        },
        "translation_title": "MMLongBench: Long-Context Vision-Language 모델 벤치마킹",
        "purpose": "Long-Context Vision-Language 모델(LCVLM)의 성능을 효과적으로 평가하기 위한 기준 마련",
        "method": [
            "다양한 장기 맥락 Vision-Language 작업을 포함하는 MMLongBench라는 벤치마크를 도입함(we introduce MMLongBench, the first benchmark covering a diverse set of long-context vision-language tasks)",
            "13,331개의 예시를 다섯 가지 다양한 하위 작업 범주에 걸쳐 구성함(MMLongBench is composed of 13,331 examples spanning five different categories of downstream tasks)",
            "모델의 입력 길이에 대한 강인성을 평가하기 위해 모든 예시는 5개의 표준화된 입력 길이(8K-128K tokens)로 제공됨(all examples are delivered at five standardized input lengths (8K-128K tokens))"
        ],
        "conclusion": "MMLongBench는 LCVLM의 진단과 발전을 위한 중요한 기초를 제공하며, 장기 맥락 Vision-Language 작업에서 모델이 겪는 한계를 강조함.",
        "keywords": [
            "Vision-Language Models",
            "Natural Language Processing",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2505.11409",
            "authors": [
                {
                    "_id": "682abb7984695084c1a48eab",
                    "name": "Yi Xu",
                    "hidden": false
                },
                {
                    "_id": "682abb7984695084c1a48eac",
                    "user": {
                        "_id": "650e9b0288cdfe73a8575923",
                        "avatarUrl": "/avatars/0fc7fcd0776f63ea5f50a310e7def2f5.svg",
                        "isPro": false,
                        "fullname": "Chengzu Li",
                        "user": "chengzu",
                        "type": "user"
                    },
                    "name": "Chengzu Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T13:09:14.188Z",
                    "hidden": false
                },
                {
                    "_id": "682abb7984695084c1a48ead",
                    "user": {
                        "_id": "62b279e92375526ae51a537b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
                        "isPro": false,
                        "fullname": "Han Zhou",
                        "user": "hzhouml",
                        "type": "user"
                    },
                    "name": "Han Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T06:42:16.276Z",
                    "hidden": false
                },
                {
                    "_id": "682abb7984695084c1a48eae",
                    "user": {
                        "_id": "65bf213f8467e2a3d6374d4b",
                        "avatarUrl": "/avatars/0194cdba95d7a4c01fbbdd505e384a3d.svg",
                        "isPro": false,
                        "fullname": "X Wan",
                        "user": "masonxw",
                        "type": "user"
                    },
                    "name": "Xingchen Wan",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-19T05:02:52.536Z",
                    "hidden": false
                },
                {
                    "_id": "682abb7984695084c1a48eaf",
                    "user": {
                        "_id": "63920dfac47e36ddeb8f1864",
                        "avatarUrl": "/avatars/c36cbf7b084d62368312e5c9292e4260.svg",
                        "isPro": false,
                        "fullname": "Caiqi Zhang",
                        "user": "caiqizh",
                        "type": "user"
                    },
                    "name": "Caiqi Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:23:48.005Z",
                    "hidden": false
                },
                {
                    "_id": "682abb7984695084c1a48eb0",
                    "user": {
                        "_id": "617a6284941993035fbaf299",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635410461794-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Anna Korhonen",
                        "user": "akorhonen",
                        "type": "user"
                    },
                    "name": "Anna Korhonen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:23:42.059Z",
                    "hidden": false
                },
                {
                    "_id": "682abb7984695084c1a48eb1",
                    "user": {
                        "_id": "6273e70dc8d55dd434bd8e52",
                        "avatarUrl": "/avatars/3483eeda218e95b1eb00c3dc63c7d000.svg",
                        "isPro": false,
                        "fullname": "Ivan Vulić",
                        "user": "ivulic",
                        "type": "user"
                    },
                    "name": "Ivan Vulić",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:23:36.111Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62b279e92375526ae51a537b/VYeWx-h6G2brVuuu-Wg5i.png"
            ],
            "publishedAt": "2025-05-16T16:17:22.000Z",
            "submittedOnDailyAt": "2025-05-19T03:37:48.826Z",
            "title": "Visual Planning: Let's Think Only with Images",
            "submittedOnDailyBy": {
                "_id": "62b279e92375526ae51a537b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
                "isPro": false,
                "fullname": "Han Zhou",
                "user": "hzhouml",
                "type": "user"
            },
            "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.",
            "upvotes": 25,
            "discussionId": "682abb7c84695084c1a48fb4",
            "githubRepo": "https://github.com/yix8/VisualPlanning",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "multimodal extensions (MLLMs)",
                "machine reasoning",
                "visual information",
                "Visual Planning",
                "purely visual representations",
                "sequences of images",
                "step-by-step inference",
                "Visual Planning via Reinforcement Learning (VPRL)",
                "GRPO",
                "post-training large vision models",
                "planning",
                "visual navigation tasks",
                "FrozenLake",
                "Maze",
                "MiniBehavior",
                "text-only space",
                "intuitive, image-based inference"
            ]
        },
        "translation_title": "시각적 계획: 이미지만으로 생각해보자",
        "purpose": "텍스트에 의존하지 않고 순수한 시각적 표현을 통해 계획을 세우기 위한 새로운 패러다임 제안",
        "method": [
            "시각 정보를 사용하여 단계별 추론을 수행하는 이미지 시퀀스를 통해 계획을 실행함(we propose a new paradigm, Visual Planning, which enables planning through purely visual representations).",
            "강화 학습 프레임워크인 VPRL을 도입하여 대규모 비전 모델의 훈련 후 개선을 이룸(We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models).",
            "FrozenLake, Maze, MiniBehavior와 같은 대표적인 시각 탐색 작업에서 계획의 개선을 확인함(leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior)."
        ],
        "conclusion": "Visual Planning은 언어 기반 추론의 대안으로 효과적이며 이미지 기반 추론을 활용한 새로운 작업 가능성을 열어줌.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "3D Vision"
        ]
    },
    {
        "paper": {
            "id": "2505.07675",
            "authors": [
                {
                    "_id": "6829dcab0daa5ccc817e6ec8",
                    "user": {
                        "_id": "6788641365213237551a0f59",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/K2nOtlmIYas_sJfElPKlH.png",
                        "isPro": false,
                        "fullname": "KangSeongJae",
                        "user": "namusum",
                        "type": "user"
                    },
                    "name": "Seongjae Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:20:17.291Z",
                    "hidden": false
                },
                {
                    "_id": "6829dcab0daa5ccc817e6ec9",
                    "user": {
                        "_id": "64f000769e7770db74d44bba",
                        "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
                        "isPro": false,
                        "fullname": "Dong-Bok Lee",
                        "user": "dongboklee",
                        "type": "user"
                    },
                    "name": "Dong Bok Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T06:42:58.152Z",
                    "hidden": false
                },
                {
                    "_id": "6829dcab0daa5ccc817e6eca",
                    "user": {
                        "_id": "633ad9e4976a7d6910e84d15",
                        "avatarUrl": "/avatars/bb464e7503d6e865d1c351430982f7dd.svg",
                        "isPro": false,
                        "fullname": "Hyungjoon Jang",
                        "user": "hjoon",
                        "type": "user"
                    },
                    "name": "Hyungjoon Jang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:20:23.560Z",
                    "hidden": false
                },
                {
                    "_id": "6829dcab0daa5ccc817e6ecb",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T15:39:51.000Z",
            "submittedOnDailyAt": "2025-05-19T06:17:24.942Z",
            "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization",
            "submittedOnDailyBy": {
                "_id": "64f000769e7770db74d44bba",
                "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
                "isPro": false,
                "fullname": "Dong-Bok Lee",
                "user": "dongboklee",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\ntexttt{D}ual-texttt{H}ead\ntexttt{O}ptimization (texttt{DHO}) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that DHO mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that DHO\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.",
            "upvotes": 13,
            "discussionId": "6829dcad0daa5ccc817e6f40",
            "ai_keywords": [
                "Vision-language models (VLMs)",
                "knowledge distillation (KD)",
                "dual prediction heads",
                "gradient conflicts",
                "feature learning",
                "semi-supervised settings",
                "state-of-the-art performance",
                "ImageNet",
                "accuracy"
            ]
        },
        "translation_title": "Vision-Language 모델에서 단순 준지도 지식을 증류하는 DHO 최적화 기법",
        "purpose": "자원 제약 환경에서 간단한 방식으로 Vision-Language 모델에서 지식을 압축 모델로 전달하여 성능 향상",
        "method": [
            "Dual-head를 통해 레이블이 있는 데이터와 교사 예측에서 독립적으로 학습하도록 구성함(we introduce dual prediction heads that independently learn from labeled data and teacher predictions.)",
            "Inference 시 두 개의 출력 결과를 선형 결합함(propose to linearly combine their outputs during inference.)",
            "DHO가 지도 학습 신호와 증류 신호 간의 gradient 충돌을 완화함(DHO mitigates gradient conflicts between supervised and distillation signals.)"
        ],
        "conclusion": "DHO는 여러 도메인과 세부 데이터셋에서 우수한 성능을 보이며, ImageNet에서 1%와 10%의 라벨 데이터를 사용할 때 각각 3%와 0.1% 향상된 정확도를 달성함.",
        "keywords": [
            "Knowledge Distillation",
            "Vision-Language Models",
            "Image Classification"
        ]
    }
]