[
    {
        "paper": {
            "id": "2507.23726",
            "authors": [
                {
                    "_id": "688c16788c434640078cc348",
                    "name": "Luoxin Chen",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc349",
                    "name": "Jinming Gu",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc34a",
                    "name": "Liankai Huang",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc34b",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc34c",
                    "name": "Zhicheng Jiang",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc34d",
                    "name": "Allan Jie",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc34e",
                    "name": "Xiaoran Jin",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc34f",
                    "name": "Xing Jin",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc350",
                    "name": "Chenggang Li",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc351",
                    "name": "Kaijing Ma",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc352",
                    "name": "Cheng Ren",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc353",
                    "name": "Jiawei Shen",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc354",
                    "name": "Wenlei Shi",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc355",
                    "name": "Tong Sun",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc356",
                    "name": "He Sun",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc357",
                    "name": "Jiahui Wang",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc358",
                    "name": "Siran Wang",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc359",
                    "name": "Zhihong Wang",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc35a",
                    "name": "Chenrui Wei",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc35b",
                    "name": "Shufa Wei",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc35c",
                    "name": "Yonghui Wu",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc35d",
                    "name": "Yuchen Wu",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc35e",
                    "name": "Yihang Xia",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc35f",
                    "name": "Huajian Xin",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc360",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc361",
                    "name": "Huaiyuan Ying",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc362",
                    "name": "Hongyi Yuan",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc363",
                    "name": "Zheng Yuan",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc364",
                    "name": "Tianyang Zhan",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc365",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc366",
                    "name": "Yue Zhang",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc367",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc368",
                    "name": "Tianyun Zhao",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc369",
                    "name": "Jianqiu Zhao",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc36a",
                    "name": "Yichi Zhou",
                    "hidden": false
                },
                {
                    "_id": "688c16788c434640078cc36b",
                    "name": "Thomas Hanwen Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-31T17:00:30.000Z",
            "submittedOnDailyAt": "2025-08-01T00:07:55.891Z",
            "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
            "submittedOnDailyBy": {
                "_id": "5f694b8b5e78cc6b0ed31bd2",
                "avatarUrl": "/avatars/63f21d30918c34fff9ea592e1039b3f0.svg",
                "isPro": false,
                "fullname": "Zheng Yuan",
                "user": "GanjinZero",
                "type": "user"
            },
            "summary": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\nSeed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine Seed-Geometry, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning.",
            "upvotes": 61,
            "discussionId": "688c16798c434640078cc36c",
            "githubRepo": "https://github.com/ByteDance-Seed/Seed-Prover",
            "ai_summary": "Seed-Prover, a lemma-style reasoning model using Lean, achieves high performance in formal theorem proving and automated mathematical reasoning through iterative refinement and specialized geometry support.",
            "ai_keywords": [
                "reinforcement learning",
                "long chain-of-thought",
                "theorem proving",
                "formal verification",
                "lemma-style reasoning",
                "Seed-Prover",
                "MiniF2F",
                "PutnamBench",
                "Seed-Geometry",
                "automated mathematical reasoning"
            ],
            "githubStars": 122
        },
        "translation_title": "Seed-Prover: 자동 정리 증명을 위한 깊고 광범위한 추론",
        "purpose": "자동 정리 증명을 위한 효과적인 훈련과 추론 모델 개발",
        "method": [
            "LLM을 활용하여 긴 사고 과정을 통한 강화 학습을 통해 수학적 추론 능력을 보여줌(LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought.)",
            "전문 도메인 특화 언어인 Lean을 통해 명확한 감독 신호를 제공하여 효과적인 훈련이 가능하도록 함(Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning.)",
            "Seed-Prover라는 lemma 스타일의 전체 증명 추론 모델을 제안하고, Lean 피드백, 증명된 보조 정리 및 자기 요약을 기반으로 증명을 반복적으로 개선함. (In this work, we propose Seed-Prover, a lemma-style whole-proof reasoning model.)"
        ],
        "conclusion": "Seed-Prover는 IMO 문제의 78.1%를 증명하고, PutnamBench에서 50% 이상을 달성하며 이전의 최첨단 기술을 크게 초월하는 성과를 기록함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Automated Theorem Proving"
        ]
    },
    {
        "paper": {
            "id": "2507.23779",
            "authors": [
                {
                    "_id": "688c55378c434640078cc488",
                    "name": "Miaosen Zhang",
                    "hidden": false
                },
                {
                    "_id": "688c55378c434640078cc489",
                    "name": "Ziqiang Xu",
                    "hidden": false
                },
                {
                    "_id": "688c55378c434640078cc48a",
                    "name": "Jialiang Zhu",
                    "hidden": false
                },
                {
                    "_id": "688c55378c434640078cc48b",
                    "name": "Qi Dai",
                    "hidden": false
                },
                {
                    "_id": "688c55378c434640078cc48c",
                    "name": "Kai Qiu",
                    "hidden": false
                },
                {
                    "_id": "688c55378c434640078cc48d",
                    "name": "Yifan Yang",
                    "hidden": false
                },
                {
                    "_id": "688c55378c434640078cc48e",
                    "name": "Chong Luo",
                    "hidden": false
                },
                {
                    "_id": "688c55378c434640078cc48f",
                    "name": "Tianyi Chen",
                    "hidden": false
                },
                {
                    "_id": "688c55378c434640078cc490",
                    "name": "Justin Wagle",
                    "hidden": false
                },
                {
                    "_id": "688c55378c434640078cc491",
                    "name": "Tim Franklin",
                    "hidden": false
                },
                {
                    "_id": "688c55378c434640078cc492",
                    "name": "Baining Guo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62a30bf72dac39c2173c0a8c/Y83hgDElGoQEw90UC2-H6.png"
            ],
            "publishedAt": "2025-07-31T17:59:09.000Z",
            "submittedOnDailyAt": "2025-08-01T04:31:40.973Z",
            "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding",
            "submittedOnDailyBy": {
                "_id": "62a30bf72dac39c2173c0a8c",
                "avatarUrl": "/avatars/15fb1ea3dcc7ccd8bc8002ce282e27b3.svg",
                "isPro": false,
                "fullname": "Miaosen Zhang",
                "user": "Miaosen",
                "type": "user"
            },
            "summary": "With the development of multimodal reasoning models, Computer Use Agents\n(CUAs), akin to Jarvis from \"Iron Man\", are becoming a reality. GUI\ngrounding is a core component for CUAs to execute actual actions, similar to\nmechanical control in robotics, and it directly leads to the success or failure\nof the system. It determines actions such as clicking and typing, as well as\nrelated parameters like the coordinates for clicks. Current end-to-end\ngrounding models still achieve less than 65\\% accuracy on challenging\nbenchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from\nbeing ready for deployment. % , as a single misclick can result in unacceptable\nconsequences. In this work, we conduct an empirical study on the training of\ngrounding models, examining details from data collection to model training.\nUltimately, we developed the Phi-Ground model family, which achieves\nstate-of-the-art performance across all five grounding benchmarks for models\nunder 10B parameters in agent settings. In the end-to-end model setting, our\nmodel still achieves SOTA results with scores of \\textbf{43.2} on\nScreenSpot-pro and \\textbf{27.2} on UI-Vision. We believe that the\nvarious details discussed in this paper, along with our successes and failures,\nnot only clarify the construction of grounding models but also benefit other\nperception tasks. Project homepage:\nhttps://zhangmiaosen2000.github.io/Phi-Ground/{https://zhangmiaosen2000.github.io/Phi-Ground/}",
            "upvotes": 26,
            "discussionId": "688c55388c434640078cc493",
            "projectPage": "https://zhangmiaosen2000.github.io/Phi-Ground/",
            "githubRepo": "https://github.com/zhangmiaosen2000/Phi-Ground",
            "ai_summary": "The Phi-Ground model family achieves state-of-the-art performance in GUI grounding for multimodal reasoning models, improving accuracy across various benchmarks.",
            "ai_keywords": [
                "multimodal reasoning models",
                "Computer Use Agents",
                "GUI grounding",
                "end-to-end grounding models",
                "ScreenSpot-pro",
                "UI-Vision",
                "Phi-Ground model family"
            ],
            "githubStars": 8
        },
        "translation_title": "Phi-Ground 기술 보고서: GUI 그라운딩에서의 인식 향상",
        "purpose": "Computer Use Agents(CUAs)가 실제 행동을 수행할 수 있도록 GUI 그라운딩의 정확성을 향상시키기 위한 연구",
        "method": [
            "기초 데이터 수집 및 모델 훈련을 포함한 그라운딩 모델의 훈련에 대한 실증 연구를 진행함(We conduct an empirical study on the training of grounding models, examining details from data collection to model training.)",
            "10B 미만의 매개변수를 가진 모델에 대해 모든 5개 그라운딩 벤치마크에서 최첨단 성능을 달성하는 Phi-Ground 모델 가족을 개발함(Ultimately, we developed the Phi-Ground model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under 10B parameters in agent settings.)"
        ],
        "conclusion": "Phi-Ground 모델은 ScreenSpot-pro에서 43.2, UI-Vision에서 27.2의 점수를 기록하며 최첨단 성과를 달성함.",
        "keywords": [
            "Computer Vision",
            "Multimodal Learning",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2507.22968",
            "authors": [
                {
                    "_id": "688c13408c434640078cc335",
                    "name": "Chengqian Ma",
                    "hidden": false
                },
                {
                    "_id": "688c13408c434640078cc336",
                    "name": "Wei Tao",
                    "hidden": false
                },
                {
                    "_id": "688c13408c434640078cc337",
                    "name": "Yiwen Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-30T17:56:23.000Z",
            "submittedOnDailyAt": "2025-08-01T00:51:53.259Z",
            "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring\n  Challenges in Complex Conversations",
            "submittedOnDailyBy": {
                "_id": "6355473d525beaee688b7ba1",
                "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
                "isPro": false,
                "fullname": "Wei Tao",
                "user": "itaowe",
                "type": "user"
            },
            "summary": "Spoken Dialogue Models (SDMs) have recently attracted significant attention\nfor their ability to generate voice responses directly to users' spoken\nqueries. Despite their increasing popularity, there exists a gap in research\nfocused on comprehensively understanding their practical effectiveness in\ncomprehending and emulating human conversations. This is especially true\ncompared to text-based Large Language Models (LLMs), which benefit from\nextensive benchmarking. Human voice interactions are inherently more complex\nthan text due to characteristics unique to spoken dialogue. Ambiguity poses one\nchallenge, stemming from semantic factors like polysemy, as well as\nphonological aspects such as heterograph, heteronyms, and stress patterns.\nAdditionally, context-dependency, like omission, coreference, and multi-turn\ninteraction, adds further complexity to human conversational dynamics. To\nilluminate the current state of SDM development and to address these\nchallenges, we present a benchmark dataset in this paper, which comprises 1,079\ninstances in English and Chinese. Accompanied by an LLM-based evaluation method\nthat closely aligns with human judgment, this dataset facilitates a\ncomprehensive exploration of the performance of SDMs in tackling these\npractical challenges.",
            "upvotes": 18,
            "discussionId": "688c13418c434640078cc338",
            "projectPage": "https://step-out.github.io/C3-web/",
            "githubRepo": "https://github.com/step-out/C3",
            "ai_summary": "A benchmark dataset for Spoken Dialogue Models (SDMs) in English and Chinese is presented to evaluate their performance in understanding and emulating human conversations, addressing challenges like ambiguity and context-dependency.",
            "ai_keywords": [
                "Spoken Dialogue Models",
                "SDMs",
                "Large Language Models",
                "LLMs",
                "benchmark dataset",
                "human judgment",
                "ambiguity",
                "polysemy",
                "heterograph",
                "heteronyms",
                "stress patterns",
                "context-dependency",
                "omission",
                "coreference",
                "multi-turn interaction"
            ],
            "githubStars": 17
        },
        "translation_title": "C3: 복잡한 대화를 탐구하는 음성 대화 모델을 위한 이중 언어 벤치마크",
        "purpose": "음성 대화 모델(SDM)의 실질적인 효과를 이해하고 평가하기 위한 벤치마크 데이터셋 제공",
        "method": [
            "음성 대화의 복잡성을 극복하기 위해 영어와 중국어로 1,079개의 사례를 포함한 데이터셋을 제시함(To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese.)",
            "인간의 판단과 밀접하게 일치하는 LLM 기반 평가 방법을 포함함(Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.)"
        ],
        "conclusion": "제공된 데이터셋은 음성 대화 모델의 성능을 포괄적으로 탐구하는 데 기여하며, 복잡한 대화의 도전을 다루는 데 효과적임.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Spoken Dialogue Models"
        ]
    },
    {
        "paper": {
            "id": "2507.22879",
            "authors": [
                {
                    "_id": "688aebf98b724c8c7187dd92",
                    "name": "Chao Yi",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd93",
                    "name": "Dian Chen",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd94",
                    "name": "Gaoyang Guo",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd95",
                    "name": "Jiakai Tang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd96",
                    "name": "Jian Wu",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd97",
                    "name": "Jing Yu",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd98",
                    "name": "Sunhao Dai",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd99",
                    "name": "Wen Chen",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd9a",
                    "name": "Wenjun Yang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd9b",
                    "name": "Yuning Jiang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd9c",
                    "name": "Zhujin Gao",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd9d",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd9e",
                    "name": "Chi Li",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dd9f",
                    "name": "Dimin Wang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dda0",
                    "name": "Dixuan Wang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dda1",
                    "name": "Fan Li",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dda2",
                    "name": "Fan Zhang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dda3",
                    "name": "Haibin Chen",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dda4",
                    "name": "Haozhuang Liu",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dda5",
                    "name": "Jialin Zhu",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dda6",
                    "name": "Jiamang Wang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dda7",
                    "name": "Jiawei Wu",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dda8",
                    "name": "Jin Cui",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187dda9",
                    "name": "Ju Huang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddaa",
                    "name": "Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddab",
                    "name": "Kan Liu",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddac",
                    "name": "Lang Tian",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddad",
                    "name": "Liang Rao",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddae",
                    "name": "Longbin Li",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddaf",
                    "name": "Lulu Zhao",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddb0",
                    "name": "Mao Zhang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddb1",
                    "name": "Na He",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddb2",
                    "name": "Peiyang Wang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddb3",
                    "name": "Qiqi Huang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddb4",
                    "name": "Tao Luo",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddb5",
                    "name": "Wenbo Su",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddb6",
                    "name": "Xiaoxiao He",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddb7",
                    "name": "Xin Tong",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddb8",
                    "name": "Xu Chen",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddb9",
                    "name": "Xunke Xi",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddba",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddbb",
                    "name": "Yaxuan Wu",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddbc",
                    "name": "Yeqiu Yang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddbd",
                    "name": "Yi Hu",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddbe",
                    "name": "Yinnan Song",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddbf",
                    "name": "Yuchen Li",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddc0",
                    "name": "Yujie Luo",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddc1",
                    "name": "Yujin Yuan",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddc2",
                    "name": "Yuliang Yan",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddc3",
                    "name": "Zhengyang Wang",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddc4",
                    "name": "Zhibo Xiao",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddc5",
                    "name": "Zhixin Ma",
                    "hidden": false
                },
                {
                    "_id": "688aebf98b724c8c7187ddc6",
                    "name": "Zile Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-30T17:55:06.000Z",
            "submittedOnDailyAt": "2025-08-01T00:07:39.236Z",
            "title": "RecGPT Technical Report",
            "submittedOnDailyBy": {
                "_id": "65acfb3a14e6582c30b4ce76",
                "avatarUrl": "/avatars/3402ba72fe2436a9c2c2f92e56b15deb.svg",
                "isPro": false,
                "fullname": "TangJiakai",
                "user": "TangJiakai5704",
                "type": "user"
            },
            "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.",
            "upvotes": 15,
            "discussionId": "688aebf98b724c8c7187ddc7",
            "ai_summary": "RecGPT integrates large language models into recommender systems to focus on user intent, improving content diversity and satisfaction while enhancing merchant and platform performance.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "user intent",
                "user interest mining",
                "item retrieval",
                "explanation generation",
                "reasoning-enhanced pre-alignment",
                "self-training evolution",
                "Human-LLM cooperative judge system"
            ]
        },
        "translation_title": "RecGPT 기술 보고서",
        "purpose": "사용자의 의도를 중심으로 한 추천 시스템 설계를 통해 지속 가능한 추천 생태계를 구축하고자 함.",
        "method": [
            "추천 시스템의 전반적 설계 패러다임을 재구성하고, RecGPT라는 차세대 프레임워크를 제안함(we rethink the overall design paradigm of recommender systems and propose RecGPT).",
            "대규모 언어 모델(LLMs)을 사용자 관심 조사, 항목 검색 및 설명 생성의 주요 단계에 통합함(By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation).",
            "다단계 훈련 패러다임을 도입해 LLM을 도메인별 추천 작업에 효과적으로 조정함(To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm)."
        ],
        "conclusion": "RecGPT는 모든 이해관계자에게 일관된 성과 향상을 제공하며, LLM 중심의 의도 기반 설계가 지속 가능하고 상호 유익한 추천 생태계를 조성할 수 있음을 보여줌.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Recommender Systems"
        ]
    },
    {
        "paper": {
            "id": "2507.23682",
            "authors": [
                {
                    "_id": "688c14038c434640078cc33a",
                    "name": "Xiaoyu Chen",
                    "hidden": false
                },
                {
                    "_id": "688c14038c434640078cc33b",
                    "name": "Hangxing Wei",
                    "hidden": false
                },
                {
                    "_id": "688c14038c434640078cc33c",
                    "name": "Pushi Zhang",
                    "hidden": false
                },
                {
                    "_id": "688c14038c434640078cc33d",
                    "name": "Chuheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "688c14038c434640078cc33e",
                    "name": "Kaixin Wang",
                    "hidden": false
                },
                {
                    "_id": "688c14038c434640078cc33f",
                    "name": "Yanjiang Guo",
                    "hidden": false
                },
                {
                    "_id": "688c14038c434640078cc340",
                    "name": "Rushuai Yang",
                    "hidden": false
                },
                {
                    "_id": "688c14038c434640078cc341",
                    "name": "Yucen Wang",
                    "hidden": false
                },
                {
                    "_id": "688c14038c434640078cc342",
                    "name": "Xinquan Xiao",
                    "hidden": false
                },
                {
                    "_id": "688c14038c434640078cc343",
                    "name": "Li Zhao",
                    "hidden": false
                },
                {
                    "_id": "688c14038c434640078cc344",
                    "name": "Jianyu Chen",
                    "hidden": false
                },
                {
                    "_id": "688c14038c434640078cc345",
                    "name": "Jiang Bian",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62a4ab8106b784ece3860b88/svZ9laEP2bfiEPcXh7bHy.mp4"
            ],
            "publishedAt": "2025-07-31T15:57:46.000Z",
            "submittedOnDailyAt": "2025-08-01T01:52:58.615Z",
            "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action\n  Models",
            "submittedOnDailyBy": {
                "_id": "62a4ab8106b784ece3860b88",
                "avatarUrl": "/avatars/042be18e0586984e73fa7c9c0a124ed9.svg",
                "isPro": false,
                "fullname": "Pushi",
                "user": "zpschang",
                "type": "user"
            },
            "summary": "Visual-Language-Action (VLA) models have emerged as a popular paradigm for\nlearning robot manipulation policies that can follow language instructions and\ngeneralize to novel scenarios. Recent work has begun to explore the\nincorporation of latent actions, an abstract representation of visual change\nbetween two frames, into VLA pre-training. In this paper, we introduce villa-X,\na novel Visual-Language-Latent-Action (ViLLA) framework that advances latent\naction modeling for learning generalizable robot manipulation policies. Our\napproach improves both how latent actions are learned and how they are\nincorporated into VLA pre-training. Together, these contributions enable\nvilla-X to achieve superior performance across simulated environments including\nSIMPLER and LIBERO, as well as on two real-world robot setups including gripper\nand dexterous hand manipulation. We believe the ViLLA paradigm holds\nsignificant promise, and that our villa-X provides a strong foundation for\nfuture research.",
            "upvotes": 14,
            "discussionId": "688c14038c434640078cc346",
            "projectPage": "https://microsoft.github.io/villa-x/",
            "githubRepo": "https://github.com/microsoft/villa-x/",
            "ai_summary": "The ViLLA framework enhances VLA models by incorporating latent actions, improving performance in both simulated and real-world robot manipulation tasks.",
            "ai_keywords": [
                "Visual-Language-Action (VLA) models",
                "latent actions",
                "ViLLA framework",
                "SIMPLER",
                "LIBERO",
                "gripper manipulation",
                "dexterous hand manipulation"
            ],
            "githubStars": 22
        },
        "translation_title": "villa-X: 비전-언어-행동 모델에서 잠재적 행동 모델링 향상",
        "purpose": "로봇 조작 정책 학습을 위한 잠재적 행동 모델링의 개선",
        "method": [
            "잠재적 행동을 학습하는 방법을 개선하고 이것을 VLA 사전 학습에 통합하는 새로운 프레임워크인 villa-X를 도입함(we introduce villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies.)",
            "simulated 환경에서의 성능을 높이고, 실제 로봇 설정에서의 조작 성능 보장을 위한 접근 방식을 제시함(our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training.)"
        ],
        "conclusion": "villa-X는 시뮬레이션된 환경(SIMPLER, LIBERO) 및 실제 로봇 설정에서 우수한 성능을 달성하였으며, ViLLA 패러다임이 앞으로의 연구에 큰 잠재력을 가지고 있다고 믿음.",
        "keywords": [
            "Vision-Language Models",
            "Robotics",
            "Image Understanding"
        ]
    }
]