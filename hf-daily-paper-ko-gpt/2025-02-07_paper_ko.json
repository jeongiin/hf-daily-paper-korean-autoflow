[
    {
        "paper": {
            "id": "2502.03032",
            "authors": [
                {
                    "_id": "67a59c4e7ffacd843a56404a",
                    "user": {
                        "_id": "634c5f8cfb80cc6bcaf42c03",
                        "avatarUrl": "/avatars/1f37db0e70cbaf9707f4c8cbcee37ca0.svg",
                        "isPro": false,
                        "fullname": "Daniil Laptev",
                        "user": "dlaptev",
                        "type": "user"
                    },
                    "name": "Daniil Laptev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:04.546Z",
                    "hidden": false
                },
                {
                    "_id": "67a59c4e7ffacd843a56404b",
                    "user": {
                        "_id": "60b364e7f88532cd79eaff7b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
                        "isPro": false,
                        "fullname": "Nikita Balagansky",
                        "user": "elephantmipt",
                        "type": "user"
                    },
                    "name": "Nikita Balagansky",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:02.693Z",
                    "hidden": false
                },
                {
                    "_id": "67a59c4e7ffacd843a56404c",
                    "name": "Yaroslav Aksenov",
                    "hidden": false
                },
                {
                    "_id": "67a59c4e7ffacd843a56404d",
                    "user": {
                        "_id": "62a9c8edc19f92ae443ab37f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
                        "isPro": false,
                        "fullname": "Daniil Gavrilov",
                        "user": "kefirski",
                        "type": "user"
                    },
                    "name": "Daniil Gavrilov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:06.718Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T09:39:34.000Z",
            "title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language\n  Models",
            "summary": "We introduce a new approach to systematically map features discovered by\nsparse autoencoder across consecutive layers of large language models,\nextending earlier work that examined inter-layer feature links. By using a\ndata-free cosine similarity technique, we trace how specific features persist,\ntransform, or first appear at each stage. This method yields granular flow\ngraphs of feature evolution, enabling fine-grained interpretability and\nmechanistic insights into model computations. Crucially, we demonstrate how\nthese cross-layer feature maps facilitate direct steering of model behavior by\namplifying or suppressing chosen features, achieving targeted thematic control\nin text generation. Together, our findings highlight the utility of a causal,\ncross-layer interpretability framework that not only clarifies how features\ndevelop through forward passes but also provides new means for transparent\nmanipulation of large language models.",
            "upvotes": 40,
            "discussionId": "67a59c4f7ffacd843a56408f"
        },
        "translation_title": "언어 모델의 해석 및 조작 향상을 위한 특성 흐름 분석",
        "purpose": "대규모 언어 모델에서 발견된 특성을 체계적으로 매핑하여 해석 가능성과 조작 능력을 향상시키기 위한 방법 연구",
        "method": [
            "희소 오토인코더를 사용하여 대규모 언어 모델의 여러 층에서 발견된 특성을 매핑하는 접근 방식을 제안함(We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models.)",
            "데이터 없는 코사인 유사도 기법을 통해 특성이 어떻게 지속되거나 변형되는지 추적함(By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage.)",
            "특성 진화를 나타내는 세부적인 흐름 그래프를 생성하여 모델 계산에 대한 해석 및 메커니즘 통찰을 제공함(This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations.)"
        ],
        "conclusion": "크로스 레이어 특성 맵을 통해 모델 동작을 직접적으로 조작할 수 있는 방법을 보여주고, 대규모 언어 모델의 투명한 조작을 위한 새로운 수단을 제공함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2502.03621",
            "authors": [
                {
                    "_id": "67a59e5298f41a0460ee5282",
                    "user": {
                        "_id": "6301d8324ccccaa23d3864f4",
                        "avatarUrl": "/avatars/148b1b1d1460e26f03a1f2ce0feacf78.svg",
                        "isPro": false,
                        "fullname": "Danah Yatim",
                        "user": "DanahY",
                        "type": "user"
                    },
                    "name": "Danah Yatim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:37:14.464Z",
                    "hidden": false
                },
                {
                    "_id": "67a59e5298f41a0460ee5283",
                    "user": {
                        "_id": "62627f3c02cd5952e013c843",
                        "avatarUrl": "/avatars/1d76689d75d670630b6fa0307309c31f.svg",
                        "isPro": false,
                        "fullname": "Rafail Fridman",
                        "user": "RafailFridman",
                        "type": "user"
                    },
                    "name": "Rafail Fridman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:37:21.549Z",
                    "hidden": false
                },
                {
                    "_id": "67a59e5298f41a0460ee5284",
                    "user": {
                        "_id": "62e29044a133a252b5cf70b2",
                        "avatarUrl": "/avatars/6d09ddcba9bc47c309150a8d77815891.svg",
                        "isPro": false,
                        "fullname": "Omer Bar-Tal",
                        "user": "omerbartal",
                        "type": "user"
                    },
                    "name": "Omer Bar-Tal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:37:28.382Z",
                    "hidden": false
                },
                {
                    "_id": "67a59e5298f41a0460ee5285",
                    "user": {
                        "_id": "631cddec68f7da9ad24f6fc7",
                        "avatarUrl": "/avatars/7d4f1ce805e5889ca6594bd4a93f2583.svg",
                        "isPro": false,
                        "fullname": "Tali Dekel",
                        "user": "talidekel",
                        "type": "user"
                    },
                    "name": "Tali Dekel",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:37:34.275Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T21:14:55.000Z",
            "title": "DynVFX: Augmenting Real Videos with Dynamic Content",
            "summary": "We present a method for augmenting real-world videos with newly generated\ndynamic content. Given an input video and a simple user-provided text\ninstruction describing the desired content, our method synthesizes dynamic\nobjects or complex scene effects that naturally interact with the existing\nscene over time. The position, appearance, and motion of the new content are\nseamlessly integrated into the original footage while accounting for camera\nmotion, occlusions, and interactions with other dynamic objects in the scene,\nresulting in a cohesive and realistic output video. We achieve this via a\nzero-shot, training-free framework that harnesses a pre-trained text-to-video\ndiffusion transformer to synthesize the new content and a pre-trained Vision\nLanguage Model to envision the augmented scene in detail. Specifically, we\nintroduce a novel inference-based method that manipulates features within the\nattention mechanism, enabling accurate localization and seamless integration of\nthe new content while preserving the integrity of the original scene. Our\nmethod is fully automated, requiring only a simple user instruction. We\ndemonstrate its effectiveness on a wide range of edits applied to real-world\nvideos, encompassing diverse objects and scenarios involving both camera and\nobject motion.",
            "upvotes": 15,
            "discussionId": "67a59e5798f41a0460ee5389"
        },
        "translation_title": "DynVFX: 동적 콘텐츠로 실제 비디오 보강하기",
        "purpose": "간단한 텍스트 지시사항을 기반으로 실제 비디오에 동적 콘텐츠를 추가하는 방법을 제시",
        "method": [
            "사용자가 제공한 텍스트 지시사항에 따라 새로운 동적 객체나 복잡한 장면 효과를 합성함(our method synthesizes dynamic objects or complex scene effects that naturally interact with the existing scene over time.)",
            "기존 장면과의 자연스러운 통합을 위해 카메라 움직임, 가림 현상, 다른 동적 객체와의 상호작용을 고려함(the position, appearance, and motion of the new content are seamlessly integrated into the original footage while accounting for camera motion, occlusions, and interactions with other dynamic objects in the scene.)",
            "사전 훈련된 text-to-video diffusion transformer를 활용하여 새로운 콘텐츠를 합성하고, Vision Language Model을 통해 보강된 장면을 상세히 구상함(we achieve this via a zero-shot, training-free framework that harnesses a pre-trained text-to-video diffusion transformer to synthesize the new content and a pre-trained Vision Language Model to envision the augmented scene in detail.)"
        ],
        "conclusion": "이 방법은 사용자가 간단한 지시만으로도 실제 비디오에 다양한 동적 콘텐츠를 보강할 수 있도록 자동화되어 있으며, 다양한 객체와 시나리오에서 효과적임을 입증함.",
        "keywords": [
            "Video Generation",
            "Image Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2502.04153",
            "authors": [
                {
                    "_id": "67a57b1fdea89ffe80d9fe56",
                    "user": {
                        "_id": "66c89152d33e34fbc29497d7",
                        "avatarUrl": "/avatars/bbddabf6532393951c4759e5915a065b.svg",
                        "isPro": false,
                        "fullname": "KaikaiAn",
                        "user": "kkk-an",
                        "type": "user"
                    },
                    "name": "Kaikai An",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:18.320Z",
                    "hidden": false
                },
                {
                    "_id": "67a57b1fdea89ffe80d9fe57",
                    "name": "Li Sheng",
                    "hidden": false
                },
                {
                    "_id": "67a57b1fdea89ffe80d9fe58",
                    "user": {
                        "_id": "650eba9555dc1e841746f132",
                        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
                        "isPro": false,
                        "fullname": "Ganqu Cui",
                        "user": "ganqu",
                        "type": "user"
                    },
                    "name": "Ganqu Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:31:05.333Z",
                    "hidden": false
                },
                {
                    "_id": "67a57b1fdea89ffe80d9fe59",
                    "user": {
                        "_id": "637c99bbfe115289cfedfb44",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
                        "isPro": false,
                        "fullname": "ssz",
                        "user": "ssz1111",
                        "type": "user"
                    },
                    "name": "Shuzheng Si",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:16.229Z",
                    "hidden": false
                },
                {
                    "_id": "67a57b1fdea89ffe80d9fe5a",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "67a57b1fdea89ffe80d9fe5b",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "67a57b1fdea89ffe80d9fe5c",
                    "name": "Baobao Chang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T15:39:16.000Z",
            "title": "UltraIF: Advancing Instruction Following from the Wild",
            "summary": "Instruction-following made modern large language models (LLMs) helpful\nassistants. However, the key to taming LLMs on complex instructions remains\nmysterious, for that there are huge gaps between models trained by open-source\ncommunity and those trained by leading companies. To bridge the gap, we propose\na simple and scalable approach UltraIF for building LLMs that can follow\ncomplex instructions with open-source data. UltraIF first decomposes real-world\nuser prompts into simpler queries, constraints, and corresponding evaluation\nquestions for the constraints. Then, we train an UltraComposer to compose\nconstraint-associated prompts with evaluation questions. This prompt composer\nallows us to synthesize complicated instructions as well as filter responses\nwith evaluation questions. In our experiment, for the first time, we\nsuccessfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5\ninstruction-following benchmarks without any benchmark information, using only\n8B model as response generator and evaluator. The aligned model also achieved\ncompetitive scores on other benchmarks. Moreover, we also show that UltraIF\ncould further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating\nbroader use cases for the method. Our code will be available at\nhttps://github.com/kkk-an/UltraIF.",
            "upvotes": 14,
            "discussionId": "67a57b1fdea89ffe80d9fe93"
        },
        "translation_title": "UltraIF: 복잡한 지침 수행 향상을 위한 접근법",
        "purpose": "LLMs가 복잡한 지침을 잘 따를 수 있도록 하는 방법 연구",
        "method": [
            "실제 사용자 프롬프트를 단순한 쿼리, 제약 조건 및 해당 평가 질문으로 분해함(UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints.)",
            "UltraComposer를 학습하여 제약 조건에 연결된 프롬프트를 구성하고 평가 질문과 함께 필터링할 수 있도록 함(Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions.)",
            "8B 모델을 반응 생성기 및 평가자로 사용하여 LLaMA-3.1-8B-Base를 지침 버전과 조정함(we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks.)"
        ],
        "conclusion": "UltraIF는 LLaMA 모델의 지침 수행 능력을 크게 향상시키며, 더 넓은 응용 사례에서도 효과적임.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2502.04313",
            "authors": [
                {
                    "_id": "67a5b9107897c8f5406155e0",
                    "user": {
                        "_id": "6506832221ac448013f94995",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
                        "isPro": false,
                        "fullname": "Shashwat Goel",
                        "user": "shash42",
                        "type": "user"
                    },
                    "name": "Shashwat Goel",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:39:36.508Z",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e1",
                    "user": {
                        "_id": "6728c6113d35dd53cfe9f30c",
                        "avatarUrl": "/avatars/7f93b9d41446cce382f63c78ca5059a1.svg",
                        "isPro": false,
                        "fullname": "Joschka Strüber",
                        "user": "Klingspor",
                        "type": "user"
                    },
                    "name": "Joschka Struber",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:39:43.250Z",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e2",
                    "user": {
                        "_id": "671b49503fd1d03dc69194b0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tnkR0j1VaWClUcumXcgjQ.png",
                        "isPro": false,
                        "fullname": "Ilze Amanda Auzina",
                        "user": "iaa01",
                        "type": "user"
                    },
                    "name": "Ilze Amanda Auzina",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:40:04.242Z",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e3",
                    "name": "Karuna K Chandra",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e4",
                    "name": "Ponnurangam Kumaraguru",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e5",
                    "user": {
                        "_id": "61dc997715b47073db1620dc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1641847245435-61dc997715b47073db1620dc.jpeg",
                        "isPro": false,
                        "fullname": "Douwe Kiela",
                        "user": "douwekiela",
                        "type": "user"
                    },
                    "name": "Douwe Kiela",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:40:22.153Z",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e6",
                    "user": {
                        "_id": "6464a0d41683d3c81f51924a",
                        "avatarUrl": "/avatars/bfa89f568302fa34a641e0d8744bf8b5.svg",
                        "isPro": false,
                        "fullname": "Ameya Prabhu",
                        "user": "AmeyaPrabhu",
                        "type": "user"
                    },
                    "name": "Ameya Prabhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:40:34.763Z",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e7",
                    "name": "Matthias Bethge",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e8",
                    "user": {
                        "_id": "63d86dbf3130cadcaf8bdd11",
                        "avatarUrl": "/avatars/29d79a0c6dcec01111ef192fecd0fa7a.svg",
                        "isPro": false,
                        "fullname": "Jonas Geiping",
                        "user": "JonasGeiping",
                        "type": "user"
                    },
                    "name": "Jonas Geiping",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:40:49.233Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T18:56:01.000Z",
            "title": "Great Models Think Alike and this Undermines AI Oversight",
            "summary": "As Language Model (LM) capabilities advance, evaluating and supervising them\nat scale is getting harder for humans. There is hope that other language models\ncan automate both these tasks, which we refer to as \"AI Oversight\". We study\nhow model similarity affects both aspects of AI oversight by proposing a\nprobabilistic metric for LM similarity based on overlap in model mistakes.\nUsing this metric, we first show that LLM-as-a-judge scores favor models\nsimilar to the judge, generalizing recent self-preference results. Then, we\nstudy training on LM annotations, and find complementary knowledge between the\nweak supervisor and strong student model plays a crucial role in gains from\n\"weak-to-strong generalization\". As model capabilities increase, it becomes\nharder to find their mistakes, and we might defer more to AI oversight.\nHowever, we observe a concerning trend -- model mistakes are becoming more\nsimilar with increasing capabilities, pointing to risks from correlated\nfailures. Our work underscores the importance of reporting and correcting for\nmodel similarity, especially in the emerging paradigm of AI oversight.",
            "upvotes": 12,
            "discussionId": "67a5b9137897c8f540615673"
        },
        "translation_title": "대모델들은 비슷하게 생각하며 이는 AI 감독을 저해한다",
        "purpose": "대규모로 언어 모델을 평가하고 감독하는 과정을 자동화하기 위한 기준 연구",
        "method": [
            "모델이 저지르는 실수의 중첩을 기반으로 한 확률적 지표를 제안함(We propose a probabilistic metric for LM similarity based on overlap in model mistakes.)",
            "LLM-as-a-judge 점수가 판단자와 유사한 모델에 유리한 결과를 보여줌(Using this metric, we first show that LLM-as-a-judge scores favor models similar to the judge.)",
            "약한 감독자와 강한 학생 모델 간의 보완적 지식이 '약한 일반화에서 강한 일반화'의 결과에 중요한 역할을 함(we find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from 'weak-to-strong generalization').",
            "모델 능력이 증가함에 따라 실수의 유사성이 증가하는 경향을 관찰함(we observe a concerning trend -- model mistakes are becoming more similar with increasing capabilities.)"
        ],
        "conclusion": "모델 유사성을 보고하고 수정하는 것이 중요하며, 이는 AI 감독의 새로운 패러다임에서 특별히 강조되어야 함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "AI Oversight"
        ]
    },
    {
        "paper": {
            "id": "2502.04328",
            "authors": [
                {
                    "_id": "67a586fad177de2eeba7de7b",
                    "user": {
                        "_id": "64f001bfabd9fb1914398bd5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
                        "isPro": false,
                        "fullname": "liuzuyan",
                        "user": "Zuyan",
                        "type": "user"
                    },
                    "name": "Zuyan Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:10.679Z",
                    "hidden": false
                },
                {
                    "_id": "67a586fad177de2eeba7de7c",
                    "user": {
                        "_id": "652965773a416e1f2173443b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
                        "isPro": false,
                        "fullname": "Yuhao Dong",
                        "user": "THUdyh",
                        "type": "user"
                    },
                    "name": "Yuhao Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:37:45.556Z",
                    "hidden": false
                },
                {
                    "_id": "67a586fad177de2eeba7de7d",
                    "name": "Jiahui Wang",
                    "hidden": false
                },
                {
                    "_id": "67a586fad177de2eeba7de7e",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:38:14.852Z",
                    "hidden": false
                },
                {
                    "_id": "67a586fad177de2eeba7de7f",
                    "user": {
                        "_id": "63673bb9d0ee6e2662be0ec1",
                        "avatarUrl": "/avatars/1b8976785d64bc4e3f7159ccdb7f06c5.svg",
                        "isPro": false,
                        "fullname": "Qingqiao Hu",
                        "user": "WinstonHu",
                        "type": "user"
                    },
                    "name": "Winston Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:38:23.468Z",
                    "hidden": false
                },
                {
                    "_id": "67a586fad177de2eeba7de80",
                    "user": {
                        "_id": "66c44203ea476bea05e9fcd7",
                        "avatarUrl": "/avatars/b061eebec609446e669f5ad6365959f9.svg",
                        "isPro": false,
                        "fullname": "lu",
                        "user": "jiwenlu",
                        "type": "user"
                    },
                    "name": "Jiwen Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:38:29.456Z",
                    "hidden": false
                },
                {
                    "_id": "67a586fad177de2eeba7de81",
                    "user": {
                        "_id": "63e4865354f51ea342d45d78",
                        "avatarUrl": "/avatars/2e7eccc878751331ca8b282f53e38899.svg",
                        "isPro": false,
                        "fullname": "Yongming Rao",
                        "user": "raoyongming",
                        "type": "user"
                    },
                    "name": "Yongming Rao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:38:35.766Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T18:59:55.000Z",
            "title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment",
            "summary": "Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.",
            "upvotes": 11,
            "discussionId": "67a586fbd177de2eeba7deae"
        },
        "translation_title": "Ola: 점진적인 모달리티 정렬을 통한 옴니모달 언어 모델의 한계 확장",
        "purpose": "옴니모달 모델을 개발하여 이미지, 비디오 및 오디오 이해에서 경쟁력 있는 성능을 달성하는 것",
        "method": [
            "Ola라는 옴니모달 언어 모델을 제시하여 전통적인 단일 모달리티 모델과의 성능 차이를 줄임(we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts.)",
            "모델의 훈련 파이프라인은 이미지와 텍스트 모달리티부터 시작하여 점진적으로 음성 및 비디오 데이터를 사용해 모델의 기술 세트를 확장함(Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data.)",
            "상대적으로 작은 크기의 교차 모달 정렬 데이터를 유지하며 기존의 비전-언어 모델로부터 옴니모달 모델을 개발함(The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly.)"
        ],
        "conclusion": "Ola는 기존의 옴니모달 LLM을 초월하며, 유사한 크기의 전문 모델에 대해 경쟁력 있는 성능을 기록함.",
        "keywords": [
            "Natural Language Processing",
            "Multimodal Learning",
            "Vision-Language Models"
        ]
    }
]