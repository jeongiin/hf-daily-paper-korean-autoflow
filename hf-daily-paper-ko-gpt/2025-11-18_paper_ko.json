[
    {
        "paper": {
            "id": "2511.13612",
            "authors": [
                {
                    "_id": "691bfdc96bfd5965c0fd3951",
                    "user": {
                        "_id": "65352acb7139c5dd8d9a8590",
                        "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
                        "isPro": false,
                        "fullname": "JiachengChen",
                        "user": "JC-Chen",
                        "type": "user"
                    },
                    "name": "Jiacheng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-18T14:07:04.293Z",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3952",
                    "name": "Qianjia Cheng",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3953",
                    "name": "Fangchen Yu",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3954",
                    "name": "Haiyuan Wan",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3955",
                    "name": "Yuchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3956",
                    "name": "Shenghe Zheng",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3957",
                    "name": "Junchi Yao",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3958",
                    "name": "Qingyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3959",
                    "name": "Haonan He",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd395a",
                    "name": "Yun Luo",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd395b",
                    "name": "Yufeng Zhao",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd395c",
                    "name": "Futing Wang",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd395d",
                    "name": "Li Sheng",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd395e",
                    "name": "Chengxing Xie",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd395f",
                    "name": "Yuxin Zuo",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3960",
                    "name": "Yizhuo Li",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3961",
                    "name": "Wenxauan Zeng",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3962",
                    "name": "Yulun Wu",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3963",
                    "user": {
                        "_id": "6731caae58ba0f0984c188ea",
                        "avatarUrl": "/avatars/445275cca29f97c5f8754d6c27075887.svg",
                        "isPro": false,
                        "fullname": "Rui Huang",
                        "user": "Rui1121",
                        "type": "user"
                    },
                    "name": "Rui Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-18T14:07:02.211Z",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3964",
                    "name": "Dongzhan Zhou",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3965",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3966",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3967",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3968",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd3969",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd396a",
                    "name": "Bowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd396b",
                    "name": "Peng Ye",
                    "hidden": false
                },
                {
                    "_id": "691bfdc96bfd5965c0fd396c",
                    "name": "Ganqu Cui",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-17T17:18:13.000Z",
            "submittedOnDailyAt": "2025-11-18T02:37:44.601Z",
            "title": "P1: Mastering Physics Olympiads with Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "65352acb7139c5dd8d9a8590",
                "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
                "isPro": false,
                "fullname": "JiachengChen",
                "user": "JC-Chen",
                "type": "user"
            },
            "summary": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.",
            "upvotes": 102,
            "discussionId": "691bfdca6bfd5965c0fd396d",
            "projectPage": "https://prime-rl.github.io/P1/",
            "githubRepo": "https://github.com/PRIME-RL/P1",
            "githubStars": 44
        },
        "translation_title": "P1: 강화 학습을 통한 물리 올림피아드 마스터하기",
        "purpose": "물리 문제에 대한 뛰어난 추론 능력을 갖춘 대규모 언어 모델을 개발하여 물리 연구를 발전시키는 것",
        "method": [
            "강화 학습(RL)을 통해 훈련된 물리 추론 모델인 P1 시리즈를 도입함(In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, specially excel at solving Olympiad-level physics problems.)",
            "P1-235B-A22B는 최신 국제 물리 올림피아드(IPhO 2025)에서 금메달 성과를 기록한 첫 번째 오픈 소스 모델임(Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025).)",
            "P1 모델은 수학 및 코딩과 같은 다른 추론 작업에서도 뛰어난 성과를 보여줌(Besides physics, P1 models also present great performance on other reasoning tasks like math and coding.)"
        ],
        "conclusion": "P1-235B-A22B는 IPhO 2025에서 1위 및 13개의 물리 대회에서 최고의 평균 점수를 기록하며 전반적으로 뛰어난 성과를 달성함.",
        "keywords": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2511.12609",
            "authors": [
                {
                    "_id": "691be5fc6bfd5965c0fd37d4",
                    "name": "Yunxin Li",
                    "hidden": false
                },
                {
                    "_id": "691be5fc6bfd5965c0fd37d5",
                    "user": {
                        "_id": "64d9da538767727dff1e8f19",
                        "avatarUrl": "/avatars/aaad38795007c6cbcb94c7eed1706e51.svg",
                        "isPro": false,
                        "fullname": "Xinyu Chen",
                        "user": "Ghaser",
                        "type": "user"
                    },
                    "name": "Xinyu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-18T14:08:12.997Z",
                    "hidden": false
                },
                {
                    "_id": "691be5fc6bfd5965c0fd37d6",
                    "name": "Shenyuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "691be5fc6bfd5965c0fd37d7",
                    "user": {
                        "_id": "652fb8bcc9dd2692a25ef2e3",
                        "avatarUrl": "/avatars/461e6cc1c3441cde18192b080b0b8576.svg",
                        "isPro": false,
                        "fullname": "Haoyuan Shi",
                        "user": "MrSunshy",
                        "type": "user"
                    },
                    "name": "Haoyuan Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-18T14:08:19.326Z",
                    "hidden": false
                },
                {
                    "_id": "691be5fc6bfd5965c0fd37d8",
                    "name": "Zhenyu Liu",
                    "hidden": false
                },
                {
                    "_id": "691be5fc6bfd5965c0fd37d9",
                    "name": "Xuanyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "691be5fc6bfd5965c0fd37da",
                    "name": "Nanhao Deng",
                    "hidden": false
                },
                {
                    "_id": "691be5fc6bfd5965c0fd37db",
                    "name": "Zhenran Xu",
                    "hidden": false
                },
                {
                    "_id": "691be5fc6bfd5965c0fd37dc",
                    "name": "Yicheng Ma",
                    "hidden": false
                },
                {
                    "_id": "691be5fc6bfd5965c0fd37dd",
                    "name": "Meishan Zhang",
                    "hidden": false
                },
                {
                    "_id": "691be5fc6bfd5965c0fd37de",
                    "name": "Baotian Hu",
                    "hidden": false
                },
                {
                    "_id": "691be5fc6bfd5965c0fd37df",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-16T14:10:55.000Z",
            "submittedOnDailyAt": "2025-11-18T01:14:55.926Z",
            "title": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
            "submittedOnDailyBy": {
                "_id": "62fdb01bc1588e1d4c6c1a7c",
                "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
                "isPro": false,
                "fullname": "Yunxin Li",
                "user": "YunxinLi",
                "type": "user"
            },
            "summary": "We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.",
            "upvotes": 83,
            "discussionId": "691be5fc6bfd5965c0fd37e0",
            "projectPage": "https://idealistxy.github.io/Uni-MoE-v2.github.io/",
            "githubRepo": "https://github.com/HITsz-TMG/Uni-MoE",
            "githubStars": 825,
            "organization": {
                "_id": "629867d7f2bf8bd3e468706e",
                "name": "HIT-TMG",
                "fullname": "HITsz-Text and Multimodal Generative Intelligence Group(TMG)",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658735061824-62986540f2bf8bd3e468622a.png"
            }
        },
        "translation_title": "Uni-MoE-2.0-Omni: 고급 MoE, 훈련 및 데이터로 언어 중심의 오미모달 대형 모델 확장하기",
        "purpose": "언어 중심의 멀티모달 이해, 추론 및 생성을 발전시키기 위한 오픈소스 오미모달 대형 모델 제안",
        "method": [
            "Mixture-of-Experts (MoE) 설계를 통해 동적 용량을 구현함(we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design.)",
            "모달리티별 전문가를 활성화하고 균형 잡힌 데이터 구성을 통해 발전적인 감독 하에 미세 조정함(we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition.)",
            "약 75B 토큰의 오픈소스 데이터를 기반으로 훈련하여 언어적 단서를 기반으로 생성 작업을 수행함(the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens.)"
        ],
        "conclusion": "모델은 85개의 벤치마크에서 SOTA 또는 경쟁력 있는 성능을 달성하였으며, 특히 비디오 이해와 오미모달 이해에서 향상된 성능을 보여줌.",
        "keywords": [
            "Multimodal Learning",
            "Image Generation",
            "Video Understanding"
        ]
    },
    {
        "paper": {
            "id": "2511.11793",
            "authors": [
                {
                    "_id": "691be81b6bfd5965c0fd37e2",
                    "name": "MiroMind Team",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37e3",
                    "name": "Song Bai",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37e4",
                    "name": "Lidong Bing",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37e5",
                    "name": "Carson Chen",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37e6",
                    "name": "Guanzheng Chen",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37e7",
                    "name": "Yuntao Chen",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37e8",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37e9",
                    "name": "Ziyi Chen",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37ea",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37eb",
                    "name": "Xuan Dong",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37ec",
                    "name": "Yue Deng",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37ed",
                    "name": "Yunjie Fu",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37ee",
                    "name": "Junqi Ge",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37ef",
                    "name": "Chenxia Han",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37f0",
                    "name": "Tammy Huang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37f1",
                    "name": "Zhenhang Huang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37f2",
                    "name": "Jerry Jiao",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37f3",
                    "name": "Shilei Jiang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37f4",
                    "name": "Tianyu Jiao",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37f5",
                    "name": "Xiaoqi Jian",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37f6",
                    "name": "Lei Lei",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37f7",
                    "name": "Ruilin Li",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37f8",
                    "name": "Ryan Luo",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37f9",
                    "name": "Tiantong Li",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37fa",
                    "name": "Xiang Lin",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37fb",
                    "name": "Ziyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37fc",
                    "name": "Zhiqi Li",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37fd",
                    "name": "Jie Ni",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37fe",
                    "name": "Qiang Ren",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd37ff",
                    "name": "Pax Sun",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3800",
                    "name": "Shiqian Su",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3801",
                    "name": "Chenxin Tao",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3802",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3803",
                    "name": "Hellen Wang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3804",
                    "name": "Haonan Wang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3805",
                    "name": "James Wang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3806",
                    "name": "Jin Wang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3807",
                    "name": "Jojo Wang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3808",
                    "name": "Letian Wang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3809",
                    "name": "Shizun Wang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd380a",
                    "name": "Weizhi Wang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd380b",
                    "name": "Zixuan Wang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd380c",
                    "name": "Jinfan Xu",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd380d",
                    "name": "Sen Xing",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd380e",
                    "name": "Chenyu Yang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd380f",
                    "name": "Hai Ye",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3810",
                    "name": "Jiaheng Yu",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3811",
                    "name": "Yue Yu",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3812",
                    "name": "Muyan Zhong",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3813",
                    "name": "Tianchen Zhao",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3814",
                    "name": "Xizhou Zhu",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3815",
                    "name": "Yanpeng Zhou",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3816",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "691be81b6bfd5965c0fd3817",
                    "name": "Zhi Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-14T18:52:07.000Z",
            "submittedOnDailyAt": "2025-11-18T02:00:07.077Z",
            "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.",
            "upvotes": 65,
            "discussionId": "691be81b6bfd5965c0fd3818",
            "projectPage": "https://dr.miromind.ai/",
            "githubRepo": "https://github.com/MiroMindAI/MiroThinker",
            "githubStars": 742
        },
        "translation_title": "MiroThinker: 모델, 컨텍스트, 인터랙티브 스케일링을 통한 오픈 소스 연구 에이전트의 성능 한계 확대",
        "purpose": "도구 보강 추론 및 정보 탐색 능력을 향상시키기 위한 오픈 소스 연구 에이전트 개발",
        "method": [
            "MiroThinker는 모델 차원에서 인터랙션 스케일링을 탐구하여 에이전트-환경 상호작용을 더 깊고 빈번하게 처리하도록 체계적으로 모델을 훈련함(Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement.)",
            "강화 학습을 통해 256K 컨텍스트 창에서 한 작업당 최대 600개의 도구 호출을 수행하여 지속적인 다중 턴 추론과 복잡한 실제 연구 작업을 가능하게 함(Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows.)",
            "4개의 대표 벤치마크(GAIA, HLE, BrowseComp, BrowseComp-ZH)에서 72B 변형이 각각 81.9%, 37.7%, 47.1%, 55.6%의 정확도를 달성함(Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively)."
        ],
        "conclusion": "MiroThinker는 인터랙티브 스케일링의 이점을 활용하여 연구 성능을 향상시키며, 이는 모델 용량 및 컨텍스트 창과 함께 차세대 오픈 연구 에이전트를 구축하는 데 중요한 세 번째 차원으로 입증됨.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2511.13647",
            "authors": [
                {
                    "_id": "691bf0036bfd5965c0fd38e6",
                    "user": {
                        "_id": "686fc1fb779dd387eb29a46b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/686fc1fb779dd387eb29a46b/jdBko2vuxOB9HGAoryMZS.jpeg",
                        "isPro": false,
                        "fullname": "Chunshi Wang",
                        "user": "AiEson2",
                        "type": "user"
                    },
                    "name": "Chunshi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-18T14:07:33.977Z",
                    "hidden": false
                },
                {
                    "_id": "691bf0036bfd5965c0fd38e7",
                    "user": {
                        "_id": "65a420cd90e65dc39a6abe9e",
                        "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
                        "isPro": false,
                        "fullname": "yejunliang",
                        "user": "yejunliang23",
                        "type": "user"
                    },
                    "name": "Junliang Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-18T14:07:38.454Z",
                    "hidden": false
                },
                {
                    "_id": "691bf0036bfd5965c0fd38e8",
                    "name": "Yunhan Yang",
                    "hidden": false
                },
                {
                    "_id": "691bf0036bfd5965c0fd38e9",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "691bf0036bfd5965c0fd38ea",
                    "name": "Zizhuo Lin",
                    "hidden": false
                },
                {
                    "_id": "691bf0036bfd5965c0fd38eb",
                    "name": "Jun Zhu",
                    "hidden": false
                },
                {
                    "_id": "691bf0036bfd5965c0fd38ec",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "691bf0036bfd5965c0fd38ed",
                    "name": "Yawei Luo",
                    "hidden": false
                },
                {
                    "_id": "691bf0036bfd5965c0fd38ee",
                    "name": "Chunchao Guo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/eQD2nZGEZhwmzc6HIhQRq.png"
            ],
            "publishedAt": "2025-11-17T17:59:52.000Z",
            "submittedOnDailyAt": "2025-11-18T01:35:04.756Z",
            "title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model",
            "submittedOnDailyBy": {
                "_id": "65a420cd90e65dc39a6abe9e",
                "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
                "isPro": false,
                "fullname": "yejunliang",
                "user": "yejunliang23",
                "type": "user"
            },
            "summary": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/",
            "upvotes": 61,
            "discussionId": "691bf0036bfd5965c0fd38ef",
            "projectPage": "https://chunshi.wang/Part-X-MLLM/",
            "organization": {
                "_id": "6645f953c39288df638dbdd5",
                "name": "Tencent-Hunyuan",
                "fullname": "Tencent Hunyuan",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
            }
        },
        "translation_title": "Part-X-MLLM: 부품 인식을 기반으로 한 3D 다중 모달 대규모 언어 모델",
        "purpose": "다양한 3D 작업을 통합하고 효과적인 생성 및 수정을 가능하게 하는 모델 개발",
        "method": [
            "3D 작업을 구조화된 실행 가능한 문법으로 프로그램화하여 접근함(Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands.)",
            "부품 중심의 대규모 데이터셋에서 모델을 연습하고 지침 기반 조정을 통해 성능을 향상시킴(We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset.)",
            "실험을 통해 모델이 고품질 구조 계획을 생성하고, Q&A, 생성 및 편집의 성능에서 최첨단 결과를 달성함(Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q&A, compositional generation, and localized editing through one unified interface.)"
        ],
        "conclusion": "Part-X-MLLM은 다중 모달 작업을 처리하는 데 있어 혁신적이며, 높은 성능의 구조화된 출력을 통해 다양한 3D 작업에 효과적으로 활용될 수 있음.",
        "keywords": [
            "3D Vision",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2511.13254",
            "authors": [
                {
                    "_id": "691c1c836bfd5965c0fd39e4",
                    "user": {
                        "_id": "6419b34a9a27800807c34a63",
                        "avatarUrl": "/avatars/ee1391a9a153bae0dd04323b1fa5b5d6.svg",
                        "isPro": false,
                        "fullname": "Shalini M",
                        "user": "shalinimaiti",
                        "type": "user"
                    },
                    "name": "Shalini Maiti",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-18T14:06:47.486Z",
                    "hidden": false
                },
                {
                    "_id": "691c1c836bfd5965c0fd39e5",
                    "user": {
                        "_id": "6687ee79eee600e418404cc9",
                        "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg",
                        "isPro": false,
                        "fullname": "Amar Budhiraja",
                        "user": "ambud26",
                        "type": "user"
                    },
                    "name": "Amar Budhiraja",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-18T14:06:49.892Z",
                    "hidden": false
                },
                {
                    "_id": "691c1c836bfd5965c0fd39e6",
                    "name": "Bhavul Gauri",
                    "hidden": false
                },
                {
                    "_id": "691c1c836bfd5965c0fd39e7",
                    "user": {
                        "_id": "691c6b9b660c15d270b5838a",
                        "avatarUrl": "/avatars/2dc40e079bd8b7af7ae4a4ac43acc552.svg",
                        "isPro": false,
                        "fullname": "Gaurav Chaurasia",
                        "user": "gchauras",
                        "type": "user"
                    },
                    "name": "Gaurav Chaurasia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-18T14:06:45.558Z",
                    "hidden": false
                },
                {
                    "_id": "691c1c836bfd5965c0fd39e8",
                    "name": "Anton Protopopov",
                    "hidden": false
                },
                {
                    "_id": "691c1c836bfd5965c0fd39e9",
                    "name": "Alexis Audran-Reiss",
                    "hidden": false
                },
                {
                    "_id": "691c1c836bfd5965c0fd39ea",
                    "name": "Michael Slater",
                    "hidden": false
                },
                {
                    "_id": "691c1c836bfd5965c0fd39eb",
                    "name": "Despoina Magka",
                    "hidden": false
                },
                {
                    "_id": "691c1c836bfd5965c0fd39ec",
                    "name": "Tatiana Shavrina",
                    "hidden": false
                },
                {
                    "_id": "691c1c836bfd5965c0fd39ed",
                    "name": "Roberta Raileanu",
                    "hidden": false
                },
                {
                    "_id": "691c1c836bfd5965c0fd39ee",
                    "name": "Yoram Bachrach",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-17T11:13:34.000Z",
            "submittedOnDailyAt": "2025-11-18T04:47:00.815Z",
            "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance",
            "submittedOnDailyBy": {
                "_id": "6687ee79eee600e418404cc9",
                "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg",
                "isPro": false,
                "fullname": "Amar Budhiraja",
                "user": "ambud26",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.",
            "upvotes": 55,
            "discussionId": "691c1c846bfd5965c0fd39fc",
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "translation_title": "Souper-Model: 간단한 산술이 최첨단 LLM 성능을 여는 방법",
        "purpose": "자원과 시간 소모를 줄이면서 LLM 성능을 향상시키기 위한 새로운 모델 소핑 기법 연구",
        "method": [
            "모델 소핑에 대한 원칙 있는 접근법인 SoCE를 제안함(we introduce Soup Of Category Experts (SoCE), a principled approach for model souping)",
            "벤치마크 구성으로 최적 모델 후보를 식별하고 비균일 가중 평균을 적용함(To identify optimal model candidates and applies non-uniform weighted averaging to maximize performance)",
            "약한 상관관계를 가진 벤치마크 카테고리 클러스터에 대해 '전문가' 모델을 식별하고 최적화된 가중 평균을 활용하여 결합함(SoCE identifies 'expert' models for each weakly-correlated category cluster and combines them using optimized weighted averaging)."
        ],
        "conclusion": "제안된 방법이 다국어 능력, 도구 호출, 수학 문제 해결 등 여러 분야에서 성능과 견고성을 향상시키고, Berkeley Function Calling Leaderboard에서 최첨단 결과를 달성함.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Model Soupering"
        ]
    }
]