[
    {
        "paper": {
            "id": "2601.16725",
            "authors": [
                {
                    "_id": "6976d5405d41524304c13537",
                    "name": "Meituan LongCat Team",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13538",
                    "name": "Anchun Gui",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13539",
                    "name": "Bei Li",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1353a",
                    "name": "Bingyang Tao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1353b",
                    "name": "Bole Zhou",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1353c",
                    "name": "Borun Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1353e",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "69772bc15d41524304c13739",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1353f",
                    "name": "Chen Gao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13540",
                    "name": "Chen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13541",
                    "name": "Chengcheng Han",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13542",
                    "name": "Chenhui Yang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13543",
                    "name": "Chuyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13544",
                    "name": "Cong Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13545",
                    "name": "Cunguang Wang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13546",
                    "name": "Daoru Pan",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13547",
                    "name": "Defei Bu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13548",
                    "name": "Dengchang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13549",
                    "name": "Di Xiu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1354a",
                    "name": "Dishan Liu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1354b",
                    "name": "Dongyu Ru",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1354c",
                    "name": "Dunwei Tu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1354d",
                    "name": "Fan Wu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1354e",
                    "name": "Fengcheng Yuan",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1354f",
                    "name": "Fengcun Li",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13550",
                    "name": "Gang Xu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13551",
                    "name": "Guanyu Wu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13552",
                    "name": "Guoyuan Lin",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13553",
                    "name": "Haibin Wang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13554",
                    "name": "Hansi Yang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13555",
                    "name": "Hao Yang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13556",
                    "name": "Haonan Yan",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13557",
                    "name": "Haoxiang Ma",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13558",
                    "name": "Haoxing Wen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13559",
                    "name": "Hongyan Hao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1355a",
                    "name": "Hongyin Tang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1355b",
                    "name": "Hongyu Zang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1355c",
                    "name": "Hongzhi Ni",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1355d",
                    "name": "Hui Su",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1355e",
                    "name": "Jiacheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1355f",
                    "name": "Jiahong Zhou",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13560",
                    "name": "Jiahuan Li",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13561",
                    "name": "Jiaming Wang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13562",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13563",
                    "user": {
                        "_id": "64008a0af4ff62c2616d8858",
                        "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg",
                        "isPro": false,
                        "fullname": "zhangjf",
                        "user": "zhangjf",
                        "type": "user"
                    },
                    "name": "Jianfei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-26T09:09:09.272Z",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13564",
                    "name": "Jianhao Xu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13565",
                    "name": "Jianing Wang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13566",
                    "name": "Jiapeng Zhu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13567",
                    "name": "Jiaqi Sun",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13568",
                    "name": "Jiarong Shi",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13569",
                    "name": "Jiarui Zhao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1356a",
                    "name": "Jingang Wang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1356b",
                    "user": {
                        "_id": "6592472fccbc1e2cc7250903",
                        "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg",
                        "isPro": false,
                        "fullname": "Jinluan Yang",
                        "user": "Jinluan",
                        "type": "user"
                    },
                    "name": "Jinluan Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-26T08:28:47.175Z",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1356c",
                    "name": "Jinrui Ding",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1356d",
                    "name": "Jinwei Xiao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1356e",
                    "name": "Jiyuan He",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1356f",
                    "name": "Juncan Xu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13570",
                    "name": "Kefeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13571",
                    "name": "Keheng Wang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13572",
                    "name": "Li Wei",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13573",
                    "name": "Lianhui Ma",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13574",
                    "name": "Lin Qiu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13575",
                    "name": "Lingbing Kong",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13576",
                    "name": "Lingchuan Liu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13577",
                    "name": "Linsen Guo",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13578",
                    "name": "Mengshen Zhu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13579",
                    "name": "Mengxia Shen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1357a",
                    "name": "Mingyang Zhu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1357b",
                    "name": "Peiguang Li",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1357c",
                    "name": "Peng Pei",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1357d",
                    "name": "Pengcheng Jia",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1357e",
                    "name": "Pengtao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1357f",
                    "name": "Peng Zhao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13580",
                    "name": "Qi Gu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13581",
                    "name": "Qiong Huang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13582",
                    "name": "Qiyuan Duan",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13583",
                    "name": "Quanchi Weng",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13584",
                    "name": "Rongxiang Weng",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13585",
                    "name": "Rongzhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13586",
                    "name": "Rumei Li",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13587",
                    "name": "Shanglin Lei",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13588",
                    "user": {
                        "_id": "64db5f5dd68a6ddcc7bd89e9",
                        "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg",
                        "isPro": false,
                        "fullname": "Shengnan An",
                        "user": "ShengnanAn",
                        "type": "user"
                    },
                    "name": "Shengnan An",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-26T09:09:11.410Z",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13589",
                    "name": "Shijun Dai",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1358a",
                    "name": "Shuaikang Liu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1358b",
                    "name": "Shuang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1358c",
                    "name": "Shuo Wang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1358d",
                    "name": "Songyuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1358e",
                    "name": "Tao Liang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1358f",
                    "name": "Tianhao Hu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13590",
                    "name": "Tianze Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13591",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13592",
                    "name": "Wei Shi",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13593",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13594",
                    "name": "Weifeng Tang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13595",
                    "name": "Wenjie Shi",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13596",
                    "name": "Wenlong Zhu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13597",
                    "name": "Wentao Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13598",
                    "name": "Wentao Shi",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c13599",
                    "name": "Xi Su",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1359a",
                    "name": "Xiangcheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1359b",
                    "name": "Xiandi Ma",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1359c",
                    "user": {
                        "_id": "63edb098679c2cc40abc6c2e",
                        "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg",
                        "isPro": false,
                        "fullname": "Xiangyu",
                        "user": "xixy",
                        "type": "user"
                    },
                    "name": "Xiangyu Xi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-26T09:09:13.312Z",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1359d",
                    "name": "Xiangyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1359e",
                    "name": "Xiangzhou Huang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c1359f",
                    "name": "Xiao Liu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135a0",
                    "name": "Xiaodong Cai",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135a1",
                    "name": "Xiaolong Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135a2",
                    "name": "Xiaowei Shi",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135a3",
                    "name": "Xiaoyu Li",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135a4",
                    "name": "Xin Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135a5",
                    "name": "Xingchen Liu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135a6",
                    "name": "Xuan Huang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135a7",
                    "name": "Xuezhi Cao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135a8",
                    "name": "Xunliang Cai",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135a9",
                    "name": "Yan Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135aa",
                    "user": {
                        "_id": "63fc1c420aab06079200c15c",
                        "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg",
                        "isPro": false,
                        "fullname": "yang bai",
                        "user": "byang",
                        "type": "user"
                    },
                    "name": "Yang Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-26T09:09:07.036Z",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135ab",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135ac",
                    "name": "Yang Yang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135ad",
                    "name": "Yang Zheng",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135ae",
                    "name": "Yaoming Wang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135af",
                    "name": "Yaoming Zhu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135b0",
                    "name": "Yaqi Huo",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135b1",
                    "name": "Yanyu Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135b2",
                    "name": "Yaorui Shi",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135b3",
                    "name": "Yerui Sun",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135b4",
                    "name": "Yi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135b5",
                    "name": "Yihao Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135b6",
                    "name": "Yi-Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135b7",
                    "name": "Yifan Lu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135b8",
                    "name": "Yifan Zhao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135b9",
                    "name": "Yitao Zhai",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135ba",
                    "name": "Yongjing Yin",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135bb",
                    "name": "Yongwei Zhou",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135bc",
                    "name": "Youshao Xiao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135bd",
                    "name": "Yuchuan Dai",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135be",
                    "name": "Yuchen Xie",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135bf",
                    "name": "Yuchen Yu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135c0",
                    "name": "Yufei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135c1",
                    "name": "Yuhuai Wei",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135c2",
                    "name": "Yulei Qian",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135c3",
                    "name": "Yunfan Liang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135c4",
                    "name": "Yunke Zhao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135c5",
                    "name": "Yuwei Jiang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135c6",
                    "name": "Yuxin Bian",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135c7",
                    "name": "Yuxin Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135c8",
                    "name": "Yuxin Liu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135c9",
                    "name": "Yue Xu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135ca",
                    "name": "Yueqing Sun",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135cb",
                    "name": "Zeyang Yu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135cc",
                    "name": "Zhao Yang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135cd",
                    "name": "Zhengsheng Huang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135ce",
                    "name": "Zhengyu Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135cf",
                    "name": "Zhijian Liu",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135d0",
                    "name": "Zhikang Xia",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135d1",
                    "name": "Zhimin Lin",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135d2",
                    "name": "Zhiyuan Yao",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135d3",
                    "name": "Zhuofan Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135d4",
                    "name": "Zhuowen Han",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135d5",
                    "name": "Zijian Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135d6",
                    "name": "Ziran Li",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135d7",
                    "name": "Ziwen Wang",
                    "hidden": false
                },
                {
                    "_id": "6976d5405d41524304c135d8",
                    "name": "Ziyuan Zhuang",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-23T13:20:09.000Z",
            "submittedOnDailyAt": "2026-01-26T00:15:28.340Z",
            "title": "LongCat-Flash-Thinking-2601 Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.",
            "upvotes": 133,
            "discussionId": "6976d5405d41524304c135d9",
            "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "agentic reasoning",
                "domain-parallel expert training",
                "fusion",
                "asynchronous reinforcement learning",
                "DORA",
                "long-tailed generation",
                "multi-turn interactions",
                "real-world noise patterns",
                "test-time scaling",
                "reasoning depth",
                "reasoning width",
                "parallel thinking"
            ],
            "organization": {
                "_id": "68b28d79a176a9beb30d2049",
                "name": "meituan-longcat",
                "fullname": "LongCat",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
            }
        },
        "translation_title": "LongCat-Flash-Thinking-2601 기술 보고서",
        "purpose": "최고의 에이전트적 추론 기능을 가진 오픈 소스 Mixture-of-Experts (MoE) 모델 개발 및 성능 최적화",
        "method": [
            "5600억 개의 파라미터를 가진 LongCat-Flash-Thinking-2601을 소개함(We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model.)",
            "모델의 성능을 높이기 위해 환경 확장 및 태스크 구성을 심도 있게 탐구함(In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction.)",
            "DORA라는 비동기 강화 학습 프레임워크를 확장하여 안정적이고 효율적인 다중 환경 학습을 구현함(To optimize long-tailed, skewed generation and multi-turn agentic interactions, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training.)"
        ],
        "conclusion": "LongCat-Flash-Thinking-2601은 에이전트적 벤치마크에서 최신 성능을 달성하고, 복잡한 도구 상호작용 및 현실 환경에서 강력한 일반화 능력을 보여줍니다.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2601.16746",
            "authors": [
                {
                    "_id": "6976d4105d41524304c13517",
                    "name": "Yuhang Wang",
                    "hidden": false
                },
                {
                    "_id": "6976d4105d41524304c13518",
                    "user": {
                        "_id": "645b0c3ec35da9c7afd95421",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                        "isPro": false,
                        "fullname": "Yuling",
                        "user": "YerbaPage",
                        "type": "user"
                    },
                    "name": "Yuling Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-26T08:28:56.805Z",
                    "hidden": false
                },
                {
                    "_id": "6976d4105d41524304c13519",
                    "name": "Mo Yang",
                    "hidden": false
                },
                {
                    "_id": "6976d4105d41524304c1351a",
                    "name": "Rongrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d4105d41524304c1351b",
                    "name": "Shilin He",
                    "hidden": false
                },
                {
                    "_id": "6976d4105d41524304c1351c",
                    "name": "Heng Lian",
                    "hidden": false
                },
                {
                    "_id": "6976d4105d41524304c1351d",
                    "name": "Yuting Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d4105d41524304c1351e",
                    "name": "Siyu Ye",
                    "hidden": false
                },
                {
                    "_id": "6976d4105d41524304c1351f",
                    "name": "Kai Cai",
                    "hidden": false
                },
                {
                    "_id": "6976d4105d41524304c13520",
                    "name": "Xiaodong Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-23T13:51:59.000Z",
            "submittedOnDailyAt": "2026-01-26T00:10:23.451Z",
            "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.",
            "upvotes": 62,
            "discussionId": "6976d4105d41524304c13521",
            "githubRepo": "https://github.com/Ayanami1314/swe-pruner",
            "githubRepoAddedBy": "user",
            "ai_summary": "SWE-Pruner is a self-adaptive context pruning framework for coding agents that uses task-aware pruning to reduce token usage while maintaining performance.",
            "ai_keywords": [
                "context compression",
                "LongLLMLingua",
                "PPL",
                "code understanding",
                "task-aware adaptive pruning",
                "neural skimmer",
                "token reduction",
                "SWE-Bench Verified",
                "LongCodeQA"
            ],
            "githubStars": 21,
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "translation_title": "SWE-Pruner: 코딩 에이전트를 위한 자기 적응형 컨텍스트 프루닝",
        "purpose": "코딩 에이전트의 성능을 향상시키기 위해 긴 상호작용 컨텍스트에 대한 적응형 프루닝 기법 연구",
        "method": [
            "사람 프로그래머들이 소스 코드를 '선택적으로 훑어보는' 방식을 모방하여, 작업에 따라 적응형 프루닝을 수행함(SWE-Pruner performs task-aware adaptive pruning for long contexts.)",
            "현재 작업에 대한 명확한 목표를 수립하여 프루닝 대상을 안내함(The agent formulates an explicit goal as a hint to guide the pruning targets.)",
            "경량화된 신경망 스키머를 훈련시켜, 주어진 목표에 따라 관련 있는 코드 라인을 동적으로 선택하도록 함(A lightweight neural skimmer is trained to dynamically select relevant lines from the surrounding context given the goal.)"
        ],
        "conclusion": "SWE-Pruner는 다양한 작업에서 에이전트 성능에 최소한의 영향을 주면서 23-54%의 토큰을 감소시키고, 단일 턴 작업에서 최대 14.84배 압축을 달성함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2601.14133",
            "authors": [
                {
                    "_id": "69706e6ea8be625b19c2afac",
                    "user": {
                        "_id": "63d3b5f1640bb0f77173baea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg",
                        "isPro": false,
                        "fullname": "yubin",
                        "user": "VLyb",
                        "type": "user"
                    },
                    "name": "Bin Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-21T09:19:14.460Z",
                    "hidden": false
                },
                {
                    "_id": "69706e6ea8be625b19c2afad",
                    "user": {
                        "_id": "65ec01fd770aa0e25d9374dc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg",
                        "isPro": false,
                        "fullname": "Shijie Lian",
                        "user": "LiamLian0727",
                        "type": "user"
                    },
                    "name": "Shijie Lian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-22T08:47:15.246Z",
                    "hidden": false
                },
                {
                    "_id": "69706e6ea8be625b19c2afae",
                    "name": "Xiaopeng Lin",
                    "hidden": false
                },
                {
                    "_id": "69706e6ea8be625b19c2afaf",
                    "name": "Yuliang Wei",
                    "hidden": false
                },
                {
                    "_id": "69706e6ea8be625b19c2afb0",
                    "name": "Zhaolong Shen",
                    "hidden": false
                },
                {
                    "_id": "69706e6ea8be625b19c2afb1",
                    "name": "Changti Wu",
                    "hidden": false
                },
                {
                    "_id": "69706e6ea8be625b19c2afb2",
                    "name": "Yuzhuo Miao",
                    "hidden": false
                },
                {
                    "_id": "69706e6ea8be625b19c2afb3",
                    "name": "Xinming Wang",
                    "hidden": false
                },
                {
                    "_id": "69706e6ea8be625b19c2afb4",
                    "name": "Bailing Wang",
                    "hidden": false
                },
                {
                    "_id": "69706e6ea8be625b19c2afb5",
                    "name": "Cong Huang",
                    "hidden": false
                },
                {
                    "_id": "69706e6ea8be625b19c2afb6",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-20T16:30:07.000Z",
            "submittedOnDailyAt": "2026-01-26T00:14:31.380Z",
            "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
            "submittedOnDailyBy": {
                "_id": "63d3b5f1640bb0f77173baea",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg",
                "isPro": false,
                "fullname": "yubin",
                "user": "VLyb",
                "type": "user"
            },
            "summary": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.",
            "upvotes": 51,
            "discussionId": "69706e6ea8be625b19c2afb7",
            "githubRepo": "https://github.com/ZGC-EmbodyAI/TwinBrainVLA",
            "githubRepoAddedBy": "user",
            "ai_summary": "TwinBrainVLA addresses the tension between semantic understanding and motor skills in robot control by coordinating a generalist vision-language model with a specialist model through an asymmetric mixture-of-transformers mechanism.",
            "ai_keywords": [
                "Vision-Language-Action models",
                "Vision-Language Models",
                "robotic control",
                "catastrophic forgetting",
                "frozen Left Brain",
                "trainable Right Brain",
                "Asymmetric Mixture-of-Transformers",
                "Flow-Matching Action Expert",
                "embodied perception",
                "proprioception"
            ],
            "githubStars": 9,
            "organization": {
                "_id": "68896d3a716ee5bfb1428441",
                "name": "ZGCA",
                "fullname": "Zhongguancun Academy",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
            }
        },
        "translation_title": "TwinBrainVLA: 비대칭 혼합 변환기를 통한 일반ist VLM의 실체 작업 잠재력 발휘",
        "purpose": "로봇 제어를 위한 일반ist VLM과 저수준 센서 운동 기술을 균형 있게 학습하기 위함.",
        "method": [
            "TwinBrainVLA라는 새로운 아키텍처를 소개하여 일반ist VLM과 전문 VLM을 조정함(we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control).",
            "비대칭 혼합 변환기(Asymmetric Mixture-of-Transformers) 메커니즘을 통해 '왼쪽 뇌'와 '오른쪽 뇌'를 결합함(TwinBrainVLA synergizes a frozen 'Left Brain' with a trainable 'Right Brain' via a novel Asymmetric Mixture-of-Transformers mechanism).",
            "오른쪽 뇌가 왼쪽 뇌의 의미 지식을 동적으로 쿼리하고 이를 자기 인지 상태와 융합하게 함(This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states)."
        ],
        "conclusion": "TwinBrainVLA는 기존의 VLM의 종합적인 시각 이해 능력을 보존하면서도 우수한 조작 성능을 발휘하며, 일반 목적의 로봇 개발에 유망한 방향을 제시함.",
        "keywords": [
            "Vision-Language Models",
            "Robotics",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2601.16973",
            "authors": [
                {
                    "_id": "6976d4695d41524304c13523",
                    "user": {
                        "_id": "641a38fdfb5ffff5ac78ceb0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a38fdfb5ffff5ac78ceb0/53nGstZ9Ya5WfeGdXAXCr.png",
                        "isPro": true,
                        "fullname": "Zirui Wang",
                        "user": "zwcolin",
                        "type": "user"
                    },
                    "name": "Zirui Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-26T08:28:54.291Z",
                    "hidden": false
                },
                {
                    "_id": "6976d4695d41524304c13524",
                    "name": "Junyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6976d4695d41524304c13525",
                    "user": {
                        "_id": "6629dac35e13d8145e3a605e",
                        "avatarUrl": "/avatars/95938f20ab9e067838f37aca6ea235ae.svg",
                        "isPro": false,
                        "fullname": "Jiaxin Ge",
                        "user": "JiaxinGe",
                        "type": "user"
                    },
                    "name": "Jiaxin Ge",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-26T08:31:33.595Z",
                    "hidden": false
                },
                {
                    "_id": "6976d4695d41524304c13526",
                    "user": {
                        "_id": "63797c273f575acc2f6893c0",
                        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
                        "isPro": true,
                        "fullname": "Long(Tony) Lian",
                        "user": "longlian",
                        "type": "user"
                    },
                    "name": "Long Lian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-26T08:28:52.034Z",
                    "hidden": false
                },
                {
                    "_id": "6976d4695d41524304c13527",
                    "name": "Letian Fu",
                    "hidden": false
                },
                {
                    "_id": "6976d4695d41524304c13528",
                    "name": "Lisa Dunlap",
                    "hidden": false
                },
                {
                    "_id": "6976d4695d41524304c13529",
                    "name": "Ken Goldberg",
                    "hidden": false
                },
                {
                    "_id": "6976d4695d41524304c1352a",
                    "name": "XuDong Wang",
                    "hidden": false
                },
                {
                    "_id": "6976d4695d41524304c1352b",
                    "name": "Ion Stoica",
                    "hidden": false
                },
                {
                    "_id": "6976d4695d41524304c1352c",
                    "name": "David M. Chan",
                    "hidden": false
                },
                {
                    "_id": "6976d4695d41524304c1352d",
                    "name": "Sewon Min",
                    "hidden": false
                },
                {
                    "_id": "6976d4695d41524304c1352e",
                    "name": "Joseph E. Gonzalez",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/641a38fdfb5ffff5ac78ceb0/sC7mJkpqiaPhEB6RZvHAY.mp4"
            ],
            "publishedAt": "2026-01-23T18:43:34.000Z",
            "submittedOnDailyAt": "2026-01-26T00:14:43.910Z",
            "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
            "submittedOnDailyBy": {
                "_id": "641a38fdfb5ffff5ac78ceb0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a38fdfb5ffff5ac78ceb0/53nGstZ9Ya5WfeGdXAXCr.png",
                "isPro": true,
                "fullname": "Zirui Wang",
                "user": "zwcolin",
                "type": "user"
            },
            "summary": "Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.",
            "upvotes": 21,
            "discussionId": "6976d46a5d41524304c1352f",
            "projectPage": "https://visgym.github.io/",
            "githubRepo": "https://github.com/visgym/VIsGym",
            "githubRepoAddedBy": "user",
            "ai_summary": "Modern vision-language models exhibit significant challenges in multi-step visual interaction tasks, particularly in long-horizon perception-memory-action integration, with performance declining when handling unbounded historical contexts.",
            "ai_keywords": [
                "vision-language models",
                "multi-step visual interactions",
                "perception",
                "memory",
                "action",
                "symbolic puzzles",
                "real-image understanding",
                "navigation",
                "manipulation",
                "supervised fine-tuning",
                "goal observations",
                "textual feedback",
                "exploratory demonstrations",
                "partially observable environments",
                "unknown-dynamics settings"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "66c3b97cd7a9770138e4ce9a",
                "name": "UCB-Sky-Computing-Lab",
                "fullname": "Sky Computing Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66c3740cc55655c7155c47ab/0ZqaCA_kmq5k6ahdnsIJ1.png"
            }
        },
        "translation_title": "VisGym: 다양한, 맞춤형, 확장 가능한 다중 모달 에이전트를 위한 환경",
        "purpose": "다단계 시각적 상호작용에서 Vision-Language Models(VLMs)의 성능을 평가하고 훈련하기 위한 환경을 제공",
        "method": [
            "17개의 다양한 환경으로 구성된 VisGym을 소개하며, 상징 퍼즐, 실제 이미지 이해, 탐색, 조작을 포괄하여 난이도, 입력 표현, 계획 수명, 피드백을 조절할 수 있도록 함(We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs.)",
            "구조화된 데모를 생성하는 다단계 해결책을 제공하여 감독 학습을 가능하게 함(We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning.)",
            "모델들이 불리한 설정에서 낮은 성공률을 기록하며 장기 맥락을 효과적으로 활용하지 못함을 보여주는 실험을 수행함(Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations.)"
        ],
        "conclusion": "명확한 목표 관찰과 텍스트 기반 피드백이 다단계 시각적 의사결정을 개선할 수 있는 방법을 강조하며, 이러한 방법을 통해 꾸준한 성과 향상을 규명함.",
        "keywords": [
            "Vision-Language Models",
            "Multimodal Learning",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2601.16296",
            "authors": [
                {
                    "_id": "6976d29a5d41524304c1350a",
                    "name": "Dohun Lee",
                    "hidden": false
                },
                {
                    "_id": "6976d29a5d41524304c1350b",
                    "name": "Chun-Hao Paul Huang",
                    "hidden": false
                },
                {
                    "_id": "6976d29a5d41524304c1350c",
                    "name": "Xuelin Chen",
                    "hidden": false
                },
                {
                    "_id": "6976d29a5d41524304c1350d",
                    "name": "Jong Chul Ye",
                    "hidden": false
                },
                {
                    "_id": "6976d29a5d41524304c1350e",
                    "name": "Duygu Ceylan",
                    "hidden": false
                },
                {
                    "_id": "6976d29a5d41524304c1350f",
                    "name": "Hyeonho Jeong",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/jX8O1mKnYcn5zBMk0V0bu.mp4"
            ],
            "publishedAt": "2026-01-22T19:59:17.000Z",
            "submittedOnDailyAt": "2026-01-26T00:06:22.236Z",
            "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V",
            "upvotes": 13,
            "discussionId": "6976d29b5d41524304c13510",
            "projectPage": "https://dohunlee1.github.io/MemoryV2V",
            "ai_summary": "Memory-V2V enhances multi-turn video editing by maintaining cross-consistency through explicit memory mechanisms and efficient token compression in video-to-video diffusion models.",
            "ai_keywords": [
                "video-to-video diffusion models",
                "cross-consistency",
                "multi-turn video editing",
                "memory augmentation",
                "retrieval",
                "dynamic tokenization",
                "token compressor",
                "DiT backbone",
                "video novel view synthesis",
                "text-conditioned long video editing"
            ],
            "organization": {
                "_id": "61e5d14f77496de0a6d95c6b",
                "name": "adobe",
                "fullname": "Adobe",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
            }
        },
        "translation_title": "Memory-V2V: 메모리로 비디오-비디오 확산 모델 강화하기",
        "purpose": "비디오 편집의 다단계 과정에서 일관성을 유지하기 위한 방법을 연구함",
        "method": [
            "외부 캐시에서 이전에 편집된 비디오를 활용함으로써 현재 편집 단계를 이전 결과에 조건화하는 Memory-V2V 프레임워크를 제안함(We introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory.)",
            "중복을 줄이고 계산 부담을 경감하기 위해 DiT 백본 내에서 학습 가능한 토큰 압축기를 제안하고, 이 압축기가 필수적인 시각적 단서를 유지하면서도 중복되는 조건 토큰을 압축함(To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues.)"
        ],
        "conclusion": "Memory-V2V는 이전보다 일관성 있는 비디오를 생산하며, 최소한의 계산 부담으로도 성능을 유지하거나 개선함.",
        "keywords": [
            "Video Generation",
            "Image Understanding",
            "Multimodal Learning"
        ]
    }
]