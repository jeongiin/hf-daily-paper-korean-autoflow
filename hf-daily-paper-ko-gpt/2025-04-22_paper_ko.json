[
    {
        "paper": {
            "id": "2504.14945",
            "authors": [
                {
                    "_id": "6806fdeda296cac1cf860505",
                    "user": {
                        "_id": "6086838b19137b3a6ba760e7",
                        "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
                        "isPro": false,
                        "fullname": "Jianhao Yan",
                        "user": "Elliott",
                        "type": "user"
                    },
                    "name": "Jianhao Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:43:23.332Z",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf860506",
                    "user": {
                        "_id": "63f3502a520c14618925825a",
                        "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
                        "isPro": false,
                        "fullname": "Yafu Li",
                        "user": "yaful",
                        "type": "user"
                    },
                    "name": "Yafu Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:43:32.087Z",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf860507",
                    "user": {
                        "_id": "662de927b10f82aa4f46557f",
                        "avatarUrl": "/avatars/19878db42ecf8e35456378571eae4643.svg",
                        "isPro": false,
                        "fullname": "Zican Hu",
                        "user": "huzican",
                        "type": "user"
                    },
                    "name": "Zican Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:43:39.458Z",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf860508",
                    "name": "Zhi Wang",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf860509",
                    "user": {
                        "_id": "650eba9555dc1e841746f132",
                        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
                        "isPro": false,
                        "fullname": "Ganqu Cui",
                        "user": "ganqu",
                        "type": "user"
                    },
                    "name": "Ganqu Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:43:50.636Z",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf86050a",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf86050b",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf86050c",
                    "name": "Yue Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T08:09:13.000Z",
            "submittedOnDailyAt": "2025-04-22T00:57:31.276Z",
            "title": "Learning to Reason under Off-Policy Guidance",
            "submittedOnDailyBy": {
                "_id": "6086838b19137b3a6ba760e7",
                "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
                "isPro": false,
                "fullname": "Jianhao Yan",
                "user": "Elliott",
                "type": "user"
            },
            "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.",
            "upvotes": 53,
            "discussionId": "6806fdeea296cac1cf860553",
            "githubRepo": "https://github.com/ElliottYan/LUFFY",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "zero-RL",
                "on-policy",
                "off-policy",
                "LUFFY",
                "imitation learning",
                "on-policy rollouts",
                "policy shaping",
                "regularized importance sampling",
                "mixed-policy training",
                "math benchmarks",
                "out-of-distribution tasks",
                "imitation-based supervised fine-tuning (SFT)",
                "generalizable reasoning models"
            ]
        },
        "translation_title": "Off-Policy 지침 하에서 학습하기",
        "purpose": "기존 모델의 능력을 초과한 Reasoning 능력을 학습할 수 있는 방법 연구",
        "method": [
            "LUFFY라는 프레임워크를 통해 zero-RL에 off-policy reasoning traces를 추가하여 학습함(We introduce LUFFY (Learning to reason Under oFF-policY guidance), a framework that augments zero-RL with off-policy reasoning traces.)",
            "훈련 중 off-policy 데모와 on-policy 롤아웃을 결합하여 모방과 탐험을 동적으로 균형 잡음(LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training.)",
            "정규화된 중요도 샘플링을 통해 혼합 정책 훈련 중 피상적이고 경직된 모방을 피하는 정책 형성을 제안함(Notably, we propose policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training.)"
        ],
        "conclusion": "LUFFY는 6개의 수학 벤치마크에서 평균 +7.0의 성과 향상을 이루었고, 일반화에 있어 특히 모방 기반의 SFT보다 뛰어난 성능을 보임.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2504.15271",
            "authors": [
                {
                    "_id": "680748d088578d9444349293",
                    "user": {
                        "_id": "6392c73390b8e99a6779a7b0",
                        "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
                        "isPro": false,
                        "fullname": "Guo Chen",
                        "user": "cg1177",
                        "type": "user"
                    },
                    "name": "Guo Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:46:58.348Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d9444349294",
                    "user": {
                        "_id": "6582d86e58df0a2e21db80b8",
                        "avatarUrl": "/avatars/a8245b1644183bd3ee7dc06b218c6e47.svg",
                        "isPro": true,
                        "fullname": "ZhiqiLi",
                        "user": "RealZhiqiLi",
                        "type": "user"
                    },
                    "name": "Zhiqi Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:20.873Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d9444349295",
                    "name": "Shihao Wang",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d9444349296",
                    "name": "Jindong Jiang",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d9444349297",
                    "name": "Yicheng Liu",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d9444349298",
                    "user": {
                        "_id": "64a3de701698ad2985277148",
                        "avatarUrl": "/avatars/09eebadbbea53ed2800591564ff5c931.svg",
                        "isPro": false,
                        "fullname": "lulidong",
                        "user": "lulidong",
                        "type": "user"
                    },
                    "name": "Lidong Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:51:00.432Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d9444349299",
                    "user": {
                        "_id": "641d1c5ec3983aa94915c162",
                        "avatarUrl": "/avatars/127985b837ecf61e43c835deee578b5e.svg",
                        "isPro": false,
                        "fullname": "De-An Huang",
                        "user": "deahuang",
                        "type": "user"
                    },
                    "name": "De-An Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:50:49.235Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d944434929a",
                    "user": {
                        "_id": "67da13552f60f9492fb908c6",
                        "avatarUrl": "/avatars/342ad1a5d3ab33a483ed74d6734f4b03.svg",
                        "isPro": false,
                        "fullname": "Wonmin Byeon",
                        "user": "WonminByeon",
                        "type": "user"
                    },
                    "name": "Wonmin Byeon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:50:43.675Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d944434929b",
                    "name": "Matthieu Le",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d944434929c",
                    "user": {
                        "_id": "67d3a14adfd19a63b25f0871",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fwYFVlhNGgJo3sSxWDfoP.png",
                        "isPro": false,
                        "fullname": "Tuomas Rintamaki",
                        "user": "trintamaki",
                        "type": "user"
                    },
                    "name": "Tuomas Rintamaki",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:49:24.223Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d944434929d",
                    "user": {
                        "_id": "67d0d2d85f0fcc0c38872acc",
                        "avatarUrl": "/avatars/27bdff7f058a515261b9bec654941ca0.svg",
                        "isPro": false,
                        "fullname": "Tyler Poon",
                        "user": "tpoon-nvidia",
                        "type": "user"
                    },
                    "name": "Tyler Poon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:49:41.245Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d944434929e",
                    "name": "Max Ehrlich",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d944434929f",
                    "name": "Tuomas Rintamaki",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a0",
                    "name": "Tyler Poon",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a1",
                    "name": "Tong Lu",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a2",
                    "user": {
                        "_id": "62c77f4352d8ae531f5511f9",
                        "avatarUrl": "/avatars/50198ccb02ccd286975a4613fbabee28.svg",
                        "isPro": false,
                        "fullname": "Limin Wang",
                        "user": "lmwang",
                        "type": "user"
                    },
                    "name": "Limin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:48:58.836Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a3",
                    "user": {
                        "_id": "6311021788942700629e6247",
                        "avatarUrl": "/avatars/e7adc1632b76e80e7e4a590033d1c20a.svg",
                        "isPro": false,
                        "fullname": "Bryan Catanzaro",
                        "user": "ctnzr",
                        "type": "user"
                    },
                    "name": "Bryan Catanzaro",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:48:38.596Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a4",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a5",
                    "name": "Andrew Tao",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a6",
                    "user": {
                        "_id": "66c8037c737ba92ae3fe0322",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c8037c737ba92ae3fe0322/WR_Yh5DWOVVh7IFlF24NM.jpeg",
                        "isPro": false,
                        "fullname": "Zhiding Yu",
                        "user": "Zhiding",
                        "type": "user"
                    },
                    "name": "Zhiding Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:48:23.522Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a7",
                    "user": {
                        "_id": "6656eb16e50d7c40881a14f0",
                        "avatarUrl": "/avatars/c6822a51c8d5918debf6ee1d25fe1825.svg",
                        "isPro": false,
                        "fullname": "GuilinLiu",
                        "user": "GuilinLiu",
                        "type": "user"
                    },
                    "name": "Guilin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:48:17.611Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T17:57:28.000Z",
            "submittedOnDailyAt": "2025-04-22T06:14:17.808Z",
            "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "6392c73390b8e99a6779a7b0",
                "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
                "isPro": false,
                "fullname": "Guo Chen",
                "user": "cg1177",
                "type": "user"
            },
            "summary": "We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\nfor long-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension and high-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerous efficiency optimizations in the pipeline for\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-context multimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B.",
            "upvotes": 39,
            "discussionId": "680748d188578d9444349311",
            "projectPage": "https://nvlabs.github.io/EAGLE/",
            "githubRepo": "https://github.com/NVlabs/EAGLE",
            "ai_keywords": [
                "vision-language models (VLMs)",
                "long-context multimodal learning",
                "long video comprehension",
                "high-resolution image understanding",
                "Automatic Degrade Sampling",
                "Image Area Preservation",
                "efficiency optimizations",
                "long-context data training",
                "Eagle-Video-110K",
                "story-level annotations",
                "clip-level annotations",
                "long-video understanding",
                "multimodal benchmarks",
                "Video-MME"
            ]
        },
        "translation_title": "Eagle 2.5: 최전선 Vision-Language 모델을 위한 장기 맥락 포스트-학습 향상",
        "purpose": "장기 비디오 이해와 고해상도 이미지 이해의 문제를 해결하기 위한 Vision-Language 모델 개발",
        "method": [
            "Automatic Degrade Sampling 및 Image Area Preservation 기술을 통해 맥락 무결성과 시각적 세부사항을 보존하는 훈련 프레임워크를 제안함(Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist framework for both tasks.)",
            "장기 맥락 데이터 훈련을 위한 효율성 최적화를 여려 단계에서 포함시킴(The framework also includes numerous efficiency optimizations in the pipeline for long-context data training.)",
            "스토리 수준 및 클립 수준 주석을 통합한 새로운 데이터셋 Eagle-Video-110K을 제안함(Finally, we propose Eagle-Video-110K, a novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding.)"
        ],
        "conclusion": "Eagle 2.5는 기존 VLM의 한계를 극복하며 장기 맥락 멀티모달 벤치마크에서 상당한 향상을 보여줌.",
        "keywords": [
            "Vision-Language Models",
            "Multimodal Learning",
            "Video Understanding"
        ]
    },
    {
        "paper": {
            "id": "2504.15257",
            "authors": [
                {
                    "_id": "68070ee593d1301c2f2ade99",
                    "user": {
                        "_id": "62728f4f6253fe2068da1021",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
                        "isPro": false,
                        "fullname": "Hongcheng Gao",
                        "user": "HongchengGao",
                        "type": "user"
                    },
                    "name": "Hongcheng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:45:06.014Z",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2ade9a",
                    "user": {
                        "_id": "6650c77a74664a42ddfb9187",
                        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
                        "isPro": false,
                        "fullname": "yueliu1999",
                        "user": "yueliu1999",
                        "type": "user"
                    },
                    "name": "Yue Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:44:41.556Z",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2ade9b",
                    "name": "Yufei He",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2ade9c",
                    "user": {
                        "_id": "6214e4ee1e35c843d42d1f88",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
                        "isPro": false,
                        "fullname": "Longxu Dou",
                        "user": "dreamerdeo",
                        "type": "user"
                    },
                    "name": "Longxu Dou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:46.818Z",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2ade9d",
                    "name": "Chao Du",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2ade9e",
                    "name": "Zhijie Deng",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2ade9f",
                    "user": {
                        "_id": "651d8032c50012d33e914f2f",
                        "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
                        "isPro": false,
                        "fullname": "Bryan Hooi",
                        "user": "bhooi",
                        "type": "user"
                    },
                    "name": "Bryan Hooi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:45:53.602Z",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2adea0",
                    "name": "Min Lin",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2adea1",
                    "user": {
                        "_id": "63d91b6d255ef6add20e1b38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
                        "isPro": false,
                        "fullname": "Tianyu Pang",
                        "user": "P2333",
                        "type": "user"
                    },
                    "name": "Tianyu Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:46:22.858Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T17:35:42.000Z",
            "submittedOnDailyAt": "2025-04-22T02:12:27.161Z",
            "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
            "submittedOnDailyBy": {
                "_id": "6650c77a74664a42ddfb9187",
                "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
                "isPro": false,
                "fullname": "yueliu1999",
                "user": "yueliu1999",
                "type": "user"
            },
            "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate\nthe design of query-level multi-agent systems, i.e., one system per user query.\nOur core idea is to incentivize a reasoning-based meta-agent via external\nexecution feedback. Concretely, by distilling DeepSeek R1, we first endow the\nbasic reasoning ability regarding the generation of multi-agent systems to\nFlowReasoner. Then, we further enhance it via reinforcement learning (RL) with\nexternal execution feedback. A multi-purpose reward is designed to guide the RL\ntraining from aspects of performance, complexity, and efficiency. In this\nmanner, FlowReasoner is enabled to generate a personalized multi-agent system\nfor each user query via deliberative reasoning. Experiments on both engineering\nand competition code benchmarks demonstrate the superiority of FlowReasoner.\nRemarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.\nThe code is available at https://github.com/sail-sg/FlowReasoner.",
            "upvotes": 30,
            "discussionId": "68070ee693d1301c2f2aded1",
            "ai_keywords": [
                "DeepSeek R1",
                "reinforcement learning",
                "reward",
                "performance",
                "complexity",
                "efficiency",
                "deliberative reasoning"
            ]
        },
        "translation_title": "FlowReasoner: 쿼리 수준 메타 에이전트 강화하기",
        "purpose": "쿼리 수준의 다중 에이전트 시스템 설계를 자동화하기 위해 FlowReasoner라는 메타 에이전트를 제안함",
        "method": [
            "DeepSeek R1을 증류하여 FlowReasoner에게 다중 에이전트 시스템 생성을 위한 기본 추론 능력을 부여함(Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner.)",
            "외부 실행 피드백을 통해 강화 학습(RL)을 이용해 추가적으로 능력을 향상시킴(Then, we further enhance it via reinforcement learning (RL) with external execution feedback.)",
            "성능, 복잡성, 효율성 측면에서 RL 훈련을 안내하는 다목적 보상을 설계함(A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency.)"
        ],
        "conclusion": "FlowReasoner는 각 사용자 쿼리별로 개인화된 다중 에이전트 시스템을 생성할 수 있으며, 세 가지 벤치마크에서 o1-mini보다 10.52% 높은 정확도를 기록하였음.",
        "keywords": [
            "Robotics",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2504.13958",
            "authors": [
                {
                    "_id": "6806fe1bec9fb3764b875ea0",
                    "name": "Cheng Qian",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea1",
                    "user": {
                        "_id": "63888d3fd68e37abd599f428",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
                        "isPro": true,
                        "fullname": "emre can",
                        "user": "emrecanacikgoz",
                        "type": "user"
                    },
                    "name": "Emre Can Acikgoz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:57.521Z",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea2",
                    "name": "Qi He",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea3",
                    "user": {
                        "_id": "65f906e5c3dbdcae83ff7aac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
                        "isPro": false,
                        "fullname": "Hongru Wang",
                        "user": "Merlin-Hongru",
                        "type": "user"
                    },
                    "name": "Hongru Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:52:19.954Z",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea4",
                    "user": {
                        "_id": "6270ff726417aed8a7340c8b",
                        "avatarUrl": "/avatars/3f14913c55cc4fc78678ac43fb603e80.svg",
                        "isPro": false,
                        "fullname": "Xiusi Chen",
                        "user": "XtremSup",
                        "type": "user"
                    },
                    "name": "Xiusi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:51:58.709Z",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea5",
                    "name": "Dilek Hakkani-Tür",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea6",
                    "user": {
                        "_id": "640f687c06c3b5ca88412985",
                        "avatarUrl": "/avatars/6c1e538b78f52ce45ff124b0a5ec3369.svg",
                        "isPro": false,
                        "fullname": "gokhan tur",
                        "user": "Gokhantur",
                        "type": "user"
                    },
                    "name": "Gokhan Tur",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:51:42.581Z",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea7",
                    "name": "Heng Ji",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/v-Pmd0mDnq-v5479tRcgC.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/pCPBgXm4IAZdhYjc-ZSrw.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/0c5XM8ZGXnieGz3IzuYTh.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/omlm8xwODvorHTUkfFOgR.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/1n73QmTy-PA5FfFQJMTej.png"
            ],
            "publishedAt": "2025-04-16T21:45:32.000Z",
            "submittedOnDailyAt": "2025-04-22T01:37:59.400Z",
            "title": "ToolRL: Reward is All Tool Learning Needs",
            "submittedOnDailyBy": {
                "_id": "63888d3fd68e37abd599f428",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
                "isPro": true,
                "fullname": "emre can",
                "user": "emrecanacikgoz",
                "type": "user"
            },
            "summary": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.",
            "upvotes": 25,
            "discussionId": "6806fe1dec9fb3764b875f0c",
            "githubRepo": "https://github.com/qiancheng0/ToolRL",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "R1-like models",
                "reasoning and generalization",
                "tool selection",
                "tool application",
                "reward design",
                "finegrained feedback",
                "reward strategies",
                "Group Relative Policy Optimization (GRPO)"
            ]
        },
        "translation_title": "ToolRL: 도구 학습에 필요한 것은 보상뿐입니다",
        "purpose": "도구 선택 및 적용 작업에 대한 보상 설계를 연구하여 LLM의 도구 사용 능력을 향상시키기 위한 목표",
        "method": [
            "보상 설계를 위한 다양한 전략을 체계적으로 탐색하고 분석함(We systematically explore a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics.)",
            "도구 사용 작업에 맞춘 원칙적인 보상 설계를 제안하고 Group Relative Policy Optimization (GRPO)를 사용해 LLM을 훈련함(Building on these insights, we propose a principled reward design tailored for tool use tasks and apply it to train LLMs using Group Relative Policy Optimization (GRPO).)",
            "다양한 벤치마크에서 우리의 접근 방식이 강력하고 안정적인 훈련을 제공함을 입증함(Empirical evaluations across diverse benchmarks demonstrate that our approach yields robust, scalable, and stable training.)"
        ],
        "conclusion": "보상 설계는 LLM의 도구 사용 능력과 일반화 성능을 향상시키는 데 중요한 역할을 한다는 것을 강조함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Reinforcement Learning"
        ]
    },
    {
        "paper": {
            "id": "2504.14396",
            "authors": [
                {
                    "_id": "6806fcc4f349e60f6c1b928c",
                    "user": {
                        "_id": "630461624ec2dfa82a5ad7e7",
                        "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
                        "isPro": false,
                        "fullname": "Minho Park",
                        "user": "mpark",
                        "type": "user"
                    },
                    "name": "Minho Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:59.880Z",
                    "hidden": false
                },
                {
                    "_id": "6806fcc4f349e60f6c1b928d",
                    "user": {
                        "_id": "679ecfd537a46d35e42f068d",
                        "avatarUrl": "/avatars/d3e891401105b5ef7196cb057ad92be0.svg",
                        "isPro": false,
                        "fullname": "TaewoongKang",
                        "user": "TaewoongKang",
                        "type": "user"
                    },
                    "name": "Taewoong Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:52:46.869Z",
                    "hidden": false
                },
                {
                    "_id": "6806fcc4f349e60f6c1b928e",
                    "user": {
                        "_id": "6369f693bf21b20c5692937b",
                        "avatarUrl": "/avatars/e937dc8234b3e456149882bfce34841f.svg",
                        "isPro": false,
                        "fullname": "Jooyeol Yun",
                        "user": "YeolJoo",
                        "type": "user"
                    },
                    "name": "Jooyeol Yun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:51:02.918Z",
                    "hidden": false
                },
                {
                    "_id": "6806fcc4f349e60f6c1b928f",
                    "user": {
                        "_id": "642fcfc0a043f0ac7deeaae0",
                        "avatarUrl": "/avatars/6cc46dd480cdc0d86c7a509e22782a13.svg",
                        "isPro": false,
                        "fullname": "Sungwon Hwang",
                        "user": "sungwon95",
                        "type": "user"
                    },
                    "name": "Sungwon Hwang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:52:52.332Z",
                    "hidden": false
                },
                {
                    "_id": "6806fcc4f349e60f6c1b9290",
                    "user": {
                        "_id": "64be3127805e5b64572da65c",
                        "avatarUrl": "/avatars/edd3e94ba9e375827cc75b164602bcac.svg",
                        "isPro": false,
                        "fullname": "Jaegul Choo",
                        "user": "joyfull78",
                        "type": "user"
                    },
                    "name": "Jaegul Choo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:52:58.097Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-19T19:59:11.000Z",
            "submittedOnDailyAt": "2025-04-22T00:50:52.270Z",
            "title": "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video\n  Generation via Spherical Latent Representation",
            "submittedOnDailyBy": {
                "_id": "630461624ec2dfa82a5ad7e7",
                "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
                "isPro": false,
                "fullname": "Minho Park",
                "user": "mpark",
                "type": "user"
            },
            "summary": "The increasing demand for AR/VR applications has highlighted the need for\nhigh-quality 360-degree panoramic content. However, generating high-quality\n360-degree panoramic images and videos remains a challenging task due to the\nsevere distortions introduced by equirectangular projection (ERP). Existing\napproaches either fine-tune pretrained diffusion models on limited ERP datasets\nor attempt tuning-free methods that still rely on ERP latent representations,\nleading to discontinuities near the poles. In this paper, we introduce\nSphereDiff, a novel approach for seamless 360-degree panoramic image and video\ngeneration using state-of-the-art diffusion models without additional tuning.\nWe define a spherical latent representation that ensures uniform distribution\nacross all perspectives, mitigating the distortions inherent in ERP. We extend\nMultiDiffusion to spherical latent space and propose a spherical latent\nsampling method to enable direct use of pretrained diffusion models. Moreover,\nwe introduce distortion-aware weighted averaging to further improve the\ngeneration quality in the projection process. Our method outperforms existing\napproaches in generating 360-degree panoramic content while maintaining high\nfidelity, making it a robust solution for immersive AR/VR applications. The\ncode is available here. https://github.com/pmh9960/SphereDiff",
            "upvotes": 21,
            "discussionId": "6806fcc7f349e60f6c1b93ab",
            "projectPage": "https://pmh9960.github.io/research/SphereDiff/",
            "githubRepo": "https://github.com/pmh9960/SphereDiff",
            "ai_keywords": [
                "SphereDiff",
                "diffusion models",
                "equirectangular projection (ERP)",
                "spherical latent representation",
                "MultiDiffusion",
                "spherical latent space",
                "spherical latent sampling method",
                "distortion-aware weighted averaging",
                "high-fidelity",
                "immersive AR/VR applications"
            ]
        },
        "translation_title": "SphereDiff: 조정 없이 구형 잠재 표현을 통한 전 방향 파노라마 이미지 및 비디오 생성",
        "purpose": "고품질의 360도 파노라마 콘텐츠 생성을 위한 새로운 접근 방식을 연구",
        "method": [
            "기존의 방법들은 출발점으로서 equirectangular projection(ERP)을 사용했지만, 이는 극지점에서 단절이 발생함.(However, generating high-quality 360-degree panoramic images and videos remains a challenging task due to the severe distortions introduced by equirectangular projection (ERP).)",
            "구형 잠재 표현을 정의하여 모든 각도에 균일한 분포를 보장하고 ERP의 왜곡을 줄임.(We define a spherical latent representation that ensures uniform distribution across all perspectives, mitigating the distortions inherent in ERP.)",
            "MultiDiffusion을 구형 잠재 공간으로 확장하고, 사전 학습된 확산 모델을 직접 사용할 수 있도록 구형 잠재 샘플링 방법을 제안함.(We extend MultiDiffusion to spherical latent space and propose a spherical latent sampling method to enable direct use of pretrained diffusion models.)",
            "왜곡을 인지한 가중 평균 방식을 도입하여 생성 품질을 개선함.(Moreover, we introduce distortion-aware weighted averaging to further improve the generation quality in the projection process.)"
        ],
        "conclusion": "우리의 방법은 360도 파노라마 콘텐츠 생성에서 기존 접근 방식을 능가하며, 몰입형 AR/VR 애플리케이션에 적합한 강력한 솔루션이 됨.",
        "keywords": [
            "Computer Vision",
            "Image Generation",
            "Video Generation"
        ]
    }
]