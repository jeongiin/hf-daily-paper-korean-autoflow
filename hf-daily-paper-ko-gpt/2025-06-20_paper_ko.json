[
    {
        "paper": {
            "id": "2506.14965",
            "authors": [
                {
                    "_id": "68538be099bf39f9665c79b9",
                    "name": "Zhoujun Cheng",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79ba",
                    "name": "Shibo Hao",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79bb",
                    "user": {
                        "_id": "629e2bcc46b4826be2c57fe3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629e2bcc46b4826be2c57fe3/41BiA52XlZi31ABsljFiq.jpeg",
                        "isPro": false,
                        "fullname": "Tianyang Liu",
                        "user": "tianyang",
                        "type": "user"
                    },
                    "name": "Tianyang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-20T12:19:48.511Z",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79bc",
                    "user": {
                        "_id": "628f6e5ab90dde28ef57d293",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f6e5ab90dde28ef57d293/AxNzR2nvrND6Rf3RPkYMk.jpeg",
                        "isPro": false,
                        "fullname": "Fan Zhou",
                        "user": "koalazf99",
                        "type": "user"
                    },
                    "name": "Fan Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-20T12:19:46.579Z",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79bd",
                    "name": "Yutao Xie",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79be",
                    "user": {
                        "_id": "64c8b2c5c547ed5243e14a6e",
                        "avatarUrl": "/avatars/96d4a9010f96001c8cff235915926390.svg",
                        "isPro": false,
                        "fullname": "Feng Yao",
                        "user": "fengyao1909",
                        "type": "user"
                    },
                    "name": "Feng Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-20T12:19:50.740Z",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79bf",
                    "name": "Yuexin Bian",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c0",
                    "name": "Yonghao Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c1",
                    "name": "Nilabjo Dey",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c2",
                    "name": "Yuheng Zha",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c3",
                    "name": "Yi Gu",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c4",
                    "name": "Kun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c5",
                    "name": "Yuqi Wang",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c6",
                    "name": "Yuan Li",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c7",
                    "name": "Richard Fan",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c8",
                    "name": "Jianshu She",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c9",
                    "name": "Chengqian Gao",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79ca",
                    "name": "Abulhair Saparov",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79cb",
                    "name": "Haonan Li",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79cc",
                    "name": "Taylor W. Killian",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79cd",
                    "name": "Mikhail Yurochkin",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79ce",
                    "name": "Zhengzhong Liu",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79cf",
                    "name": "Eric P. Xing",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79d0",
                    "name": "Zhiting Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T20:24:00.000Z",
            "submittedOnDailyAt": "2025-06-20T06:25:47.447Z",
            "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective",
            "submittedOnDailyBy": {
                "_id": "6083902e1e36b13a64497d91",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6083902e1e36b13a64497d91/h4rGHMn2c6z5GesF0F6VU.png",
                "isPro": false,
                "fullname": "cheng",
                "user": "zhoujun",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360",
            "upvotes": 11,
            "discussionId": "68538be099bf39f9665c79d1",
            "projectPage": "https://guru-reasoning.github.io/",
            "githubRepo": "https://github.com/LLM360/Reasoning360",
            "ai_summary": "Guru, a diverse RL reasoning corpus, highlights domain-specific training needs and demonstrates improved performance in complex tasks for RL-enhanced LLMs.",
            "ai_keywords": [
                "reinforcement learning",
                "large language model",
                "RL reasoning",
                "curated RL reasoning corpus",
                "domain-specific reward design",
                "dereplication",
                "filtering",
                "cross-domain RL training",
                "in-domain training",
                "Guru-7B",
                "Guru-32B",
                "Pass@k performance"
            ]
        },
        "translation_title": "여러 도메인 관점에서 다시 살펴보는 LLM 추론을 위한 강화 학습",
        "purpose": "LLM(대규모 언어 모델)의 추론 능력을 향상시키기 위한 신뢰할 수 있고 확장 가능한 강화 학습 보상 신호 연구",
        "method": [
            "Guru라는 92K 검증 가능한 샘플로 구성된 강화 학습 추론 데이터세트를 도입함(We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domains.)",
            "여섯 가지 추론 도메인 각각에 대해 도메인 특화된 보상 설계를 통해 신뢰성과 효율성을 보장함(each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training.)",
            "강화 학습의 기존 결과를 체계적으로 재검토하고 도메인 간의 유의미한 차이를 관찰함(Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains.)"
        ],
        "conclusion": "Guru-7B와 Guru-32B 모델을 통해 공개 데이터로 RL 훈련된 모델이 최고의 성능을 달성함과 함께, 복잡한 작업에서 기존 모델의 성능 향상을 효과적으로 이루어냄.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Reinforcement Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.09827",
            "authors": [
                {
                    "_id": "685519bb4f1add9d4c5c5cbd",
                    "user": {
                        "_id": "61a24fc72101184cfb29c965",
                        "avatarUrl": "/avatars/e32aa61016caef50de28c16b30196799.svg",
                        "isPro": false,
                        "fullname": "Christoph Schuhmann",
                        "user": "ChristophSchuhmann",
                        "type": "user"
                    },
                    "name": "Christoph Schuhmann",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-20T10:22:26.676Z",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cbe",
                    "name": "Robert Kaczmarczyk",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cbf",
                    "user": {
                        "_id": "64ac21f11cacea8d4b8f2b3f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ac21f11cacea8d4b8f2b3f/asQOf8wFZ4vmqIeyxfvUR.jpeg",
                        "isPro": false,
                        "fullname": "Gollam Rabby",
                        "user": "tourist800",
                        "type": "user"
                    },
                    "name": "Gollam Rabby",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:29:47.779Z",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cc0",
                    "user": {
                        "_id": "62e7dd4036a8e8a82700041c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
                        "isPro": false,
                        "fullname": "Felix Friedrich",
                        "user": "felfri",
                        "type": "user"
                    },
                    "name": "Felix Friedrich",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-20T08:57:36.090Z",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cc1",
                    "name": "Maurice Kraus",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cc2",
                    "name": "Kourosh Nadi",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cc3",
                    "name": "Huu Nguyen",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cc4",
                    "name": "Kristian Kersting",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cc5",
                    "user": {
                        "_id": "62cd8f74342b1d5dab8da3a6",
                        "avatarUrl": "/avatars/51c237653aadc98c73df207d9d054597.svg",
                        "isPro": false,
                        "fullname": "Sören Auer",
                        "user": "soeren1611",
                        "type": "user"
                    },
                    "name": "Sören Auer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:30:06.624Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62e7dd4036a8e8a82700041c/tvkMYrIKiGhbIVAuaxvlt.png"
            ],
            "publishedAt": "2025-06-11T15:06:59.000Z",
            "submittedOnDailyAt": "2025-06-20T06:53:47.262Z",
            "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion Detection",
            "submittedOnDailyBy": {
                "_id": "62e7dd4036a8e8a82700041c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
                "isPro": false,
                "fullname": "Felix Friedrich",
                "user": "felfri",
                "type": "user"
            },
            "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.",
            "upvotes": 5,
            "discussionId": "685519bb4f1add9d4c5c5cc6",
            "ai_summary": "EmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.",
            "ai_keywords": [
                "speech emotion recognition",
                "SER",
                "EmoNet-Voice",
                "EmoNet-Voice Big",
                "EmoNet-Voice Bench",
                "human expert annotations",
                "synthetic audio snippets",
                "psychology experts",
                "high-arousal emotions",
                "low-arousal states",
                "Empathic Insight Voice models"
            ]
        },
        "translation_title": "EmoNet-Voice: 음성 감정 인식을 위한 정밀하고 전문가 검증된 기준",
        "purpose": "AI 시스템의 감정 이해 능력을 평가하기 위한 강력한 기준을 마련하고, 음성 감정 인식 모델을 정밀하게 평가하기 위함",
        "method": [
            "EmoNet-Voice Big이라는 대규모 사전 훈련 데이터셋(11개의 목소리와 40개의 감정, 4개 언어로 4,500시간 이상의 음성을 포함)을 생성함(EmoNet-Voice Big, a large-scale pre-training dataset featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages).",
            "감정 강도 수준이 다양한 40개 감정 카테고리로 SER 모델을 평가하도록 새롭게 구성한 기준 데이터셋인 EmoNet-Voice Bench를 제공함(EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities).",
            "정신학 전문가의 검증을 통해 감정 강도 레이블을 부여받은 합성 음성 클립을 생성함(We conducted rigorous validation by psychology experts who assigned perceived intensity labels)."
        ],
        "conclusion": "EmoNet-Voice는 기존 데이터셋에서 부족했던 민감한 감정 상태도 포함하여, 음성 감정 인식의 새로운 기준을 설정함.",
        "keywords": [
            "Natural Language Processing",
            "Speech Emotion Recognition",
            "Mulitmodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.15154",
            "authors": [
                {
                    "_id": "685393f499bf39f9665c79db",
                    "user": {
                        "_id": "65fd1805883a1c3b4b1fde10",
                        "avatarUrl": "/avatars/ff475f757e190021f66e1f3c0fe5bd17.svg",
                        "isPro": false,
                        "fullname": "Anuradha Chopra",
                        "user": "annabeth97c",
                        "type": "user"
                    },
                    "name": "Anuradha Chopra",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:30:18.910Z",
                    "hidden": false
                },
                {
                    "_id": "685393f499bf39f9665c79dc",
                    "name": "Abhinaba Roy",
                    "hidden": false
                },
                {
                    "_id": "685393f499bf39f9665c79dd",
                    "user": {
                        "_id": "655431b2997379e9b0999d23",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
                        "isPro": false,
                        "fullname": "Dorien Herremans",
                        "user": "dorienh",
                        "type": "user"
                    },
                    "name": "Dorien Herremans",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:30:29.316Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
            ],
            "publishedAt": "2025-06-18T05:51:36.000Z",
            "submittedOnDailyAt": "2025-06-20T04:32:59.096Z",
            "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
            "submittedOnDailyBy": {
                "_id": "655431b2997379e9b0999d23",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
                "isPro": false,
                "fullname": "Dorien Herremans",
                "user": "dorienh",
                "type": "user"
            },
            "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.",
            "upvotes": 3,
            "discussionId": "685393f599bf39f9665c79de",
            "ai_summary": "SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.",
            "ai_keywords": [
                "multi-task music captioning",
                "SonicVerse",
                "caption generation",
                "key detection",
                "vocals detection",
                "projection-based architecture",
                "language tokens",
                "auxiliary heads",
                "time-informed descriptions",
                "large-language model",
                "MusicBench dataset",
                "MIRFLEX",
                "music feature extractor"
            ]
        },
        "translation_title": "SonicVerse: 음악 특징을 반영한 캡션 생성을 위한 다중 작업 학습",
        "purpose": "음악 AI 연구를 발전시키기 위해 음악의 특징을 정확히 반영한 캡션 생성을 개선",
        "method": [
            "음악 캡션 생성을 음악 특징 탐지 작업과 결합한 SonicVerse라는 다중 작업 모델을 소개함(This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more.)",
            "오디오 입력을 언어 토큰으로 변환하는 투사 기반 아키텍처를 개발함(The key contribution is a projection-based architecture that transforms audio input into language tokens.)",
            "MusicBench 데이터셋을 확장하고 MIRFLEX를 사용해 음악 특징을 주석 처리해 캡션과 음악 특징 데이터 쌍을 생성함(To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX.)"
        ],
        "conclusion": "제안된 방법은 캡션의 품질과 세부성을 향상시키며, 짧은 음악 조각뿐만 아니라 긴 음악 조각에 대한 시간 정보가 반영된 표현도 생성 가능.",
        "keywords": [
            "Natural Language Processing",
            "Multimodal Learning",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2506.14837",
            "authors": [
                {
                    "_id": "6854ea7a7bc8d012d4ca998d",
                    "user": {
                        "_id": "672a29efe53061b3dc76fd70",
                        "avatarUrl": "/avatars/20c7100884f4a69a9ec781315f68ff0b.svg",
                        "isPro": false,
                        "fullname": "xuchengzhi",
                        "user": "dazhiga",
                        "type": "user"
                    },
                    "name": "Chengzhi Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:30:45.306Z",
                    "hidden": false
                },
                {
                    "_id": "6854ea7a7bc8d012d4ca998e",
                    "name": "Yuyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6854ea7a7bc8d012d4ca998f",
                    "user": {
                        "_id": "64a16b1aeacb4b50ba1c889d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
                        "isPro": false,
                        "fullname": "Lai Wei",
                        "user": "WaltonFuture",
                        "type": "user"
                    },
                    "name": "Lai Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-20T08:57:40.628Z",
                    "hidden": false
                },
                {
                    "_id": "6854ea7a7bc8d012d4ca9990",
                    "user": {
                        "_id": "65a52766215aabac489e3468",
                        "avatarUrl": "/avatars/fe05e22cd7e12e961296426434e17c76.svg",
                        "isPro": false,
                        "fullname": "Lichao Sun",
                        "user": "sunlichao137",
                        "type": "user"
                    },
                    "name": "Lichao Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:30:53.972Z",
                    "hidden": false
                },
                {
                    "_id": "6854ea7a7bc8d012d4ca9991",
                    "user": {
                        "_id": "65e095da35ad8b2fe8c80e71",
                        "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
                        "isPro": false,
                        "fullname": "Weiran Huang",
                        "user": "weiranhuang",
                        "type": "user"
                    },
                    "name": "Weiran Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:31:00.023Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-15T14:10:16.000Z",
            "submittedOnDailyAt": "2025-06-20T03:28:58.032Z",
            "title": "Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction",
            "submittedOnDailyBy": {
                "_id": "64a16b1aeacb4b50ba1c889d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
                "isPro": false,
                "fullname": "Lai Wei",
                "user": "WaltonFuture",
                "type": "user"
            },
            "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.",
            "upvotes": 1,
            "discussionId": "6854ea7a7bc8d012d4ca9992",
            "ai_summary": "ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "visual understanding",
                "code translation",
                "structured instruction",
                "description instruction",
                "difference instruction",
                "language representations",
                "initial code generation",
                "iterative refinement",
                "Qwen2-VL",
                "GPT-4o"
            ]
        },
        "translation_title": "구조적 지침을 통한 차트-코드 생성을 위한 개선된 반복적 세련화",
        "purpose": "차트를 재현하는 실행 가능한 코드를 생성하기 위한 MLLM의 성능을 향상시키기 위한 방법 연구",
        "method": [
            "차트 생성을 위한 두 가지 작업인 시각적 이해와 코드 번역을 구분함(We distinguish two tasks: visual understanding and code translation.)",
            "시각적 이해를 위해 참조 차트의 시각적 요소를 캡처하는 설명 지침과 참조 차트와 생성된 차트 간의 차이를 나타내는 차이 지침을 설계함(We design two types of structured instructions: description and difference.)",
            "전체 차트 생성 파이프라인을 초기 코드 생성과 반복적 세련화의 두 단계로 분해하여 최종 출력을 점진적으로 향상시킴(We decompose the overall chart generation pipeline into two stages: initial code generation and iterative refinement.)"
        ],
        "conclusion": "우리의 방법은 Qwen2-VL과 GPT-4o 모델 모두에서 다른 방법보다 우수한 성능을 나타내며, 차트-코드 생성에서 효과적임.",
        "keywords": [
            "Multimodal Learning",
            "Natural Language Processing",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2506.15455",
            "authors": [
                {
                    "_id": "68546a187bc8d012d4ca991f",
                    "name": "Xinnuo Xu",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9920",
                    "name": "Rachel Lawrence",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9921",
                    "user": {
                        "_id": "669e070afbc9ba8a128a2807",
                        "avatarUrl": "/avatars/babae90dc758a865dfb76a710ac1da5f.svg",
                        "isPro": false,
                        "fullname": "KD",
                        "user": "Ksh000",
                        "type": "user"
                    },
                    "name": "Kshitij Dubey",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-20T12:19:19.484Z",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9922",
                    "name": "Atharva Pandey",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9923",
                    "name": "Risa Ueno",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9924",
                    "name": "Fabian Falck",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9925",
                    "name": "Aditya V. Nori",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9926",
                    "name": "Rahul Sharma",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9927",
                    "name": "Amit Sharma",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9928",
                    "name": "Javier Gonzalez",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/669e070afbc9ba8a128a2807/O6wK_gM9xelNdW9seuZXC.webp"
            ],
            "publishedAt": "2025-06-18T13:35:47.000Z",
            "submittedOnDailyAt": "2025-06-20T13:48:53.334Z",
            "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation",
            "submittedOnDailyBy": {
                "_id": "669e070afbc9ba8a128a2807",
                "avatarUrl": "/avatars/babae90dc758a865dfb76a710ac1da5f.svg",
                "isPro": false,
                "fullname": "KD",
                "user": "Ksh000",
                "type": "user"
            },
            "summary": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.",
            "upvotes": 0,
            "discussionId": "68546a187bc8d012d4ca9929",
            "ai_summary": "RE-IMAGINE evaluates the reasoning abilities of Large Language Models by generating variations of problems that cannot be solved by memorization, indicating reliance on statistical recall.",
            "ai_keywords": [
                "Large Language Models",
                "reasoning benchmarks",
                "ladder of causation",
                "RE-IMAGINE",
                "reasoning hierarchy",
                "symbolic representation",
                "problem variations",
                "reasoning domains",
                "memorization",
                "performance evaluation"
            ]
        },
        "translation_title": "RE-IMAGINE: 추론 평가를 위한 기호 벤치마크 생성",
        "purpose": "최근 대형 언어 모델(LLMs)의 추론 성능을 평가하기 위한 새로운 기법 개발",
        "method": [
            "RE-IMAGINE 프레임워크를 도입하여 LLM의 추론 능력을 계층적으로 특성화함(This paper introduces RE-IMAGINE, a framework to characterize a hierarchy of reasoning ability in LLMs.)",
            "문제를 기호적 표현으로 수정하여 기억에 의존하지 않는 문제 변형을 생성함(By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone.)",
            "네 가지 광범위한 벤치마크를 통해 여러 가족의 LLM 성능 평가를 수행함(We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs.)"
        ],
        "conclusion": "모델의 성능 저하는 LLM이 통계적 회상에 어느 정도 의존하고 있음을 나타내며, 추론 계층 내 기술을 목표로 하는 추가 연구의 가능성을 열어줌.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    }
]