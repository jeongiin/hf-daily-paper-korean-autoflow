[
    {
        "paper": {
            "id": "2510.05684",
            "authors": [
                {
                    "_id": "68ec8eb7cd07fb414898ca8a",
                    "name": "Suwhan Choi",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca8b",
                    "user": {
                        "_id": "646484cfb90150b2706df03b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
                        "isPro": false,
                        "fullname": "Jaeyoon Jung",
                        "user": "lastdefiance20",
                        "type": "user"
                    },
                    "name": "Jaeyoon Jung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:04:31.376Z",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca8c",
                    "name": "Haebin Seong",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca8d",
                    "user": {
                        "_id": "647046995b8225e31ae91359",
                        "avatarUrl": "/avatars/dd288377385efc7b8b24bc88bd9c27d6.svg",
                        "isPro": false,
                        "fullname": "Minchan Kim",
                        "user": "shovelingpig",
                        "type": "user"
                    },
                    "name": "Minchan Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:04:33.915Z",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca8e",
                    "name": "Minyeong Kim",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca8f",
                    "name": "Yongjun Cho",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca90",
                    "name": "Yoonshik Kim",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca91",
                    "name": "Yubeen Park",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca92",
                    "name": "Youngjae Yu",
                    "hidden": false
                },
                {
                    "_id": "68ec8eb7cd07fb414898ca93",
                    "user": {
                        "_id": "63198bf1615c77c25d63e9ab",
                        "avatarUrl": "/avatars/6d591f87366e9990fed3c221dafdfae0.svg",
                        "isPro": false,
                        "fullname": "Yunsung Lee",
                        "user": "Maangeek",
                        "type": "user"
                    },
                    "name": "Yunsung Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:04:28.653Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/646484cfb90150b2706df03b/c36wRRNT6d9wbZQ1iXQ9Y.mp4"
            ],
            "publishedAt": "2025-10-07T08:40:33.000Z",
            "submittedOnDailyAt": "2025-10-13T06:15:34.299Z",
            "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to\n  Embodied AI",
            "submittedOnDailyBy": {
                "_id": "646484cfb90150b2706df03b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
                "isPro": false,
                "fullname": "Jaeyoon Jung",
                "user": "lastdefiance20",
                "type": "user"
            },
            "summary": "Large language models leverage internet-scale text data, yet embodied AI\nremains constrained by the prohibitive costs of physical trajectory collection.\nDesktop environments -- particularly gaming -- offer a compelling alternative:\nthey provide rich sensorimotor interactions at scale while maintaining the\nstructured observation-action coupling essential for embodied learning. We\npresent D2E (Desktop to Embodied AI), a framework that demonstrates desktop\ninteractions can serve as an effective pretraining substrate for robotics\nembodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT\nfor Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a\ncomplete pipeline from scalable desktop data collection to verified transfer in\nembodied domains. Our framework comprises three components: (1) the OWA Toolkit\nthat unifies diverse desktop interactions into a standardized format with 152x\ncompression, (2) the Generalist-IDM that achieves strong zero-shot\ngeneralization across unseen games through timestamp-based event prediction,\nenabling internet-scale pseudo-labeling, and (3) VAPT that transfers\ndesktop-pretrained representations to physical manipulation and navigation.\nUsing 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of\npseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO\nmanipulation and 83.3% on CANVAS navigation benchmarks. This validates that\nsensorimotor primitives in digital interactions exhibit sufficient invariance\nto transfer meaningfully to physical embodied tasks, establishing desktop\npretraining as a practical paradigm for robotics. We will make all our work\npublic, including the OWA toolkit, datasets of human-collected and\npseudo-labeled, and VAPT-trained models available at\nhttps://worv-ai.github.io/d2e/",
            "upvotes": 91,
            "discussionId": "68ec8eb7cd07fb414898ca94",
            "projectPage": "https://worv-ai.github.io/d2e/",
            "githubRepo": "https://github.com/worv-ai/D2E",
            "ai_summary": "D2E framework uses desktop interactions to pretrain embodied AI, achieving high success rates in physical manipulation and navigation tasks.",
            "ai_keywords": [
                "embodied AI",
                "desktop environments",
                "sensorimotor interactions",
                "OWA Toolkit",
                "Generalist-IDM",
                "timestamp-based event prediction",
                "zero-shot generalization",
                "internet-scale pseudo-labeling",
                "VAPT",
                "LIBERO manipulation",
                "CANVAS navigation",
                "desktop pretraining"
            ],
            "githubStars": 14
        },
        "translation_title": "D2E: 데스크톱 데이터를 통한 비대면 AI로의 비전-행동 사전학습 확장",
        "purpose": "데스크톱 환경에서 수집한 데이터를 사용하여 로봇의 비대면 AI 임무를 위한 사전학습 방법을 제시하려는 목표",
        "method": [
            "OWA Toolkit을 이용해 다양한 데스크톱 상호작용을 통합하여 표준화된 형식으로 변환하고 152배 압축을 실현함(we present D2E, a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks.)",
            "Generalist-IDM을 활용해 시간 기반 이벤트 예측을 통해 보지 못한 게임에서도 강력한 제로샷 일반화를 달성함(Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction).",
            "VAPT를 통해 데스크톱에서 사전훈련된 표현을 실제 물리적 조작과 내비게이션으로 전이함(VAPT that transfers desktop-pretrained representations to physical manipulation and navigation.)"
        ],
        "conclusion": "이 연구는 디지털 상호작용의 감각운동 원형이 물리적 임무로 의미 있게 이전될 수 있음을 검증하여, 데스크톱 사전학습이 로봇 공학의 실용적인 학습 패러다임이 됨을 입증함.",
        "keywords": [
            "Robotics",
            "Large Language Models",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2510.08673",
            "authors": [
                {
                    "_id": "68ec5edfcd07fb414898c93e",
                    "user": {
                        "_id": "65bc98383b879593a5a2f5e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65bc98383b879593a5a2f5e5/p2ZtoTFN6tW-QkcPJf7YT.jpeg",
                        "isPro": true,
                        "fullname": "Kang Liao",
                        "user": "KangLiao",
                        "type": "user"
                    },
                    "name": "Kang Liao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:05:58.256Z",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c93f",
                    "name": "Size Wu",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c940",
                    "name": "Zhonghua Wu",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c941",
                    "name": "Linyi Jin",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c942",
                    "name": "Chao Wang",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c943",
                    "user": {
                        "_id": "63463bc4547c70e4b7d3009f",
                        "avatarUrl": "/avatars/6e5350fd998f0a7a4143d7504218164a.svg",
                        "isPro": false,
                        "fullname": "Yikai Wang",
                        "user": "yikaiwang",
                        "type": "user"
                    },
                    "name": "Yikai Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:06:01.008Z",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c944",
                    "name": "Fei Wang",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c945",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "68ec5edfcd07fb414898c946",
                    "name": "Chen Change Loy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-09T17:59:29.000Z",
            "submittedOnDailyAt": "2025-10-13T00:45:06.202Z",
            "title": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric\n  Understanding and Generation",
            "submittedOnDailyBy": {
                "_id": "65bc98383b879593a5a2f5e5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65bc98383b879593a5a2f5e5/p2ZtoTFN6tW-QkcPJf7YT.jpeg",
                "isPro": true,
                "fullname": "Kang Liao",
                "user": "KangLiao",
                "type": "user"
            },
            "summary": "Camera-centric understanding and generation are two cornerstones of spatial\nintelligence, yet they are typically studied in isolation. We present Puffin, a\nunified camera-centric multimodal model that extends spatial awareness along\nthe camera dimension. Puffin integrates language regression and diffusion-based\ngeneration to interpret and create scenes from arbitrary viewpoints. To bridge\nthe modality gap between cameras and vision-language, we introduce a novel\nparadigm that treats camera as language, enabling thinking with camera. This\nguides the model to align spatially grounded visual cues with photographic\nterminology while reasoning across geometric context. Puffin is trained on\nPuffin-4M, a large-scale dataset of 4 million vision-language-camera triplets.\nWe incorporate both global camera parameters and pixel-wise camera maps,\nyielding flexible and reliable spatial generation. Experiments demonstrate\nPuffin superior performance over specialized models for camera-centric\ngeneration and understanding. With instruction tuning, Puffin generalizes to\ndiverse cross-view tasks such as spatial imagination, world exploration, and\nphotography guidance. We will release the code, models, dataset pipeline, and\nbenchmark to advance multimodal spatial intelligence research.",
            "upvotes": 78,
            "discussionId": "68ec5ee0cd07fb414898c947",
            "projectPage": "https://kangliao929.github.io/projects/puffin/",
            "githubRepo": "https://github.com/KangLiao929/Puffin",
            "ai_summary": "Puffin, a unified multimodal model, integrates language regression and diffusion-based generation to enhance camera-centric spatial understanding and generation by treating camera parameters as language.",
            "ai_keywords": [
                "language regression",
                "diffusion-based generation",
                "camera-centric",
                "multimodal model",
                "spatial awareness",
                "vision-language",
                "camera as language",
                "geometric context",
                "Puffin-4M",
                "global camera parameters",
                "pixel-wise camera maps",
                "spatial generation",
                "instruction tuning",
                "spatial imagination",
                "world exploration",
                "photography guidance"
            ],
            "githubStars": 65
        },
        "translation_title": "Thinking with Camera: 카메라 중심 이해 및 생성을 위한 통합 멀티모달 모델",
        "purpose": "카메라 중심 이해와 생성의 통합을 통해 공간 지능을 향상시키고자 함",
        "method": [
            "Puffin이라는 통합 카메라 중심 멀티모달 모델을 제안함(We present Puffin, a unified camera-centric multimodal model that extends spatial awareness along the camera dimension.)",
            "언어 회귀와 확산 기반 생성을 통합하여 다양한 시점에서 장면을 해석하고 창조함(Puffin integrates language regression and diffusion-based generation to interpret and create scenes from arbitrary viewpoints.)",
            "카메라를 언어처럼 다루는 새로운 패러다임을 도입하여 모달리티 간의 갭을 해소함(This introduces a novel paradigm that treats camera as language, enabling thinking with camera.)"
        ],
        "conclusion": "Puffin은 카메라 중심 생성 및 이해에 대해 전문화된 모델보다 우수한 성능을 보이며, 다양한 교차 뷰 작업에 일반화됨.",
        "keywords": [
            "Multimodal Learning",
            "Vision-Language Models",
            "Image Generation"
        ]
    },
    {
        "paper": {
            "id": "2510.09558",
            "authors": [
                {
                    "_id": "68ec771ccd07fb414898ca1f",
                    "name": "Qiguang Chen",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca20",
                    "name": "Zheng Yan",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca21",
                    "name": "Mingda Yang",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca22",
                    "name": "Libo Qin",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca23",
                    "name": "Yixin Yuan",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca24",
                    "name": "Hanjing Li",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca25",
                    "name": "Jinhao Liu",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca26",
                    "name": "Yiyan Ji",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca27",
                    "name": "Dengyun Peng",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca28",
                    "name": "Jiannan Guan",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca29",
                    "name": "Mengkang Hu",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca2a",
                    "name": "Yantao Du",
                    "hidden": false
                },
                {
                    "_id": "68ec771ccd07fb414898ca2b",
                    "name": "Wanxiang Che",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T17:08:36.000Z",
            "submittedOnDailyAt": "2025-10-13T02:29:55.098Z",
            "title": "AutoPR: Let's Automate Your Academic Promotion!",
            "submittedOnDailyBy": {
                "_id": "636f526a6cd69d9a36ff2b53",
                "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
                "isPro": false,
                "fullname": "Qiguang Chen",
                "user": "LightChen2333",
                "type": "user"
            },
            "summary": "As the volume of peer-reviewed research surges, scholars increasingly rely on\nsocial platforms for discovery, while authors invest considerable effort in\npromoting their work to ensure visibility and citations. To streamline this\nprocess and reduce the reliance on human effort, we introduce Automatic\nPromotion (AutoPR), a novel task that transforms research papers into accurate,\nengaging, and timely public content. To enable rigorous evaluation, we release\nPRBench, a multimodal benchmark that links 512 peer-reviewed articles to\nhigh-quality promotional posts, assessing systems along three axes: Fidelity\n(accuracy and tone), Engagement (audience targeting and appeal), and Alignment\n(timing and channel optimization). We also introduce PRAgent, a multi-agent\nframework that automates AutoPR in three stages: content extraction with\nmultimodal preparation, collaborative synthesis for polished outputs, and\nplatform-specific adaptation to optimize norms, tone, and tagging for maximum\nreach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates\nsubstantial improvements, including a 604% increase in total watch time, a 438%\nrise in likes, and at least a 2.9x boost in overall engagement. Ablation\nstudies show that platform modeling and targeted promotion contribute the most\nto these gains. Our results position AutoPR as a tractable, measurable research\nproblem and provide a roadmap for scalable, impactful automated scholarly\ncommunication.",
            "upvotes": 35,
            "discussionId": "68ec771ccd07fb414898ca2c",
            "projectPage": "https://yzweak.github.io/autopr.github.io/",
            "githubRepo": "https://github.com/LightChen233/AutoPR",
            "ai_summary": "AutoPR, a multi-agent framework, automates the promotion of research papers by transforming them into engaging public content, significantly improving engagement metrics compared to direct LLM pipelines.",
            "ai_keywords": [
                "multimodal benchmark",
                "content extraction",
                "collaborative synthesis",
                "platform-specific adaptation",
                "LLM pipelines"
            ],
            "githubStars": 21
        },
        "translation_title": "AutoPR: 당신의 학술 승진을 자동화합시다!",
        "purpose": "학술 연구의 가시성 및 인용 수를 높이기 위해 연구 논문을 효과적으로 홍보하는 방법 개발",
        "method": [
            "논문을 정확하고 매력적인 공개 콘텐츠로 변환하는 Automatic Promotion (AutoPR)이라는 새로운 작업을 소개함(To streamline this process and reduce the reliance on human effort, we introduce Automatic Promotion (AutoPR), a novel task that transforms research papers into accurate, engaging, and timely public content.)",
            "PRBench라는 멀티모달 벤치마크를 출시하여 512개의 동료 검토된 논문과 고품질 홍보 게시물을 연결하고, 세 가지 축에 따라 시스템을 평가함(To enable rigorous evaluation, we release PRBench, a multimodal benchmark that links 512 peer-reviewed articles to high-quality promotional posts, assessing systems along three axes: Fidelity, Engagement, and Alignment.)",
            "PRAgent라는 다중 에이전트 프레임워크를 도입하여 콘텐츠 추출, 공동 합성 및 플랫폼별 적응을 통해 AutoPR을 자동화함(We also introduce PRAgent, a multi-agent framework that automates AutoPR in three stages: content extraction with multimodal preparation, collaborative synthesis for polished outputs, and platform-specific adaptation to optimize norms, tone, and tagging for maximum reach.)"
        ],
        "conclusion": "PRAgent는 PRBench에서 직접 LLM 파이프라인보다 총 웨치 타임을 604% 증가시키고, 좋아요를 438% 증가시켰으며, 전반적인 참여도를 최소 2.9배 증가시키는 성과를 보임.",
        "keywords": [
            "Natural Language Processing",
            "Multimodal Learning",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2510.04533",
            "authors": [
                {
                    "_id": "68e67c91975ac4c405ef2334",
                    "user": {
                        "_id": "680ed019a9918bb1cbc66248",
                        "avatarUrl": "/avatars/904d243f2fad99341f11795e93788993.svg",
                        "isPro": true,
                        "fullname": "Hyunmin Cho",
                        "user": "hyeoncho01",
                        "type": "user"
                    },
                    "name": "Hyunmin Cho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:51:20.523Z",
                    "hidden": false
                },
                {
                    "_id": "68e67c91975ac4c405ef2335",
                    "name": "Donghoon Ahn",
                    "hidden": false
                },
                {
                    "_id": "68e67c91975ac4c405ef2336",
                    "name": "Susung Hong",
                    "hidden": false
                },
                {
                    "_id": "68e67c91975ac4c405ef2337",
                    "name": "Jee Eun Kim",
                    "hidden": false
                },
                {
                    "_id": "68e67c91975ac4c405ef2338",
                    "name": "Seungryong Kim",
                    "hidden": false
                },
                {
                    "_id": "68e67c91975ac4c405ef2339",
                    "name": "Kyong Hwan Jin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/680ed019a9918bb1cbc66248/QFu-1RXfK2GP-IUrFzCwl.png"
            ],
            "publishedAt": "2025-10-06T06:53:29.000Z",
            "submittedOnDailyAt": "2025-10-13T00:05:19.216Z",
            "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion\n  Sampling",
            "submittedOnDailyBy": {
                "_id": "680ed019a9918bb1cbc66248",
                "avatarUrl": "/avatars/904d243f2fad99341f11795e93788993.svg",
                "isPro": true,
                "fullname": "Hyunmin Cho",
                "user": "hyeoncho01",
                "type": "user"
            },
            "summary": "Recent diffusion models achieve the state-of-the-art performance in image\ngeneration, but often suffer from semantic inconsistencies or hallucinations.\nWhile various inference-time guidance methods can enhance generation, they\noften operate indirectly by relying on external signals or architectural\nmodifications, which introduces additional computational overhead. In this\npaper, we propose Tangential Amplifying Guidance (TAG), a more efficient and\ndirect guidance method that operates solely on trajectory signals without\nmodifying the underlying diffusion model. TAG leverages an intermediate sample\nas a projection basis and amplifies the tangential components of the estimated\nscores with respect to this basis to correct the sampling trajectory. We\nformalize this guidance process by leveraging a first-order Taylor expansion,\nwhich demonstrates that amplifying the tangential component steers the state\ntoward higher-probability regions, thereby reducing inconsistencies and\nenhancing sample quality. TAG is a plug-and-play, architecture-agnostic module\nthat improves diffusion sampling fidelity with minimal computational addition,\noffering a new perspective on diffusion guidance.",
            "upvotes": 34,
            "discussionId": "68e67c91975ac4c405ef233a",
            "projectPage": "https://hyeon-cho.github.io/TAG/",
            "githubRepo": "https://github.com/hyeon-cho/Tangential-Amplifying-Guidance",
            "ai_summary": "Tangential Amplifying Guidance (TAG) improves diffusion model sample quality by directly amplifying tangential components of estimated scores without modifying the model architecture.",
            "ai_keywords": [
                "diffusion models",
                "image generation",
                "semantic inconsistencies",
                "hallucinations",
                "inference-time guidance",
                "trajectory signals",
                "intermediate sample",
                "projection basis",
                "tangential components",
                "first-order Taylor expansion",
                "sampling trajectory",
                "higher-probability regions",
                "plug-and-play",
                "architecture-agnostic"
            ],
            "githubStars": 5
        },
        "translation_title": "TAG: 환각 저항성 확산 샘플링을 위한 접선 증폭 안내",
        "purpose": "이미지 생성에서의 의미 불일치 및 환각 문제를 해결하기 위해 더 효율적인 안내 방법 개발",
        "method": [
            "기존의 구조 수정 없이 경로 신호만 활용하여 직접적인 안내 방법인 TAG를 제안함(we propose Tangential Amplifying Guidance (TAG), a more efficient and direct guidance method that operates solely on trajectory signals without modifying the underlying diffusion model.)",
            "중간 샘플을 투영 기반으로 활용하고 추정된 점수의 접선 성분을 증폭해 샘플링 경로를 수정함(TAG leverages an intermediate sample as a projection basis and amplifies the tangential components of the estimated scores with respect to this basis to correct the sampling trajectory.)",
            "첫 번째 차수 테일러 전개를 활용해 안내 과정을 형식화하고, 접선 성분을 증폭해 더 높은 확률 영역으로 유도함(We formalize this guidance process by leveraging a first-order Taylor expansion, which demonstrates that amplifying the tangential component steers the state toward higher-probability regions.)"
        ],
        "conclusion": "TAG는 최소한의 계산 오버헤드로 확산 샘플링 품질을 개선하며, 새로운 관점을 제시함.",
        "keywords": [
            "Image Generation",
            "Multimodal Learning",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2510.09201",
            "authors": [
                {
                    "_id": "68ec632fcd07fb414898c97b",
                    "user": {
                        "_id": "64cfa0b9749587dbe01d0079",
                        "avatarUrl": "/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg",
                        "isPro": false,
                        "fullname": "Yumin Choi",
                        "user": "YuminChoi",
                        "type": "user"
                    },
                    "name": "Yumin Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-13T10:05:53.909Z",
                    "hidden": false
                },
                {
                    "_id": "68ec632fcd07fb414898c97c",
                    "name": "Dongki Kim",
                    "hidden": false
                },
                {
                    "_id": "68ec632fcd07fb414898c97d",
                    "name": "Jinheon Baek",
                    "hidden": false
                },
                {
                    "_id": "68ec632fcd07fb414898c97e",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-10T09:41:25.000Z",
            "submittedOnDailyAt": "2025-10-13T00:56:45.053Z",
            "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for\n  MLLMs",
            "submittedOnDailyBy": {
                "_id": "64cfa0b9749587dbe01d0079",
                "avatarUrl": "/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg",
                "isPro": false,
                "fullname": "Yumin Choi",
                "user": "YuminChoi",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have shown remarkable success, and their\nmultimodal expansions (MLLMs) further unlock capabilities spanning images,\nvideos, and other modalities beyond text. However, despite this shift, prompt\noptimization approaches, designed to reduce the burden of manual prompt\ncrafting while maximizing performance, remain confined to text, ultimately\nlimiting the full potential of MLLMs. Motivated by this gap, we introduce the\nnew problem of multimodal prompt optimization, which expands the prior\ndefinition of prompt optimization to the multimodal space defined by the pairs\nof textual and non-textual prompts. To tackle this problem, we then propose the\nMultimodal Prompt Optimizer (MPO), a unified framework that not only performs\nthe joint optimization of multimodal prompts through alignment-preserving\nupdates but also guides the selection process of candidate prompts by\nleveraging earlier evaluations as priors in a Bayesian-based selection\nstrategy. Through extensive experiments across diverse modalities that go\nbeyond text, such as images, videos, and even molecules, we demonstrate that\nMPO outperforms leading text-only optimization methods, establishing multimodal\nprompt optimization as a crucial step to realizing the potential of MLLMs.",
            "upvotes": 33,
            "discussionId": "68ec632fcd07fb414898c97f",
            "githubRepo": "https://github.com/Dozi01/MPO",
            "ai_summary": "Multimodal Prompt Optimizer (MPO) extends prompt optimization to handle multiple data types, improving performance over text-only methods in various applications.",
            "ai_keywords": [
                "Large Language Models",
                "multimodal expansions",
                "prompt optimization",
                "Multimodal Prompt Optimizer",
                "alignment-preserving updates",
                "Bayesian-based selection strategy"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "translation_title": "다중 모달 프롬프트 최적화: MLLM을 위한 다중 모달리티 활용하기",
        "purpose": "MLLM의 잠재력을 극대화하기 위해 다중 모달 데이터에서 프롬프트 최적화 문제를 제안하고 해결하기",
        "method": [
            "프롬프트 최적화의 기존 정의를 다중 모달 공간으로 확장함(we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts.)",
            "다중 모달 프롬프트의 공동 최적화를 수행하는 Multimodal Prompt Optimizer(MPO)라는 통합 프레임워크를 제안함(we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates).",
            "Bayesian 기반 선택 전략을 이용하여 후보 프롬프트의 선택 과정을 안내함(and also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-based selection strategy.)"
        ],
        "conclusion": "MPO는 텍스트 전용 최적화 방법보다 우수한 성과를 보이며, MLLM의 잠재력을 실현하는 데 중요한 단계로 입증됨.",
        "keywords": [
            "Multimodal Learning",
            "Large Language Models",
            "Natural Language Processing"
        ]
    }
]