[
    {
        "paper": {
            "id": "2506.08007",
            "authors": [
                {
                    "_id": "684794553ec10bdd8ab4de1a",
                    "name": "Qingxiu Dong",
                    "hidden": false
                },
                {
                    "_id": "684794553ec10bdd8ab4de1b",
                    "user": {
                        "_id": "5df85abada6d0311fd3d5408",
                        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
                        "isPro": false,
                        "fullname": "Li Dong",
                        "user": "unilm",
                        "type": "user"
                    },
                    "name": "Li Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T08:44:23.723Z",
                    "hidden": false
                },
                {
                    "_id": "684794553ec10bdd8ab4de1c",
                    "user": {
                        "_id": "667119d6578448466d9531a6",
                        "avatarUrl": "/avatars/72c31909a5584b1306b6404b94a22b2a.svg",
                        "isPro": false,
                        "fullname": "Yao Tang",
                        "user": "YaoTang23",
                        "type": "user"
                    },
                    "name": "Yao Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T08:44:20.414Z",
                    "hidden": false
                },
                {
                    "_id": "684794553ec10bdd8ab4de1d",
                    "name": "Tianzhu Ye",
                    "hidden": false
                },
                {
                    "_id": "684794553ec10bdd8ab4de1e",
                    "name": "Yutao Sun",
                    "hidden": false
                },
                {
                    "_id": "684794553ec10bdd8ab4de1f",
                    "name": "Zhifang Sui",
                    "hidden": false
                },
                {
                    "_id": "684794553ec10bdd8ab4de20",
                    "user": {
                        "_id": "67ecd6178647cfa1775f75ed",
                        "avatarUrl": "/avatars/98882cc58dc0a5de94df765d523d92c9.svg",
                        "isPro": false,
                        "fullname": "FW",
                        "user": "frontierai",
                        "type": "user"
                    },
                    "name": "Furu Wei",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-10T02:11:34.050Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/OeDc7c4QFJOxdkJWvdbWB.png"
            ],
            "publishedAt": "2025-06-09T17:59:53.000Z",
            "submittedOnDailyAt": "2025-06-10T00:43:01.816Z",
            "title": "Reinforcement Pre-Training",
            "submittedOnDailyBy": {
                "_id": "5df85abada6d0311fd3d5408",
                "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
                "isPro": false,
                "fullname": "Li Dong",
                "user": "unilm",
                "type": "user"
            },
            "summary": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling\nparadigm for large language models and reinforcement learning (RL).\nSpecifically, we reframe next-token prediction as a reasoning task trained\nusing RL, where it receives verifiable rewards for correctly predicting the\nnext token for a given context. RPT offers a scalable method to leverage vast\namounts of text data for general-purpose RL, rather than relying on\ndomain-specific annotated answers. By incentivizing the capability of\nnext-token reasoning, RPT significantly improves the language modeling accuracy\nof predicting the next tokens. Moreover, RPT provides a strong pre-trained\nfoundation for further reinforcement fine-tuning. The scaling curves show that\nincreased training compute consistently improves the next-token prediction\naccuracy. The results position RPT as an effective and promising scaling\nparadigm to advance language model pre-training.",
            "upvotes": 130,
            "discussionId": "684794553ec10bdd8ab4de21",
            "ai_summary": "Reinforcement Pre-Training (RPT) improves language model accuracy through reinforcement learning and offers a scalable method for leveraging text data for general-purpose RL.",
            "ai_keywords": [
                "Reinforcement Pre-Training (RPT)",
                "next-token prediction",
                "reasoning task",
                "reinforcement learning (RL)",
                "verifiable rewards",
                "language modeling accuracy",
                "reinforcement fine-tuning",
                "scaling curves"
            ]
        },
        "translation_title": "강화 사전 훈련 (Reinforcement Pre-Training)",
        "purpose": "대규모 언어 모델과 강화 학습을 위한 새로운 확장 패러다임을 제안하고 언어 모델의 예측 정확도를 개선하기 위함",
        "method": [
            "다음 토큰 예측을 강화 학습을 통해 추론 작업으로 재구성함(we reframe next-token prediction as a reasoning task trained using RL)",
            "이전 교육 데이터의 방대한 양을 활용할 수 있는 방법을 제공함(RPT offers a scalable method to leverage vast amounts of text data for general-purpose RL)",
            "RPT를 통해 다음 토큰의 추론 능력을 강화하며 예측 정확도가 크게 향상됨(By incentivizing the capability of next-token reasoning, RPT significantly improves the language modeling accuracy)"
        ],
        "conclusion": "RPT는 언어 모델의 사전 훈련을 발전시키는 효과적이고 유망한 확장 패러다임으로 자리잡음.",
        "keywords": [
            "Large Language Models",
            "Reinforcement Learning",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2506.07044",
            "authors": [
                {
                    "_id": "684795093ec10bdd8ab4de43",
                    "name": "LASA Team",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de44",
                    "user": {
                        "_id": "64118689756b9e455c7eac62",
                        "avatarUrl": "/avatars/cdb3da22593facf545a0bafbf548b07e.svg",
                        "isPro": false,
                        "fullname": "Xu Weiwen",
                        "user": "xww033",
                        "type": "user"
                    },
                    "name": "Weiwen Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T08:44:07.459Z",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de45",
                    "user": {
                        "_id": "604f67ef0fe8ff3ec13d71ef",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
                        "isPro": false,
                        "fullname": "Hou Pong (Ken) Chan",
                        "user": "kenchan0226",
                        "type": "user"
                    },
                    "name": "Hou Pong Chan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T08:44:05.163Z",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de46",
                    "name": "Long Li",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de47",
                    "name": "Mahani Aljunied",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de48",
                    "name": "Ruifeng Yuan",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de49",
                    "user": {
                        "_id": "61e09ec13a1781f66b4e9ae2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Jianyu Wang",
                        "user": "Jianyu",
                        "type": "user"
                    },
                    "name": "Jianyu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T08:44:03.340Z",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de4a",
                    "user": {
                        "_id": "63108cc834c7d77420b0fd68",
                        "avatarUrl": "/avatars/2721e573a417a8ec0b81ee048c4b42ba.svg",
                        "isPro": false,
                        "fullname": "chenghao xiao",
                        "user": "gowitheflow",
                        "type": "user"
                    },
                    "name": "Chenghao Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T10:59:59.163Z",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de4b",
                    "name": "Guizhen Chen",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de4c",
                    "name": "Chaoqun Liu",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de4d",
                    "name": "Zhaodonghui Li",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de4e",
                    "name": "Yu Sun",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de4f",
                    "name": "Junao Shen",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de50",
                    "name": "Chaojun Wang",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de51",
                    "name": "Jie Tan",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de52",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de53",
                    "name": "Tingyang Xu",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de54",
                    "user": {
                        "_id": "64b7cd74ff6d81ae297feded",
                        "avatarUrl": "/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg",
                        "isPro": false,
                        "fullname": "ZHANG HAO",
                        "user": "26hzhang",
                        "type": "user"
                    },
                    "name": "Hao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T11:20:59.561Z",
                    "hidden": false
                },
                {
                    "_id": "684795093ec10bdd8ab4de55",
                    "user": {
                        "_id": "642eecbf9b2484d7d8526781",
                        "avatarUrl": "/avatars/773606f4a37d48861ec4f0f2df8a956f.svg",
                        "isPro": false,
                        "fullname": "Yu Rong",
                        "user": "Swrooy",
                        "type": "user"
                    },
                    "name": "Yu Rong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T08:44:01.224Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/604f67ef0fe8ff3ec13d71ef/R3ajyza5JHjd8tOwwV2ht.png"
            ],
            "publishedAt": "2025-06-08T08:47:30.000Z",
            "submittedOnDailyAt": "2025-06-10T00:48:48.080Z",
            "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning",
            "submittedOnDailyBy": {
                "_id": "604f67ef0fe8ff3ec13d71ef",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
                "isPro": false,
                "fullname": "Hou Pong (Ken) Chan",
                "user": "kenchan0226",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...",
            "upvotes": 60,
            "discussionId": "684795093ec10bdd8ab4de56",
            "ai_summary": "A medical-specialized multimodal large language model, Lingshu, is introduced with enhanced data curation and reinforcement learning to address limitations in medical applications.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "medical knowledge",
                "hallucinations",
                "data curation",
                "medical texts",
                "general-domain data",
                "accurate medical captions",
                "visual question answering",
                "VQA",
                "reasoning capabilities",
                "multi-stage training",
                "medical expertise",
                "reinforcement learning",
                "verifiable rewards paradigm",
                "MedEvalKit",
                "multimodal QA",
                "text-based QA",
                "medical report generation"
            ]
        },
        "translation_title": "Lingshu: 통합 다중모달 의료 이해 및 추리를 위한 일반ist 기반 모델",
        "purpose": "의료 응용에서 다중모달 모델의 한계를 극복하고 전문가 수준의 의료 추리 능력을 향상시키기 위한 연구",
        "method": [
            "포괄적인 데이터 관리 절차를 제안하여 의료 지식 데이터를 효과적으로 수집함(we first propose a comprehensive data curation procedure that efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data.)",
            "의료 관련 캡션, 시각적 질문 응답(VQA), 추론 샘플을 합성하여 방대한 의료 지식을 포함한 다중모달 데이터셋을 구축함(as a result, we build a multimodal dataset enriched with extensive medical knowledge.)",
            "Lingshu라는 의료 전문 MLLM을 도입하고 다단계 훈련을 통해 의료 전문 지식을 내장함(Building on the curated data, we introduce our medical-specialized MLLM: Lingshu, which undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities.)"
        ],
        "conclusion": "Lingshu는 기존의 다중모달 모델보다 의료 과제에서 일관되게 성능이 뛰어난 결과를 보였으며, 의료 추리에 관한 능력도 향상되었다.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Medical Imaging"
        ]
    },
    {
        "paper": {
            "id": "2506.07900",
            "authors": [
                {
                    "_id": "6847924d3ec10bdd8ab4ddb9",
                    "name": "MiniCPM Team",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddba",
                    "user": {
                        "_id": "608f6d72283d0a8d7be9d1f9",
                        "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
                        "isPro": false,
                        "fullname": "Chaojun XIAO",
                        "user": "xcjthu",
                        "type": "user"
                    },
                    "name": "Chaojun Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T11:00:03.612Z",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddbb",
                    "name": "Yuxuan Li",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddbc",
                    "name": "Xu Han",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddbd",
                    "name": "Yuzhuo Bai",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddbe",
                    "name": "Jie Cai",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddbf",
                    "name": "Haotian Chen",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddc0",
                    "name": "Wentong Chen",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddc1",
                    "name": "Xin Cong",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddc2",
                    "name": "Ganqu Cui",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddc3",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddc4",
                    "name": "Shengdan Fan",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddc5",
                    "name": "Yewei Fang",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddc6",
                    "name": "Zixuan Fu",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddc7",
                    "name": "Wenyu Guan",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddc8",
                    "name": "Yitong Guan",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddc9",
                    "user": {
                        "_id": "66add9c413ac672510f6cdba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66add9c413ac672510f6cdba/ubTWt8XN7aDdSihYAORah.png",
                        "isPro": false,
                        "fullname": "guojunshao",
                        "user": "guojunshaoyao",
                        "type": "user"
                    },
                    "name": "Junshao Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T11:00:01.410Z",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddca",
                    "name": "Yufeng Han",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddcb",
                    "name": "Bingxiang He",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddcc",
                    "name": "Yuxiang Huang",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddcd",
                    "name": "Cunliang Kong",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddce",
                    "name": "Qiuzuo Li",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddcf",
                    "name": "Siyuan Li",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddd0",
                    "name": "Wenhao Li",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddd1",
                    "name": "Yanghao Li",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddd2",
                    "name": "Yishan Li",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddd3",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddd4",
                    "name": "Dan Liu",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddd5",
                    "name": "Biyuan Lin",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddd6",
                    "name": "Yankai Lin",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddd7",
                    "name": "Xiang Long",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddd8",
                    "name": "Quanyu Lu",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddd9",
                    "name": "Yaxi Lu",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddda",
                    "name": "Peiyan Luo",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dddb",
                    "name": "Hongya Lyu",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dddc",
                    "name": "Litu Ou",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dddd",
                    "name": "Yinxu Pan",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddde",
                    "name": "Zekai Qu",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dddf",
                    "name": "Qundong Shi",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dde0",
                    "name": "Zijun Song",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dde1",
                    "name": "Jiayuan Su",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dde2",
                    "name": "Zhou Su",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dde3",
                    "name": "Ao Sun",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dde4",
                    "name": "Xianghui Sun",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dde5",
                    "name": "Peijun Tang",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dde6",
                    "name": "Fangzheng Wang",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dde7",
                    "name": "Feng Wang",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dde8",
                    "name": "Shuo Wang",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dde9",
                    "user": {
                        "_id": "63be286fb3b8c44f8cecc16f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63be286fb3b8c44f8cecc16f/1CIkfEKoTnBYdYDSuQ8AT.jpeg",
                        "isPro": false,
                        "fullname": "Yudong Wang",
                        "user": "BigDong",
                        "type": "user"
                    },
                    "name": "Yudong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T08:45:33.890Z",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddea",
                    "name": "Yesai Wu",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddeb",
                    "name": "Zhenyu Xiao",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddec",
                    "name": "Jie Xie",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4dded",
                    "name": "Zihao Xie",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddee",
                    "name": "Yukun Yan",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddef",
                    "name": "Jiarui Yuan",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddf0",
                    "name": "Kaihuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddf1",
                    "name": "Lei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddf2",
                    "name": "Linyue Zhang",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddf3",
                    "name": "Xueren Zhang",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddf4",
                    "name": "Yudi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddf5",
                    "name": "Hengyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddf6",
                    "name": "Weilin Zhao",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddf7",
                    "name": "Weilun Zhao",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddf8",
                    "name": "Yuanqian Zhao",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddf9",
                    "name": "Zhi Zheng",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddfa",
                    "name": "Ge Zhou",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddfb",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddfc",
                    "name": "Wei Zhou",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddfd",
                    "name": "Zihan Zhou",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddfe",
                    "name": "Zixuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4ddff",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4de00",
                    "name": "Guoyang Zeng",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4de01",
                    "name": "Chao Jia",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4de02",
                    "name": "Dahai Li",
                    "hidden": false
                },
                {
                    "_id": "6847924d3ec10bdd8ab4de03",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/608f6d72283d0a8d7be9d1f9/NF1aHsqQbJ_Dl18__cn2H.qt"
            ],
            "publishedAt": "2025-06-09T16:16:50.000Z",
            "submittedOnDailyAt": "2025-06-10T00:50:56.021Z",
            "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
            "submittedOnDailyBy": {
                "_id": "608f6d72283d0a8d7be9d1f9",
                "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
                "isPro": false,
                "fullname": "Chaojun XIAO",
                "user": "xcjthu",
                "type": "user"
            },
            "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\noutperforms open-source models of similar size across multiple benchmarks,\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\ndemonstrates significant speed improvements over Qwen3-8B when processing long\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\napplications, including trustworthy survey generation and tool use with model\ncontext protocol, clearly showcasing its broad usability.",
            "upvotes": 48,
            "discussionId": "6847924e3ec10bdd8ab4de04",
            "projectPage": "https://huggingface.co/collections/openbmb/minicpm4-6841ab29d180257e940baa9b",
            "githubRepo": "https://github.com/openbmb/minicpm",
            "ai_summary": "MiniCPM4, a highly efficient large language model for end-side devices, achieves superior performance using innovations in sparse attention, pre-training datasets, training algorithms, and inference systems.",
            "ai_keywords": [
                "InfLLM v2",
                "sparse attention mechanism",
                "UltraClean",
                "UltraChat v2",
                "prefilling",
                "decoding",
                "long-context processing",
                "ModelTunnel v2",
                "chunk-wise rollout",
                "data-efficient tenary LLM",
                "BitCPM",
                "CPM.cu",
                "model quantization",
                "speculative sampling"
            ]
        },
        "translation_title": "MiniCPM4: 최종 장치에서의 초효율 LLM",
        "purpose": "최종 장치에서 사용할 수 있는 고효율 대규모 언어 모델(LLM)을 개발하기 위한 연구",
        "method": [
            "모델 아키텍처로 InfLLM v2를 제안하여 긴 문맥 처리에 대한 대부분의 단계를 가속화함(We propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing.)",
            "효율적이고 정확한 데이터 필터링 및 생성을 위한 UltraClean 전략을 제안하고, 감독 학습을 위한 데이터세트 UltraChat v2를 사용함(We propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset.)",
            "모델 훈련을 위한 효율적인 전략 검색을 위한 ModelTunnel v2를 제안하고 기존 후속 훈련 방법을 개선함(We propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM.)"
        ],
        "conclusion": "MiniCPM4는 다양한 벤치마크에서 비슷한 크기의 오픈 소스 모델보다 뛰어난 성능을 보이며, 신뢰할 수 있는 설문 조사 생성 및 도구 사용 등 다양한 응용 프로그램에 성공적으로 활용됨.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.06444",
            "authors": [
                {
                    "_id": "68479c1e3ec10bdd8ab4de9d",
                    "name": "Ruizhong Qiu",
                    "hidden": false
                },
                {
                    "_id": "68479c1e3ec10bdd8ab4de9e",
                    "name": "Gaotang Li",
                    "hidden": false
                },
                {
                    "_id": "68479c1e3ec10bdd8ab4de9f",
                    "name": "Tianxin Wei",
                    "hidden": false
                },
                {
                    "_id": "68479c1e3ec10bdd8ab4dea0",
                    "name": "Jingrui He",
                    "hidden": false
                },
                {
                    "_id": "68479c1e3ec10bdd8ab4dea1",
                    "name": "Hanghang Tong",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65370d95019de94263ad34a7/Uhd5i-LpHkPBjdpr4KJLi.jpeg"
            ],
            "publishedAt": "2025-06-06T18:05:45.000Z",
            "submittedOnDailyAt": "2025-06-10T01:29:36.727Z",
            "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance",
            "submittedOnDailyBy": {
                "_id": "65370d95019de94263ad34a7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65370d95019de94263ad34a7/fH9SoJfX7yifXup0DvXkm.jpeg",
                "isPro": false,
                "fullname": "Ruizhong Qiu",
                "user": "q-rz",
                "type": "user"
            },
            "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
            "upvotes": 44,
            "discussionId": "68479c1e3ec10bdd8ab4dea2",
            "projectPage": "https://q-rz.github.io/p/saffron",
            "githubRepo": "https://github.com/q-rz/saffron",
            "ai_summary": "SAFFRON, a novel inference scaling paradigm, enhances LLM safety by reducing reward model evaluations through a multifurcation reward model and other optimizations.",
            "ai_keywords": [
                "LLMs",
                "inference scaling",
                "safety assurance",
                "jailbreak attacks",
                "Best-of-N Sampling",
                "process reward model",
                "exploration--efficiency dilemma",
                "multifurcation reward model",
                "partial supervision training",
                "conservative exploration constraint",
                "Trie-based key--value caching",
                "Safety4M dataset"
            ]
        },
        "translation_title": "Saffron-1: LLM 안전 보장을 위한 추론 스케일링 패러다임 연구",
        "purpose": "Emerging threats에 대한 LLM의 안전성을 보장할 수 있는 추론 스케일링 연구",
        "method": [
            "기존의 추론 스케일링 기법은 reasoning 작업에서 성공적이지만, 안전 맥락에서는 성능이 저조함을 확인함(We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts.)",
            "새롭게 정의된 exploration--efficiency dilemma 문제를 해결하기 위해 안전 보장을 위한 SAFFRON 패러다임을 제안함(We propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance.)",
            "다양한 보상 모델 평가를 줄이기 위해 multifurcation reward model(MRM)을 도입함(Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations.)"
        ],
        "conclusion": "SAFFRON의 유효성을 검증하는 실험을 통해, 우리는 LLM 안전성 향상을 위한 기초 자료로 Saffron-1과 Safety4M 데이터를 공개함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2506.07977",
            "authors": [
                {
                    "_id": "684792f03ec10bdd8ab4de06",
                    "name": "Jingjing Chang",
                    "hidden": false
                },
                {
                    "_id": "684792f03ec10bdd8ab4de07",
                    "user": {
                        "_id": "647469b9a51711a3b58bda2b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647469b9a51711a3b58bda2b/yeDf8Sa8IDEQyney1dGC9.jpeg",
                        "isPro": false,
                        "fullname": "Yixiao Fang",
                        "user": "fangyixiao",
                        "type": "user"
                    },
                    "name": "Yixiao Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T08:44:54.679Z",
                    "hidden": false
                },
                {
                    "_id": "684792f03ec10bdd8ab4de08",
                    "name": "Peng Xing",
                    "hidden": false
                },
                {
                    "_id": "684792f03ec10bdd8ab4de09",
                    "name": "Shuhan Wu",
                    "hidden": false
                },
                {
                    "_id": "684792f03ec10bdd8ab4de0a",
                    "user": {
                        "_id": "64b914c8ace99c0723ad83a9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
                        "isPro": false,
                        "fullname": "Wei Cheng",
                        "user": "wchengad",
                        "type": "user"
                    },
                    "name": "Wei Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T08:45:23.322Z",
                    "hidden": false
                },
                {
                    "_id": "684792f03ec10bdd8ab4de0b",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "684792f03ec10bdd8ab4de0c",
                    "name": "Xianfang Zeng",
                    "hidden": false
                },
                {
                    "_id": "684792f03ec10bdd8ab4de0d",
                    "name": "Gang Yu",
                    "hidden": false
                },
                {
                    "_id": "684792f03ec10bdd8ab4de0e",
                    "name": "Hai-Bao Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/Qq0Ue6mPEkoDJkIMGjRJ0.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ywhobM8HPQHIRy4CRBkLI.jpeg"
            ],
            "publishedAt": "2025-06-09T17:50:21.000Z",
            "submittedOnDailyAt": "2025-06-10T00:52:27.518Z",
            "title": "OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation",
            "submittedOnDailyBy": {
                "_id": "64b914c8ace99c0723ad83a9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
                "isPro": false,
                "fullname": "Wei Cheng",
                "user": "wchengad",
                "type": "user"
            },
            "summary": "Text-to-image (T2I) models have garnered significant attention for generating\nhigh-quality images aligned with text prompts. However, rapid T2I model\nadvancements reveal limitations in early benchmarks, lacking comprehensive\nevaluations, for example, the evaluation on reasoning, text rendering and\nstyle. Notably, recent state-of-the-art models, with their rich knowledge\nmodeling capabilities, show promising results on the image generation problems\nrequiring strong reasoning ability, yet existing evaluation systems have not\nadequately addressed this frontier. To systematically address these gaps, we\nintroduce OneIG-Bench, a meticulously designed comprehensive benchmark\nframework for fine-grained evaluation of T2I models across multiple dimensions,\nincluding prompt-image alignment, text rendering precision, reasoning-generated\ncontent, stylization, and diversity. By structuring the evaluation, this\nbenchmark enables in-depth analysis of model performance, helping researchers\nand practitioners pinpoint strengths and bottlenecks in the full pipeline of\nimage generation. Specifically, OneIG-Bench enables flexible evaluation by\nallowing users to focus on a particular evaluation subset. Instead of\ngenerating images for the entire set of prompts, users can generate images only\nfor the prompts associated with the selected dimension and complete the\ncorresponding evaluation accordingly. Our codebase and dataset are now publicly\navailable to facilitate reproducible evaluation studies and cross-model\ncomparisons within the T2I research community.",
            "upvotes": 36,
            "discussionId": "684792f03ec10bdd8ab4de0f",
            "projectPage": "https://oneig-bench.github.io/",
            "githubRepo": "https://github.com/OneIG-Bench/OneIG-Benchmark",
            "ai_summary": "OneIG-Bench is a comprehensive benchmark framework for evaluating text-to-image models across multiple dimensions including reasoning, text rendering, and diversity.",
            "ai_keywords": [
                "text-to-image (T2I) models",
                "prompt-image alignment",
                "text rendering precision",
                "reasoning-generated content",
                "stylization",
                "diversity"
            ]
        },
        "translation_title": "OneIG-Bench: 이미지 생성을 위한 다각적 세밀 평가",
        "purpose": "T2I 모델의 평가 체계를 체계적으로 개선하고 강화하여 고도의 이유 생성 및 스타일 평가를 포함하는 포괄적이고 세밀한 평가 체계를 구축하는 것.",
        "method": [
            "OneIG-Bench라는 세밀하게 설계된 평가 프레임워크를 도입하여 T2I 모델을 여러 차원에서 평가함(we introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions.)",
            "사용자들이 특정 평가 하위 집합에 집중할 수 있도록 유연한 평가 체계를 제공함(OneIG-Bench enables flexible evaluation by allowing users to focus on a particular evaluation subset.)",
            "코드베이스와 데이터셋을 공개하여 재현 가능한 평가 연구와 모델 간 비교를 촉진함(Our codebase and dataset are now publicly available to facilitate reproducible evaluation studies and cross-model comparisons within the T2I research community.)"
        ],
        "conclusion": "OneIG-Bench를 통해 T2I 모델의 성능 분석이 심층적으로 가능해지며, 연구자들이 이미지 생성 파이프라인의 강점과 병목 문제를 파악할 수 있도록 지원함.",
        "keywords": [
            "Image Generation",
            "Computer Vision",
            "Natural Language Processing"
        ]
    }
]