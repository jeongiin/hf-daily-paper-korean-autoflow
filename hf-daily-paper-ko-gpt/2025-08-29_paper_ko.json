[
    {
        "paper": {
            "id": "2508.20751",
            "authors": [
                {
                    "_id": "68b10e71b19c54000148491a",
                    "user": {
                        "_id": "654c6845bac6e6e49895a5b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QwUOUujUAyiV4tAhB_moO.png",
                        "isPro": false,
                        "fullname": "SII-Yibin Wang",
                        "user": "CodeGoat24",
                        "type": "user"
                    },
                    "name": "Yibin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-29T08:02:30.189Z",
                    "hidden": true
                },
                {
                    "_id": "68b10e71b19c54000148491b",
                    "name": "Zhimin Li",
                    "hidden": false
                },
                {
                    "_id": "68b10e71b19c54000148491c",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-29T08:02:32.217Z",
                    "hidden": false
                },
                {
                    "_id": "68b10e71b19c54000148491d",
                    "name": "Yujie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68b10e71b19c54000148491e",
                    "name": "Jiazi Bu",
                    "hidden": false
                },
                {
                    "_id": "68b10e71b19c54000148491f",
                    "name": "Chunyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68b10e71b19c540001484920",
                    "name": "Qinglin Lu",
                    "hidden": false
                },
                {
                    "_id": "68b10e71b19c540001484921",
                    "name": "Cheng Jin",
                    "hidden": false
                },
                {
                    "_id": "68b10e71b19c540001484922",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-28T13:11:24.000Z",
            "submittedOnDailyAt": "2025-08-29T00:53:42.823Z",
            "title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable\n  Text-to-Image Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "654c6845bac6e6e49895a5b5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QwUOUujUAyiV4tAhB_moO.png",
                "isPro": false,
                "fullname": "SII-Yibin Wang",
                "user": "CodeGoat24",
                "type": "user"
            },
            "summary": "Recent advancements highlight the importance of GRPO-based reinforcement\nlearning methods and benchmarking in enhancing text-to-image (T2I) generation.\nHowever, current methods using pointwise reward models (RM) for scoring\ngenerated images are susceptible to reward hacking. We reveal that this happens\nwhen minimal score differences between images are amplified after\nnormalization, creating illusory advantages that drive the model to\nover-optimize for trivial gains, ultimately destabilizing the image generation\nprocess. To address this, we propose Pref-GRPO, a pairwise preference\nreward-based GRPO method that shifts the optimization objective from score\nmaximization to preference fitting, ensuring more stable training. In\nPref-GRPO, images are pairwise compared within each group using preference RM,\nand the win rate is used as the reward signal. Extensive experiments\ndemonstrate that PREF-GRPO differentiates subtle image quality differences,\nproviding more stable advantages and mitigating reward hacking. Additionally,\nexisting T2I benchmarks are limited by coarse evaluation criteria, hindering\ncomprehensive model assessment. To solve this, we introduce UniGenBench, a\nunified T2I benchmark comprising 600 prompts across 5 main themes and 20\nsubthemes. It evaluates semantic consistency through 10 primary and 27\nsub-criteria, leveraging MLLM for benchmark construction and evaluation. Our\nbenchmarks uncover the strengths and weaknesses of both open and closed-source\nT2I models and validate the effectiveness of Pref-GRPO.",
            "upvotes": 57,
            "discussionId": "68b10e71b19c540001484923",
            "projectPage": "https://codegoat24.github.io/UnifiedReward/Pref-GRPO",
            "githubRepo": "https://github.com/CodeGoat24/Pref-GRPO",
            "ai_summary": "Pref-GRPO, a pairwise preference reward-based GRPO method, enhances text-to-image generation by mitigating reward hacking and improving stability, while UniGenBench provides a comprehensive benchmark for evaluating T2I models.",
            "ai_keywords": [
                "GRPO",
                "reinforcement learning",
                "text-to-image",
                "pointwise reward models",
                "reward hacking",
                "pairwise preference",
                "preference fitting",
                "win rate",
                "UniGenBench",
                "semantic consistency",
                "MLLM"
            ],
            "githubStars": 72
        },
        "translation_title": "Pref-GRPO: 안정적인 Text-to-Image 강화 학습을 위한 쌍별 선호 보상 기반 GRPO",
        "purpose": "Text-to-Image 생성의 안정성을 높이기 위한 쌍별 선호 보상 모델 연구",
        "method": [
            "기존의 포인트 보상 모델이 감소한 점수 차이를 확대하여 모델의 최적화를 왜곡시키는 문제를 밝혀냄(we reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages).",
            "선호 보상을 이용하여 직접적으로 이미지의 선호도를 맞추는 Pref-GRPO 방법을 제안함(To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting).",
            "각 그룹 내에서 이미지 쌍을 비교하고 승률을 보상 신호로 사용하여 안정적인 학습을 실행함(in Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal).",
            "MLLM을 활용하여 600개의 프롬프트로 구성된 통합 Benchmark UniGenBench를 도입함(to solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes)."
        ],
        "conclusion": "Pref-GRPO는 Text-to-Image 생성 과정을 안정화하고, UniGenBench는 다양한 T2I 모델의 강점과 약점을 밝히는 평가 기준을 제공함.",
        "keywords": [
            "Image Generation",
            "Reinforcement Learning",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.20722",
            "authors": [
                {
                    "_id": "68b10494b19c5400014848cb",
                    "name": "Ning Shang",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848cc",
                    "name": "Yifei Liu",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848cd",
                    "name": "Yi Zhu",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848ce",
                    "user": {
                        "_id": "62b0009c72043b05d29492b2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
                        "isPro": false,
                        "fullname": "Li Lyna Zhang",
                        "user": "lynazhang",
                        "type": "user"
                    },
                    "name": "Li Lyna Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-29T11:09:40.680Z",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848cf",
                    "name": "Weijiang Xu",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848d0",
                    "name": "Xinyu Guan",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848d1",
                    "name": "Buze Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848d2",
                    "name": "Bingcheng Dong",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848d3",
                    "name": "Xudong Zhou",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848d4",
                    "name": "Bowen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848d5",
                    "name": "Ying Xin",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848d6",
                    "name": "Ziming Miao",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848d7",
                    "name": "Scarlett Li",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848d8",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "68b10494b19c5400014848d9",
                    "name": "Mao Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-28T12:45:25.000Z",
            "submittedOnDailyAt": "2025-08-29T00:25:20.345Z",
            "title": "rStar2-Agent: Agentic Reasoning Technical Report",
            "submittedOnDailyBy": {
                "_id": "62b0009c72043b05d29492b2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
                "isPro": false,
                "fullname": "Li Lyna Zhang",
                "user": "lynazhang",
                "type": "user"
            },
            "summary": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic\nreinforcement learning to achieve frontier-level performance. Beyond current\nlong CoT, the model demonstrates advanced cognitive behaviors, such as thinking\ncarefully before using Python coding tools and reflecting on code execution\nfeedback to autonomously explore, verify, and refine intermediate steps in\ncomplex problem-solving. This capability is enabled through three key\ninnovations that makes agentic RL effective at scale: (i) an efficient RL\ninfrastructure with a reliable Python code environment that supports\nhigh-throughput execution and mitigates the high rollout costs, enabling\ntraining on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic\nRL algorithm with a Resample-on-Correct rollout strategy that addresses the\ninherent environment noises from coding tools, allowing the model to reason\nmore effectively in a code environment; (iii) An efficient agent training\nrecipe that starts with non-reasoning SFT and progresses through multi-RL\nstages, yielding advanced cognitive abilities with minimal compute cost. To\nthis end, rStar2-Agent boosts a pre-trained 14B model to state of the art in\nonly 510 RL steps within one week, achieving average pass@1 scores of 80.6% on\nAIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly\nshorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates\nstrong generalization to alignment, scientific reasoning, and agentic tool-use\ntasks. Code and training recipes are available at\nhttps://github.com/microsoft/rStar.",
            "upvotes": 43,
            "discussionId": "68b10495b19c5400014848da",
            "githubRepo": "https://github.com/microsoft/rStar",
            "ai_summary": "rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning, achieves state-of-the-art performance by efficiently handling complex problem-solving with advanced cognitive behaviors and minimal computational resources.",
            "ai_keywords": [
                "agentic reinforcement learning",
                "CoT",
                "Python coding tools",
                "GRPO-RoC",
                "Resample-on-Correct",
                "SFT",
                "multi-RL",
                "AIME24",
                "AIME25",
                "DeepSeek-R1",
                "alignment",
                "scientific reasoning",
                "agentic tool-use"
            ],
            "githubStars": 655
        },
        "translation_title": "rStar2-Agent: 에이전트적 추론 기술 보고서",
        "purpose": "에이전트적 강화 학습을 이용해 최첨단 수학 추론 모델인 rStar2-Agent 개발",
        "method": [
            "14B 모델을 훈련하기 위해 효율적인 RL 인프라 구축 및 신뢰할 수 있는 Python 코드 환경을 사용함(An efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs.)",
            "코딩 도구의 환경 잡음을 해결하기 위한 GRPO-RoC 알고리즘을 개발함(an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools.)",
            "비추론 SFT에서 시작하여 여러 RL 단계를 거치는 효율적인 에이전트 훈련 전략을 적용함(An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages.)"
        ],
        "conclusion": "rStar2-Agent는 510 RL 단계 만에 놀라운 성능을 달성하며, 수학 외에도 여러 과학적 추론 및 도구 사용 과제에서도 우수한 일반화를 보여줌.",
        "keywords": [
            "Natural Language Processing",
            "Robotics",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2508.18966",
            "authors": [
                {
                    "_id": "68ae8011364411bea07df7fd",
                    "user": {
                        "_id": "660114b38ae190912a61be5d",
                        "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
                        "isPro": false,
                        "fullname": "ShaojinWu",
                        "user": "fenfan",
                        "type": "user"
                    },
                    "name": "Shaojin Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-27T07:10:26.369Z",
                    "hidden": false
                },
                {
                    "_id": "68ae8011364411bea07df7fe",
                    "name": "Mengqi Huang",
                    "hidden": false
                },
                {
                    "_id": "68ae8011364411bea07df7ff",
                    "user": {
                        "_id": "66a0aed210de1ccd3d409721",
                        "avatarUrl": "/avatars/bc6e6c1d60b6601dfb3fe3a697e02ce9.svg",
                        "isPro": false,
                        "fullname": "Yufeng Cheng",
                        "user": "cb1cyf",
                        "type": "user"
                    },
                    "name": "Yufeng Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-29T08:03:16.038Z",
                    "hidden": false
                },
                {
                    "_id": "68ae8011364411bea07df800",
                    "user": {
                        "_id": "635634171c93c1ef4e9eb1c2",
                        "avatarUrl": "/avatars/66b31b801960612057ecfd1e26410075.svg",
                        "isPro": false,
                        "fullname": "wuwenxu",
                        "user": "wuwx",
                        "type": "user"
                    },
                    "name": "Wenxu Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:55:56.250Z",
                    "hidden": false
                },
                {
                    "_id": "68ae8011364411bea07df801",
                    "name": "Jiahe Tian",
                    "hidden": false
                },
                {
                    "_id": "68ae8011364411bea07df802",
                    "name": "Yiming Luo",
                    "hidden": false
                },
                {
                    "_id": "68ae8011364411bea07df803",
                    "name": "Fei Ding",
                    "hidden": false
                },
                {
                    "_id": "68ae8011364411bea07df804",
                    "name": "Qian He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-26T12:10:24.000Z",
            "submittedOnDailyAt": "2025-08-29T01:32:11.306Z",
            "title": "USO: Unified Style and Subject-Driven Generation via Disentangled and\n  Reward Learning",
            "submittedOnDailyBy": {
                "_id": "660114b38ae190912a61be5d",
                "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
                "isPro": false,
                "fullname": "ShaojinWu",
                "user": "fenfan",
                "type": "user"
            },
            "summary": "Existing literature typically treats style-driven and subject-driven\ngeneration as two disjoint tasks: the former prioritizes stylistic similarity,\nwhereas the latter insists on subject consistency, resulting in an apparent\nantagonism. We argue that both objectives can be unified under a single\nframework because they ultimately concern the disentanglement and\nre-composition of content and style, a long-standing theme in style-driven\nresearch. To this end, we present USO, a Unified Style-Subject Optimized\ncustomization model. First, we construct a large-scale triplet dataset\nconsisting of content images, style images, and their corresponding stylized\ncontent images. Second, we introduce a disentangled learning scheme that\nsimultaneously aligns style features and disentangles content from style\nthrough two complementary objectives, style-alignment training and\ncontent-style disentanglement training. Third, we incorporate a style\nreward-learning paradigm denoted as SRL to further enhance the model's\nperformance. Finally, we release USO-Bench, the first benchmark that jointly\nevaluates style similarity and subject fidelity across multiple metrics.\nExtensive experiments demonstrate that USO achieves state-of-the-art\nperformance among open-source models along both dimensions of subject\nconsistency and style similarity. Code and model:\nhttps://github.com/bytedance/USO",
            "upvotes": 30,
            "discussionId": "68ae8011364411bea07df805",
            "projectPage": "https://bytedance.github.io/USO/",
            "githubRepo": "https://github.com/bytedance/USO",
            "ai_summary": "USO, a unified model, achieves state-of-the-art performance in both style similarity and subject consistency by disentangling and re-composing content and style through a disentangled learning scheme and style reward-learning paradigm.",
            "ai_keywords": [
                "disentangled learning",
                "style-alignment training",
                "content-style disentanglement training",
                "style reward-learning",
                "USO-Bench"
            ],
            "githubStars": 189
        },
        "translation_title": "USO: 분리된 학습과 보상 학습을 통한 통합된 스타일 및 주제 기반 생성",
        "purpose": "스타일 기반과 주제 기반 생성을 통합하여 콘텐츠와 스타일을 분리하고 재구성하는 새로운 접근 방식을 제안",
        "method": [
            "대규모 트리플릿 데이터셋을 구성하고 콘텐츠 이미지, 스타일 이미지, 이와 관련된 스타일화된 콘텐츠 이미지를 수집함(we construct a large-scale triplet dataset consisting of content images, style images, and their corresponding stylized content images.)",
            "스타일 피처를 정렬하고 콘텐츠와 스타일을 분리하기 위한 분리된 학습 방식을 도입함(we introduce a disentangled learning scheme that simultaneously aligns style features and disentangles content from style through two complementary objectives.)",
            "스타일 향상을 위한 보상 학습 패러다임인 SRL을 사용하여 모델 성능을 극대화함(we incorporate a style reward-learning paradigm denoted as SRL to further enhance the model's performance.)"
        ],
        "conclusion": "USO는 주제 일관성과 스타일 유사성 측면에서 최신의 성능을 달성하며, USO-Bench를 통해 스타일 유사성 및 주제 충실도를 평가하는 최초의 벤치마크를 공개함.",
        "keywords": [
            "Image Generation",
            "Multimodal Learning",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2508.20404",
            "authors": [
                {
                    "_id": "68b11548b19c540001484981",
                    "name": "Chengyue Yu",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c540001484982",
                    "user": {
                        "_id": "6696954da7fd582ae70db39d",
                        "avatarUrl": "/avatars/92bf42710b206a3164b3ace8090443fa.svg",
                        "isPro": false,
                        "fullname": "Siyuan Lu",
                        "user": "IcyFish",
                        "type": "user"
                    },
                    "name": "Siyuan Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-29T08:01:48.010Z",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c540001484983",
                    "name": "Chenyi Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c540001484984",
                    "name": "Dong Wang",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c540001484985",
                    "name": "Qintong Wu",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c540001484986",
                    "name": "Zongyue Li",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c540001484987",
                    "name": "Runsheng Gan",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c540001484988",
                    "name": "Chunfeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c540001484989",
                    "name": "Siqi Hou",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c54000148498a",
                    "name": "Gaochi Huang",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c54000148498b",
                    "name": "Wenlong Yan",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c54000148498c",
                    "name": "Lifeng Hong",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c54000148498d",
                    "name": "Aohui Xue",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c54000148498e",
                    "name": "Yanfeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c54000148498f",
                    "name": "Jinjie Gu",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c540001484990",
                    "name": "David Tsai",
                    "hidden": false
                },
                {
                    "_id": "68b11548b19c540001484991",
                    "name": "Tao Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-28T04:04:30.000Z",
            "submittedOnDailyAt": "2025-08-29T02:20:06.800Z",
            "title": "AWorld: Orchestrating the Training Recipe for Agentic AI",
            "submittedOnDailyBy": {
                "_id": "64e847ab5ddcace745b8f5b1",
                "avatarUrl": "/avatars/89525e66ff2900f86f66a11043a298f9.svg",
                "isPro": true,
                "fullname": "chenyi zhuang",
                "user": "chengle",
                "type": "user"
            },
            "summary": "The learning from practice paradigm is crucial for developing capable Agentic\nAI systems, yet it is severely hampered by inefficient experience generation, a\nbottleneck especially pronounced in complex benchmarks like GAIA. To address\nthis, we introduce AWorld, an open-source system engineered for large-scale\nagent-environment interaction. By distributing tasks across a cluster, AWorld\naccelerates experience collection by 14.6x compared to standard single-node,\nsequential execution. This critical speedup makes extensive reinforcement\nlearning practical and scalable. Leveraging this capability, we trained a\nQwen3-32B-based agent that significantly outperforms its base model, increasing\nits overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most\nchallenging levels, our agent achieves a score of 16.33%, surpassing the\nperformance of leading proprietary models. Our open-source system and resulting\nagent provide a practical blueprint for a complete agentic AI training\npipeline, from efficient interaction to demonstrable model improvement.",
            "upvotes": 23,
            "discussionId": "68b11548b19c540001484992",
            "githubRepo": "https://github.com/inclusionAI/AWorld/tree/main",
            "ai_summary": "AWorld, an open-source system for large-scale agent-environment interaction, accelerates experience collection and enhances reinforcement learning, leading to significant improvements in agentic AI performance on complex benchmarks.",
            "ai_keywords": [
                "reinforcement learning",
                "Qwen3-32B",
                "GAIA benchmark",
                "agent-environment interaction",
                "experience generation",
                "distributed tasks",
                "cluster",
                "sequential execution",
                "agentic AI",
                "model improvement"
            ],
            "githubStars": 625
        },
        "translation_title": "AWorld: 능동적 AI 훈련을 위한 체계적 접근",
        "purpose": "효율적인 경험 생성을 통해 강력한 능동적 AI 시스템을 개발하기 위한 방법론 제시",
        "method": [
            "AWorld라는 오픈 소스 시스템을 개발하여 대규모 에이전트-환경 상호작용을 가능하게 함(To address this, we introduce AWorld, an open-source system engineered for large-scale agent-environment interaction.)",
            "클러스터를 통한 작업 분산으로 경험 수집 속도를 14.6배 향상시킴(By distributing tasks across a cluster, AWorld accelerates experience collection by 14.6x compared to standard single-node, sequential execution.)",
            "Qwen3-32B 기반의 에이전트를 학습시켜 GAIA 정확성을 21.59%에서 32.23%로 향상시킴(Leveraging this capability, we trained a Qwen3-32B-based agent that significantly outperforms its base model, increasing its overall GAIA accuracy from 21.59% to 32.23%.)"
        ],
        "conclusion": "AWorld는 능동적 AI 훈련을 위한 실용적인 청사진을 제공하며, 성능이 우수한 에이전트를 개발하는데 기여함.",
        "keywords": [
            "Reinforcement Learning",
            "Agentic AI",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.20374",
            "authors": [
                {
                    "_id": "68b11ef0b19c5400014849ae",
                    "name": "Simin Ma",
                    "hidden": false
                },
                {
                    "_id": "68b11ef0b19c5400014849af",
                    "user": {
                        "_id": "67d0574cc2ef59a2ab641b80",
                        "avatarUrl": "/avatars/31362733e6b8e745ac3d0586909a7d55.svg",
                        "isPro": false,
                        "fullname": "Shujian",
                        "user": "shujian2025",
                        "type": "user"
                    },
                    "name": "Shujian Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-29T08:01:45.976Z",
                    "hidden": false
                },
                {
                    "_id": "68b11ef0b19c5400014849b0",
                    "name": "Jun Tan",
                    "hidden": false
                },
                {
                    "_id": "68b11ef0b19c5400014849b1",
                    "user": {
                        "_id": "61638ee4877577e50b485cd4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633914587203-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Yebowen Hu",
                        "user": "huuuyeah",
                        "type": "user"
                    },
                    "name": "Yebowen Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-29T08:01:41.414Z",
                    "hidden": false
                },
                {
                    "_id": "68b11ef0b19c5400014849b2",
                    "user": {
                        "_id": "624be9911fb0c140d4e8768a",
                        "avatarUrl": "/avatars/fe8a253f2f4fcdf9c60eead7bb90180a.svg",
                        "isPro": false,
                        "fullname": "SongW",
                        "user": "songwang41",
                        "type": "user"
                    },
                    "name": "Song Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-29T08:01:38.795Z",
                    "hidden": false
                },
                {
                    "_id": "68b11ef0b19c5400014849b3",
                    "name": "Sathish Reddy Indurthi",
                    "hidden": false
                },
                {
                    "_id": "68b11ef0b19c5400014849b4",
                    "name": "Sanqiang Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b11ef0b19c5400014849b5",
                    "name": "Liwei Wu",
                    "hidden": false
                },
                {
                    "_id": "68b11ef0b19c5400014849b6",
                    "name": "Jianbing Han",
                    "hidden": false
                },
                {
                    "_id": "68b11ef0b19c5400014849b7",
                    "user": {
                        "_id": "63a261f13c003e409322c191",
                        "avatarUrl": "/avatars/dc4be6ce1983d0d32620275df124fe45.svg",
                        "isPro": false,
                        "fullname": "Kaiqiang Song",
                        "user": "kqsong",
                        "type": "user"
                    },
                    "name": "Kaiqiang Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-29T08:01:43.795Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-28T02:42:10.000Z",
            "submittedOnDailyAt": "2025-08-29T02:02:23.288Z",
            "title": "TCIA: A Task-Centric Instruction Augmentation Method for Instruction\n  Finetuning",
            "submittedOnDailyBy": {
                "_id": "67d0574cc2ef59a2ab641b80",
                "avatarUrl": "/avatars/31362733e6b8e745ac3d0586909a7d55.svg",
                "isPro": false,
                "fullname": "Shujian",
                "user": "shujian2025",
                "type": "user"
            },
            "summary": "Diverse instruction data is vital for effective instruction tuning of large\nlanguage models, as it enables the model to generalize across different types\nof inputs . Building such diversified instruction dataset is an essential step\nin this process. Existing approaches often leverage large language models to\nautomatically explore and generate diverse instructions, ensuring both data\ndiversity and quality. However, they tend to overlook an important factor in\nreal-world applications: on-task relevance. In practice, only a few real-world\napplications require a truly general-purpose model; most benefit from\ntask-specific knowledge tailored to their particular use case. Therefore, it is\nvital to develop instruction augmentation methods that not only maintain\ndiversity but are also optimized for specific, real-world scenarios.\n  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework\nthat systematically expands instructions while preserving both diversity and\ntask alignment. By representing instructions in a discrete query-constraints\nspace, TCIA creates a rich set of task-relevant instructions and enables models\nto generalize to these task-specific instructions without sacrificing overall\nperformance. Experiments show that TCIA improves open-source LLMs' performance\nby an average of 8.7% across four real-world, task-specific applications, and\nin some cases outperforming leading closed-source models. These improvements do\nnot compromise general instruction-following ability, making TCIA a scalable\nand efficient solution for adapting LLMs to real-world, task-focused\napplications.",
            "upvotes": 15,
            "discussionId": "68b11ef0b19c5400014849b8",
            "ai_summary": "Task Centric Instruction Augmentation (TCIA) enhances large language models' performance on specific tasks while maintaining general instruction-following ability.",
            "ai_keywords": [
                "instruction tuning",
                "large language models",
                "instruction dataset",
                "data diversity",
                "task-specific knowledge",
                "instruction augmentation",
                "discrete query-constraints space",
                "task-relevant instructions",
                "open-source LLMs",
                "closed-source models"
            ]
        },
        "translation_title": "TCIA: 지시사항 세분화를 위한 작업 중심의 지시 추가 방법",
        "purpose": "대규모 언어 모델의 효과적인 지시 조정을 위한 다양한 지시 데이터를 구축하고, 이를 특정 작업에 맞게 최적화하기 위한 방법 연구",
        "method": [
            "지시사항을 체계적으로 확장하는 프레임워크인 Task Centric Instruction Augmentation(TCIA)를 소개함.",
            "TCIA는 지시사항을 이산 쿼리-제약 공간에서 표현하여 작업 관련 지시사항 세트를 생성함(We thus introduce Task Centric Instruction Augmentation (TCIA), a framework that systematically expands instructions while preserving both diversity and task alignment.)",
            "실험을 통해 TCIA가 네 개의 실제 작업 적용 사례에서 오픈 소스 LLM의 성능을 평균 8.7% 향상시킴(Experiments show that TCIA improves open-source LLMs' performance by an average of 8.7% across four real-world, task-specific applications.)"
        ],
        "conclusion": "TCIA는 대규모 언어 모델을 실제 작업 중심 응용 프로그램에 효과적으로 적응시킬 수 있는 확장 가능하고 효율적인 솔루션임.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Natural Language Processing"
        ]
    }
]