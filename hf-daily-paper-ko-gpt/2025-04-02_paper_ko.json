[
    {
        "paper": {
            "id": "2503.24379",
            "authors": [
                {
                    "_id": "67ec0a7262144ec35d0e571d",
                    "user": {
                        "_id": "64c139d867eff857ea51caa8",
                        "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
                        "isPro": false,
                        "fullname": "Shengqiong Wu",
                        "user": "ChocoWu",
                        "type": "user"
                    },
                    "name": "Shengqiong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:23:35.392Z",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e571e",
                    "user": {
                        "_id": "6360d9f0472131c3bc4f61df",
                        "avatarUrl": "/avatars/c5d884e5ef19b781e3405aba6dd68ca8.svg",
                        "isPro": false,
                        "fullname": "WeicaiYe",
                        "user": "WeicaiYe",
                        "type": "user"
                    },
                    "name": "Weicai Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:36:51.426Z",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e571f",
                    "name": "Jiahao Wang",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5720",
                    "name": "Quande Liu",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5721",
                    "user": {
                        "_id": "60e272ca6c78a8c122b12127",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
                        "isPro": false,
                        "fullname": "Xintao Wang",
                        "user": "Xintao",
                        "type": "user"
                    },
                    "name": "Xintao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:38:20.890Z",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5722",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5723",
                    "user": {
                        "_id": "644c8324f02250233d0d67d9",
                        "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "dizhang",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:38:50.116Z",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5724",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5725",
                    "user": {
                        "_id": "67eaa070b9fa8908e151fd7d",
                        "avatarUrl": "/avatars/1fe2fd678d2e71099a83a9bcb9ab517e.svg",
                        "isPro": false,
                        "fullname": "shuicheng yan",
                        "user": "shuicheng",
                        "type": "user"
                    },
                    "name": "Shuicheng Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:39:18.884Z",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5726",
                    "user": {
                        "_id": "647773a1168cb428e00e9a8f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
                        "isPro": false,
                        "fullname": "Hao Fei",
                        "user": "scofield7419",
                        "type": "user"
                    },
                    "name": "Hao Fei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:23:38.087Z",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5727",
                    "user": {
                        "_id": "6570ae84c4993b8fb96f41a8",
                        "avatarUrl": "/avatars/21f7d79d46ac4df0ecff8eca7678b33f.svg",
                        "isPro": false,
                        "fullname": "Tat-Seng Chua",
                        "user": "chuats",
                        "type": "user"
                    },
                    "name": "Tat-Seng Chua",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:39:25.537Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T17:59:01.000Z",
            "submittedOnDailyAt": "2025-04-02T01:29:41.083Z",
            "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "64c139d867eff857ea51caa8",
                "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
                "isPro": false,
                "fullname": "Shengqiong Wu",
                "user": "ChocoWu",
                "type": "user"
            },
            "summary": "To address the bottleneck of accurate user intent interpretation within the\ncurrent video generation community, we present Any2Caption, a novel framework\nfor controllable video generation under any condition. The key idea is to\ndecouple various condition interpretation steps from the video synthesis step.\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\ninterprets diverse inputs--text, images, videos, and specialized cues such as\nregion, motion, and camera poses--into dense, structured captions that offer\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\nlarge-scale dataset with 337K instances and 407K conditions for\nany-condition-to-caption instruction tuning. Comprehensive evaluations\ndemonstrate significant improvements of our system in controllability and video\nquality across various aspects of existing video generation models. Project\nPage: https://sqwu.top/Any2Cap/",
            "upvotes": 41,
            "discussionId": "67ec0a7562144ec35d0e57fc",
            "projectPage": "https://sqwu.top/Any2Cap/",
            "ai_keywords": [
                "multimodal large language models (MLLMs)",
                "dense, structured captions",
                "region",
                "motion",
                "camera poses",
                "any-condition-to-caption instruction tuning"
            ]
        },
        "translation_title": "Any2Caption: 제어 가능한 비디오 생성을 위한 다양한 조건 해석 시스템",
        "purpose": "사용자의 의도를 보다 정확하게 해석하여 비디오 생성을 개선하기 위해 새로운 프레임워크 개발",
        "method": [
            "비디오 합성 단계와 다양한 조건 해석 단계를 분리함(By decoupling various condition interpretation steps from the video synthesis step.)",
            "최신 멀티모달 대형 언어 모델(MLLMs)을 활용하여 텍스트, 이미지, 비디오 및 특수 신호를 해석해 구조화된 캡션을 생성함(By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs into dense, structured captions.)",
            "337K 인스턴스와 407K 조건으로 구성된 Any2CapIns라는 대규모 데이터셋을 구축함(We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning.)"
        ],
        "conclusion": "시스템 평가 결과, 제어성과 비디오 품질이 기존 비디오 생성 모델보다 현저히 향상됨.",
        "keywords": [
            "Video Generation",
            "Multimodal Learning",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2503.24376",
            "authors": [
                {
                    "_id": "67eca2b8351721d62aa537df",
                    "user": {
                        "_id": "60d045c4778bafd0fbcfa3f5",
                        "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
                        "isPro": false,
                        "fullname": "Yi Chen",
                        "user": "ChenYi99",
                        "type": "user"
                    },
                    "name": "Yi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:30.143Z",
                    "hidden": false
                },
                {
                    "_id": "67eca2b8351721d62aa537e0",
                    "user": {
                        "_id": "6455cc8f654d8bccae50e4d4",
                        "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
                        "isPro": false,
                        "fullname": "Yuying Ge",
                        "user": "tttoaster",
                        "type": "user"
                    },
                    "name": "Yuying Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:40:05.288Z",
                    "hidden": false
                },
                {
                    "_id": "67eca2b8351721d62aa537e1",
                    "user": {
                        "_id": "62e0f1314db2175cd270ad08",
                        "avatarUrl": "/avatars/1d3d6af6c63557f4abf0484e028fa942.svg",
                        "isPro": false,
                        "fullname": "Rui Wang",
                        "user": "ruiwang",
                        "type": "user"
                    },
                    "name": "Rui Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:40:12.320Z",
                    "hidden": false
                },
                {
                    "_id": "67eca2b8351721d62aa537e2",
                    "user": {
                        "_id": "640e9762b03f4cd29f58d982",
                        "avatarUrl": "/avatars/81da37d628163fe3e094b247c7c3a3b5.svg",
                        "isPro": false,
                        "fullname": "Yixiao Ge",
                        "user": "yxgeee",
                        "type": "user"
                    },
                    "name": "Yixiao Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:40:19.876Z",
                    "hidden": false
                },
                {
                    "_id": "67eca2b8351721d62aa537e3",
                    "name": "Lu Qiu",
                    "hidden": false
                },
                {
                    "_id": "67eca2b8351721d62aa537e4",
                    "user": {
                        "_id": "63ca3ddc04c979828310bfcb",
                        "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
                        "isPro": false,
                        "fullname": "Ying Shan",
                        "user": "yshan2u",
                        "type": "user"
                    },
                    "name": "Ying Shan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:40:26.551Z",
                    "hidden": false
                },
                {
                    "_id": "67eca2b8351721d62aa537e5",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:40:32.831Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T17:55:23.000Z",
            "submittedOnDailyAt": "2025-04-02T01:06:48.435Z",
            "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1",
            "submittedOnDailyBy": {
                "_id": "60d045c4778bafd0fbcfa3f5",
                "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
                "isPro": false,
                "fullname": "Yi Chen",
                "user": "ChenYi99",
                "type": "user"
            },
            "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.",
            "upvotes": 24,
            "discussionId": "67eca2b9351721d62aa53822",
            "githubRepo": "https://github.com/TencentARC/SEED-Bench-R1",
            "ai_keywords": [
                "Chain of Thought (COT)",
                "Large Language Models (LLMs)",
                "reinforcement learning (RL)",
                "Multimodal Large Language Models (MLLMs)",
                "SEED-Bench-R1",
                "video understanding",
                "multiple-choice questions",
                "in-distribution",
                "cross-environment",
                "cross-environment-task scenarios",
                "Qwen2-VL-Instruct-7B",
                "supervised fine-tuning (SFT)",
                "LongVideoBench",
                "visual perception",
                "reasoning chains",
                "inconsistent reasoning",
                "reward modeling"
            ]
        },
        "translation_title": "비디오 이해에 대한 강화 학습의 효과 탐색: SEED-Bench-R1의 통찰",
        "purpose": "Multimodal Large Language Models의 비디오 이해를 평가하기 위한 기준 연구",
        "method": [
            "SEED-Bench-R1이라는 벤치마크를 도입하여 MLLMs의 비디오 이해 후속 훈련 방법을 체계적으로 평가함(To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding.)",
            "Qwen2-VL-Instruct-7B 모델을 사용하여 강화 학습(RL)과 감독된 미세 조정(SFT)을 비교함(Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT).)",
            "RL이 데이터 효율성과 비디오 이해 과제에서 우수한 성능을 보임을 보여줌(we demonstrate RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks.)"
        ],
        "conclusion": "강화 학습은 비주얼 인식을 향상시키지만, 논리적 일관성은 떨어질 수 있으며, 향후 개선 사항으로 모델의 사고, 보상 모델링 및 RL의 강건성을 제안함.",
        "keywords": [
            "Video Understanding",
            "Multimodal Learning",
            "Reinforcement Learning"
        ]
    },
    {
        "paper": {
            "id": "2504.00050",
            "authors": [
                {
                    "_id": "67ec9bf3e58745dc7d652587",
                    "name": "Nuo Chen",
                    "hidden": false
                },
                {
                    "_id": "67ec9bf3e58745dc7d652588",
                    "user": {
                        "_id": "64351475901c5734bcb64248",
                        "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
                        "isPro": false,
                        "fullname": "Zhiyuan Hu",
                        "user": "zhiyuanhucs",
                        "type": "user"
                    },
                    "name": "Zhiyuan Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:44:43.089Z",
                    "hidden": false
                },
                {
                    "_id": "67ec9bf3e58745dc7d652589",
                    "user": {
                        "_id": "6730a1fed66bf1b6378cd451",
                        "avatarUrl": "/avatars/5ec9b7313213a951b7c325d35ca26692.svg",
                        "isPro": false,
                        "fullname": "qy",
                        "user": "qingyunzou",
                        "type": "user"
                    },
                    "name": "Qingyun Zou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:44:49.663Z",
                    "hidden": false
                },
                {
                    "_id": "67ec9bf3e58745dc7d65258a",
                    "name": "Jiaying Wu",
                    "hidden": false
                },
                {
                    "_id": "67ec9bf3e58745dc7d65258b",
                    "name": "Qian Wang",
                    "hidden": false
                },
                {
                    "_id": "67ec9bf3e58745dc7d65258c",
                    "user": {
                        "_id": "651d8032c50012d33e914f2f",
                        "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
                        "isPro": false,
                        "fullname": "Bryan Hooi",
                        "user": "bhooi",
                        "type": "user"
                    },
                    "name": "Bryan Hooi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:45:24.844Z",
                    "hidden": false
                },
                {
                    "_id": "67ec9bf3e58745dc7d65258d",
                    "name": "Bingsheng He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T02:18:51.000Z",
            "submittedOnDailyAt": "2025-04-02T04:52:20.239Z",
            "title": "JudgeLRM: Large Reasoning Models as a Judge",
            "submittedOnDailyBy": {
                "_id": "64351475901c5734bcb64248",
                "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
                "isPro": false,
                "fullname": "Zhiyuan Hu",
                "user": "zhiyuanhucs",
                "type": "user"
            },
            "summary": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning.",
            "upvotes": 22,
            "discussionId": "67ec9bf4e58745dc7d6525c1",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Supervised Fine-Tuning (SFT)",
                "reasoning capabilities",
                "reinforcement learning (RL)",
                "judge-wise, outcome-driven rewards",
                "JudgeLRM",
                "GPT-4",
                "DeepSeek-R1"
            ]
        },
        "translation_title": "JudgeLRM: 대규모 추론 모델을 심사자로 활용하기",
        "purpose": "대규모 언어 모델(LLMs)이 복잡한 추론을 요구하는 평가 작업에서 보다 효과적인 성과를 낼 수 있도록 개선하기",
        "method": [
            "각 평가 작업에 필요한 추론 요구 사항을 분석하여 SFT 성과와 추론 샘플 비율 간의 부정적 상관관계를 확인함(Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples.)",
            "강화 학습(RL)을 통해 심사자 중심의 보상을 사용하여 JudgeLRM 모델을 훈련시킴(To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards.)",
            "JudgeLRM 모델이 SFT 조정 모델 및 최신 추론 모델을 지속적으로 능가하는 결과를 도출함(JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models.)"
        ],
        "conclusion": "JudgeLRM 모델은 깊은 추론이 필요한 심사 작업에서 특히 뛰어난 성과를 보이며, 기존의 GPT-4보다도 우수한 결과를 나타냄.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2503.23145",
            "authors": [
                {
                    "_id": "67eb710f1f8a09c48b2e3ba1",
                    "user": {
                        "_id": "674286496efe2b931f7ce354",
                        "avatarUrl": "/avatars/8f920618b777dbff5f2a117ebd9e9caa.svg",
                        "isPro": false,
                        "fullname": "Anjiang Wei",
                        "user": "anjiangwei",
                        "type": "user"
                    },
                    "name": "Anjiang Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:46:42.109Z",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba2",
                    "user": {
                        "_id": "65e7bb35e5e78134ab049942",
                        "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
                        "isPro": false,
                        "fullname": "Tarun Suresh",
                        "user": "tarsur909",
                        "type": "user"
                    },
                    "name": "Tarun Suresh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:23:41.048Z",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba3",
                    "name": "Jiannan Cao",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba4",
                    "name": "Naveen Kannan",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba5",
                    "name": "Yuheng Wu",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba6",
                    "user": {
                        "_id": "65de7628deee79773f0f46f6",
                        "avatarUrl": "/avatars/6c509dbe96e47b47271eb74178c1c9ba.svg",
                        "isPro": false,
                        "fullname": "Kai Yan",
                        "user": "kaiyan289",
                        "type": "user"
                    },
                    "name": "Kai Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:43:08.150Z",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba7",
                    "name": "Thiago S. F. X. Teixeira",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba8",
                    "name": "Ke Wang",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba9",
                    "name": "Alex Aiken",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-29T16:50:39.000Z",
            "submittedOnDailyAt": "2025-04-02T04:28:51.051Z",
            "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis",
            "submittedOnDailyBy": {
                "_id": "65e7bb35e5e78134ab049942",
                "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
                "isPro": false,
                "fullname": "Tarun Suresh",
                "user": "tarsur909",
                "type": "user"
            },
            "summary": "Inductive program synthesis, or programming by example, requires synthesizing\nfunctions from input-output examples that generalize to unseen inputs. While\nlarge language model agents have shown promise in programming tasks guided by\nnatural language, their ability to perform inductive program synthesis is\nunderexplored. Existing evaluation protocols rely on static sets of examples\nand held-out tests, offering no feedback when synthesized functions are\nincorrect and failing to reflect real-world scenarios such as reverse\nengineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge,\na new evaluation framework where agents interact with a hidden target function\nby querying it with new inputs, synthesizing candidate functions, and\niteratively refining their solutions using a differential testing oracle. This\ninteractive setting encourages agents to perform function calls and\nself-correction based on feedback. We construct the first large-scale benchmark\nfor general-purpose inductive program synthesis, featuring 1114 functions.\nAmong 18 models evaluated, o3-mini performs best with a success rate of 52.7%,\nhighlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on\ncurated synthesis traces yields up to a 31% relative performance gain. CodeARC\nprovides a more realistic and challenging testbed for evaluating LLM-based\nprogram synthesis and inductive reasoning.",
            "upvotes": 21,
            "discussionId": "67eb71101f8a09c48b2e3bee",
            "ai_keywords": [
                "inductive program synthesis",
                "programming by example",
                "function synthesis",
                "input-output examples",
                "large language model agents",
                "natural language",
                "evaluation protocols",
                "feedback mechanism",
                "real-world scenarios",
                "reverse engineering",
                "CodeARC",
                "Code Abstraction and Reasoning Challenge",
                "hidden target function",
                "querying",
                "candidate functions",
                "differential testing oracle",
                "interactive setting",
                "function calls",
                "self-correction",
                "large-scale benchmark",
                "general-purpose inductive program synthesis",
                "fine-tuning",
                "LLaMA-3.1-8B-Instruct",
                "synthesis traces",
                "relative performance gain"
            ]
        },
        "translation_title": "CodeARC: 귀납적 프로그램 합성을 위한 LLM 에이전트의 추론 능력 벤치마크",
        "purpose": "LLM을 기반으로 한 프로그램 합성의 성능을 보다 현실적으로 평가하고 개선하기 위함",
        "method": [
            "CodeARC라는 새로운 평가 프레임워크를 제안함(We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function...)",
            "에이전트가 새로운 입력으로 요청하고, 후보 함수를 합성하며, 피드백을 이용해 솔루션을 반복적으로 개선하도록 유도함(This interactive setting encourages agents to perform function calls and self-correction based on feedback.)",
            "1114개의 함수를 포함한 대규모 벤치마크를 구축하여 다양한 모델을 평가함(We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions.)"
        ],
        "conclusion": "CodeARC는 LLM 기반 프로그램 합성과 귀납적 추론을 평가하기 위한 보다 현실적이고 도전적인 테스트베드를 제공합니다.",
        "keywords": [
            "Large Language Models",
            "Inductive Program Synthesis",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2504.01016",
            "authors": [
                {
                    "_id": "67ec958ebb1d6dd924f94a31",
                    "user": {
                        "_id": "65f8e4778dc7bb5b4db97f92",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/gBmQdovmANmjV72k3gaW8.png",
                        "isPro": false,
                        "fullname": "Tian-Xing Xu",
                        "user": "slothfulxtx",
                        "type": "user"
                    },
                    "name": "Tian-Xing Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:51:06.226Z",
                    "hidden": false
                },
                {
                    "_id": "67ec958ebb1d6dd924f94a32",
                    "user": {
                        "_id": "64c0953a8137192a1e2474dc",
                        "avatarUrl": "/avatars/546405a7eaf2f60ad108ceaa0dda7d08.svg",
                        "isPro": false,
                        "fullname": "xiangjun gao",
                        "user": "xiangjun0211",
                        "type": "user"
                    },
                    "name": "Xiangjun Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:51:14.978Z",
                    "hidden": false
                },
                {
                    "_id": "67ec958ebb1d6dd924f94a33",
                    "user": {
                        "_id": "657a7458afbb0117ba15c59f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
                        "isPro": false,
                        "fullname": "Wenbo Hu",
                        "user": "wbhu-tc",
                        "type": "user"
                    },
                    "name": "Wenbo Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:52.313Z",
                    "hidden": false
                },
                {
                    "_id": "67ec958ebb1d6dd924f94a34",
                    "name": "Xiaoyu Li",
                    "hidden": false
                },
                {
                    "_id": "67ec958ebb1d6dd924f94a35",
                    "name": "Song-Hai Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ec958ebb1d6dd924f94a36",
                    "user": {
                        "_id": "63ca3ddc04c979828310bfcb",
                        "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
                        "isPro": false,
                        "fullname": "Ying Shan",
                        "user": "yshan2u",
                        "type": "user"
                    },
                    "name": "Ying Shan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:51:45.869Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/6He3mQcB_AO1G_Sq8xYc0.mp4"
            ],
            "publishedAt": "2025-04-01T17:58:03.000Z",
            "submittedOnDailyAt": "2025-04-02T00:15:17.585Z",
            "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors",
            "submittedOnDailyBy": {
                "_id": "657a7458afbb0117ba15c59f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
                "isPro": false,
                "fullname": "Wenbo Hu",
                "user": "wbhu-tc",
                "type": "user"
            },
            "summary": "Despite remarkable advancements in video depth estimation, existing methods\nexhibit inherent limitations in achieving geometric fidelity through the\naffine-invariant predictions, limiting their applicability in reconstruction\nand other metrically grounded downstream tasks. We propose GeometryCrafter, a\nnovel framework that recovers high-fidelity point map sequences with temporal\ncoherence from open-world videos, enabling accurate 3D/4D reconstruction,\ncamera parameter estimation, and other depth-based applications. At the core of\nour approach lies a point map Variational Autoencoder (VAE) that learns a\nlatent space agnostic to video latent distributions for effective point map\nencoding and decoding. Leveraging the VAE, we train a video diffusion model to\nmodel the distribution of point map sequences conditioned on the input videos.\nExtensive evaluations on diverse datasets demonstrate that GeometryCrafter\nachieves state-of-the-art 3D accuracy, temporal consistency, and generalization\ncapability.",
            "upvotes": 14,
            "discussionId": "67ec9593bb1d6dd924f94b3e",
            "projectPage": "https://geometrycrafter.github.io/",
            "githubRepo": "https://github.com/TencentARC/GeometryCrafter",
            "ai_keywords": [
                "GeometryCrafter",
                "point map Variational Autoencoder (VAE)",
                "latent space",
                "video latent distributions",
                "point map encoding",
                "point map decoding",
                "video diffusion model",
                "point map sequences",
                "3D accuracy",
                "temporal consistency",
                "generalization capability"
            ]
        },
        "translation_title": "GeometryCrafter: 오픈 월드 비디오를 위한 일관성 있는 기하학 추정",
        "purpose": "비디오 깊이 추정을 위한 고충실도 기하학 정보를 회복하여 3D 및 4D 재구성을 가능하게 하기 위한 새로운 프레임워크 제안",
        "method": [
            "기하학적 충실도를 달성하기 위해 개방형 비디오에서 시간적 일관성을 가진 고충실도 포인트 맵 시퀀스를 복원하는 GeometryCrafter 프레임워크를 제안함(We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction.)",
            "포인트 맵 인코딩 및 디코딩을 위한 비디오 잠재 분포와 무관한 잠재 공간을 학습하는 포인트 맵 Variational Autoencoder (VAE)를 기반으로 함(For effective point map encoding and decoding, at the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions.)",
            "VAE를 활용해 입력 비디오에 조건화된 포인트 맵 시퀀스의 분포를 모델링하는 비디오 디퓨전 모델을 학습함(Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos.)"
        ],
        "conclusion": "GeometryCrafter는 최첨단 3D 정확도, 시간적 일관성 및 일반화 능력을 달성함.",
        "keywords": [
            "3D Vision",
            "Video Understanding",
            "Robotics"
        ]
    }
]