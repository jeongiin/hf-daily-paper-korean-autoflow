[
    {
        "paper": {
            "id": "2510.06590",
            "authors": [
                {
                    "_id": "68e743637ae125f9582e6a5d",
                    "user": {
                        "_id": "655c5709948930b0fcf88a22",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/4Qk9Nlyus-su16GdIPJao.png",
                        "isPro": false,
                        "fullname": "Ziyuan",
                        "user": "zyhuangnus",
                        "type": "user"
                    },
                    "name": "Ziyuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T07:57:48.507Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a5e",
                    "user": {
                        "_id": "63747fe30938c075424108e6",
                        "avatarUrl": "/avatars/fd2d82c4fb2834edae516e904424a462.svg",
                        "isPro": false,
                        "fullname": "zheng",
                        "user": "forde450",
                        "type": "user"
                    },
                    "name": "DanDan Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T07:57:46.443Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a5f",
                    "name": "Cheng Zou",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a60",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a61",
                    "user": {
                        "_id": "64e848dd9a3cd93b371166cf",
                        "avatarUrl": "/avatars/6fcee1fe59624113fcf233caf0197729.svg",
                        "isPro": false,
                        "fullname": "Xiaolong Wang",
                        "user": "Xiaolong-Wang",
                        "type": "user"
                    },
                    "name": "Xiaolong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T07:57:38.365Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a62",
                    "user": {
                        "_id": "68e7773faf3a162f9301a117",
                        "avatarUrl": "/avatars/86010187beb5cd964eb9cc1db6e64dee.svg",
                        "isPro": false,
                        "fullname": "Kaixiang Ji",
                        "user": "TorryJ",
                        "type": "user"
                    },
                    "name": "Kaixiang Ji",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:31.833Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a63",
                    "user": {
                        "_id": "64d2fe8ed8b712baf1948c03",
                        "avatarUrl": "/avatars/e8f5f368632065e16bd758a7a2cf182e.svg",
                        "isPro": false,
                        "fullname": "Willie Chai",
                        "user": "Williechai",
                        "type": "user"
                    },
                    "name": "Weilong Chai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:26.937Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a64",
                    "name": "Jianxin Sun",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a65",
                    "user": {
                        "_id": "66b180741fa16824b629943b",
                        "avatarUrl": "/avatars/42d1076da1ae16ce2277f254416ff839.svg",
                        "isPro": false,
                        "fullname": "wang",
                        "user": "lbwang",
                        "type": "user"
                    },
                    "name": "Libin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:29.030Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a66",
                    "name": "Yongjie Lv",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a67",
                    "name": "Taozhi Huang",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a68",
                    "name": "Jiajia Liu",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a69",
                    "user": {
                        "_id": "6482dd5ec2ec7df31fd5cfd5",
                        "avatarUrl": "/avatars/c74fb04b2b4488a9f3f7872b2f91cd7b.svg",
                        "isPro": false,
                        "fullname": "qingpei.gqp",
                        "user": "qingpei",
                        "type": "user"
                    },
                    "name": "Qingpei Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T07:57:42.652Z",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a6a",
                    "name": "Ming Yang",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a6b",
                    "name": "Jingdong Chen",
                    "hidden": false
                },
                {
                    "_id": "68e743637ae125f9582e6a6c",
                    "name": "Jun Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63747fe30938c075424108e6/j9oLJLM284ndzAO9csZY8.qt"
            ],
            "publishedAt": "2025-10-08T02:50:14.000Z",
            "submittedOnDailyAt": "2025-10-09T06:46:28.405Z",
            "title": "Ming-UniVision: Joint Image Understanding and Generation with a Unified\n  Continuous Tokenizer",
            "submittedOnDailyBy": {
                "_id": "63747fe30938c075424108e6",
                "avatarUrl": "/avatars/fd2d82c4fb2834edae516e904424a462.svg",
                "isPro": false,
                "fullname": "zheng",
                "user": "forde450",
                "type": "user"
            },
            "summary": "Visual tokenization remains a core challenge in unifying visual understanding\nand generation within the autoregressive paradigm. Existing methods typically\nemploy tokenizers in discrete latent spaces to align with the tokens from large\nlanguage models, where the quantization errors can limit semantic\nexpressiveness and degrade the capability of vision-language understanding. To\naddress this, we introduce MingTok, a new family of visual tokenizers with a\ncontinuous latent space, for unified autoregressive generation and\nunderstanding. While understanding tasks favor discriminative high-dimensional\nfeatures, generation tasks prefer compact low-level codes. Thus, to reconcile\nthese competing demands, MingTok adopts a three-stage sequential architecture\ninvolving low-level encoding, semantic expansion, and visual reconstruction.\nBuilt on top of it, Ming-UniVision eliminates the need for task-specific visual\nrepresentations, and unifies diverse vision-language tasks under a single\nautoregrsssive prediction paradigm. By formulating both understanding and\ngeneration as next-token prediction in a shared continuous space, it seamlessly\nsupports multi-round, in-context tasks such as iterative understanding,\ngeneration and editing. Empirically, we find that using a unified continuous\nvisual representation reconciles the competing requirements on the tokenizers\nby the understanding and generation tasks, thereby leading to state-of-the-art\nlevel performance across both domains. We hope our findings will facilitate\nunified visual tokenization in the continuous domain. Inference code and model\nweights are released to benefit community.",
            "upvotes": 54,
            "discussionId": "68e743637ae125f9582e6a6d",
            "githubRepo": "https://github.com/inclusionAI/Ming-UniVision",
            "ai_summary": "MingTok, a continuous latent space visual tokenizer, unifies vision-language understanding and generation within an autoregressive framework, achieving state-of-the-art performance across both domains.",
            "ai_keywords": [
                "visual tokenization",
                "autoregressive paradigm",
                "discrete latent spaces",
                "large language models",
                "quantization errors",
                "continuous latent space",
                "three-stage sequential architecture",
                "low-level encoding",
                "semantic expansion",
                "visual reconstruction",
                "Ming-UniVision",
                "task-specific visual representations",
                "next-token prediction",
                "iterative understanding",
                "generation and editing"
            ],
            "githubStars": 53,
            "organization": {
                "_id": "67aea5c8f086ab0f70ed97c9",
                "name": "inclusionAI",
                "fullname": "inclusionAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
            }
        },
        "translation_title": "Ming-UniVision: 통합 연속 토크나이저를 통한 이미지 이해 및 생성 통합",
        "purpose": "시각 이해와 생성의 통합을 위한 연속적인 시각 토크나이저 개발 및 향상된 성능 달성",
        "method": [
            "MingTok이라는 새로운 연속 잠재 공간을 가진 시각 토크나이저 패밀리를 도입함(To address this, we introduce MingTok, a new family of visual tokenizers with a continuous latent space.)",
            "세 단계의 순차적 아키텍처인 저수준 인코딩, 의미 확장 및 시각 재구성을 채택함(Thus, to reconcile these competing demands, MingTok adopts a three-stage sequential architecture involving low-level encoding, semantic expansion, and visual reconstruction.)",
            "공유된 연속 공간에서 이해와 생성을 다음 토큰 예측으로 공식화함(By formulating both understanding and generation as next-token prediction in a shared continuous space)."
        ],
        "conclusion": "Ming-UniVision은 이해와 생성 작업을 통합하여 최신 성능을 달성하였으며, 연속적인 도메인에서의 통합 시각 토크나이징을 촉진할 것으로 기대함.",
        "keywords": [
            "Image Understanding",
            "Image Generation",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2510.03215",
            "authors": [
                {
                    "_id": "68e7171b7ae125f9582e6952",
                    "user": {
                        "_id": "6445fd9ba56444c355dcbcba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
                        "isPro": false,
                        "fullname": "Tianyu Fu",
                        "user": "fuvty",
                        "type": "user"
                    },
                    "name": "Tianyu Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:21.158Z",
                    "hidden": false
                },
                {
                    "_id": "68e7171b7ae125f9582e6953",
                    "user": {
                        "_id": "6607b4a850d2b7a7109f0496",
                        "avatarUrl": "/avatars/ab52887b31b5ac8d20f91bf3b0db674d.svg",
                        "isPro": false,
                        "fullname": "Zihan Min",
                        "user": "minzh23",
                        "type": "user"
                    },
                    "name": "Zihan Min",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:53.564Z",
                    "hidden": false
                },
                {
                    "_id": "68e7171b7ae125f9582e6954",
                    "name": "Hanling Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e7171b7ae125f9582e6955",
                    "user": {
                        "_id": "6800abdd637c165b6c03c429",
                        "avatarUrl": "/avatars/739dbc76409cbef66528417710a9e7c6.svg",
                        "isPro": false,
                        "fullname": "JICHAO Yan",
                        "user": "elentsing",
                        "type": "user"
                    },
                    "name": "Jichao Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:51.583Z",
                    "hidden": false
                },
                {
                    "_id": "68e7171b7ae125f9582e6956",
                    "name": "Guohao Dai",
                    "hidden": false
                },
                {
                    "_id": "68e7171b7ae125f9582e6957",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68e7171b7ae125f9582e6958",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-03T17:52:32.000Z",
            "submittedOnDailyAt": "2025-10-09T00:33:46.090Z",
            "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "6445fd9ba56444c355dcbcba",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
                "isPro": false,
                "fullname": "Tianyu Fu",
                "user": "fuvty",
                "type": "user"
            },
            "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
            "upvotes": 51,
            "discussionId": "68e7171b7ae125f9582e6959",
            "projectPage": "https://fuvty.github.io/C2C_Project_Page/",
            "githubRepo": "https://github.com/thu-nics/C2C",
            "ai_summary": "Cache-to-Cache (C2C) enables direct semantic communication between LLMs using neural network projections, improving accuracy and reducing latency compared to text-based communication.",
            "ai_keywords": [
                "Large Language Models",
                "KV-Cache",
                "Cache-to-Cache",
                "neural network",
                "semantic communication",
                "gating mechanism",
                "accuracy",
                "latency"
            ],
            "githubStars": 18,
            "organization": {
                "_id": "64b74b5fb727f8771ab887f9",
                "name": "nics-efc",
                "fullname": "Tsinghua-NICS-EFC",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641031b1a78453b8d96b8420/vmgxct2WsyHcKT2x2NAYT.jpeg"
            }
        },
        "translation_title": "Cache-to-Cache: 대형 언어 모델 간 직접 의미적 통신",
        "purpose": "대형 언어 모델 간의 통신 방식을 개선하여 성능과 효율성을 높이기 위한 새로운 방법 연구",
        "method": [
            "LLM 간의 통신을 텍스트가 아닌 KV-Cache를 통해 수행하도록 설계함(we ask: Can LLMs communicate beyond text?)",
            "신경망을 사용해 소스 모델의 KV-cache를 타겟 모델과 융합하여 직접적인 의미 전송을 가능하게 하는 Cache-to-Cache(C2C) 제안함(C2C uses a neural network to project and fuse the source model's KV-cache with that of the target model to enable direct semantic transfer.)",
            "학습 가능한 게이팅 메커니즘을 통해 캐시 통신에서 혜택을 볼 수 있는 타겟 레이어를 선택함(A learnable gating mechanism selects the target layers that benefit from cache communication.)",
            "실험 결과, C2C가 개별 모델보다 평균 8.5-10.5% 높은 정확도를 달성하며 지연시간에서도 평균 2.0배의 속도 향상을 보여줌(Experiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models.)"
        ],
        "conclusion": "C2C 방식은 텍스트 통신 방법보다 성능이 더 우수하고, 캐시 통신을 통해 더 빠르고 효과적인 데이터 전송을 가능하게 함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2510.06308",
            "authors": [
                {
                    "_id": "68e70bdd7ae125f9582e6883",
                    "name": "Yi Xin",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6884",
                    "user": {
                        "_id": "66bb136002fd8eb58bc84ffb",
                        "avatarUrl": "/avatars/122cb8f59c502392768099b3c2afe043.svg",
                        "isPro": false,
                        "fullname": "qinqi",
                        "user": "Dakerqi",
                        "type": "user"
                    },
                    "name": "Qi Qin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:46.173Z",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6885",
                    "name": "Siqi Luo",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6886",
                    "name": "Kaiwen Zhu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6887",
                    "name": "Juncheng Yan",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6888",
                    "name": "Yan Tai",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6889",
                    "user": {
                        "_id": "64c3c72e8f31d1e6c664b052",
                        "avatarUrl": "/avatars/af1ad5048eaa9dc417837ad02f927911.svg",
                        "isPro": false,
                        "fullname": "jiayi lei",
                        "user": "jyjyjyjy",
                        "type": "user"
                    },
                    "name": "Jiayi Lei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T06:50:43.828Z",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e688a",
                    "name": "Yuewen Cao",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e688b",
                    "name": "Keqi Wang",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e688c",
                    "name": "Yibin Wang",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e688d",
                    "name": "Jinbin Bai",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e688e",
                    "name": "Qian Yu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e688f",
                    "name": "Dengyang Jiang",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6890",
                    "name": "Yuandong Pu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6891",
                    "name": "Haoxing Chen",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6892",
                    "name": "Le Zhuo",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6893",
                    "name": "Junjun He",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6894",
                    "name": "Gen Luo",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6895",
                    "name": "Tianbin Li",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6896",
                    "name": "Ming Hu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6897",
                    "name": "Jin Ye",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6898",
                    "name": "Shenglong Ye",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e6899",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e689a",
                    "name": "Chang Xu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e689b",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e689c",
                    "name": "Hongsheng Li",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e689d",
                    "name": "Guangtao Zhai",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e689e",
                    "name": "Tianfan Xue",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e689f",
                    "name": "Bin Fu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e68a0",
                    "name": "Xiaohong Liu",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e68a1",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68e70bdd7ae125f9582e68a2",
                    "name": "Yihao Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-07T17:59:20.000Z",
            "submittedOnDailyAt": "2025-10-09T01:00:29.190Z",
            "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal\n  Generation and Understanding",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce Lumina-DiMOO, an open-source foundational model for seamless\nmulti-modal generation and understanding. Lumina-DiMOO sets itself apart from\nprior unified models by utilizing a fully discrete diffusion modeling to handle\ninputs and outputs across various modalities. This innovative approach allows\nLumina-DiMOO to achieve higher sampling efficiency compared to previous\nautoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a\nbroad spectrum of multi-modal tasks, including text-to-image generation,\nimage-to-image generation (e.g., image editing, subject-driven generation, and\nimage inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves\nstate-of-the-art performance on multiple benchmarks, surpassing existing\nopen-source unified multi-modal models. To foster further advancements in\nmulti-modal and discrete diffusion model research, we release our code and\ncheckpoints to the community. Project Page:\nhttps://synbol.github.io/Lumina-DiMOO.",
            "upvotes": 35,
            "discussionId": "68e70bde7ae125f9582e68a3",
            "projectPage": "https://synbol.github.io/Lumina-DiMOO/",
            "githubRepo": "https://github.com/Alpha-VLLM/Lumina-DiMOO",
            "ai_summary": "Lumina-DiMOO, an open-source foundational model, uses fully discrete diffusion modeling for efficient multi-modal generation and understanding, outperforming existing models.",
            "ai_keywords": [
                "discrete diffusion modeling",
                "autoregressive",
                "hybrid AR-Diffusion",
                "text-to-image generation",
                "image-to-image generation",
                "image editing",
                "subject-driven generation",
                "image inpainting",
                "image understanding"
            ],
            "githubStars": 736,
            "organization": {
                "_id": "64bd3427b567ae97c332277f",
                "name": "Alpha-VLLM",
                "fullname": "Alpha-VLLM",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646f1bef075e11ca78da3bb7/mD7gobrnznzDXnpZ9ZiT8.png"
            }
        },
        "translation_title": "Lumina-DiMOO: 다중 모달 생성을 위한 옴니 디퓨전 대규모 언어 모델",
        "purpose": "다양한 모달리티의 입력 및 출력을 효과적으로 처리하는 오픈 소스 모델을 개발하기 위함",
        "method": [
            "완전 이산 디퓨전 모델링을 활용하여 다양한 모달리티를 처리함(Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities.)",
            "기존의 autoregressive(AR) 또는 하이브리드 AR-디퓨전 패러다임보다 높은 샘플링 효율성을 달성함(This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms.)",
            "텍스트-이미지 생성과 이미지 이해 등 다양한 다중 모달 작업을 지원함(adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation, as well as image understanding.)"
        ],
        "conclusion": "Lumina-DiMOO는 여러 기준에서 최첨단 성능을 기록하며, 지금까지의 오픈 소스 통합 다중 모달 모델을 초과하는 성과를 달성함.",
        "keywords": [
            "Multimodal Learning",
            "Image Generation",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2510.06917",
            "authors": [
                {
                    "_id": "68e70dc97ae125f9582e68c8",
                    "name": "Cheng-Han Chiang",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68c9",
                    "name": "Xiaofei Wang",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68ca",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68cb",
                    "name": "Chung-Ching Lin",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68cc",
                    "name": "Kevin Lin",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68cd",
                    "name": "Shujie Liu",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68ce",
                    "name": "Zhendong Wang",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68cf",
                    "name": "Zhengyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68d0",
                    "name": "Hung-yi Lee",
                    "hidden": false
                },
                {
                    "_id": "68e70dc97ae125f9582e68d1",
                    "name": "Lijuan Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/622326ae0129f2097d69a3e2/N4e_VG2KksHuxbjoCTmWW.mp4"
            ],
            "publishedAt": "2025-10-08T11:48:59.000Z",
            "submittedOnDailyAt": "2025-10-09T00:11:57.362Z",
            "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
            "submittedOnDailyBy": {
                "_id": "622326ae0129f2097d69a3e2",
                "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
                "isPro": false,
                "fullname": "Cheng-Han Chiang",
                "user": "dcml0714",
                "type": "user"
            },
            "summary": "Current large language models (LLMs) and spoken language models (SLMs) begin\nthinking and taking actions only after the user has finished their turn. This\nprevents the model from interacting during the user's turn and can lead to high\nresponse latency while it waits to think. Consequently, thinking after\nreceiving the full input is not suitable for speech-to-speech interaction,\nwhere real-time, low-latency exchange is important. We address this by noting\nthat humans naturally \"think while listening.\" In this paper, we propose\nSHANKS, a general inference framework that enables SLMs to generate unspoken\nchain-of-thought reasoning while listening to the user input. SHANKS streams\nthe input speech in fixed-duration chunks and, as soon as a chunk is received,\ngenerates unspoken reasoning based on all previous speech and reasoning, while\nthe user continues speaking. SHANKS uses this unspoken reasoning to decide\nwhether to interrupt the user and to make tool calls to complete the task. We\ndemonstrate that SHANKS enhances real-time user-SLM interaction in two\nscenarios: (1) when the user is presenting a step-by-step solution to a math\nproblem, SHANKS can listen, reason, and interrupt when the user makes a\nmistake, achieving 37.1% higher interruption accuracy than a baseline that\ninterrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can\ncomplete 56.9% of the tool calls before the user finishes their turn. Overall,\nSHANKS moves toward models that keep thinking throughout the conversation, not\nonly after a turn ends. Animated illustrations of Shanks can be found at\nhttps://d223302.github.io/SHANKS/",
            "upvotes": 29,
            "discussionId": "68e70dca7ae125f9582e68d2",
            "projectPage": "https://d223302.github.io/SHANKS/",
            "ai_summary": "SHANKS, a general inference framework, enables spoken language models to generate unspoken reasoning while listening to user input, enhancing real-time interaction and task completion.",
            "ai_keywords": [
                "SHANKS",
                "spoken language models",
                "unspoken chain-of-thought reasoning",
                "real-time interaction",
                "tool calls",
                "interruption accuracy"
            ]
        },
        "translation_title": "SHANKS: 발화 언어 모델을 위한 동시 청취 및 사고",
        "purpose": "사용자 발화 중에 실시간으로 상호작용을 가능하게 하는 언어 모델 개발",
        "method": [
            "SHANKS라는 일반 추론 프레임워크를 제안하여 SLMs가 사용자의 입력을 듣는 동안 비언어적 추론을 생성하도록 함(We propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input.)",
            "입력을 고정된 시간 간격의 청크로 나누어 스트리밍하고, 청크 수신 즉시 이전의 모든 발언과 추론을 바탕으로 비언어적 추론을 생성함(SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning.)",
            "사용자의 발화 중에 이를 통해 사용자를 방해할 필요가 있는지 결정하고 작업을 완료하기 위한 도구 호출을 함(SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task.)"
        ],
        "conclusion": "SHANKS는 사용자가 문제를 설명하는 상황에서 실시간 상호작용을 개선하고, 도구 호출을 미리 수행할 수 있어 대화의 질을 향상시킴.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2510.06710",
            "authors": [
                {
                    "_id": "68e764c97ae125f9582e6bbc",
                    "name": "Hongzhi Zang",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bbd",
                    "name": "Mingjie Wei",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bbe",
                    "name": "Si Xu",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bbf",
                    "name": "Yongji Wu",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc0",
                    "name": "Zhen Guo",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc1",
                    "name": "Yuanqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc2",
                    "name": "Hao Lin",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc3",
                    "name": "Liangzhi Shi",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc4",
                    "name": "Yuqing Xie",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc5",
                    "name": "Zhexuan Xu",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc6",
                    "name": "Zhihao Liu",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc7",
                    "name": "Kang Chen",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc8",
                    "user": {
                        "_id": "66a51845f9565635ebec3b55",
                        "avatarUrl": "/avatars/3cfff055e79da1bc43293b94b28ca210.svg",
                        "isPro": false,
                        "fullname": "Wenhao Tang",
                        "user": "tangwh0517",
                        "type": "user"
                    },
                    "name": "Wenhao Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-10-09T12:48:14.490Z",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bc9",
                    "name": "Quanlu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bca",
                    "name": "Weinan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bcb",
                    "name": "Chao Yu",
                    "hidden": false
                },
                {
                    "_id": "68e764c97ae125f9582e6bcc",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-08T07:05:13.000Z",
            "submittedOnDailyAt": "2025-10-09T06:06:46.600Z",
            "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
            "submittedOnDailyBy": {
                "_id": "64ba0f8d842aa47891cb972b",
                "avatarUrl": "/avatars/3bcacd9b778a146e88e20887b0b00720.svg",
                "isPro": false,
                "fullname": "Chao Yu",
                "user": "zoeyuchao",
                "type": "user"
            },
            "summary": "Recent progress in vision and language foundation models has significantly\nadvanced multimodal understanding, reasoning, and generation, inspiring a surge\nof interest in extending such capabilities to embodied settings through\nvision-language-action (VLA) models. Yet, most VLA models are still trained\nwith supervised fine-tuning (SFT), which struggles to generalize under\ndistribution shifts due to error accumulation. Reinforcement learning (RL)\noffers a promising alternative by directly optimizing task performance through\ninteraction, but existing attempts remain fragmented and lack a unified\nplatform for fair and systematic comparison across model architectures and\nalgorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and\nefficient framework for scalable RL training of VLA models. The system adopts a\nhighly flexible resource allocation design that addresses the challenge of\nintegrating rendering, training, and inference in RL+VLA training. In\nparticular, for GPU-parallelized simulators, RLinf-VLA implements a novel\nhybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup\nin training. Through a unified interface, RLinf-VLA seamlessly supports diverse\nVLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,\nPPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a\nunified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25\nManiSkill tasks. Beyond empirical performance, our study distills a set of best\npractices for applying RL to VLA training and sheds light on emerging patterns\nin this integration. Furthermore, we present preliminary deployment on a\nreal-world Franka robot, where RL-trained policies exhibit stronger\ngeneralization than those trained with SFT. We envision RLinf-VLA as a\nfoundation to accelerate and standardize research on embodied intelligence.",
            "upvotes": 27,
            "discussionId": "68e764c97ae125f9582e6bcd",
            "projectPage": "https://rlinf.readthedocs.io/en/latest/",
            "githubRepo": "https://github.com/RLinf/RLinf",
            "ai_summary": "RLinf-VLA is a unified framework for scalable reinforcement learning training of vision-language-action models, offering improved performance and generalization compared to supervised fine-tuning.",
            "ai_keywords": [
                "vision-language-action models",
                "supervised fine-tuning",
                "reinforcement learning",
                "RLinf-VLA",
                "resource allocation",
                "GPU-parallelized simulators",
                "hybrid fine-grained pipeline allocation",
                "OpenVLA",
                "OpenVLA-OFT",
                "PPO",
                "GRPO",
                "ManiSkill",
                "LIBERO",
                "Franka robot"
            ],
            "githubStars": 472,
            "organization": {
                "_id": "689ea978824b212c988bc8f5",
                "name": "RLinf",
                "fullname": "RLinf",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"
            }
        },
        "translation_title": "RLinf-VLA: VLA+RL 훈련을 위한 통합적이고 효율적인 프레임워크",
        "purpose": "VLA 모델의 훈련을 개선하고 일반화 능력을 향상시키기 위한 RL 기반의 통합 프레임워크 개발",
        "method": [
            "기존의 VLA 모델들이 지닌 일반화 문제를 해결하기 위해 RLinf-VLA라는 통합형 효율 프레임워크를 도입함(To address this gap, we introduce RLinf-VLA, a unified and efficient framework for scalable RL training of VLA models.)",
            "GPU 병렬 시뮬레이터를 통한 훈련 속도를 최적화하는 새로운 하이브리드 파이프라인 할당 모드를 구현함(In particular, for GPU-parallelized simulators, RLinf-VLA implements a novel hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup in training.)",
            "다양한 VLA 아키텍처와 RL 알고리즘, 그리고 시뮬레이터를 지원하는 통합 인터페이스를 제공함(Through a unified interface, RLinf-VLA seamlessly supports diverse VLA architectures and multiple RL algorithms.)"
        ],
        "conclusion": "RLinf-VLA는 RL 기반의 VLA 훈련에서 우수한 성과를 보여주었으며, 실제 로봇에 배포 시 SFT로 훈련된 정책보다 더 강한 일반화를 나타냄.",
        "keywords": [
            "Reinforcement Learning",
            "Vision-Language Models",
            "Robotics"
        ]
    }
]