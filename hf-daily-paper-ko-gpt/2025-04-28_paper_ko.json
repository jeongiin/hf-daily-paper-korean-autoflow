[
    {
        "paper": {
            "id": "2504.15376",
            "authors": [
                {
                    "_id": "680bda9c34c8d0bd08e01a25",
                    "user": {
                        "_id": "64c170190bfb901b04399295",
                        "avatarUrl": "/avatars/c30ce7566ae3497ddc989ec8918d37cc.svg",
                        "isPro": false,
                        "fullname": "Zhiqiu Lin",
                        "user": "zhiqiulin",
                        "type": "user"
                    },
                    "name": "Zhiqiu Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-26T08:53:01.030Z",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a26",
                    "user": {
                        "_id": "65f82fb0de5e636ca20184fa",
                        "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
                        "isPro": false,
                        "fullname": "Alan",
                        "user": "syCen",
                        "type": "user"
                    },
                    "name": "Siyuan Cen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-26T08:53:05.915Z",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a27",
                    "name": "Daniel Jiang",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a28",
                    "user": {
                        "_id": "65b92084327a7e54db8289ae",
                        "avatarUrl": "/avatars/a53b6cda119ed195bd87ba2c1c7a5e13.svg",
                        "isPro": false,
                        "fullname": "Jay Karhade",
                        "user": "JayKarhade",
                        "type": "user"
                    },
                    "name": "Jay Karhade",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-28T13:52:06.066Z",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a29",
                    "user": {
                        "_id": "67b2db158904ba09ca8feb79",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b2db158904ba09ca8feb79/faCKKdyroDNCcylEAQZKu.png",
                        "isPro": false,
                        "fullname": "Hewei Wang",
                        "user": "Stephen624",
                        "type": "user"
                    },
                    "name": "Hewei Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-26T08:53:03.301Z",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a2a",
                    "user": {
                        "_id": "6484bce8d86bf0201eaff647",
                        "avatarUrl": "/avatars/a824216890c0fcd1fcff4a398c4c7cc6.svg",
                        "isPro": false,
                        "fullname": "Chancharik Mitra",
                        "user": "chancharikm",
                        "type": "user"
                    },
                    "name": "Chancharik Mitra",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-28T13:52:12.932Z",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a2b",
                    "name": "Tiffany Ling",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a2c",
                    "name": "Yuhan Huang",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a2d",
                    "name": "Sifan Liu",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a2e",
                    "name": "Mingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a2f",
                    "name": "Rushikesh Zawar",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a30",
                    "name": "Xue Bai",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a31",
                    "user": {
                        "_id": "63c9bd445fdc575773c732fe",
                        "avatarUrl": "/avatars/def472d1ab3fbf751225357c0932ae7e.svg",
                        "isPro": false,
                        "fullname": "Yilun Du",
                        "user": "yilundu",
                        "type": "user"
                    },
                    "name": "Yilun Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-28T13:52:43.497Z",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a32",
                    "name": "Chuang Gan",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a33",
                    "user": {
                        "_id": "6337151b0267ebcf02640eb6",
                        "avatarUrl": "/avatars/14a723cafc5587043bdfb19304fc202d.svg",
                        "isPro": false,
                        "fullname": "Deva Ramanan",
                        "user": "devakramanan",
                        "type": "user"
                    },
                    "name": "Deva Ramanan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-28T13:52:26.336Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T18:34:57.000Z",
            "submittedOnDailyAt": "2025-04-28T00:10:15.204Z",
            "title": "Towards Understanding Camera Motions in Any Video",
            "submittedOnDailyBy": {
                "_id": "65f82fb0de5e636ca20184fa",
                "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
                "isPro": false,
                "fullname": "Alan",
                "user": "syCen",
                "type": "user"
            },
            "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video.",
            "upvotes": 115,
            "discussionId": "680bda9e34c8d0bd08e01ae9",
            "projectPage": "https://linzhiqiu.github.io/papers/camerabench/",
            "githubRepo": "https://github.com/sy77777en/CameraBench",
            "ai_keywords": [
                "Structure-from-Motion (SfM)",
                "Video-Language Models (VLMs)",
                "semantic primitives",
                "geometric primitives",
                "generative VLM",
                "motion-augmented captioning",
                "video question answering",
                "video-text retrieval"
            ]
        },
        "translation_title": "모든 비디오에서 카메라 모션 이해를 위한 연구",
        "purpose": "카메라 모션 이해를 향상시키기 위한 대규모 데이터 세트와 벤치마크 개발",
        "method": [
            "CameraBench라는 약 3,000개의 다양한 인터넷 비디오로 구성된 데이터세트를 생성하고 전문가들이 엄격한 품질 관리 프로세스를 통해 주석을 달도록 함(We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding.)",
            "카메라 모션 프리미티브에 대한 분류법을 도입하고, 이는 영화 제작자와 협력하여 설계함(One of our contributions is a taxonomy of camera motion primitives, designed in collaboration with cinematographers.)",
            "대규모 인간 연구를 통해 전문가 교육이 주석 정확도를 향상시킬 수 있음을 증명함(We conduct a large-scale human study to quantify human annotation performance, revealing that domain expertise and tutorial-based training can significantly enhance accuracy.)",
            "CameraBench를 이용해 Structure-from-Motion(SfM) 모델과 Video-Language Models(VLMs)를 평가함(Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language Models (VLMs))."
        ],
        "conclusion": "CameraBench를 통해 카메라 모션을 이해하는 데 중요한 발전을 이루고, 우리의 분류법과 벤치마크는 향후 연구에 기여할 것으로 기대함.",
        "keywords": [
            "Video Understanding",
            "Computer Vision",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2504.16656",
            "authors": [
                {
                    "_id": "6809a4ac81a95c83f0c81c83",
                    "name": "Chris",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c84",
                    "name": "Yichen Wei",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c85",
                    "name": "Yi Peng",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c86",
                    "name": "Xiaokun Wang",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c87",
                    "name": "Weijie Qiu",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c88",
                    "name": "Wei Shen",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c89",
                    "user": {
                        "_id": "63fdb1aa27abbe6b3ce098f5",
                        "avatarUrl": "/avatars/c22e3a77ff84b3b87c16cff2469f6d3d.svg",
                        "isPro": false,
                        "fullname": "xietian",
                        "user": "sealical",
                        "type": "user"
                    },
                    "name": "Tianyidan Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-28T12:50:25.445Z",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c8a",
                    "name": "Jiangbo Pei",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c8b",
                    "name": "Jianhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c8c",
                    "name": "Yunzhuo Hao",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c8d",
                    "user": {
                        "_id": "6462b241b438438da3c25a5d",
                        "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
                        "isPro": false,
                        "fullname": "Xuchen Song",
                        "user": "xuchensong",
                        "type": "user"
                    },
                    "name": "Xuchen Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:35:17.241Z",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c8e",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c8f",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T12:24:10.000Z",
            "submittedOnDailyAt": "2025-04-28T05:19:19.230Z",
            "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
            "submittedOnDailyBy": {
                "_id": "6462b241b438438da3c25a5d",
                "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
                "isPro": false,
                "fullname": "Xuchen Song",
                "user": "xuchensong",
                "type": "user"
            },
            "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that harmonizes\nreward-model guidance with rule-based strategies, thereby addressing the\nlong-standing challenge of balancing sophisticated reasoning capabilities with\nbroad generalization. To further enhance training efficiency, we propose the\nSelective Sample Buffer (SSB) mechanism, which effectively counters the\n``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization\n(GRPO) by prioritizing high-value samples throughout the optimization process.\nNotably, we observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and\n74.0 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B.",
            "upvotes": 41,
            "discussionId": "6809a4ae81a95c83f0c81cda",
            "githubRepo": "https://github.com/SkyworkAI/Skywork-R1V",
            "ai_keywords": [
                "reinforcement learning",
                "reward-model guidance",
                "rule-based strategies",
                "Selective Sample Buffer (SSB)",
                "Vanishing Advantages",
                "Group Relative Policy Optimization (GRPO)",
                "visual hallucinations",
                "calibrated reward thresholds",
                "benchmark-leading performances",
                "OlympiadBench",
                "AIME2024",
                "LiveCodeBench",
                "MMMU",
                "Skywork R1V2-38B"
            ]
        },
        "translation_title": "Skywork R1V2: 추론을 위한 다중 양식 하이브리드 강화 학습",
        "purpose": "복잡한 추론 능력과 넓은 일반화를 균형 있게 맞추기 위한 다중 양식 추론 모델 개발",
        "method": [
            "하이브리드 강화 학습 패러다임 도입으로 보상 모델과 규칙 기반 전략의 조화를 이루어냄(At its core, R1V2 introduces a hybrid reinforcement learning paradigm that harmonizes reward-model guidance with rule-based strategies.)",
            "Selective Sample Buffer(SSB) 메커니즘을 제안하여 최적화 과정에서 고가치 샘플을 우선시함(To further enhance training efficiency, we propose the Selective Sample Buffer (SSB) mechanism, which effectively counters the 'Vanishing Advantages' dilemma inherent in Group Relative Policy Optimization (GRPO) by prioritizing high-value samples throughout the optimization process.)",
            "훈련 과정에서 과도한 강화 신호로 인한 시각적 환각을 모니터링하고 보상 임계값 조정으로 완화함(Notably, we observe that excessive reinforcement signals can induce visual hallucinations—a phenomenon we systematically monitor and mitigate through calibrated reward thresholds throughout the training process.)"
        ],
        "conclusion": "R1V2는 기존 모델을 능가하는 성과를 보였으며, 상업용 시스템과의 성능 격차를 줄이는 데 중요한 진전을 이루었다.",
        "keywords": [
            "Reinforcement Learning",
            "Multimodal Learning",
            "Reasoning"
        ]
    },
    {
        "paper": {
            "id": "2504.18415",
            "authors": [
                {
                    "_id": "680ef1549cc294f617fb14b4",
                    "name": "Hongyu Wang",
                    "hidden": false
                },
                {
                    "_id": "680ef1549cc294f617fb14b5",
                    "name": "Shuming Ma",
                    "hidden": false
                },
                {
                    "_id": "680ef1549cc294f617fb14b6",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-25T15:17:52.000Z",
            "submittedOnDailyAt": "2025-04-28T01:39:22.422Z",
            "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
            "submittedOnDailyBy": {
                "_id": "63f71771d36951307fcb4dcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
                "isPro": false,
                "fullname": "Hongyu Wang",
                "user": "hongyuw",
                "type": "user"
            },
            "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.",
            "upvotes": 21,
            "discussionId": "680ef1559cc294f617fb1536",
            "ai_keywords": [
                "BitNet v2",
                "1-bit Large Language Models (LLMs)",
                "activation outliers",
                "quantization",
                "4-bit activation quantization",
                "H-BitLinear",
                "Hadamard transformation",
                "activation distributions",
                "Gaussian-like forms",
                "low-bit representation",
                "8-bit activations",
                "BitNet b1.58",
                "batched inference"
            ]
        },
        "translation_title": "BitNet v2: 1-bit LLM을 위한 Hadamard 변환을 활용한 네이티브 4-bit 활성화",
        "purpose": "1-bit Large Language Models (LLMs)의 효율적인 배포를 위해 native 4-bit 활성화 양자화를 구현하는 방법 제안",
        "method": [
            "H-BitLinear 모듈을 제안하여 활성화 양자화 전 온라인 Hadamard 변환을 적용함(we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization.)",
            "Hadamard 변환을 통해 활성화 분포를 더 Gaussian-like 형태로 변환함(This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation.)",
            "실험을 통해 8-bit 활성화로 처음부터 훈련된 BitNet v2의 성능을 확인함(Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance.)"
        ],
        "conclusion": "BitNet v2는 native 4-bit 활성화로 훈련될 때 성능 저하를 최소화하여 메모리 사용량과 배치 추론의 계산 비용을 크게 줄이는 데 성공함.",
        "keywords": [
            "Large Language Models",
            "Quantization",
            "Computational Efficiency"
        ]
    },
    {
        "paper": {
            "id": "2504.17821",
            "authors": [
                {
                    "_id": "680f56b8da9639d22c64443f",
                    "user": {
                        "_id": "64d9da538767727dff1e8f19",
                        "avatarUrl": "/avatars/aaad38795007c6cbcb94c7eed1706e51.svg",
                        "isPro": false,
                        "fullname": "Yu",
                        "user": "Ghaser",
                        "type": "user"
                    },
                    "name": "Xinyu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-28T12:50:03.092Z",
                    "hidden": false
                },
                {
                    "_id": "680f56b8da9639d22c644440",
                    "name": "Yunxin Li",
                    "hidden": false
                },
                {
                    "_id": "680f56b8da9639d22c644441",
                    "name": "Haoyuan Shi",
                    "hidden": false
                },
                {
                    "_id": "680f56b8da9639d22c644442",
                    "name": "Baotian Hu",
                    "hidden": false
                },
                {
                    "_id": "680f56b8da9639d22c644443",
                    "name": "Wenhan Luo",
                    "hidden": false
                },
                {
                    "_id": "680f56b8da9639d22c644444",
                    "name": "Yaowei Wang",
                    "hidden": false
                },
                {
                    "_id": "680f56b8da9639d22c644445",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T13:47:30.000Z",
            "submittedOnDailyAt": "2025-04-28T09:15:13.533Z",
            "title": "VideoVista-CulturalLingo: 360^circ Horizons-Bridging Cultures,\n  Languages, and Domains in Video Comprehension",
            "submittedOnDailyBy": {
                "_id": "62fdb01bc1588e1d4c6c1a7c",
                "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
                "isPro": false,
                "fullname": "Yunxin Li",
                "user": "YunxinLi",
                "type": "user"
            },
            "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics.",
            "upvotes": 18,
            "discussionId": "680f56bdda9639d22c64456b",
            "projectPage": "https://videovista-culturallingo.github.io/",
            "githubRepo": "https://github.com/HITsz-TMG/VideoVista"
        },
        "translation_title": "VideoVista-CulturalLingo: 문화, 언어, 그리고 분야를 잇는 360도 비디오 이해",
        "purpose": "비디오 이해도를 평가하고 다양한 문화와 언어를 아우르는 비디오 평가 기준을 수립하기 위한 연구",
        "method": [
            "VideoVista-CulturalLingo라는 첫 번째 비디오 평가 기준을 제안함(cultural, linguistic, and domain divide in video comprehension)",
            "중국, 북미, 유럽 등 다양한 문화 요소를 포함하고 있음(cultural diversity, incorporating cultures from China, North America, and Europe)",
            "중국어와 영어로 질문을 제시함(multi-linguistics, with questions presented in Chinese and English)",
            "수백 개의 인간 창작 분야에서 비디오를 수집하여 폭넓은 도메인을 포괄함(broad domain, featuring videos sourced from hundreds of human-created domains)"
        ],
        "conclusion": "기존 모델들이 중국 중심 질문에서 낮은 성과를 보였고, 특히 이벤트 위치 지정 작업에서 시간적 이해에 한계가 있음을 발견함.",
        "keywords": [
            "Video Understanding",
            "Multimodal Learning",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2504.16427",
            "authors": [
                {
                    "_id": "680c48805ec65044c2861a6a",
                    "user": {
                        "_id": "669090c01e3f5b16ce22b535",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
                        "isPro": false,
                        "fullname": "Hanlei Zhang",
                        "user": "HanleiZhang",
                        "type": "user"
                    },
                    "name": "Hanlei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-26T08:52:58.965Z",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a6b",
                    "user": {
                        "_id": "680ddad00a5976c000344004",
                        "avatarUrl": "/avatars/82913c7239fdb56e5830d5ddabfa90c1.svg",
                        "isPro": false,
                        "fullname": "Zhuohang Li",
                        "user": "lzh0275",
                        "type": "user"
                    },
                    "name": "Zhuohang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-28T12:33:19.643Z",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a6c",
                    "name": "Yeshuang Zhu",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a6d",
                    "name": "Hua Xu",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a6e",
                    "name": "Peiwu Wang",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a6f",
                    "name": "Haige Zhu",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a70",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a71",
                    "name": "Jinchao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T05:25:13.000Z",
            "submittedOnDailyAt": "2025-04-28T00:53:49.838Z",
            "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
            "submittedOnDailyBy": {
                "_id": "669090c01e3f5b16ce22b535",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
                "isPro": false,
                "fullname": "Hanlei Zhang",
                "user": "HanleiZhang",
                "type": "user"
            },
            "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.",
            "upvotes": 11,
            "discussionId": "680c48825ec65044c2861ac4",
            "githubRepo": "https://github.com/thuiar/MMLA",
            "ai_keywords": [
                "multimodal language models (MLLMs)",
                "MMLA (Multimodal Language Analysis)",
                "multimodal utterances",
                "intent",
                "emotion",
                "dialogue act",
                "sentiment",
                "speaking style",
                "communication behavior",
                "zero-shot inference",
                "supervised fine-tuning",
                "instruction tuning",
                "large language models (LLMs)"
            ]
        },
        "translation_title": "대형 언어 모델이 다중 모달 언어 분석에 기여할 수 있을까? MMLA: 포괄적인 벤치마크",
        "purpose": "다중 모달 언어 분석에서 대형 언어 모델의 인지 수준 의미 이해 능력을 평가하기 위한 포괄적인 벤치마크 개발",
        "method": [
            "MMLA를 통해 61,000개 이상의 다중 모달 발화를 수집하여 여섯 가지 핵심 차원(의도, 감정, 대화 행위, 감정, 말하기 스타일, 커뮤니케이션 행동)을 분석함(MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics.)",
            "여덟 가지 주요 LLM 및 MLLM 모델을 평가하고, 제로샷 추론, 감독하의 미세 조정, 지시 조정 방법을 사용함(We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning.)",
            "모델을 평가한 결과, 미세 조정된 모델도 인간 언어 이해에서 60%~70% 정확도로 제한적임을 확인함(Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language.)"
        ],
        "conclusion": "MMLA는 대형 언어 모델의 다중 모달 언어 분석 잠재력을 탐색하는 데 기초 자료로서 유용하며, 이 분야의 발전에 기여할 수 있을 것임.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    }
]