[
    {
        "paper": {
            "id": "2510.22115",
            "authors": [
                {
                    "_id": "690977f9812eca10f9cc61c7",
                    "name": "Ling-Team",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61c8",
                    "name": "Ang Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61c9",
                    "name": "Ben Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ca",
                    "name": "Binbin Hu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61cb",
                    "name": "Bing Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61cc",
                    "name": "Bingwei Zeng",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61cd",
                    "name": "Borui Ye",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ce",
                    "name": "Caizhi Tang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61cf",
                    "name": "Changxin Tian",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d0",
                    "name": "Chao Huang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d1",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d2",
                    "name": "Chen Qian",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d3",
                    "name": "Chenchen Ju",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d4",
                    "name": "Chenchen Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d5",
                    "name": "Chengfu Tang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d6",
                    "name": "Chili Fu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d7",
                    "name": "Chunshao Ren",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d8",
                    "name": "Chunwei Wu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61d9",
                    "name": "Cong Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61da",
                    "name": "Cunyin Peng",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61db",
                    "name": "Dafeng Xu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61dc",
                    "name": "Daixin Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61dd",
                    "name": "Dalong Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61de",
                    "name": "Dingnan Jin",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61df",
                    "name": "Dingyuan Zhu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e0",
                    "name": "Dongke Hu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e1",
                    "name": "Fangzheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e2",
                    "name": "Feifan Wu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e3",
                    "name": "Feng Zhu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e4",
                    "name": "Gangshan Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e5",
                    "name": "Haitao Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e6",
                    "name": "Hailin Zhao",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e7",
                    "name": "Hanxiao Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e8",
                    "name": "Hanzi Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61e9",
                    "name": "Hao Qian",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ea",
                    "name": "Haoyi Yu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61eb",
                    "name": "Heng Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ec",
                    "name": "Hongliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ed",
                    "name": "Hongzhi Luan",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ee",
                    "name": "Huirong Dong",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ef",
                    "name": "Huizhong Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f0",
                    "name": "Jia Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f1",
                    "name": "Jia Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f2",
                    "name": "Jialong Zhu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f3",
                    "name": "Jian Sha",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f4",
                    "name": "Jianping Wei",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f5",
                    "name": "Jiaolong Yang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f6",
                    "name": "Jieyue Ma",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f7",
                    "name": "Jiewei Wu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f8",
                    "name": "Jinjing Huang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61f9",
                    "name": "Jingyun Tian",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61fa",
                    "name": "Jingyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61fb",
                    "name": "Jinquan Sun",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61fc",
                    "name": "Juanhui Tu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61fd",
                    "name": "Jun Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61fe",
                    "name": "Jun Xu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc61ff",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6200",
                    "name": "Junjie Ou",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6201",
                    "name": "Junpeng Fang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6202",
                    "name": "Kaihong Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6203",
                    "name": "Kaiqin Hu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6204",
                    "name": "Ke Shi",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6205",
                    "name": "Kun Tang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6206",
                    "name": "Kunlong Chen",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6207",
                    "name": "Lanyin Mei",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6208",
                    "name": "Lei Liang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6209",
                    "name": "Lei Xu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc620a",
                    "name": "Libo Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc620b",
                    "name": "Lin Ju",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc620c",
                    "name": "Lin Yuan",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc620d",
                    "name": "Ling Zhong",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc620e",
                    "name": "Lintao Ma",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc620f",
                    "name": "Lu Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6210",
                    "name": "Lu Yu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6211",
                    "name": "Lun Cai",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6212",
                    "name": "Meiqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6213",
                    "name": "Mengying Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6214",
                    "name": "Min Chen",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6215",
                    "name": "Minghao Xue",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6216",
                    "name": "Minghong Cai",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6217",
                    "name": "Mingming Yin",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6218",
                    "name": "Peijie Jiang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6219",
                    "name": "Peilong Zhao",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc621a",
                    "name": "Pingping Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc621b",
                    "name": "Qian Zhao",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc621c",
                    "name": "Qing Cui",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc621d",
                    "name": "Qingxiang Huang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc621e",
                    "name": "Qingyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc621f",
                    "name": "Quankun Yu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6220",
                    "name": "Shaowei Wei",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6221",
                    "name": "Shijie Lian",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6222",
                    "name": "Shoujian Zheng",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6223",
                    "name": "Shun Song",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6224",
                    "name": "Shungen Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6225",
                    "name": "Shuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6226",
                    "name": "Siyuan Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6227",
                    "name": "Song Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6228",
                    "name": "Ting Guo",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6229",
                    "name": "Tong Zhao",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc622a",
                    "name": "Wanli Gu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc622b",
                    "name": "Weichang Wu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc622c",
                    "name": "Weiguang Han",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc622d",
                    "name": "Wenjing Fang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc622e",
                    "name": "Wubin Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc622f",
                    "name": "Xiang Shu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6230",
                    "name": "Xiao Shi",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6231",
                    "name": "Xiaoshun Lan",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6232",
                    "name": "Xiaolu Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6233",
                    "name": "Xiaqing Sun",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6234",
                    "name": "Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6235",
                    "name": "Xingyu Lu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6236",
                    "name": "Xiong Xu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6237",
                    "name": "Xudong Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6238",
                    "name": "Xudong Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6239",
                    "name": "Xuemin Yang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc623a",
                    "name": "Yajie Yang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc623b",
                    "name": "Yang Xiang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc623c",
                    "name": "Yanzhe Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc623d",
                    "name": "Yi Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc623e",
                    "name": "Yilong Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc623f",
                    "name": "Yingxue Li",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6240",
                    "name": "Yongzhen Guo",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6241",
                    "name": "Yuzhuo Fu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6242",
                    "name": "Yuanyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6243",
                    "name": "Yue Yang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6244",
                    "name": "Yue Yu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6245",
                    "name": "Yufeng Deng",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6246",
                    "name": "Yun Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6247",
                    "name": "Yunfei Xu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6248",
                    "name": "Yuqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6249",
                    "name": "Yuxiao He",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc624a",
                    "name": "Zengke Gui",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc624b",
                    "name": "Zhaoxin Huan",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc624c",
                    "name": "Zhaoyang Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc624d",
                    "name": "Zhibo Zhu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc624e",
                    "name": "Zhihao Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc624f",
                    "name": "Zhiqiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6250",
                    "name": "Zhoufei Wang",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6251",
                    "name": "Zihang Zeng",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6252",
                    "name": "Ziqi Liu",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6253",
                    "name": "Zitao Xuan",
                    "hidden": false
                },
                {
                    "_id": "690977f9812eca10f9cc6254",
                    "name": "Zuoli Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-25T01:51:37.000Z",
            "submittedOnDailyAt": "2025-11-04T01:24:23.418Z",
            "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open\n  Language Foundation",
            "submittedOnDailyBy": {
                "_id": "677e8c5624bd3d7373584b0c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e8c5624bd3d7373584b0c/36DAI9UR_q3F4LqKZEjho.jpeg",
                "isPro": false,
                "fullname": "Zhang Zhiqiang",
                "user": "zzqsmall",
                "type": "user"
            },
            "summary": "We introduce Ling 2.0, a series reasoning-oriented language foundation built\nupon the principle that every activation boosts reasoning capability. Designed\nto scale from tens of billions to one trillion parameters under a unified\nMixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,\ncross-scale consistency, and efficiency guided by empirical scaling laws. The\nseries includes three non-thinking (instruct) models - Ling-mini-2.0,\nLing-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and\nachieving up to 7-fold active-compute efficiency compared with dense\ncounterparts. Ling 2.0 integrates coordinated innovations across model\narchitecture, pre-training, post-training, and infrastructure: a high-sparsity\nMoE with MTP for efficient reasoning, reasoning-oriented data and mid-training\nCoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale\nFP8 training with fine-grained heterogeneous pipelines. At the trillion scale,\nLing-1T establishes a new Pareto frontier of reasoning accuracy versus\ncomputational efficiency, demonstrating that sparse activation, when properly\naligned with reasoning objectives, enables scalable and efficient intelligence.\nCollectively, Ling 2.0 provides a coherent, open, and efficient foundation for\nadvancing future reasoning and thinking models, including the Ring series built\nupon the same base.",
            "upvotes": 56,
            "discussionId": "690977f9812eca10f9cc6255",
            "ai_summary": "Ling 2.0, a reasoning-oriented language model series, achieves high efficiency and accuracy through a Mixture-of-Experts paradigm, sparse activation, and innovative training techniques.",
            "ai_keywords": [
                "Mixture-of-Experts (MoE)",
                "high sparsity",
                "cross-scale consistency",
                "MTP",
                "reasoning-oriented data",
                "mid-training CoT activation",
                "reinforcement-based fine-tuning (DFT",
                "Evo-CoT)",
                "full-scale FP8 training",
                "fine-grained heterogeneous pipelines",
                "sparse activation",
                "reasoning accuracy",
                "computational efficiency"
            ],
            "organization": {
                "_id": "67aea5c8f086ab0f70ed97c9",
                "name": "inclusionAI",
                "fullname": "inclusionAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
            }
        },
        "translation_title": "모든 활성화를 강화하다: 1조 개 오픈 언어 기초로 확장하기",
        "purpose": "모든 활성화가 추론 능력을 향상시킨다는 원칙 하에 언어 기초 모델을 확장하고 향상시키기 위한 연구",
        "method": [
            "Ling 2.0은 통합된 Mixture-of-Experts (MoE) 패러다임 아래에서 수십억에서 1조 개의 파라미터로 확장되도록 설계됨(Designed to scale from tens of billions to one trillion parameters under a unified Mixture-of-Experts (MoE) paradigm.)",
            "세 가지 비사고(instruct) 모델(Ling-mini-2.0, Ling-flash-2.0, Ling-1T)을 통해 최대 7배의 활성 계산 효율을 달성함(Achieving up to 7-fold active-compute efficiency compared with dense counterparts.)",
            "모델 아키텍처, 사전 훈련, 후 훈련 및 인프라에 걸친 혁신을 통합함(Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure.)"
        ],
        "conclusion": "Ling 2.0은 향후 추론 및 사고 모델을 발전시키기 위한 일관되고 효율적인 기초를 제공함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2510.24788",
            "authors": [
                {
                    "_id": "69096b57812eca10f9cc6185",
                    "name": "Xinjian Zhao",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc6186",
                    "name": "Wei Pang",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc6187",
                    "name": "Zhongkai Xue",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc6188",
                    "name": "Xiangru Jian",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc6189",
                    "name": "Lei Zhang",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc618a",
                    "name": "Yaoyao Xu",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc618b",
                    "name": "Xiaozhuang Song",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc618c",
                    "name": "Shu Wu",
                    "hidden": false
                },
                {
                    "_id": "69096b57812eca10f9cc618d",
                    "name": "Tianshu Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-27T05:11:44.000Z",
            "submittedOnDailyAt": "2025-11-04T02:03:48.972Z",
            "title": "The Underappreciated Power of Vision Models for Graph Structural\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "68380f4f231cf484dd4e87f4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xp34hfiSLf-DiE1DVVhHk.png",
                "isPro": false,
                "fullname": "Xinjian Zhao",
                "user": "Xinjiansz",
                "type": "user"
            },
            "summary": "Graph Neural Networks operate through bottom-up message-passing,\nfundamentally differing from human visual perception, which intuitively\ncaptures global structures first. We investigate the underappreciated potential\nof vision models for graph understanding, finding they achieve performance\ncomparable to GNNs on established benchmarks while exhibiting distinctly\ndifferent learning patterns. These divergent behaviors, combined with\nlimitations of existing benchmarks that conflate domain features with\ntopological understanding, motivate our introduction of GraphAbstract. This\nbenchmark evaluates models' ability to perceive global graph properties as\nhumans do: recognizing organizational archetypes, detecting symmetry, sensing\nconnectivity strength, and identifying critical elements. Our results reveal\nthat vision models significantly outperform GNNs on tasks requiring holistic\nstructural understanding and maintain generalizability across varying graph\nscales, while GNNs struggle with global pattern abstraction and degrade with\nincreasing graph size. This work demonstrates that vision models possess\nremarkable yet underutilized capabilities for graph structural understanding,\nparticularly for problems requiring global topological awareness and\nscale-invariant reasoning. These findings open new avenues to leverage this\nunderappreciated potential for developing more effective graph foundation\nmodels for tasks dominated by holistic pattern recognition.",
            "upvotes": 26,
            "discussionId": "69096b57812eca10f9cc618e",
            "ai_summary": "Vision models outperform Graph Neural Networks on tasks requiring global structural understanding and scale-invariant reasoning, as demonstrated by the new GraphAbstract benchmark.",
            "ai_keywords": [
                "Graph Neural Networks",
                "message-passing",
                "vision models",
                "GraphAbstract",
                "global graph properties",
                "organizational archetypes",
                "symmetry",
                "connectivity strength",
                "critical elements",
                "holistic structural understanding",
                "scale-invariant reasoning"
            ]
        },
        "translation_title": "그래프 구조 이해를 위한 비전 모델의 과소평가된 힘",
        "purpose": "그래프 이해에 있어 비전 모델의 잠재력을 발견하고 성능을 평가하기 위한 새로운 벤치마크 개발",
        "method": [
            "비전 모델의 성능을 평가하기 위해 GraphAbstract라는 새로운 벤치마크를 도입함(we introduce GraphAbstract. This benchmark evaluates models' ability to perceive global graph properties as humans do.)",
            "비전 모델이 GNN보다 다양한 그래프 스케일에서 전반적인 구조 이해를 요구하는 작업에서 우수한 성능을 보여줌(vision models significantly outperform GNNs on tasks requiring holistic structural understanding and maintain generalizability across varying graph scales.)"
        ],
        "conclusion": "비전 모델은 그래프 구조 이해에 있어 뛰어난 능력을 가지며, 이를 통해 보다 효과적인 그래프 기반 모델 개발이 가능함을 보여줌.",
        "keywords": [
            "Image Understanding",
            "Graph Neural Networks",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2511.01678",
            "authors": [
                {
                    "_id": "6909a211812eca10f9cc63ca",
                    "name": "Ropeway Liu",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63cb",
                    "name": "Hangjie Yuan",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63cc",
                    "name": "Bo Dong",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63cd",
                    "name": "Jiazheng Xing",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63ce",
                    "name": "Jinwang Wang",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63cf",
                    "user": {
                        "_id": "652b83b73b5997ed71a310f2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652b83b73b5997ed71a310f2/ipCpdeHUp4-0OmRz5z8IW.png",
                        "isPro": false,
                        "fullname": "Rui Zhao",
                        "user": "ruizhaocv",
                        "type": "user"
                    },
                    "name": "Rui Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-04T09:24:59.454Z",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63d0",
                    "name": "Yan Xing",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63d1",
                    "name": "Weihua Chen",
                    "hidden": false
                },
                {
                    "_id": "6909a211812eca10f9cc63d2",
                    "name": "Fan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T15:41:41.000Z",
            "submittedOnDailyAt": "2025-11-04T04:21:00.451Z",
            "title": "UniLumos: Fast and Unified Image and Video Relighting with\n  Physics-Plausible Feedback",
            "submittedOnDailyBy": {
                "_id": "649d54b314afbb10ce2a9eeb",
                "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                "isPro": false,
                "fullname": "Hangjie Yuan",
                "user": "JacobYuan",
                "type": "user"
            },
            "summary": "Relighting is a crucial task with both practical demand and artistic value,\nand recent diffusion models have shown strong potential by enabling rich and\ncontrollable lighting effects. However, as they are typically optimized in\nsemantic latent space, where proximity does not guarantee physical correctness\nin visual space, they often produce unrealistic results, such as overexposed\nhighlights, misaligned shadows, and incorrect occlusions. We address this with\nUniLumos, a unified relighting framework for both images and videos that brings\nRGB-space geometry feedback into a flow matching backbone. By supervising the\nmodel with depth and normal maps extracted from its outputs, we explicitly\nalign lighting effects with the scene structure, enhancing physical\nplausibility. Nevertheless, this feedback requires high-quality outputs for\nsupervision in visual space, making standard multi-step denoising\ncomputationally expensive. To mitigate this, we employ path consistency\nlearning, allowing supervision to remain effective even under few-step training\nregimes. To enable fine-grained relighting control and supervision, we design a\nstructured six-dimensional annotation protocol capturing core illumination\nattributes. Building upon this, we propose LumosBench, a disentangled\nattribute-level benchmark that evaluates lighting controllability via large\nvision-language models, enabling automatic and interpretable assessment of\nrelighting precision across individual dimensions. Extensive experiments\ndemonstrate that UniLumos achieves state-of-the-art relighting quality with\nsignificantly improved physical consistency, while delivering a 20x speedup for\nboth image and video relighting. Code is available at\nhttps://github.com/alibaba-damo-academy/Lumos-Custom.",
            "upvotes": 25,
            "discussionId": "6909a212812eca10f9cc63d3",
            "projectPage": "https://github.com/alibaba-damo-academy/Lumos-Custom",
            "githubRepo": "https://github.com/alibaba-damo-academy/Lumos-Custom",
            "ai_summary": "UniLumos, a unified relighting framework, enhances physical plausibility by integrating RGB-space geometry feedback into a flow matching backbone, achieving state-of-the-art results with improved consistency and speed.",
            "ai_keywords": [
                "diffusion models",
                "semantic latent space",
                "RGB-space",
                "flow matching",
                "depth maps",
                "normal maps",
                "path consistency learning",
                "six-dimensional annotation protocol",
                "disentangled attribute-level benchmark",
                "LumosBench",
                "large vision-language models",
                "relighting quality",
                "physical consistency"
            ],
            "githubStars": 5,
            "organization": {
                "_id": "6808e7522a4d69d5111da55f",
                "name": "Alibaba-DAMO-Academy",
                "fullname": "DAMO Academy",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
            }
        },
        "translation_title": "UniLumos: 물리적으로 신뢰할 수 있는 피드백을 통한 빠르고 통합된 이미지 및 비디오 리라이트닝",
        "purpose": "이미지 및 비디오 리라이트닝 품질을 개선하고 물리적 일관성을 높이기 위한 통합 프레임워크 개발",
        "method": [
            "RGB 공간의 기하학적 피드백을 흐름 일치 백본에 통합하여 리라이트닝 프레임워크를 구축함(we address this with UniLumos, a unified relighting framework for both images and videos that brings RGB-space geometry feedback into a flow matching backbone.)",
            "모델의 출력에서 깊이 및 노말 맵을 추출하여 조명 효과를 장면 구조와 명시적으로 정렬함(By supervising the model with depth and normal maps extracted from its outputs, we explicitly align lighting effects with the scene structure.)",
            "경량화 훈련 효과를 유지하기 위해 경로 일관성 학습을 적용함(To mitigate this, we employ path consistency learning, allowing supervision to remain effective even under few-step training regimes.)"
        ],
        "conclusion": "UniLumos는 높은 품질의 리라이트닝을 제공하고 물리적 일관성을 크게 개선하며, 이미지와 비디오 리라이트닝 속도를 20배 향상시킴.",
        "keywords": [
            "Image Generation",
            "Video Generation",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2511.00086",
            "authors": [
                {
                    "_id": "69096c6c812eca10f9cc6190",
                    "name": "Fali Wang",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6191",
                    "name": "Jihai Chen",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6192",
                    "name": "Shuhua Yang",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6193",
                    "name": "Runxue Bao",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6194",
                    "name": "Tianxiang Zhao",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6195",
                    "name": "Zhiwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6196",
                    "name": "Xianfeng Tang",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6197",
                    "name": "Hui Liu",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6198",
                    "name": "Qi He",
                    "hidden": false
                },
                {
                    "_id": "69096c6c812eca10f9cc6199",
                    "name": "Suhang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-10-29T22:14:25.000Z",
            "submittedOnDailyAt": "2025-11-04T00:31:32.141Z",
            "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph",
            "submittedOnDailyBy": {
                "_id": "644a8ca97c5c68c7762906a0",
                "avatarUrl": "/avatars/c2f6507fa7dcf00fe0151462533f1c2c.svg",
                "isPro": false,
                "fullname": "Fali Wang",
                "user": "FairyFali",
                "type": "user"
            },
            "summary": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating\nadditional computation during inference, typically through parallel,\nsequential, or hybrid scaling. However, prior studies often assume fixed\ncollaboration architectures (e.g., topologies) and single-model usage,\noverlooking that optimal architectures and model combinations can vary across\ntasks. Therefore, we study the novel problem of searching for compute-optimal\nmodel combinations and architectures in TTS under a fixed budget. We formalize\nit as a multi-LLM collaboration graph, where nodes encode roles and LLM model\nassignments, and edges capture information flow. This problem is challenging\nbecause (i) the combinatorial search space is prohibitively large, and (ii)\ntask-specific requirements demand tailored designs. To address these, we\nreformulate the problem as probabilistic graph optimization and, through pilot\nexperiments, derive three empirical insights into TTS collaboration graphs.\nGuided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented\nframework that mirrors the REINFORCE pipeline by mapping\nsampling-gradient-update to sampling-feedback-update, where feedback serves as\na textual gradient to update the probabilistic graph and efficiently search for\noptimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE\noutperforms both traditional and LLM-based baselines in sample efficiency and\nsearch performance, and effectively identifies optimal graphs under joint\nobjectives of accuracy and inference latency.",
            "upvotes": 25,
            "discussionId": "69096c6c812eca10f9cc619a",
            "ai_summary": "Agent-REINFORCE optimizes multi-LLM collaboration graphs for test-time scaling, improving sample efficiency and search performance under accuracy and latency constraints.",
            "ai_keywords": [
                "Test-Time Scaling",
                "large language models",
                "parallel scaling",
                "sequential scaling",
                "hybrid scaling",
                "collaboration architectures",
                "multi-LLM collaboration graph",
                "probabilistic graph optimization",
                "Agent-REINFORCE",
                "REINFORCE pipeline",
                "textual gradient"
            ],
            "organization": {
                "_id": "623c72b6483fb88b35620a27",
                "name": "PennState",
                "fullname": "Pennsylvania State University"
            }
        },
        "translation_title": "테스트 시간 최적 컴퓨팅 최적화 확장을 위한 최적화 가능한 그래프 일반화",
        "purpose": "제한된 예산 내에서 최적의 모델 조합과 아키텍처를 찾기 위한 연구",
        "method": [
            "TTS를 다중 LLM 협업 그래프로 정식화하여 노드가 역할과 모델 할당을 나타내고 엣지가 정보 흐름을 캡처하도록 함(To address these, we reformulate the problem as probabilistic graph optimization.)",
            "Agent-REINFORCE라는 LLM-agent 증강 프레임워크를 제안하여 피드백을 텍스트 그래디언트로 활용함(Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline.)",
            "파일럿 실험을 통해 TTS 협업 그래프에 대한 세 가지 경험적 통찰력을 도출함(through pilot experiments, derive three empirical insights into TTS collaboration graphs.)"
        ],
        "conclusion": "Agent-REINFORCE는 샘플 효율성과 검색 성능 모두에서 전통적인 방법 및 LLM 기반의 기준보다 뛰어남을 보여주며, 정확성과 추론 지연의 공동 목표 하에서 최적의 그래프를 효과적으로 식별할 수 있음을 입증함.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Probabilistic Graph Optimization"
        ]
    },
    {
        "paper": {
            "id": "2511.01295",
            "authors": [
                {
                    "_id": "69097a04812eca10f9cc62fe",
                    "name": "Feng Han",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc62ff",
                    "name": "Yibin Wang",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6300",
                    "name": "Chenglin Li",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6301",
                    "name": "Zheming Liang",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6302",
                    "name": "Dianyi Wang",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6303",
                    "name": "Yang Jiao",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6304",
                    "name": "Zhipeng Wei",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6305",
                    "name": "Chao Gong",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6306",
                    "name": "Cheng Jin",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6307",
                    "name": "Jingjing Chen",
                    "hidden": false
                },
                {
                    "_id": "69097a04812eca10f9cc6308",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-03T07:24:57.000Z",
            "submittedOnDailyAt": "2025-11-04T01:29:52.436Z",
            "title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark",
            "submittedOnDailyBy": {
                "_id": "654c6845bac6e6e49895a5b5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png",
                "isPro": false,
                "fullname": "SII-Yibin Wang",
                "user": "CodeGoat24",
                "type": "user"
            },
            "summary": "Recent advances in multi-modal generative models have driven substantial\nimprovements in image editing. However, current generative models still\nstruggle with handling diverse and complex image editing tasks that require\nimplicit reasoning, underscoring the need for a comprehensive benchmark to\nsystematically assess their performance across various reasoning scenarios.\nExisting benchmarks primarily focus on single-object attribute transformation\nin realistic scenarios, which, while effective, encounter two key challenges:\n(1) they largely overlook multi-object interactions as well as game-world\nscenarios that involve human-defined rules, which are common in real-life\napplications; (2) they only rely on textual references to evaluate the\ngenerated images, potentially leading to systematic misjudgments, especially in\ncomplex reasoning scenarios. To this end, this work proposes UniREditBench, a\nunified benchmark for reasoning-based image editing evaluation. It comprises\n2,700 meticulously curated samples, covering both real- and game-world\nscenarios across 8 primary dimensions and 18 sub-dimensions. To improve\nevaluation reliability, we introduce multimodal dual-reference evaluation,\nproviding both textual and ground-truth image references for each sample\nassessment. Furthermore, we design an automated multi-scenario data synthesis\npipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with\nhigh-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel\non this dataset and develop UniREdit-Bagel, demonstrating substantial\nimprovements in both in-domain and out-of-distribution settings. Through\nthorough benchmarking of both open-source and closed-source image editing\nmodels, we reveal their strengths and weaknesses across various aspects.",
            "upvotes": 20,
            "discussionId": "69097a04812eca10f9cc6309",
            "projectPage": "https://maplebb.github.io/UniREditBench/",
            "githubRepo": "https://github.com/Maplebb/UniREditBench",
            "ai_summary": "UniREditBench is a unified benchmark for reasoning-based image editing that addresses limitations in existing benchmarks by including multi-object interactions, game-world scenarios, and multimodal dual-reference evaluation.",
            "ai_keywords": [
                "multi-modal generative models",
                "image editing",
                "implicit reasoning",
                "benchmark",
                "single-object attribute transformation",
                "real-world scenarios",
                "game-world scenarios",
                "human-defined rules",
                "multimodal dual-reference evaluation",
                "automated multi-scenario data synthesis",
                "chain-of-thought reasoning",
                "fine-tuning",
                "UniREdit-Data-100K",
                "UniREdit-Bagel",
                "in-domain",
                "out-of-distribution"
            ],
            "githubStars": 22,
            "organization": {
                "_id": "643cb0625fcffe09fb6ca688",
                "name": "Fudan-University",
                "fullname": "Fudan University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
            }
        },
        "translation_title": "UniREditBench: 통합 추론 기반 이미지 편집 벤치마크",
        "purpose": "다양한 추론 시나리오에서 이미지 편집 모델의 성능을 체계적으로 평가할 수 있는 벤치마크 개발",
        "method": [
            "UniREditBench라는 통합 벤치마크를 제안하고, 2,700개의 샘플을 신중하게 큐레이션함(we propose UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples.)",
            "8개의 주요 차원과 18개의 하위 차원을 통해 현실 세계 및 게임 세계의 시나리오를 포함함(covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions.)",
            "다양한 평가를 위해 텍스트 및 실제 이미지 참조를 제공하는 다중모드 이중 참조 평가를 도입함(we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment.)"
        ],
        "conclusion": "자세한 벤치마크 분석을 통해 다양한 이미지 편집 모델의 강점과 약점을 밝힘.",
        "keywords": [
            "Image Editing",
            "Multimodal Learning",
            "Computer Vision"
        ]
    }
]