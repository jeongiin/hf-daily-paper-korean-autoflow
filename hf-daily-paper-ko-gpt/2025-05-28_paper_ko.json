[
    {
        "paper": {
            "id": "2505.19897",
            "authors": [
                {
                    "_id": "6835e75d649a767a0771f8a4",
                    "user": {
                        "_id": "6064a0eeb1703ddba0d458b9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
                        "isPro": false,
                        "fullname": "Qiushi",
                        "user": "QiushiSun",
                        "type": "user"
                    },
                    "name": "Qiushi Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:59:14.344Z",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8a5",
                    "name": "Zhoumianze Liu",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8a6",
                    "name": "Chang Ma",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8a7",
                    "user": {
                        "_id": "642b9861bb77f8456634b048",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/ZT-oJrw5BsADC-gZT_i25.jpeg",
                        "isPro": false,
                        "fullname": "Zichen Ding",
                        "user": "heroding77",
                        "type": "user"
                    },
                    "name": "Zichen Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:59:08.025Z",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8a8",
                    "user": {
                        "_id": "64e6cf78ecce34cb442dc889",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
                        "isPro": false,
                        "fullname": "Fangzhi Xu",
                        "user": "xufangzhi",
                        "type": "user"
                    },
                    "name": "Fangzhi Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:59:12.170Z",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8a9",
                    "name": "Zhangyue Yin",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8aa",
                    "name": "Haiteng Zhao",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8ab",
                    "name": "Zhenyu Wu",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8ac",
                    "user": {
                        "_id": "63340dbbd92c5842ae71d1e9",
                        "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
                        "isPro": false,
                        "fullname": "Kanzhi Cheng",
                        "user": "cckevinn",
                        "type": "user"
                    },
                    "name": "Kanzhi Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:59:03.364Z",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8ad",
                    "name": "Zhaoyang Liu",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8ae",
                    "name": "Jianing Wang",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8af",
                    "name": "Qintong Li",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b0",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b1",
                    "name": "Tianbao Xie",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b2",
                    "user": {
                        "_id": "6776ae0c91b4c75dac91249c",
                        "avatarUrl": "/avatars/a43139f65ad7086426d9757a1bcb7080.svg",
                        "isPro": false,
                        "fullname": "Oran Feng",
                        "user": "xiachongfeng",
                        "type": "user"
                    },
                    "name": "Xiachong Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:59:05.718Z",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b3",
                    "name": "Xiang Li",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b4",
                    "name": "Ben Kao",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b5",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b6",
                    "name": "Biqing Qi",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b7",
                    "name": "Lingpeng Kong",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b8",
                    "name": "Zhiyong Wu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/kBkmkf8TUS9QTLLNJh9Cc.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/MuRvbGQbB4n2WBymZqGh1.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/hpztzin1L2YJpACBBIbdb.gif"
            ],
            "publishedAt": "2025-05-26T12:27:27.000Z",
            "submittedOnDailyAt": "2025-05-28T12:25:48.176Z",
            "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic\n  Scientific Workflows",
            "submittedOnDailyBy": {
                "_id": "6064a0eeb1703ddba0d458b9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
                "isPro": false,
                "fullname": "Qiushi",
                "user": "QiushiSun",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/.",
            "upvotes": 72,
            "discussionId": "6835e760649a767a0771f984",
            "projectPage": "https://qiushisun.github.io/ScienceBoard-Home/",
            "githubRepo": "https://github.com/OS-Copilot/ScienceBoard",
            "ai_summary": "ScienceBoard provides a realistic scientific workflow environment and benchmark to evaluate the performance of LLM-based agents, demonstrating their current limitations in complex scientific tasks.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "computer-using agents",
                "scientific discovery",
                "dynamic workflows",
                "integrated professional software",
                "autonomous interaction",
                "ScienceBoard",
                "benchmark",
                "evaluation",
                "state-of-the-art backbones",
                "GPT-4o",
                "Claude 3.7",
                "UI-TARS",
                "biochemistry",
                "astronomy",
                "geoinformatics",
                "design principles"
            ]
        },
        "translation_title": "ScienceBoard: 현실적인 과학 작업 흐름에서 멀티모달 자율 에이전트 평가",
        "purpose": "과학적 발견을 지원하는 멀티모달 자율 에이전트의 평가 및 개선을 위해 현실적인 환경과 도전적인 벤치마크 제공",
        "method": [
            "dynamic하고 시각적으로 풍부한 과학 작업 흐름을 갖춘 현실적인 환경을 구축하고, 에이전트가 다양한 인터페이스를 통해 자율적으로 상호작용하도록 함(we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows...)",
            "169개의 높은 품질의 실제 작업을 포함하는 도전적인 벤치마크를 수집하고 평가함(a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans...)",
            "최신 에이전트의 강점을 활용해 평가를 수행하고, 현재의 한계를 분석하여 더 효과적인 설계 원칙을 제공함(Extensive evaluations of agents with state-of-the-art backbones show that, despite some promising results, they still fall short...)"
        ],
        "conclusion": "현재의 멀티모달 자율 에이전트는 복잡한 과학 작업 흐름에서 신뢰성 있는 지원이 부족하며, 이를 개선하기 위한 귀중한 통찰력을 제공함.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2505.21327",
            "authors": [
                {
                    "_id": "6836799db9b35de1c4a90d73",
                    "user": {
                        "_id": "64a3d1ddb3239f3e3892b24b",
                        "avatarUrl": "/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg",
                        "isPro": false,
                        "fullname": "Jiakang Yuan",
                        "user": "JiakangYuan",
                        "type": "user"
                    },
                    "name": "Jiakang Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:10:39.171Z",
                    "hidden": false
                },
                {
                    "_id": "6836799db9b35de1c4a90d74",
                    "user": {
                        "_id": "6538dd471ad9b3ba7c2df861",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538dd471ad9b3ba7c2df861/MbEa7KHAK6u7PRb7WiPUC.jpeg",
                        "isPro": false,
                        "fullname": "Tianshuo Peng",
                        "user": "Potentialts",
                        "type": "user"
                    },
                    "name": "Tianshuo Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:10:47.745Z",
                    "hidden": false
                },
                {
                    "_id": "6836799db9b35de1c4a90d75",
                    "user": {
                        "_id": "66838e043f885cb431c48fd9",
                        "avatarUrl": "/avatars/b14e238dabb99a692ce2189f1c7ec9c7.svg",
                        "isPro": false,
                        "fullname": "JiangYilei",
                        "user": "Yilei-Jiang",
                        "type": "user"
                    },
                    "name": "Yilei Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:11:01.744Z",
                    "hidden": false
                },
                {
                    "_id": "6836799db9b35de1c4a90d76",
                    "name": "Yiting Lu",
                    "hidden": false
                },
                {
                    "_id": "6836799db9b35de1c4a90d77",
                    "name": "Renrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "6836799db9b35de1c4a90d78",
                    "user": {
                        "_id": "67079840a9bcb7459b8d2a46",
                        "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
                        "isPro": false,
                        "fullname": "Kaituo Feng",
                        "user": "KaituoFeng",
                        "type": "user"
                    },
                    "name": "Kaituo Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:12:45.346Z",
                    "hidden": false
                },
                {
                    "_id": "6836799db9b35de1c4a90d79",
                    "name": "Chaoyou Fu",
                    "hidden": false
                },
                {
                    "_id": "6836799db9b35de1c4a90d7a",
                    "name": "Tao Chen",
                    "hidden": false
                },
                {
                    "_id": "6836799db9b35de1c4a90d7b",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "6836799db9b35de1c4a90d7c",
                    "user": {
                        "_id": "643dfd235aafbdca3a5792c0",
                        "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
                        "isPro": false,
                        "fullname": "Bo Zhang",
                        "user": "BoZhang",
                        "type": "user"
                    },
                    "name": "Bo Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:13:12.150Z",
                    "hidden": false
                },
                {
                    "_id": "6836799db9b35de1c4a90d7d",
                    "user": {
                        "_id": "666a8f24e2990b0cb16b7bf9",
                        "avatarUrl": "/avatars/fcbaf8f1e3e53a2a4a819b7cb2c53aa4.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Yue",
                        "user": "xyyue",
                        "type": "user"
                    },
                    "name": "Xiangyu Yue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:12:59.396Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T15:23:23.000Z",
            "submittedOnDailyAt": "2025-05-28T01:30:04.674Z",
            "title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
            "submittedOnDailyBy": {
                "_id": "64a3d1ddb3239f3e3892b24b",
                "avatarUrl": "/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg",
                "isPro": false,
                "fullname": "Jiakang Yuan",
                "user": "JiakangYuan",
                "type": "user"
            },
            "summary": "Logical reasoning is a fundamental aspect of human intelligence and an\nessential capability for multimodal large language models (MLLMs). Despite the\nsignificant advancement in multimodal reasoning, existing benchmarks fail to\ncomprehensively evaluate their reasoning abilities due to the lack of explicit\ncategorization for logical reasoning types and an unclear understanding of\nreasoning. To address these issues, we introduce MME-Reasoning, a comprehensive\nbenchmark designed to evaluate the reasoning ability of MLLMs, which covers all\nthree types of reasoning (i.e., inductive, deductive, and abductive) in its\nquestions. We carefully curate the data to ensure that each question\neffectively evaluates reasoning ability rather than perceptual skills or\nknowledge breadth, and extend the evaluation protocols to cover the evaluation\nof diverse questions. Our evaluation reveals substantial limitations of\nstate-of-the-art MLLMs when subjected to holistic assessments of logical\nreasoning capabilities. Even the most advanced MLLMs show limited performance\nin comprehensive logical reasoning, with notable performance imbalances across\nreasoning types. In addition, we conducted an in-depth analysis of approaches\nsuch as ``thinking mode'' and Rule-based RL, which are commonly believed to\nenhance reasoning abilities. These findings highlight the critical limitations\nand performance imbalances of current MLLMs in diverse logical reasoning\nscenarios, providing comprehensive and systematic insights into the\nunderstanding and evaluation of reasoning capabilities.",
            "upvotes": 64,
            "discussionId": "6836799fb9b35de1c4a90df0",
            "projectPage": "https://alpha-innovator.github.io/mmereasoning.github.io/",
            "githubRepo": "https://github.com/Alpha-Innovator/MME-Reasoning",
            "ai_summary": "MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.",
            "ai_keywords": [
                "multimodal large language models",
                "MME-Reasoning",
                "logical reasoning",
                "inductive reasoning",
                "deductive reasoning",
                "abductive reasoning",
                "reasoning ability",
                "thinking mode",
                "Rule-based RL"
            ]
        },
        "translation_title": "MME-Reasoning: MLLMs에서 논리 추론을 위한 포괄적인 벤치마크",
        "purpose": "MLLM의 논리 추론 능력을 포괄적으로 평가하기 위한 벤치마크 개발",
        "method": [
            "논리 추론 유형을 명확히 정의하고, 세 가지 유형의 추론(귀납적, 연역적, 외재적)을 포함한 질문을 설계함(To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning.)",
            "각 질문이 인지 능력이나 지식의 범위를 평가하는 것이 아니라 추론 능력을 효과적으로 평가하도록 데이터 선별함(We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth.)",
            "다양한 질문 평가를 위한 평가 프로토콜을 확장함(extend the evaluation protocols to cover the evaluation of diverse questions.)"
        ],
        "conclusion": "현재 MLLMs는 포괄적인 논리 추론 평가에서 상당한 한계를 보이며, 다양한 논리 추론 시나리오에서의 성능 불균형이 나타남.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2505.21497",
            "authors": [
                {
                    "_id": "68366e5a2ae719660434bb5a",
                    "user": {
                        "_id": "65164444bc0631719873af81",
                        "avatarUrl": "/avatars/dab8b90db8bbd00806268fe276e3ea36.svg",
                        "isPro": false,
                        "fullname": "Wei Pang",
                        "user": "weipang142857",
                        "type": "user"
                    },
                    "name": "Wei Pang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:58:18.507Z",
                    "hidden": false
                },
                {
                    "_id": "68366e5a2ae719660434bb5b",
                    "user": {
                        "_id": "64440be5af034cdfd69ca3a7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
                        "isPro": false,
                        "fullname": "Qinghong (Kevin) Lin",
                        "user": "KevinQHLin",
                        "type": "user"
                    },
                    "name": "Kevin Qinghong Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T10:11:26.491Z",
                    "hidden": false
                },
                {
                    "_id": "68366e5a2ae719660434bb5c",
                    "user": {
                        "_id": "636865b8cca0a0a962c21f3f",
                        "avatarUrl": "/avatars/ed0b5eb84ba91afa263c1069db25d909.svg",
                        "isPro": false,
                        "fullname": "Xiangru (Edward) Jian",
                        "user": "HideOnBush",
                        "type": "user"
                    },
                    "name": "Xiangru Jian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:58:16.627Z",
                    "hidden": false
                },
                {
                    "_id": "68366e5a2ae719660434bb5d",
                    "name": "Xi He",
                    "hidden": false
                },
                {
                    "_id": "68366e5a2ae719660434bb5e",
                    "user": {
                        "_id": "6565ed28a5ec0231cb07225f",
                        "avatarUrl": "/avatars/7f95bba9aa7811d56eecb380827abfac.svg",
                        "isPro": false,
                        "fullname": "prof philip torr",
                        "user": "philiptorr",
                        "type": "user"
                    },
                    "name": "Philip Torr",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:13:28.232Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T17:58:49.000Z",
            "submittedOnDailyAt": "2025-05-28T00:45:27.484Z",
            "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers",
            "submittedOnDailyBy": {
                "_id": "64440be5af034cdfd69ca3a7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
                "isPro": false,
                "fullname": "Qinghong (Kevin) Lin",
                "user": "KevinQHLin",
                "type": "user"
            },
            "summary": "Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.",
            "upvotes": 55,
            "discussionId": "68366e5d2ae719660434bc70",
            "projectPage": "https://paper2poster.github.io/",
            "githubRepo": "https://github.com/Paper2Poster/Paper2Poster",
            "ai_summary": "A benchmark and metric suite for poster generation evaluates visual quality, coherence, and content accuracy, leading to a multi-agent pipeline that outperforms existing models with reduced computational cost.",
            "ai_keywords": [
                "top-down pipeline",
                "multi-agent pipeline",
                "VLM-as-judge",
                "binary-tree layout",
                "rendering code",
                "VLM feedback",
                "parser",
                "planner",
                "painter-commenter loop",
                "GPT-4",
                "Qwen-2.5",
                "automated poster-generation models"
            ]
        },
        "translation_title": "Paper2Poster: 과학 논문에서 다중 모달 포스터 자동화로 나아가기",
        "purpose": "과학 커뮤니케이션을 위한 포스터 생성의 효율성을 높이기 위해, 논문 내용을 압축하여 시각적으로 일관된 페이지로 만드는 작업을 개선하려 함",
        "method": [
            "포스터 생성을 위한 기준 및 평가 지표를 제시하며, 최근 회의 논문과 저자가 디자인한 포스터를 쌍으로 수집함(we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters)",
            "VLM을 사용하여 포스터의 비주얼 품질과 언어 유창성을 평가하고 문서의 핵심 내용을 전달하는 능력을 측정하기 위해 PaperQuiz를 설정함(PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes)",
            "PosterAgent라는 파이프라인을 통해 논문을 구조화된 자산 라이브러리로 압축하고, 텍스트-비주얼 쌍을 이진 트리 레이아웃으로 정렬 및 최적화함(we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a) Parser distills the paper into a structured asset library)"
        ],
        "conclusion": "PosterAgent는 기존의 다중 에이전트 시스템보다 우수한 성능을 보이며, 22페이지 논문을 손쉽게 편집 가능한 포스터로 변환 가능하다는 점에서 자동화된 포스터 생성 모델의 다음 방향을 제시하였음.",
        "keywords": [
            "Multimodal Learning",
            "Visual Language Models",
            "Document Parsing"
        ]
    },
    {
        "paper": {
            "id": "2505.19641",
            "authors": [
                {
                    "_id": "683686a4bec1d6dbb3d8728d",
                    "user": {
                        "_id": "6493fbb3085e14d7933b936d",
                        "avatarUrl": "/avatars/85723bedac9e81fecc33b36ff94ecada.svg",
                        "isPro": false,
                        "fullname": "Junteng Liu",
                        "user": "Junteng",
                        "type": "user"
                    },
                    "name": "Junteng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:12.066Z",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d8728e",
                    "user": {
                        "_id": "64813e4b3fb124fc98503a7e",
                        "avatarUrl": "/avatars/f871b7d84f04f827041d4a23cb1cdc9f.svg",
                        "isPro": false,
                        "fullname": "Yuanxiang Fan",
                        "user": "ShiroFFF",
                        "type": "user"
                    },
                    "name": "Yuanxiang Fan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:18.907Z",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d8728f",
                    "name": "Zhuo Jiang",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87290",
                    "name": "Han Ding",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87291",
                    "name": "Yongyi Hu",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87292",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87293",
                    "name": "Yiqi Shi",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87294",
                    "name": "Shitong Weng",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87295",
                    "name": "Aili Chen",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87296",
                    "user": {
                        "_id": "62bf99cae140faaa85335ab8",
                        "avatarUrl": "/avatars/bcd2aa1823995b385096e2a68ce3e071.svg",
                        "isPro": false,
                        "fullname": "Shiqi Chen",
                        "user": "ShiqiChen",
                        "type": "user"
                    },
                    "name": "Shiqi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:58.772Z",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87297",
                    "user": {
                        "_id": "660e95ac3ee7a71ef0107347",
                        "avatarUrl": "/avatars/f96fb06158e8953a0d5898021ea1fe35.svg",
                        "isPro": false,
                        "fullname": "Yu-Nan Huang",
                        "user": "YN83",
                        "type": "user"
                    },
                    "name": "Yunan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:45.974Z",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87298",
                    "user": {
                        "_id": "62f3baf389ded29a9f93876e",
                        "avatarUrl": "/avatars/6c6cfbee17782bb9b37ccfc79e2f15b2.svg",
                        "isPro": false,
                        "fullname": "Mozhi Zhang",
                        "user": "zhangmozhi",
                        "type": "user"
                    },
                    "name": "Mozhi Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:40.524Z",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87299",
                    "name": "Pengyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d8729a",
                    "user": {
                        "_id": "63390ce41718795719635b1e",
                        "avatarUrl": "/avatars/ad03a2b349f01c1ac1fedfb95d02d43e.svg",
                        "isPro": false,
                        "fullname": "JunjieYan",
                        "user": "JunjieYan",
                        "type": "user"
                    },
                    "name": "Junjie Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:34.200Z",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d8729b",
                    "user": {
                        "_id": "615f34ec3f6d24d67c1b5c78",
                        "avatarUrl": "/avatars/6dcff6477993d9e57c5cb92b6f95eb66.svg",
                        "isPro": false,
                        "fullname": "Junxian He",
                        "user": "jxhe",
                        "type": "user"
                    },
                    "name": "Junxian He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:27.539Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T07:59:36.000Z",
            "submittedOnDailyAt": "2025-05-28T06:37:58.638Z",
            "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning\n  Logical Reasoning and Beyond",
            "submittedOnDailyBy": {
                "_id": "676e38ad04af5bec20bc9faf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/676e38ad04af5bec20bc9faf/AG8Q9wAUzGtPWyjd5QO2l.jpeg",
                "isPro": false,
                "fullname": "MiniMax",
                "user": "MiniMax-AI",
                "type": "user"
            },
            "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.",
            "upvotes": 37,
            "discussionId": "683686a5bec1d6dbb3d872c8",
            "projectPage": "https://huggingface.co/datasets/MiniMaxAI/SynLogic",
            "githubRepo": "https://github.com/MiniMax-AI/SynLogic",
            "ai_summary": "SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.",
            "ai_keywords": [
                "Reinforcement Learning (RL)",
                "Large Language Models (LLMs)",
                "Logical Reasoning",
                "Data Synthesis",
                "BBEH",
                "Mixed Training",
                "DeepSeek-R1",
                "DeepSeek-R1-Distill-Qwen-32B",
                "DeepSeek-R1-Zero-Qwen-32B"
            ]
        },
        "translation_title": "SynLogic: 대량으로 검증 가능한 추론 데이터 생성 프레임워크",
        "purpose": "일반적인 추론 능력을 개발하기 위한 다양한 검증 가능한 추론 데이터를 수집하려는 목표",
        "method": [
            "SynLogic이라는 데이터 합성 프레임워크와 데이터세트를 제안하여 35개의 다양한 논리적 추론 작업을 포함한 대량의 데이터를 생성함(we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks.)",
            "데이터의 난이도와 양을 조절할 수 있는 통제된 합성을 가능하게 함(The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity.)",
            "모든 예제는 간단한 규칙으로 검증 가능하여 RL에 적합한 보상을 받을 수 있도록 함(Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards.)"
        ],
        "conclusion": "SynLogic은 공개 데이터셋 중에서 논리적 추론 성능을 최첨단으로 개선했으며, 기존의 DeepSeek-R1-Zero-Qwen-32B 모델보다 여러 벤치마크에서 성능을 능가함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Reinforcement Learning"
        ]
    },
    {
        "paper": {
            "id": "2505.21189",
            "authors": [
                {
                    "_id": "6836babd75a4c5486bac4149",
                    "user": {
                        "_id": "672e0638ee49faac3ad53af7",
                        "avatarUrl": "/avatars/3a273b4beba8286309296a1e25bc34a9.svg",
                        "isPro": false,
                        "fullname": "Gleb Mezentsev",
                        "user": "glebzok",
                        "type": "user"
                    },
                    "name": "Gleb Mezentsev",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-28T07:43:05.521Z",
                    "hidden": false
                },
                {
                    "_id": "6836babd75a4c5486bac414a",
                    "user": {
                        "_id": "6169a581d05945bfd8718dfa",
                        "avatarUrl": "/avatars/1892ab06a7ddb557232777de3cbec470.svg",
                        "isPro": false,
                        "fullname": "Ivan Oseledets",
                        "user": "oseledets",
                        "type": "user"
                    },
                    "name": "Ivan Oseledets",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:02.162Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/672e0638ee49faac3ad53af7/M2gWNMdYYUjkCuamqmHJ4.png",
                "https://cdn-uploads.huggingface.co/production/uploads/672e0638ee49faac3ad53af7/ZAl4A9HCN24-VRZwER0H1.png",
                "https://cdn-uploads.huggingface.co/production/uploads/672e0638ee49faac3ad53af7/zFXr1JmOc8jHoNtVJoiSM.png"
            ],
            "publishedAt": "2025-05-27T13:39:24.000Z",
            "submittedOnDailyAt": "2025-05-28T06:03:21.363Z",
            "title": "Exploring the Latent Capacity of LLMs for One-Step Text Generation",
            "submittedOnDailyBy": {
                "_id": "672e0638ee49faac3ad53af7",
                "avatarUrl": "/avatars/3a273b4beba8286309296a1e25bc34a9.svg",
                "isPro": false,
                "fullname": "Gleb Mezentsev",
                "user": "glebzok",
                "type": "user"
            },
            "summary": "A recent study showed that large language models (LLMs) can reconstruct\nsurprisingly long texts - up to thousands of tokens - via autoregressive\ngeneration from just one specially trained input embedding. In this work, we\nexplore whether such reconstruction is possible without autoregression. We show\nthat frozen LLMs can generate hundreds of accurate tokens in just one forward\npass, when provided with only two learned embeddings. This reveals a surprising\nand underexplored capability of LLMs - multi-token generation without iterative\ndecoding. We investigate the behaviour of these embeddings and provide insight\ninto the type of information they encode. We also empirically show that\nalthough these representations are not unique for a given text, they form\nconnected and local regions in embedding space - a property that suggests the\npotential of learning a dedicated encoder into that space.",
            "upvotes": 34,
            "discussionId": "6836babe75a4c5486bac4170",
            "githubRepo": "https://github.com/Glebzok/OneStepLLMGeneration",
            "ai_summary": "LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.",
            "ai_keywords": [
                "large language models",
                "autoregressive generation",
                "input embedding",
                "frozen LLMs",
                "multi-token generation",
                "iterative decoding",
                "learned embeddings",
                "embedding space",
                "dedicated encoder"
            ]
        },
        "translation_title": "LLMs의 잠재 용량 탐색: 일회성 텍스트 생성",
        "purpose": "자동 회귀 없이도 LLM이 긴 텍스트를 생성할 수 있는 가능성을 연구",
        "method": [
            "고정된 LLM을 사용하여 두 개의 학습된 임베딩만으로 수백 개의 정확한 토큰을 한 번의 전방 패스에서 생성하는 방법을 보여줌(We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings.)",
            "이러한 임베딩의 동작을 조사하고 그들이 인코딩하는 정보 유형에 대한 통찰을 제공함(This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding.)"
        ],
        "conclusion": "LLM의 이러한 특성은 반복적인 디코딩 없이도 다중 토큰 생성을 가능하게 하며, 더 전용 인코더 학습의 잠재력을 시사함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    }
]