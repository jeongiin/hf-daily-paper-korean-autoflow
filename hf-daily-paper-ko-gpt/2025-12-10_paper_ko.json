[
    {
        "paper": {
            "id": "2512.08765",
            "authors": [
                {
                    "_id": "6938da63dfc35938ba129f3c",
                    "user": {
                        "_id": "642e3bcb958faf258a40e89c",
                        "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg",
                        "isPro": false,
                        "fullname": "Ruihang Chu",
                        "user": "Ruihang",
                        "type": "user"
                    },
                    "name": "Ruihang Chu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T09:42:07.767Z",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f3d",
                    "name": "Yefei He",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f3e",
                    "user": {
                        "_id": "62d812e143df7719860d05d1",
                        "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg",
                        "isPro": false,
                        "fullname": "zhekai chen",
                        "user": "Azily",
                        "type": "user"
                    },
                    "name": "Zhekai Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T09:42:00.513Z",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f3f",
                    "name": "Shiwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f40",
                    "user": {
                        "_id": "637ee45b2438d7485b8d8f6a",
                        "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg",
                        "isPro": false,
                        "fullname": "Xiaogang Xu",
                        "user": "xiaogang00",
                        "type": "user"
                    },
                    "name": "Xiaogang Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T09:41:51.241Z",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f41",
                    "name": "Bin Xia",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f42",
                    "name": "Dingdong Wang",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f43",
                    "name": "Hongwei Yi",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f44",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T09:41:32.582Z",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f45",
                    "user": {
                        "_id": "690090cca41c454e4786c0e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png",
                        "isPro": false,
                        "fullname": "Hengshuang Zhao",
                        "user": "Hengshuang",
                        "type": "user"
                    },
                    "name": "Hengshuang Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T09:41:26.372Z",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f46",
                    "name": "Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f47",
                    "name": "Yingya Zhang",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f48",
                    "user": {
                        "_id": "64ca1fe838837b12d5e529b7",
                        "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg",
                        "isPro": false,
                        "fullname": "Yujiu Yang",
                        "user": "Thu-redrobot",
                        "type": "user"
                    },
                    "name": "Yujiu Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T09:41:10.566Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"
            ],
            "publishedAt": "2025-12-09T16:13:55.000Z",
            "submittedOnDailyAt": "2025-12-10T00:20:18.797Z",
            "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.",
            "upvotes": 90,
            "discussionId": "6938da64dfc35938ba129f49",
            "githubRepo": "https://github.com/ali-vilab/Wan-Move",
            "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.",
            "ai_keywords": [
                "motion control",
                "video generative models",
                "dense point trajectories",
                "latent space",
                "spatiotemporal feature map",
                "motion guidance",
                "image-to-video model",
                "auxiliary motion encoders",
                "fine-tuning",
                "MoveBench",
                "motion annotations"
            ],
            "githubStars": 160,
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "translation_title": "Wan-Move: 잠재 궤적 안내를 통한 모션 제어 비디오 생성",
        "purpose": "비디오 생성 모델에 모션 제어 기능 추가 및 향상된 사용성을 위한 연구",
        "method": [
            "객체의 움직임을 밀집 포인트 궤적으로 표현하여 세밀한 제어 구현(Our core idea is to directly make the original condition features motion-aware for guiding video synthesis.)",
            "이 궤적을 잠재 공간에 투영하고 첫 번째 프레임의 특징을 따라 전파하여 정렬된 시공간 특징 맵 생성(We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map.)",
            "기존 이미지-비디오 모델에 이 특징 맵을 통합하여 별도의 아키텍처 변경 없이 모션 가이드를 구현(This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change.)"
        ],
        "conclusion": "Wan-Move는 5초 길이의 480p 비디오에서 뛰어난 모션 제어 품질을 보여주며, MoveBench 벤치마크를 통해 종합적인 평가에서 우수성을 입증하였습니다.",
        "keywords": [
            "Video Generation",
            "Motion Control",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.08478",
            "authors": [
                {
                    "_id": "6938e00fdfc35938ba129f4f",
                    "name": "Yuning Gong",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f50",
                    "name": "Yifei Liu",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f51",
                    "name": "Yifan Zhan",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f52",
                    "name": "Muyao Niu",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f53",
                    "name": "Xueying Li",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f54",
                    "name": "Yuanjun Liao",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f55",
                    "name": "Jiaming Chen",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f56",
                    "name": "Yuanyuan Gao",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f57",
                    "name": "Jiaqi Chen",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f58",
                    "name": "Minming Chen",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f59",
                    "name": "Li Zhou",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f5a",
                    "name": "Yuning Zhang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f5b",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f5c",
                    "name": "Xiaoqing Hou",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f5d",
                    "name": "Huaxi Huang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f5e",
                    "name": "Shixiang Tang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f5f",
                    "name": "Le Ma",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f60",
                    "name": "Dingwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f61",
                    "name": "Xue Yang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f62",
                    "name": "Junchi Yan",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f63",
                    "name": "Yanchi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f64",
                    "name": "Yinqiang Zheng",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f65",
                    "name": "Xiao Sun",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f66",
                    "user": {
                        "_id": "6938f4de790b5cd0f6df6462",
                        "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg",
                        "isPro": false,
                        "fullname": "Zhihang Zhong",
                        "user": "Zuica96",
                        "type": "user"
                    },
                    "name": "Zhihang Zhong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T08:56:28.162Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"
            ],
            "publishedAt": "2025-12-09T10:54:58.000Z",
            "submittedOnDailyAt": "2025-12-10T07:43:37.566Z",
            "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
            "submittedOnDailyBy": {
                "_id": "6938f4de790b5cd0f6df6462",
                "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg",
                "isPro": false,
                "fullname": "Zhihang Zhong",
                "user": "Zuica96",
                "type": "user"
            },
            "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.",
            "upvotes": 61,
            "discussionId": "6938e00fdfc35938ba129f67",
            "projectPage": "https://visionary-laboratory.github.io/visionary/",
            "githubRepo": "https://github.com/Visionary-Laboratory/visionary",
            "ai_summary": "Visionary is an open web-native platform enabling real-time rendering of 3D Gaussian Splatting and meshes with efficient GPU-based inference, supporting dynamic content and generative models.",
            "ai_keywords": [
                "Neural rendering",
                "3D Gaussian Splatting",
                "3DGS",
                "WebGPU",
                "ONNX inference",
                "Gaussian Generator contract",
                "three.js",
                "TypeScript API",
                "MLP-based 3DGS",
                "4DGS",
                "neural avatars",
                "style transformation",
                "GPU-based primitive sorting",
                "World Model Carrier"
            ],
            "githubStars": 109
        },
        "translation_title": "Visionary: WebGPU 기반 가우시안 스플래팅 플랫폼을 활용한 월드 모델 캐리어",
        "purpose": "실시간 다양한 가우시안 스플래팅 및 메쉬 렌더링을 지원하는 웹 기반 플랫폼 개발",
        "method": [
            "효율적인 WebGPU 렌더러를 사용하여 동적 신경 처리를 가능하게 함(Built on an efficient WebGPU renderer with per-frame ONNX inference.)",
            "표준화된 가우시안 생성기 계약을 도입하여 각 프레임마다 가우시안을 생성하거나 업데이트 할 수 있도록 함(It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame.)",
            "three.js 라이브러리에 간결한 TypeScript API를 제공하여 기존 웹 애플리케이션에 원활하게 통합되도록 함(The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications.)"
        ],
        "conclusion": "Visionary는 3DGS 자산에 대해 현재 웹 뷰어보다 더 나은 렌더링 효율성을 달성하여 3DGS 관련 방법의 재현, 비교 및 배포 장벽을 크게 낮추는 데 기여함.",
        "keywords": [
            "Computer Vision",
            "Image Generation",
            "3D Vision"
        ]
    },
    {
        "paper": {
            "id": "2512.07951",
            "authors": [
                {
                    "_id": "6938e892dfc35938ba129ff5",
                    "name": "Zekai Luo",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ff6",
                    "name": "Zongze Du",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ff7",
                    "name": "Zhouhang Zhu",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ff8",
                    "name": "Hao Zhong",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ff9",
                    "user": {
                        "_id": "632179745fc60c44fd91fc33",
                        "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
                        "isPro": false,
                        "fullname": "zhumuzhi",
                        "user": "Z-MU-Z",
                        "type": "user"
                    },
                    "name": "Muzhi Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T10:51:43.541Z",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ffa",
                    "name": "Wen Wang",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ffb",
                    "name": "Yuling Xi",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ffc",
                    "name": "Chenchen Jing",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ffd",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ffe",
                    "name": "Chunhua Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-08T19:00:04.000Z",
            "submittedOnDailyAt": "2025-12-10T00:59:39.366Z",
            "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality",
            "submittedOnDailyBy": {
                "_id": "632179745fc60c44fd91fc33",
                "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
                "isPro": false,
                "fullname": "zhumuzhi",
                "user": "Z-MU-Z",
                "type": "user"
            },
            "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap",
            "upvotes": 36,
            "discussionId": "6938e892dfc35938ba129fff",
            "projectPage": "https://aim-uofa.github.io/LivingSwap",
            "ai_summary": "LivingSwap enhances video face swapping by using keyframes and reference guidance to maintain identity and fidelity over long sequences, reducing manual effort and achieving state-of-the-art results.",
            "ai_keywords": [
                "keyframe conditioning",
                "video reference guidance",
                "temporal stitching",
                "identity preservation",
                "high-fidelity reconstruction",
                "Face2Face dataset"
            ],
            "organization": {
                "_id": "61bac2af530e5c78d7b99667",
                "name": "zju",
                "fullname": "Zhejiang University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
            }
        },
        "translation_title": "소스 비디오 현실성 보존: 영화 품질을 위한 고충실도 얼굴 교환",
        "purpose": "긴 복잡한 비디오 시퀀스에서 높은 충실도와 시간 일관성을 유지하기 위한 얼굴 교환 방법 연구",
        "method": [
            "최근의 참조 중심 이미지 편집 기법에서 영감을 받아 소스 비디오의 시각적 속성을 활용해 얼굴 교환의 충실도와 시간 일관성을 향상시킴(we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping.)",
            "키프레임을 조건 신호로 사용해 목표 정체성을 주입하고 유연하고 제어 가능한 편집을 가능하게 함(Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing.)",
            "제안한 모델은 긴 비디오 시퀀스에서 안정적인 정체성 보존과 고충실도 재구성을 위해 시간 스티칭을 수행함(The model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences.)"
        ],
        "conclusion": "제안한 방법은 소스 비디오의 표현, 조명, 동작과 목표 정체성을 매끄럽게 통합하며, 제작 워크플로우에서 수작업 노력을 크게 줄이는 성과를 거둠.",
        "keywords": [
            "Video Generation",
            "Image Generation",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2512.07802",
            "authors": [
                {
                    "_id": "69392ceedfc35938ba12a187",
                    "user": {
                        "_id": "65e5eae6958b39864e8b683e",
                        "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg",
                        "isPro": true,
                        "fullname": "Zhaochong An",
                        "user": "ZhaochongAn",
                        "type": "user"
                    },
                    "name": "Zhaochong An",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T13:07:50.839Z",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a188",
                    "name": "Menglin Jia",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a189",
                    "name": "Haonan Qiu",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a18a",
                    "name": "Zijian Zhou",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a18b",
                    "name": "Xiaoke Huang",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a18c",
                    "name": "Zhiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a18d",
                    "name": "Weiming Ren",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a18e",
                    "user": {
                        "_id": "65a12ec4c9873890d15c4ab9",
                        "avatarUrl": "/avatars/ce4d46f575ba757f78eabdb25b394171.svg",
                        "isPro": false,
                        "fullname": "Kumara Kahatapitiya",
                        "user": "kumarak",
                        "type": "user"
                    },
                    "name": "Kumara Kahatapitiya",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T10:51:00.011Z",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a18f",
                    "name": "Ding Liu",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a190",
                    "name": "Sen He",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a191",
                    "name": "Chenyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a192",
                    "name": "Tao Xiang",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a193",
                    "name": "Fanny Yang",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a194",
                    "name": "Serge Belongie",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a195",
                    "name": "Tian Xie",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65e5eae6958b39864e8b683e/iJ5L2BbZWW8XiE7ORZuFK.mp4"
            ],
            "publishedAt": "2025-12-08T18:32:24.000Z",
            "submittedOnDailyAt": "2025-12-10T06:06:45.364Z",
            "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
            "submittedOnDailyBy": {
                "_id": "65e5eae6958b39864e8b683e",
                "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg",
                "isPro": true,
                "fullname": "Zhaochong An",
                "user": "ZhaochongAn",
                "type": "user"
            },
            "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.",
            "upvotes": 27,
            "discussionId": "69392ceedfc35938ba12a196",
            "projectPage": "https://zhaochongan.github.io/projects/OneStory/",
            "ai_summary": "OneStory generates coherent multi-shot videos by modeling global cross-shot context through a Frame Selection module and an Adaptive Conditioner, leveraging pretrained image-to-video models and a curated dataset.",
            "ai_keywords": [
                "multi-shot video generation",
                "next-shot generation",
                "autoregressive shot synthesis",
                "pretrained image-to-video models",
                "Frame Selection module",
                "Adaptive Conditioner",
                "semantically-relevant global memory",
                "importance-guided patchification",
                "referential captions",
                "text-conditioned",
                "image-conditioned"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "translation_title": "OneStory: 적응 메모리를 활용한 일관된 다중 장면 비디오 생성",
        "purpose": "장면 간의 문맥을 효과적으로 모델링하여 일관된 서사를 생성하기 위한 방법을 개발하고자 함.",
        "method": [
            "MSV를 다음 장면 생성 과제로 재정의하여 자율적으로 장면 합성을 가능하게 함(OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis)",
            "중요한 프레임을 기반으로 세멘틱하게 관련된 글로벌 메모리를 구성하는 Frame Selection 모듈을 도입함(We introduce a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots.)",
            "Importance-guided patchification을 통해 compact context를 생성하는 Adaptive Conditioner 모듈을 구현함(An Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning.)"
        ],
        "conclusion": "OneStory는 다양한 복잡한 장면에서 텍스트 및 이미지 조건 설정을 이용하여 뛰어난 서사 일관성을 달성, 통제 가능한 몰입형 비디오 스토리텔링을 가능하게 함.",
        "keywords": [
            "Video Generation",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.07843",
            "authors": [
                {
                    "_id": "6938e51ddfc35938ba129fb2",
                    "user": {
                        "_id": "63797c273f575acc2f6893c0",
                        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
                        "isPro": true,
                        "fullname": "Long(Tony) Lian",
                        "user": "longlian",
                        "type": "user"
                    },
                    "name": "Long Lian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T08:56:15.315Z",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb3",
                    "name": "Sida Wang",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb4",
                    "user": {
                        "_id": "6417cf37dce1e4c0229f17b1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417cf37dce1e4c0229f17b1/7h-ZCB5f4wif7TsnF-B1M.jpeg",
                        "isPro": false,
                        "fullname": "Felix Xu",
                        "user": "katanaxu",
                        "type": "user"
                    },
                    "name": "Felix Juefei-Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T13:07:56.324Z",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb5",
                    "name": "Tsu-Jui Fu",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb6",
                    "name": "Xiuyu Li",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb7",
                    "name": "Adam Yala",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb8",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb9",
                    "name": "Alane Suhr",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fba",
                    "name": "Yuandong Tian",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fbb",
                    "name": "Xi Victoria Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T18:55:59.000Z",
            "submittedOnDailyAt": "2025-12-10T02:14:20.520Z",
            "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.",
            "upvotes": 13,
            "discussionId": "6938e51edfc35938ba129fbc",
            "ai_summary": "ThreadWeaver, a framework for adaptive parallel reasoning, achieves accuracy comparable to sequential models while reducing inference latency through parallel trajectory generation, trie-based training-inference co-design, and parallelization-aware reinforcement learning.",
            "ai_keywords": [
                "Large Language Models",
                "sequential decoding",
                "adaptive parallel reasoning",
                "long chain-of-thought",
                "CoT",
                "ThreadWeaver",
                "parallel trajectory generator",
                "trie-based training-inference co-design",
                "parallelization-aware reinforcement learning",
                "Qwen3-8B",
                "AIME24"
            ]
        },
        "translation_title": "ThreadWeaver: 효율적인 병렬 추론을 위한 적응형 스레딩 기법",
        "purpose": "병렬 추론을 적용하여 대형 언어 모델의 추론 효율성을 높이고자 함",
        "method": [
            "ThreadWeaver 프레임워크를 도입하여, 기존의 순차적 추론 모델과 유사한 정확도를 유지하면서 추론 지연을 크게 줄임(We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency.)",
            "병렬 주석을 가진 대규모 고품질 체인-오브-생각 데이터 생성 및 감독 유사 학습을 위한 두 단계 병렬 궤적 생성기를 사용함(ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning.)",
            "트리 기반의 훈련-추론 공동 설계를 통해 기존의 자동 회귀 추론 엔진에서 추가 변경 없이 병렬 추론 수행 가능하게 함(and 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches.)",
            "병렬화에 대한 인식이 있는 강화 학습 프레임워크를 통해 정확도와 병렬화의 균형을 맞춤(3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization."
        ],
        "conclusion": "ThreadWeaver는 수치적으로는 최신 순차적 추론 모델과 유사한 정확도를 유지하며, 평균 1.53배의 속도 향상을 달성하여 정확성과 효율성 간의 새로운 Pareto 경계를 설정함.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Natural Language Processing"
        ]
    }
]