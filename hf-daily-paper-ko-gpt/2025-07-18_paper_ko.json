[
    {
        "paper": {
            "id": "2507.13334",
            "authors": [
                {
                    "_id": "6879aad021b37e676c8e406b",
                    "user": {
                        "_id": "63120517ae8896941da4c5da",
                        "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
                        "isPro": false,
                        "fullname": "Lingrui Mei",
                        "user": "Chevalier",
                        "type": "user"
                    },
                    "name": "Lingrui Mei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-18T07:49:58.423Z",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e406c",
                    "user": {
                        "_id": "671f9cd9ff056a1b49444f37",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/B3Z9oiFb79Gi-_YXKP13u.png",
                        "isPro": false,
                        "fullname": "duoduo yao",
                        "user": "Theodyy",
                        "type": "user"
                    },
                    "name": "Jiayu Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-18T07:49:48.326Z",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e406d",
                    "user": {
                        "_id": "656ad93853703dd78f3de7b8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/r6VB6ICND1td3wFOy8pnz.jpeg",
                        "isPro": false,
                        "fullname": "YuyaoGe",
                        "user": "YuyaoGe",
                        "type": "user"
                    },
                    "name": "Yuyao Ge",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-18T07:49:54.689Z",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e406e",
                    "name": "Yiwei Wang",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e406f",
                    "name": "Baolong Bi",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e4070",
                    "name": "Yujun Cai",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e4071",
                    "user": {
                        "_id": "687382c6e5468c287165b2dc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/N9nIkaHWkeML8PgbQHzHy.png",
                        "isPro": false,
                        "fullname": "Liujz",
                        "user": "juhyun13",
                        "type": "user"
                    },
                    "name": "Jiazhi Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-18T12:31:49.586Z",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e4072",
                    "user": {
                        "_id": "6720cf97a0396f933ec93ab8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NAdPIISe9-TYGGM0nAgsb.png",
                        "isPro": false,
                        "fullname": "Li Max",
                        "user": "LImax72",
                        "type": "user"
                    },
                    "name": "Mingyu Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-18T07:49:56.619Z",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e4073",
                    "name": "Zhong-Zhi Li",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e4074",
                    "user": {
                        "_id": "662383a20edeabfe3b64a6a5",
                        "avatarUrl": "/avatars/a76da726002d853dd08a51a6af6311d9.svg",
                        "isPro": false,
                        "fullname": "Duzhen Zhang",
                        "user": "ShowerMaker",
                        "type": "user"
                    },
                    "name": "Duzhen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-18T07:49:50.488Z",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e4075",
                    "user": {
                        "_id": "669e27902dbf53ccd23ae47f",
                        "avatarUrl": "/avatars/5c193b542dcfe2e64467fa5c686f3e20.svg",
                        "isPro": false,
                        "fullname": "chenlin",
                        "user": "tvstfe",
                        "type": "user"
                    },
                    "name": "Chenlin Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-18T07:49:52.410Z",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e4076",
                    "name": "Jiayi Mao",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e4077",
                    "name": "Tianze Xia",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e4078",
                    "name": "Jiafeng Guo",
                    "hidden": false
                },
                {
                    "_id": "6879aad021b37e676c8e4079",
                    "name": "Shenghua Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-17T17:50:36.000Z",
            "submittedOnDailyAt": "2025-07-18T00:52:14.092Z",
            "title": "A Survey of Context Engineering for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "63120517ae8896941da4c5da",
                "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
                "isPro": false,
                "fullname": "Lingrui Mei",
                "user": "Chevalier",
                "type": "user"
            },
            "summary": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1300 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI.",
            "upvotes": 84,
            "discussionId": "6879aad021b37e676c8e407a",
            "githubRepo": "https://github.com/Meirtz/Awesome-Context-Engineering",
            "ai_summary": "Context Engineering systematically optimizes information payloads for Large Language Models, addressing gaps in generating sophisticated, long-form outputs.",
            "ai_keywords": [
                "Context Engineering",
                "context retrieval",
                "context generation",
                "context processing",
                "context management",
                "retrieval-augmented generation",
                "memory systems",
                "tool-integrated reasoning",
                "multi-agent systems"
            ],
            "githubStars": 203
        },
        "translation_title": "대규모 언어 모델을 위한 컨텍스트 엔지니어링 조사",
        "purpose": "컨텍스트 정보를 최적화하여 대규모 언어 모델의 성능을 향상시키기 위한 연구",
        "method": [
            "컨텍스트 엔지니어링의 기초 요소인 컨텍스트 검색 및 생성, 컨텍스트 처리 및 관리 검토(We first examine the foundational components: context retrieval and generation, context processing and context management.)",
            "이 기초 요소들을 결합하여 RAG, 메모리 시스템, 도구 통합 추론 및 다중 에이전트 시스템과 같은 복잡한 시스템 구현을 탐구함(We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems.)",
            "1300편 이상의 연구 논문 분석을 통해 기술 로드맵을 제시하고 연구의 중요한 격차를 공개함(Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap.)"
        ],
        "conclusion": "이 조사는 향후 연구에서 다루어야 할 우선 과제가 복잡한 맥락 이해에 능숙한 모델의 생성 능력 제한이 있음을 보여줌.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2507.13348",
            "authors": [
                {
                    "_id": "6879ba2021b37e676c8e40c9",
                    "user": {
                        "_id": "6527b7280ae663e384eb8499",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b7280ae663e384eb8499/73yF3eu2cUx7jVZrhXnXx.jpeg",
                        "isPro": false,
                        "fullname": "Senqiao Yang",
                        "user": "Senqiao",
                        "type": "user"
                    },
                    "name": "Senqiao Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-18T12:43:44.095Z",
                    "hidden": false
                },
                {
                    "_id": "6879ba2021b37e676c8e40ca",
                    "user": {
                        "_id": "6614a9a9ef2015c1b33d3095",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614a9a9ef2015c1b33d3095/_rxcYbY47yvHZrB1EpmrU.jpeg",
                        "isPro": false,
                        "fullname": "JunyiLi",
                        "user": "junyili",
                        "type": "user"
                    },
                    "name": "Junyi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-18T12:43:50.898Z",
                    "hidden": false
                },
                {
                    "_id": "6879ba2021b37e676c8e40cb",
                    "name": "Xin Lai",
                    "hidden": false
                },
                {
                    "_id": "6879ba2021b37e676c8e40cc",
                    "name": "Bei Yu",
                    "hidden": false
                },
                {
                    "_id": "6879ba2021b37e676c8e40cd",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6879ba2021b37e676c8e40ce",
                    "user": {
                        "_id": "6848d10fb95a61a03a3f9ee4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/fUwUJjGLkEDtILRefxkMZ.jpeg",
                        "isPro": false,
                        "fullname": "Jia",
                        "user": "Jiaya1",
                        "type": "user"
                    },
                    "name": "Jiaya Jia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-18T12:44:13.346Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-17T17:59:55.000Z",
            "submittedOnDailyAt": "2025-07-18T01:47:35.200Z",
            "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "6527b7280ae663e384eb8499",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b7280ae663e384eb8499/73yF3eu2cUx7jVZrhXnXx.jpeg",
                "isPro": false,
                "fullname": "Senqiao Yang",
                "user": "Senqiao",
                "type": "user"
            },
            "summary": "Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.",
            "upvotes": 48,
            "discussionId": "6879ba2121b37e676c8e40cf",
            "githubRepo": "https://github.com/dvlab-research/VisionThink",
            "ai_summary": "VisionThink dynamically adjusts image resolution and visual token processing for efficient and effective vision-language tasks, improving performance on OCR tasks while reducing token usage in simpler tasks.",
            "ai_keywords": [
                "vision-language models",
                "visual tokens",
                "text tokens",
                "downsampled image",
                "smart decision-making",
                "special token",
                "Efficient VLM",
                "token compression",
                "reinforcement learning",
                "LLM-as-Judge",
                "reward function",
                "penalty mechanism",
                "image resize call ratio"
            ],
            "githubStars": 73
        },
        "translation_title": "VisionThink: 강화 학습을 통한 스마트하고 효율적인 비전 언어 모델",
        "purpose": "비전-언어 모델의 성능을 개선하고시스템의 효율성을 높이기 위해 시각적 토큰을 동적으로 처리하는 새로운 패러다임을 제안",
        "method": [
            "다양한 해상도의 표본을 동적으로 처리하는 방법을 제안함(we propose to dynamically process distinct samples with different resolutions)",
            "저해상도 이미지를 시작으로 문제 해결에 충분한지 결정하고, 필요 시 고해상도 이미지를 요청할 수 있는 특수 토큰을 출력함(Nevertheless, the model could output a special token to request the higher-resolution image)",
            "강화 학습을 적용하고 안정적인 이미지 리사이즈 호출 비율을 달성하기 위해 보상 함수와 패널티 메커니즘을 설계함(we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio)."
        ],
        "conclusion": "VisionThink는 OCR 관련 작업에서 뛰어난 시각적 이해 능력을 보여주며, 간단한 작업에서는 시각적 토큰을 상당히 절약할 수 있음.",
        "keywords": [
            "Vision-Language Models",
            "Image Understanding",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2507.13347",
            "authors": [
                {
                    "_id": "6879b78a21b37e676c8e40b1",
                    "name": "Yifan Wang",
                    "hidden": false
                },
                {
                    "_id": "6879b78a21b37e676c8e40b2",
                    "name": "Jianjun Zhou",
                    "hidden": false
                },
                {
                    "_id": "6879b78a21b37e676c8e40b3",
                    "name": "Haoyi Zhu",
                    "hidden": false
                },
                {
                    "_id": "6879b78a21b37e676c8e40b4",
                    "name": "Wenzheng Chang",
                    "hidden": false
                },
                {
                    "_id": "6879b78a21b37e676c8e40b5",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6879b78a21b37e676c8e40b6",
                    "name": "Zizun Li",
                    "hidden": false
                },
                {
                    "_id": "6879b78a21b37e676c8e40b7",
                    "name": "Junyi Chen",
                    "hidden": false
                },
                {
                    "_id": "6879b78a21b37e676c8e40b8",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                },
                {
                    "_id": "6879b78a21b37e676c8e40b9",
                    "name": "Chunhua Shen",
                    "hidden": false
                },
                {
                    "_id": "6879b78a21b37e676c8e40ba",
                    "user": {
                        "_id": "6747ede3a9c72aebe1322382",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/inILqQ05sESbYLdsEldJ_.png",
                        "isPro": false,
                        "fullname": "Tong He",
                        "user": "tonghe90",
                        "type": "user"
                    },
                    "name": "Tong He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-18T12:44:26.300Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-17T17:59:53.000Z",
            "submittedOnDailyAt": "2025-07-18T01:26:57.410Z",
            "title": "π^3: Scalable Permutation-Equivariant Visual Geometry Learning",
            "submittedOnDailyBy": {
                "_id": "6747ede3a9c72aebe1322382",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/inILqQ05sESbYLdsEldJ_.png",
                "isPro": false,
                "fullname": "Tong He",
                "user": "tonghe90",
                "type": "user"
            },
            "summary": "We introduce pi^3, a feed-forward neural network that offers a novel\napproach to visual geometry reconstruction, breaking the reliance on a\nconventional fixed reference view. Previous methods often anchor their\nreconstructions to a designated viewpoint, an inductive bias that can lead to\ninstability and failures if the reference is suboptimal. In contrast, pi^3\nemploys a fully permutation-equivariant architecture to predict\naffine-invariant camera poses and scale-invariant local point maps without any\nreference frames. This design makes our model inherently robust to input\nordering and highly scalable. These advantages enable our simple and bias-free\napproach to achieve state-of-the-art performance on a wide range of tasks,\nincluding camera pose estimation, monocular/video depth estimation, and dense\npoint map reconstruction. Code and models are publicly available.",
            "upvotes": 34,
            "discussionId": "6879b78b21b37e676c8e40bb",
            "projectPage": "https://yyfz.github.io/pi3/",
            "githubRepo": "https://github.com/yyfz/Pi3",
            "ai_summary": "A permutation-equivariant neural network, $\\pi^3$, reconstructs visual geometry without a fixed reference view, achieving state-of-the-art performance in camera pose estimation, depth estimation, and point map reconstruction.",
            "ai_keywords": [
                "feed-forward neural network",
                "permutation-equivariant architecture",
                "affine-invariant",
                "scale-invariant",
                "camera pose estimation",
                "monocular depth estimation",
                "video depth estimation",
                "dense point map reconstruction"
            ],
            "githubStars": 135
        },
        "translation_title": "π^3: 스케일러블 순열 공변 시각 기하학 학습",
        "purpose": "기존의 고정된 기준 뷰에 대한 의존성을 없애고 보다 안정적인 시각 기하학 재구성을 목표로 함",
        "method": [
            "순열 공변 아키텍처를 사용하여 참조 프레임 없이 아핀 불변 카메라 포즈와 스케일 불변 지역 포인트 맵을 예측함(we employ a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames.)",
            "이 모델의 설계는 입력 순서에 대해 본질적으로 견고하고 확장성이 뛰어남(This design makes our model inherently robust to input ordering and highly scalable.)",
            "간단하고 편향 없는 접근 방식을 통해 다양한 작업에서 최첨단 성능을 달성함(these advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks)"
        ],
        "conclusion": "pi^3는 카메라 포즈 추정, 단안/비디오 깊이 추정 및 밀집 포인트 맵 재구성을 포함한 여러 작업에서 최첨단 성능을 달성함.",
        "keywords": [
            "Computer Vision",
            "Pose Estimation",
            "3D Vision"
        ]
    },
    {
        "paper": {
            "id": "2507.13332",
            "authors": [
                {
                    "_id": "6879b01621b37e676c8e40a8",
                    "user": {
                        "_id": "66214b4e4991d64ad0e28675",
                        "avatarUrl": "/avatars/2574928aab7e45bc581c567d556a4cfd.svg",
                        "isPro": false,
                        "fullname": "Zhouqi Hua",
                        "user": "ZhouqiHUA",
                        "type": "user"
                    },
                    "name": "Zhouqi Hua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-18T07:49:38.507Z",
                    "hidden": false
                },
                {
                    "_id": "6879b01621b37e676c8e40a9",
                    "name": "Wenwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6879b01621b37e676c8e40aa",
                    "name": "Chengqi Lyu",
                    "hidden": false
                },
                {
                    "_id": "6879b01621b37e676c8e40ab",
                    "user": {
                        "_id": "6601196cc91ba4c08ad6e270",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
                        "isPro": false,
                        "fullname": "yuzhe gu",
                        "user": "vanilla1116",
                        "type": "user"
                    },
                    "name": "Yuzhe Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-18T07:49:36.203Z",
                    "hidden": false
                },
                {
                    "_id": "6879b01621b37e676c8e40ac",
                    "name": "Songyang Gao",
                    "hidden": false
                },
                {
                    "_id": "6879b01621b37e676c8e40ad",
                    "name": "Kuikun Liu",
                    "hidden": false
                },
                {
                    "_id": "6879b01621b37e676c8e40ae",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-17T17:50:07.000Z",
            "submittedOnDailyAt": "2025-07-18T00:59:46.053Z",
            "title": "The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner",
            "submittedOnDailyBy": {
                "_id": "6601196cc91ba4c08ad6e270",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
                "isPro": false,
                "fullname": "yuzhe gu",
                "user": "vanilla1116",
                "type": "user"
            },
            "summary": "Length generalization, the ability to solve problems of longer sequences than\nthose observed during training, poses a core challenge of Transformer-based\nlarge language models (LLM). Although existing studies have predominantly\nfocused on data-driven approaches for arithmetic operations and symbolic\nmanipulation tasks, these approaches tend to be task-specific with limited\noverall performance. To pursue a more general solution, this paper focuses on a\nbroader case of reasoning problems that are computable, i.e., problems that\nalgorithms can solve, thus can be solved by the Turing Machine. From this\nperspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to\nimprove the length generalization ability of LLMs. TAIL synthesizes\nchain-of-thoughts (CoT) data that imitate the execution process of a Turing\nMachine by computer programs, which linearly expands the reasoning steps into\natomic states to alleviate shortcut learning and explicit memory fetch\nmechanism to reduce the difficulties of dynamic and long-range data access in\nelementary operations. To validate the reliability and universality of TAIL, we\nconstruct a challenging synthetic dataset covering 8 classes of algorithms and\n18 tasks. Without bells and whistles, TAIL significantly improves the length\ngeneralization ability as well as the performance of Qwen2.5-7B on various\ntasks using only synthetic data, surpassing previous methods and DeepSeek-R1.\nThe experimental results reveal that the key concepts in the Turing Machine,\ninstead of the thinking styles, are indispensable for TAIL for length\ngeneralization, through which the model exhibits read-and-write behaviors\nconsistent with the properties of the Turing Machine in their attention layers.\nThis work provides a promising direction for future research in the learning of\nLLM reasoning from synthetic data.",
            "upvotes": 33,
            "discussionId": "6879b01721b37e676c8e40af",
            "ai_summary": "TAIL, a method that imitates Turing Machine execution processes, enhances the length generalization and performance of LLMs by synthesizing chain-of-thought data and reducing shortcut learning.",
            "ai_keywords": [
                "Transformer-based large language models",
                "length generalization",
                "Turing MAchine Imitation Learning",
                "TAIL",
                "chain-of-thoughts",
                "Turing Machine",
                "synthetic dataset",
                "Qwen2.5-7B",
                "read-and-write behaviors",
                "attention layers"
            ]
        },
        "translation_title": "모방 게임: 튜링 머신 모방기는 일반적인 길이 확장이 가능하다",
        "purpose": "정교한 추론을 통해 다양한 길이의 문제를 해결할 수 있도록 LLM의 길이 일반화 능력을 향상시키기 위한 연구",
        "method": [
            "TLT을 통해 튜링 머신의 실행 프로세스를 모방한 체인 오브 생각 데이터를 합성함(From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs.)",
            "TAIL은 추론 단계를 원자 상태로 선형적으로 확장하여 단축 학습을 완화하고 동적이며 장기 데이터 접근의 어려움을 줄임(TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism.)",
            "8개 알고리즘 클래스와 18개 작업을 포함하는 도전적인 합성 데이터셋을 구성함(To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks.)"
        ],
        "conclusion": "TAIL은 LLM의 길이 일반화 능력과 Qwen2.5-7B의 다양한 작업 성능을 크게 향상시켜 이전 방법 및 DeepSeek-R1을 초월하는 성과를 달성함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2507.12841",
            "authors": [
                {
                    "_id": "6879bba621b37e676c8e4195",
                    "name": "Yiming Ren",
                    "hidden": false
                },
                {
                    "_id": "6879bba621b37e676c8e4196",
                    "name": "Zhiqiang Lin",
                    "hidden": false
                },
                {
                    "_id": "6879bba621b37e676c8e4197",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "6879bba621b37e676c8e4198",
                    "name": "Gao Meng",
                    "hidden": false
                },
                {
                    "_id": "6879bba621b37e676c8e4199",
                    "name": "Weiyun Wang",
                    "hidden": false
                },
                {
                    "_id": "6879bba621b37e676c8e419a",
                    "name": "Junjie Wang",
                    "hidden": false
                },
                {
                    "_id": "6879bba621b37e676c8e419b",
                    "name": "Zicheng Lin",
                    "hidden": false
                },
                {
                    "_id": "6879bba621b37e676c8e419c",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "6879bba621b37e676c8e419d",
                    "name": "Yujiu Yang",
                    "hidden": false
                },
                {
                    "_id": "6879bba621b37e676c8e419e",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "6879bba621b37e676c8e419f",
                    "user": {
                        "_id": "642e3bcb958faf258a40e89c",
                        "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
                        "isPro": false,
                        "fullname": "Ruihang Chu",
                        "user": "Ruihang",
                        "type": "user"
                    },
                    "name": "Ruihang Chu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-18T07:49:30.538Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-17T07:04:05.000Z",
            "submittedOnDailyAt": "2025-07-18T02:04:48.913Z",
            "title": "AnyCap Project: A Unified Framework, Dataset, and Benchmark for\n  Controllable Omni-modal Captioning",
            "submittedOnDailyBy": {
                "_id": "642e3bcb958faf258a40e89c",
                "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
                "isPro": false,
                "fullname": "Ruihang Chu",
                "user": "Ruihang",
                "type": "user"
            },
            "summary": "Controllable captioning is essential for precise multimodal alignment and\ninstruction following, yet existing models often lack fine-grained control and\nreliable evaluation protocols. To address this gap, we present the AnyCap\nProject, an integrated solution spanning model, dataset, and evaluation. We\nintroduce AnyCapModel (ACM), a lightweight plug-and-play framework that\nenhances the controllability of existing foundation models for omni-modal\ncaptioning without retraining the base model. ACM reuses the original captions\nfrom base models while incorporating user instructions and modality features to\ngenerate improved captions. To remedy the data scarcity in controllable\nmultimodal captioning, we build AnyCapDataset (ACD), covering three modalities,\n28 user-instruction types, and 300\\,k high-quality data entries. We further\npropose AnyCapEval, a new benchmark that provides more reliable evaluation\nmetrics for controllable captioning by decoupling content accuracy and\nstylistic fidelity. ACM markedly improves caption quality across a diverse set\nof base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores\nby 45\\% and style scores by 12\\%, and it also achieves substantial gains on\nwidely used benchmarks such as MIA-Bench and VidCapBench.",
            "upvotes": 30,
            "discussionId": "6879bba721b37e676c8e41a0",
            "githubRepo": "https://github.com/qishisuren123/AnyCap",
            "ai_summary": "The AnyCap Project introduces a framework, dataset, and evaluation protocol to enhance controllability and reliability in multimodal captioning.",
            "ai_keywords": [
                "AnyCapModel",
                "ACM",
                "omni-modal captioning",
                "AnyCapDataset",
                "ACD",
                "AnyCapEval",
                "content accuracy",
                "stylistic fidelity",
                "MIA-Bench",
                "VidCapBench"
            ],
            "githubStars": 29
        },
        "translation_title": "AnyCap 프로젝트: 제어 가능한 다중 모달 캡셔닝을 위한 통합 프레임워크, 데이터셋, 벤치마크",
        "purpose": "정확한 다중 모달 정렬 및 지침 준수를 위한 제어 가능한 캡셔닝의 개선",
        "method": [
            "기존의 기반 모델을 재훈련 없이 향상시키는 경량 플러그 앤 플레이 프레임워크인 AnyCapModel(ACM)을 소개함(we present the AnyCap Project, an integrated solution spanning model, dataset, and evaluation.)",
            "세 가지 모달리티와 28개 사용자 지침 유형, 300k 고품질 데이터 항목을 포함하는 AnyCapDataset(ACD)를 구축함(To remedy the data scarcity in controllable multimodal captioning, we build AnyCapDataset (ACD), covering three modalities, 28 user-instruction types, and 300k high-quality data entries.)",
            "내용 정확성과 스타일 충실도를 분리한 새로운 벤치마크인 AnyCapEval을 제안함(We further propose AnyCapEval, a new benchmark that provides more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity.)"
        ],
        "conclusion": "AnyCapModel이 캡션 품질을 현저히 개선하며, GPT-4o의 콘텐츠 및 스타일 점수를 각각 45%와 12% 향상시킴을 보여줌.",
        "keywords": [
            "Multimodal Learning",
            "Image Understanding",
            "Natural Language Processing"
        ]
    }
]