[
    {
        "paper": {
            "id": "2512.21218",
            "authors": [
                {
                    "_id": "694c9c5f746a34b55dd54018",
                    "name": "Kelvin Li",
                    "hidden": false
                },
                {
                    "_id": "694c9c5f746a34b55dd54019",
                    "user": {
                        "_id": "65a86fb810125597329a4580",
                        "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg",
                        "isPro": false,
                        "fullname": "Chuyi Shang",
                        "user": "chuyishang",
                        "type": "user"
                    },
                    "name": "Chuyi Shang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-25T20:43:50.115Z",
                    "hidden": false
                },
                {
                    "_id": "694c9c5f746a34b55dd5401a",
                    "name": "Leonid Karlinsky",
                    "hidden": false
                },
                {
                    "_id": "694c9c5f746a34b55dd5401b",
                    "name": "Rogerio Feris",
                    "hidden": false
                },
                {
                    "_id": "694c9c5f746a34b55dd5401c",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "694c9c5f746a34b55dd5401d",
                    "name": "Roei Herzig",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"
            ],
            "publishedAt": "2025-12-24T14:59:49.000Z",
            "submittedOnDailyAt": "2025-12-26T00:43:27.241Z",
            "title": "Latent Implicit Visual Reasoning",
            "submittedOnDailyBy": {
                "_id": "65a86fb810125597329a4580",
                "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg",
                "isPro": false,
                "fullname": "Chuyi Shang",
                "user": "chuyishang",
                "type": "user"
            },
            "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.",
            "upvotes": 29,
            "discussionId": "694c9c5f746a34b55dd5401e",
            "organization": {
                "_id": "61f20a9ce108f2cba2dc0730",
                "name": "Berkeley",
                "fullname": "UC Berkeley",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
            }
        },
        "translation_title": "잠재적 암묵적 시각 추리",
        "purpose": "LMM이 시각적 추리 작업을 더 잘 처리할 수 있도록 하는 기법 연구",
        "method": [
            "명시적 감독 없이 LMM이 시각 추리 토큰을 발견하고 사용할 수 있도록 학습하는 작업 비특이적 메커니즘을 제안함(To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision.)",
            "비디오 정보와 관련된 시각 정보를 전역적으로 주의하고 과제를 적응적으로 재인코딩하도록 하여 모델이 관련 시각 정보를 추출할 수 있도록 함(These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision.)"
        ],
        "conclusion": "우리의 접근 방식은 다수의 시각 중심 작업에서 최첨단 결과를 달성하며, 다중 과제 지침 조정에도 일반화됨.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2512.15716",
            "authors": [
                {
                    "_id": "694b65e7746a34b55dd53dbe",
                    "user": {
                        "_id": "64f8962bce75bb0fb50bdbdb",
                        "avatarUrl": "/avatars/c85537df848bda7ec92565f56cd32eed.svg",
                        "isPro": false,
                        "fullname": "Jinjing Zhao",
                        "user": "Jinjing713",
                        "type": "user"
                    },
                    "name": "Jinjing Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-25T20:45:42.906Z",
                    "hidden": false
                },
                {
                    "_id": "694b65e7746a34b55dd53dbf",
                    "name": "Fangyun Wei",
                    "hidden": false
                },
                {
                    "_id": "694b65e7746a34b55dd53dc0",
                    "name": "Zhening Liu",
                    "hidden": false
                },
                {
                    "_id": "694b65e7746a34b55dd53dc1",
                    "name": "Hongyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b65e7746a34b55dd53dc2",
                    "name": "Chang Xu",
                    "hidden": false
                },
                {
                    "_id": "694b65e7746a34b55dd53dc3",
                    "name": "Yan Lu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64f8962bce75bb0fb50bdbdb/eC4hVgIfk0MxzPgn6mvGC.mp4"
            ],
            "publishedAt": "2025-12-17T18:59:59.000Z",
            "submittedOnDailyAt": "2025-12-26T03:15:40.222Z",
            "title": "Spatia: Video Generation with Updatable Spatial Memory",
            "submittedOnDailyBy": {
                "_id": "64f8962bce75bb0fb50bdbdb",
                "avatarUrl": "/avatars/c85537df848bda7ec92565f56cd32eed.svg",
                "isPro": false,
                "fullname": "Jinjing Zhao",
                "user": "Jinjing713",
                "type": "user"
            },
            "summary": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.",
            "upvotes": 12,
            "discussionId": "694b65e8746a34b55dd53dc4",
            "projectPage": "https://zhaojingjing713.github.io/Spatia/",
            "githubRepo": "https://github.com/ZhaoJingjing713/Spatia",
            "githubRepoAddedBy": "user",
            "ai_summary": "Spatia, a spatial memory-aware video generation framework, maintains long-term spatial and temporal consistency by preserving and updating a 3D scene point cloud, enabling realistic video generation and interactive editing.",
            "ai_keywords": [
                "spatial memory-aware",
                "video generation framework",
                "3D scene point cloud",
                "dynamic-static disentanglement",
                "visual SLAM",
                "explicit camera control",
                "3D-aware interactive editing"
            ],
            "githubStars": 50,
            "organization": {
                "_id": "670621bc820835bbf0d2b499",
                "name": "Sydney-Uni",
                "fullname": "The University of Sydney",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628dbd0f9ec8275172da853f/YSJ_DfLPaAywMvoIoM2J4.png"
            }
        },
        "translation_title": "Spatia: 업데이트 가능한 공간 메모리를 이용한 비디오 생성",
        "purpose": "비디오 생성에서 긴 시간의 공간 및 시간 일관성을 유지하기 위한 새로운 프레임워크 개발",
        "method": [
            "3D 장면 포인트 클라우드를 영속적 공간 메모리로 보존하는 Spatia 프레임워크를 제안함(To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory.)",
            "비주얼 SLAM을 통해 공간 메모리를 지속적으로 업데이트하며 비디오 클립을 반복적으로 생성함(Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM.)",
            "동적-정적 분리 설계를 통해 생성 과정 전반에 걸쳐 공간 일관성을 향상시킴(This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities.)"
        ],
        "conclusion": "Spatia는 비디오 생성의 공간 일관성을 향상시키고, 명시적인 카메라 제어 및 3D 인식 인터랙티브 편집과 같은 다양한 응용 프로그램을 가능하게 함.",
        "keywords": [
            "Video Generation",
            "3D Vision",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.19995",
            "authors": [
                {
                    "_id": "694e1a8c746a34b55dd54540",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "694e1a8c746a34b55dd54541",
                    "name": "Chenrui Fan",
                    "hidden": false
                },
                {
                    "_id": "694e1a8c746a34b55dd54542",
                    "name": "Yize Cheng",
                    "hidden": false
                },
                {
                    "_id": "694e1a8c746a34b55dd54543",
                    "name": "Soheil Feizi",
                    "hidden": false
                },
                {
                    "_id": "694e1a8c746a34b55dd54544",
                    "name": "Tianyi Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-23T02:44:25.000Z",
            "submittedOnDailyAt": "2025-12-26T02:51:58.853Z",
            "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.",
            "upvotes": 4,
            "discussionId": "694e1a8d746a34b55dd54545",
            "githubRepo": "https://github.com/MingLiiii/ThinkARM",
            "githubRepoAddedBy": "user",
            "githubStars": 2
        },
        "translation_title": "언어 모델의 수학적 추론 해부학",
        "purpose": "언어 모델의 추론 과정을 체계적으로 분석하고 제시하기 위한 프레임워크 개발",
        "method": [
            "Schoenfeld의 Episode Theory를 활용하여 추론 과정을 기능적 단계로 추상화하는 ThinkARM 프레임워크를 도입함(We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM.)",
            "다양한 모델의 수학 문제 해결에 적용하여 추론 모델과 비추론 모델 간의 구조적 차이를 발견함(When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models.)",
            "두 가지 진단 사례 연구를 통해 탐색이 올바름과 관련된 중요한 단계임을 입증하고, 효율성 지향 방법이 평가 단계의 피드백을 selectively 억제함을 보여줌(We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness.)"
        ],
        "conclusion": "Episode 수준의 표현을 통해 추론 단계를 명확히 하여 현대 언어 모델의 구조와 변화를 체계적으로 분석할 수 있게 됨.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2512.19949",
            "authors": [
                {
                    "_id": "694e0a82746a34b55dd54516",
                    "name": "Zixuan Huang",
                    "hidden": false
                },
                {
                    "_id": "694e0a82746a34b55dd54517",
                    "name": "Xiang Li",
                    "hidden": false
                },
                {
                    "_id": "694e0a82746a34b55dd54518",
                    "name": "Zhaoyang Lv",
                    "hidden": false
                },
                {
                    "_id": "694e0a82746a34b55dd54519",
                    "name": "James M. Rehg",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/629fe0dd198a9a5a755f3ce0/4F38iBHO42lBqgkLGfGka.png"
            ],
            "publishedAt": "2025-12-23T00:38:52.000Z",
            "submittedOnDailyAt": "2025-12-26T02:44:58.632Z",
            "title": "How Much 3D Do Video Foundation Models Encode?",
            "submittedOnDailyBy": {
                "_id": "629fe0dd198a9a5a755f3ce0",
                "avatarUrl": "/avatars/e643d43f66c10729f155edca96aef1f8.svg",
                "isPro": false,
                "fullname": "Zixuan Huang",
                "user": "zxhuang1698",
                "type": "user"
            },
            "summary": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.",
            "upvotes": 4,
            "discussionId": "694e0a82746a34b55dd5451a",
            "projectPage": "https://vidfm-3d-probe.github.io/",
            "organization": {
                "_id": "60212a089f64108326fac7c2",
                "name": "illinois",
                "fullname": "University of Illinois at Urbana-Champaign",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1612786274096-6021121cfb1b47827d667074.png"
            }
        },
        "translation_title": "비디오 기초 모델이 얼마나 3D를 인코딩하고 있는가?",
        "purpose": "대규모 비디오 데이터를 학습한 후 출현하는 3D 이해도를 정량적으로 연구하기 위함",
        "method": [
            "여러 VidFM의 3D 인식을 측정하는 모델 비의존적 프레임워크를 제안함(We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs.)",
            "기존의 VidFM을 통해 다양한 3D 속성을 추정하여 3D 이해도를 Quantify 함(We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data.)",
            "최신 비디오 생성 모델이 3D 객체와 장면에 대한 강력한 이해를 보여준다는 것을 입증함(we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes.)"
        ],
        "conclusion": "우리의 연구 결과는 비디오 기초 모델이 3D 작업에 필요한 대규모 전문가 모델을 초월하는 3D 이해도를 가지고 있음을 보여줍니다.",
        "keywords": [
            "Video Generation",
            "3D Vision",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.19680",
            "authors": [
                {
                    "_id": "694b4e0c746a34b55dd53c34",
                    "name": "Xinyao Liao",
                    "hidden": false
                },
                {
                    "_id": "694b4e0c746a34b55dd53c35",
                    "name": "Qiyuan He",
                    "hidden": false
                },
                {
                    "_id": "694b4e0c746a34b55dd53c36",
                    "name": "Kai Xu",
                    "hidden": false
                },
                {
                    "_id": "694b4e0c746a34b55dd53c37",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "694b4e0c746a34b55dd53c38",
                    "name": "Yicong Li",
                    "hidden": false
                },
                {
                    "_id": "694b4e0c746a34b55dd53c39",
                    "name": "Wei Wei",
                    "hidden": false
                },
                {
                    "_id": "694b4e0c746a34b55dd53c3a",
                    "name": "Angela Yao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T18:54:30.000Z",
            "submittedOnDailyAt": "2025-12-26T02:44:27.188Z",
            "title": "VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
            "submittedOnDailyBy": {
                "_id": "64cb54da1af278541d663708",
                "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
                "isPro": false,
                "fullname": "Xiaoye Qu",
                "user": "Xiaoye08",
                "type": "user"
            },
            "summary": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-π, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-π formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-π introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-π enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.",
            "upvotes": 3,
            "discussionId": "694b4e0c746a34b55dd53c3b",
            "projectPage": "https://lil-shake.github.io/va-pi.github.io/",
            "githubRepo": "https://github.com/Lil-Shake/VA-Pi",
            "githubRepoAddedBy": "user",
            "ai_summary": "VA-$\\pi$ optimizes autoregressive visual generators using a pixel-space objective to improve image quality and performance without retraining tokenizers or using external rewards.",
            "ai_keywords": [
                "autoregressive (AR) visual generation",
                "tokenizers",
                "discrete sequences",
                "evidence lower bound (ELBO)",
                "reinforcement-based alignment",
                "policy",
                "intrinsic reward",
                "teacher forcing",
                "distributional consistency",
                "FID",
                "IS",
                "LlamaGen-XXL",
                "GenEval",
                "LlamaGen",
                "Janus-Pro"
            ],
            "githubStars": 4
        },
        "translation_title": "VA-π: 픽셀 인식 자율 회귀 생성을 위한 변분 정책 정렬",
        "purpose": "자율 회귀(AR) 모델의 품질을 개선하기 위한 픽셀 기반 목표 최적화 연구",
        "method": [
            "VA-π라는 경량화된 후속 훈련 프레임워크를 제안하여 AR 모델을 픽셀 공간에서 직접 최적화함(We propose VA-π, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective.)",
            "정책으로서 AR 생성기를 다룬 강화 학습 기반 정렬 전략을 도입하여 토큰 시퀀스를 원본 이미지로 재구성하는 품질을 보상으로 활용함(VA-π introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward.)",
            "ELBO를 활용해 픽셀 재구성과 자율 회귀 모델링을 통합함(VA-π formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling.)"
        ],
        "conclusion": "VA-π는 기존 AR 생성기를 빠르게 조정할 수 있게 하며, LlamaGen-XXL에서 FID를 14.36에서 7.65로 감소시키고 IS를 86.55에서 116.70으로 개선함.",
        "keywords": [
            "Image Generation",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    }
]