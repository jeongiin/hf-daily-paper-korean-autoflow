[
    {
        "paper": {
            "id": "2503.14456",
            "authors": [
                {
                    "_id": "67da21ed78c08b432f9fee0c",
                    "user": {
                        "_id": "62b3d8d651b07307bd12b7f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655953609090-noauth.jpeg",
                        "isPro": false,
                        "fullname": "BlinkDL",
                        "user": "BlinkDL",
                        "type": "user"
                    },
                    "name": "Bo Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:45.587Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee0d",
                    "user": {
                        "_id": "6418629fd13ffa408128d7ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
                        "isPro": false,
                        "fullname": "Zhang Ruichong",
                        "user": "ZhangRC",
                        "type": "user"
                    },
                    "name": "Ruichong Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:54.749Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee0e",
                    "user": {
                        "_id": "647f4bac45baf21ad709fcd0",
                        "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
                        "isPro": false,
                        "fullname": "Dan Goldstein",
                        "user": "SmerkyG",
                        "type": "user"
                    },
                    "name": "Daniel Goldstein",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:57.333Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee0f",
                    "name": "Eric Alcaide",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee10",
                    "name": "Haowen Hou",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee11",
                    "name": "Janna Lu",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee12",
                    "name": "William Merrill",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee13",
                    "user": {
                        "_id": "622c062645261ac5cc0bda94",
                        "avatarUrl": "/avatars/ce544b74110f7fe1ad11a3939526f5da.svg",
                        "isPro": false,
                        "fullname": "Guangyu Song",
                        "user": "Guangyu",
                        "type": "user"
                    },
                    "name": "Guangyu Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T09:50:42.957Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee14",
                    "name": "Kaifeng Tan",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee15",
                    "user": {
                        "_id": "638f1fd8c4444c6ca86ff823",
                        "avatarUrl": "/avatars/405807c3868663246cfe371a2034f351.svg",
                        "isPro": false,
                        "fullname": "saitejautpala",
                        "user": "saitejautpala",
                        "type": "user"
                    },
                    "name": "Saiteja Utpala",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:48:50.276Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee16",
                    "user": {
                        "_id": "63cac1a50932c72f13995d6f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cac1a50932c72f13995d6f/Bd9jEsCL9yWXdyAQCx61J.jpeg",
                        "isPro": false,
                        "fullname": "Nathan Wilce",
                        "user": "m8than",
                        "type": "user"
                    },
                    "name": "Nathan Wilce",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T09:50:59.151Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee17",
                    "name": "Johan S. Wind",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee18",
                    "name": "Tianyi Wu",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee19",
                    "user": {
                        "_id": "63f6706a9cbd6730302b80dc",
                        "avatarUrl": "/avatars/dcbbeeeb6b12641b5884df38c5e13766.svg",
                        "isPro": false,
                        "fullname": "Dr. Daniel Wuttke",
                        "user": "hevok",
                        "type": "user"
                    },
                    "name": "Daniel Wuttke",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T13:15:03.098Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee1a",
                    "user": {
                        "_id": "6584f042b378d311dccea501",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vkq1AQhcuZwIjpFDkdgPQ.png",
                        "isPro": false,
                        "fullname": "Christian Zhou-Zheng",
                        "user": "ChristianAzinn",
                        "type": "user"
                    },
                    "name": "Christian Zhou-Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T09:51:20.835Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:31:05.000Z",
            "submittedOnDailyAt": "2025-03-19T00:29:42.147Z",
            "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
            "submittedOnDailyBy": {
                "_id": "6418629fd13ffa408128d7ae",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
                "isPro": false,
                "fullname": "Zhang Ruichong",
                "user": "ZhangRC",
                "type": "user"
            },
            "summary": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\nTC^0. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
            "upvotes": 78,
            "discussionId": "67da21ee78c08b432f9fee71",
            "projectPage": "https://rwkv.cn",
            "githubRepo": "https://github.com/RWKV/RWKV-LM",
            "ai_keywords": [
                "sequence modeling architecture",
                "pre-trained language models",
                "downstream performance",
                "multilingual tasks",
                "in-context learning rates",
                "delta rule",
                "vector-valued gating",
                "value replacement rule",
                "state tracking",
                "regular languages",
                "parallelizability of training",
                "Transformers",
                "$\\mathsf{TC}^0$"
            ]
        },
        "translation_title": "RWKV-7 'Goose': 표현력이 뛰어난 동적 상태 진화",
        "purpose": "다국어 작업에서 새로운 최첨단 성능을 달성하고 효율적인 메모리 사용을 가진 모델 개발",
        "method": [
            "RWKV-7 아키텍처를 새롭게 제안하고, 사전 훈련된 언어 모델을 통해 3B 매개변수의 다국어 작업에서 새로운 표준을 설정함(We present RWKV-7 'Goose', a new sequence modeling architecture... establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks.)",
            "델타 규칙의 새로운 일반화된 형식, 벡터 값 게이팅 및 맥락 내 학습율을 포함시킴(RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates.)",
            "모델을 훈련시켜 다양한 매개변수 범위의 네 가지 RWKV-7 모델을 생성하여 다국어 코퍼스를 학습함(we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models.)"
        ],
        "conclusion": "RWKV-7 모델은 기존 Transformer 모델의 한계를 초과하는 언어 모델링 능력을 갖추고 있으며, 공개된 데이터셋과 모델을 통해 개발자들이 쉽게 활용할 수 있도록 제공됨.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2503.14378",
            "authors": [
                {
                    "_id": "67da1ee1f1a4a52e8a1e0241",
                    "user": {
                        "_id": "64b7833aa5018e3c7c9b50d8",
                        "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
                        "isPro": false,
                        "fullname": "Zechen Bai",
                        "user": "ZechenBai",
                        "type": "user"
                    },
                    "name": "Zechen Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:45:09.659Z",
                    "hidden": false
                },
                {
                    "_id": "67da1ee1f1a4a52e8a1e0242",
                    "user": {
                        "_id": "647352516a972f252de1fd58",
                        "avatarUrl": "/avatars/8e93ca66e01f047c5fb28e1c4f737e8e.svg",
                        "isPro": false,
                        "fullname": "Hai Ci",
                        "user": "HaiCi",
                        "type": "user"
                    },
                    "name": "Hai Ci",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:14:19.533Z",
                    "hidden": false
                },
                {
                    "_id": "67da1ee1f1a4a52e8a1e0243",
                    "user": {
                        "_id": "63a55320ce5763e06f78519c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671779060549-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Mike Shou",
                        "user": "mikeshou",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-19T01:33:23.736Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64b7833aa5018e3c7c9b50d8/1jxSsiEAyMr5fSng7GOB3.mp4"
            ],
            "publishedAt": "2025-03-18T16:10:24.000Z",
            "submittedOnDailyAt": "2025-03-19T00:11:44.927Z",
            "title": "Impossible Videos",
            "submittedOnDailyBy": {
                "_id": "64b7833aa5018e3c7c9b50d8",
                "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
                "isPro": false,
                "fullname": "Zechen Bai",
                "user": "ZechenBai",
                "type": "user"
            },
            "summary": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.",
            "upvotes": 43,
            "discussionId": "67da1ee3f1a4a52e8a1e02df",
            "projectPage": "https://showlab.github.io/Impossible-Videos/",
            "githubRepo": "https://github.com/showlab/Impossible-Videos",
            "ai_keywords": [
                "IPV-Bench",
                "taxonomy",
                "prompt suite",
                "video generation models",
                "prompt following",
                "creativity capabilities",
                "video benchmark",
                "Video-LLMs",
                "temporal dynamics",
                "world knowledge"
            ]
        },
        "translation_title": "불가능한 비디오",
        "purpose": "현재 비디오 생성 및 이해 모델이 불가능한 비디오를 효과적으로 생성하고 이해할 수 있는지를 평가하기 위한 연구",
        "method": [
            "IPV-Bench라는 새로운 벤치마크를 도입하여 비디오 생성 및 이해의 진전을 평가함(we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation.)",
            "이 벤치마크는 물리적, 생물학적, 지리적, 사회적 법칙을 위반하는 다양한 장면을 포함한 4개 도메인, 14개 카테고리로 구성됨(IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories.)",
            "프롬프트를 생성하여 비디오 생성 모델의 프롬프트 준수 및 창의력 평가를 수행함(Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities.)",
            "비디오 벤치마크를 이용해 Video-LLMs의 불가능한 비디오 이해 능력을 평가함(In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos)"
        ],
        "conclusion": "비디오 모델에 대한 종합 평가를 통해 한계를 확인하고 향후 연구 방향에 대한 통찰을 제공함.",
        "keywords": [
            "Video Generation",
            "Video Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2503.14478",
            "authors": [
                {
                    "_id": "67da34a648348387ebac36ff",
                    "user": {
                        "_id": "64f5f8dd9b17cd59c453c57f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
                        "isPro": false,
                        "fullname": "Xinyu Fang",
                        "user": "nebulae09",
                        "type": "user"
                    },
                    "name": "Xinyu Fang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:17:40.816Z",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3700",
                    "name": "Zhijian Chen",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3701",
                    "name": "Kai Lan",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3702",
                    "user": {
                        "_id": "646cd947da8e99940b6e55cf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
                        "isPro": false,
                        "fullname": "Shengyuan Ding",
                        "user": "ChrisDing1105",
                        "type": "user"
                    },
                    "name": "Shengyuan Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:18:02.611Z",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3703",
                    "name": "Yingji Liang",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3704",
                    "user": {
                        "_id": "6530e62f536dbca918e71c3e",
                        "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Z",
                        "user": "PhoenixZ",
                        "type": "user"
                    },
                    "name": "Xiangyu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T13:14:48.768Z",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3705",
                    "name": "Farong Wen",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3706",
                    "name": "Zicheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3707",
                    "name": "Guofeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3708",
                    "user": {
                        "_id": "63ee1379190ddd6214efd73a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:18:37.741Z",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3709",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac370a",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:18:43.705Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:51:34.000Z",
            "submittedOnDailyAt": "2025-03-19T01:40:40.617Z",
            "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
            "submittedOnDailyBy": {
                "_id": "64f5f8dd9b17cd59c453c57f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
                "isPro": false,
                "fullname": "Xinyu Fang",
                "user": "nebulae09",
                "type": "user"
            },
            "summary": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.",
            "upvotes": 36,
            "discussionId": "67da34ad48348387ebac3926",
            "projectPage": "https://open-compass.github.io/Creation-MMBench/",
            "githubRepo": "https://github.com/open-compass/Creation-MMBench",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "Creation-MMBench",
                "image-based tasks",
                "instance-specific evaluation criteria",
                "visual fine-tuning",
                "multimodal generative intelligence"
            ]
        },
        "translation_title": "Creation-MMBench: MLLM의 맥락 인식 창의적 지능 평가",
        "purpose": "Multimodal Large Language Models(MLLMs)의 창의적 능력을 평가하기 위한 기준 수립",
        "method": [
            "MLLM의 창의력을 평가하기 위해 Creation-MMBench라는 멀티모달 벤치마크를 도입함(To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks.)",
            "765개의 테스트 사례와 51개의 세분화된 작업을 포함하여 각 테스트 사례에 대한 평가 기준을 정의함(The benchmark comprises 765 test cases spanning 51 fine-grained tasks, and we define instance-specific evaluation criteria for each test case.)",
            "실험 결과는 오픈소스 MLLM이 창의적 작업에서 독점 모델에 비해 크게 저조함을 보여줌(Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks.)"
        ],
        "conclusion": "Creation-MMBench는 MLLM의 창의성을 향상시키기 위한 귀중한 통찰을 제공하며, 향후 멀티모달 생성 지능 개선의 기반을 마련함.",
        "keywords": [
            "Multimodal Learning",
            "Large Language Models",
            "Image Generation"
        ]
    },
    {
        "paper": {
            "id": "2503.14476",
            "authors": [
                {
                    "_id": "67da2b54e5335651349e262c",
                    "user": {
                        "_id": "6341375be5071b0c5cc946a3",
                        "avatarUrl": "/avatars/c26a32b54a3f7731854059d247b5c523.svg",
                        "isPro": false,
                        "fullname": "Qiying Yu",
                        "user": "qiying",
                        "type": "user"
                    },
                    "name": "Qiying Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:19:18.079Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e262d",
                    "user": {
                        "_id": "648e9244e6598457da62a180",
                        "avatarUrl": "/avatars/60329251f08e93260196ff67a842c188.svg",
                        "isPro": false,
                        "fullname": "Zheng Zhang",
                        "user": "zhengzhang",
                        "type": "user"
                    },
                    "name": "Zheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:19:31.824Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e262e",
                    "name": "Ruofei Zhu",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e262f",
                    "user": {
                        "_id": "64b755e500bac1088cea02f1",
                        "avatarUrl": "/avatars/f4adf70175e84e945436d1f091ba338b.svg",
                        "isPro": false,
                        "fullname": "Yufeng Yuan",
                        "user": "yufeng10",
                        "type": "user"
                    },
                    "name": "Yufeng Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:19:54.321Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2630",
                    "name": "Xiaochen Zuo",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2631",
                    "name": "Yu Yue",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2632",
                    "name": "Tiantian Fan",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2633",
                    "user": {
                        "_id": "61512f1b3e89795099b6fced",
                        "avatarUrl": "/avatars/8f19253df7343e9d95ad7d0b3d429c06.svg",
                        "isPro": false,
                        "fullname": "Gaohong Liu",
                        "user": "Cheimu",
                        "type": "user"
                    },
                    "name": "Gaohong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:20:22.957Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2634",
                    "user": {
                        "_id": "6626274011772517e5469d48",
                        "avatarUrl": "/avatars/5f2606614740da04a7fb9d80fa628fb1.svg",
                        "isPro": false,
                        "fullname": "Lingjun Liu",
                        "user": "EEchollj",
                        "type": "user"
                    },
                    "name": "Lingjun Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:20:29.992Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2635",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2636",
                    "user": {
                        "_id": "65e809f67c1853c8c180baff",
                        "avatarUrl": "/avatars/e489eaebbbf86f8826fe0e9d5423be01.svg",
                        "isPro": false,
                        "fullname": "haibin",
                        "user": "haibinlin",
                        "type": "user"
                    },
                    "name": "Haibin Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:20:36.744Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2637",
                    "name": "Zhiqi Lin",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2638",
                    "name": "Bole Ma",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2639",
                    "name": "Guangming Sheng",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e263a",
                    "user": {
                        "_id": "6448e1fbe988635a3d6aa97d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eG4R9-3hgrimttP7ep3dN.jpeg",
                        "isPro": false,
                        "fullname": "Shawn/Yuxuan Tong",
                        "user": "tongyx361",
                        "type": "user"
                    },
                    "name": "Yuxuan Tong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:21:29.044Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e263b",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e263c",
                    "user": {
                        "_id": "64f33235bbf2fd1b4816632b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sVN-M1Hj-aB4QqytdpUsn.jpeg",
                        "isPro": false,
                        "fullname": "Mofan Zhang",
                        "user": "NewMomo",
                        "type": "user"
                    },
                    "name": "Mofan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:21:51.391Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e263d",
                    "name": "Wang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e263e",
                    "name": "Hang Zhu",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e263f",
                    "user": {
                        "_id": "650b0009bddcb62e458ebadf",
                        "avatarUrl": "/avatars/0431a38b9c9ca0e11ec505ce81183be8.svg",
                        "isPro": false,
                        "fullname": "Jinhua Zhu",
                        "user": "teslazhu",
                        "type": "user"
                    },
                    "name": "Jinhua Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:22:23.684Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2640",
                    "name": "Jiaze Chen",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2641",
                    "name": "Jiangjie Chen",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2642",
                    "name": "Chengyi Wang",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2643",
                    "name": "Hongli Yu",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2644",
                    "name": "Weinan Dai",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2645",
                    "name": "Yuxuan Song",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2646",
                    "name": "Xiangpeng Wei",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2647",
                    "name": "Hao Zhou",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2648",
                    "name": "Jingjing Liu",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2649",
                    "name": "Wei-Ying Ma",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e264a",
                    "user": {
                        "_id": "656c1600665d15428a8bdc60",
                        "avatarUrl": "/avatars/dfdf5f8712c769896b4ab655c4ce32e4.svg",
                        "isPro": false,
                        "fullname": "Ya-Qin Zhang",
                        "user": "Ya-Qin",
                        "type": "user"
                    },
                    "name": "Ya-Qin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:22:33.159Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e264b",
                    "name": "Lin Yan",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e264c",
                    "name": "Mu Qiao",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e264d",
                    "user": {
                        "_id": "647a13ba5f3450e1ded4ca85",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647a13ba5f3450e1ded4ca85/0ryiqG2gpFlffOFNq3jpj.jpeg",
                        "isPro": false,
                        "fullname": "Yonghui Wu",
                        "user": "yonghuiwu",
                        "type": "user"
                    },
                    "name": "Yonghui Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:22:16.717Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e264e",
                    "name": "Mingxuan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:49:06.000Z",
            "submittedOnDailyAt": "2025-03-19T00:56:39.773Z",
            "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe Decoupled Clip and Dynamic sAmpling\nPolicy Optimization (DAPO) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.",
            "upvotes": 34,
            "discussionId": "67da2b55e5335651349e26c7",
            "ai_keywords": [
                "inference scaling",
                "LLMs (Large Language Models)",
                "reinforcement learning (RL)",
                "Decoupled Clip and Dynamic Sampling Action Policy Optimization (DAPO)",
                "Qwen2.5-32B",
                "AIME 2024",
                "verl framework"
            ]
        },
        "translation_title": "DAPO: 대규모 오픈 소스 LLM 강화 학습 시스템",
        "purpose": "대규모 LLM의 강화 학습을 통해 복잡한 추론 능력을 이끌어내는 기술 공유 및 재현성 증진",
        "method": [
            "Decoupled Clip과 Dynamic sAmpling Policy Optimization(DAPO) 알고리즘을 제안함(We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm)",
            "Qwen2.5-32B 기본 모델을 사용해 AIME 2024에서 50점을 달성하는 대규모 RL 시스템을 완전히 오픈 소스로 제공함(we fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model)",
            "기술 세부 사항과 훈련 코드를 오픈 소스화하여 재현성을 높이고, 향후 연구를 지원함(in addition, we open-source our training code... enhance reproducibility and support future research in large-scale LLM RL)"
        ],
        "conclusion": "DAPO 알고리즘은 대규모 LLM 강화 학습을 성공적으로 수행할 수 있도록 하며, 앞으로의 연구를 위한 기반 자료를 제공합니다.",
        "keywords": [
            "Large Language Models",
            "Reinforcement Learning",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2503.12797",
            "authors": [
                {
                    "_id": "67da2c83aa2c34f7d95e46ff",
                    "user": {
                        "_id": "63c3b67ec7d7f4c63a4eea3a",
                        "avatarUrl": "/avatars/4a5f98cb6b0c1e37a2c09af72f7a9946.svg",
                        "isPro": false,
                        "fullname": "Xinyu Ma",
                        "user": "MaxyLee",
                        "type": "user"
                    },
                    "name": "Xinyu Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:31.865Z",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4700",
                    "user": {
                        "_id": "65903c4aa78a277803bde77b",
                        "avatarUrl": "/avatars/061388320ccf1a66e1e99519dd426a60.svg",
                        "isPro": false,
                        "fullname": "Ziyang Ding",
                        "user": "sdudzy",
                        "type": "user"
                    },
                    "name": "Ziyang Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:35.757Z",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4701",
                    "name": "Zhicong Luo",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4702",
                    "user": {
                        "_id": "642086ed290342c5df85662d",
                        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
                        "isPro": false,
                        "fullname": "Chi Chen",
                        "user": "carboncoo",
                        "type": "user"
                    },
                    "name": "Chi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:39.356Z",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4703",
                    "user": {
                        "_id": "6491af36c1741666238f3bff",
                        "avatarUrl": "/avatars/0ee7d2ec1566e2cc5e8f144140e17f00.svg",
                        "isPro": false,
                        "fullname": "Zonghao Guo",
                        "user": "guozonghao96",
                        "type": "user"
                    },
                    "name": "Zonghao Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:23:57.046Z",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4704",
                    "user": {
                        "_id": "648bd523805e4bcc541ec320",
                        "avatarUrl": "/avatars/443ca0c7cbeda3a08eb4af6a0e2da8bc.svg",
                        "isPro": false,
                        "fullname": "Derek Wong",
                        "user": "derekfw",
                        "type": "user"
                    },
                    "name": "Derek F. Wong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:23:48.590Z",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4705",
                    "user": {
                        "_id": "6502c59b8670cc10e6b6b039",
                        "avatarUrl": "/avatars/ae96d1fa2bed83cd427d18ca25d69efa.svg",
                        "isPro": false,
                        "fullname": "Xiaoyi Feng",
                        "user": "LecterF",
                        "type": "user"
                    },
                    "name": "Xiaoyi Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:23:24.786Z",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4706",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T04:06:34.000Z",
            "submittedOnDailyAt": "2025-03-19T01:06:07.094Z",
            "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding",
            "submittedOnDailyBy": {
                "_id": "642086ed290342c5df85662d",
                "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
                "isPro": false,
                "fullname": "Chi Chen",
                "user": "carboncoo",
                "type": "user"
            },
            "summary": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.",
            "upvotes": 22,
            "discussionId": "67da2c85aa2c34f7d95e4796",
            "projectPage": "https://deepperception-kvg.github.io",
            "githubRepo": "https://github.com/thunlp/DeepPerception",
            "ai_keywords": [
                "knowledge-intensive visual grounding (KVG)",
                "DeepPerception",
                "automated data synthesis pipeline",
                "supervised fine-tuning",
                "reinforcement learning",
                "perception-cognition synergy",
                "KVG-Bench",
                "cognitive reasoning scaffolding",
                "cross-domain generalization",
                "cognitive processes",
                "multimodal reasoning"
            ]
        },
        "translation_title": "DeepPerception: MLLMs에서 인지적 시각 인식을 개선하기 위한 R1 유사한 인지 기능",
        "purpose": "MLLMs에서 시각 인식 향상을 위한 지식 집약적 비주얼 그라운딩(KVG) 문제 해결",
        "method": [
            "자동화된 데이터 합성 파이프라인을 통해 고품질의 지식 정렬된 훈련 샘플을 생성함(we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities.)",
            "인지적 추론 지원을 위한 감독식 세밀한 조정과 지각-인지 조화를 최적화하기 위한 강화 학습을 결합한 2단계 훈련 프레임워크를 고안함(our approach consists of a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy.)",
            "10개 도메인을 아우르는 KVG-Bench 데이터를 도입하여 성능을 벤치마킹함(To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases.)"
        ],
        "conclusion": "DeepPerception은 KVG-Bench에서 +8.08% 정확도 개선과 +4.60%의 뛰어난 도메인 간 일반화를 통해 MLLMs에 인지적 과정을 통합하는 것이 사람과 같은 시각 인식을 가능하게 함을 증명함.",
        "keywords": [
            "Natural Language Processing",
            "Multimodal Learning",
            "Image Understanding"
        ]
    }
]