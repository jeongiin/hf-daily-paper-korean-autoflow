[
    {
        "paper": {
            "id": "2508.18265",
            "authors": [
                {
                    "_id": "68ad214d86b21a0e2e358d5d",
                    "user": {
                        "_id": "619507e7b74b6c591f794340",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                        "isPro": false,
                        "fullname": "Weiyun Wang",
                        "user": "Weiyun1025",
                        "type": "user"
                    },
                    "name": "Weiyun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:11.076Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d5e",
                    "name": "Zhangwei Gao",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d5f",
                    "user": {
                        "_id": "6541efc9109d78427198ea40",
                        "avatarUrl": "/avatars/c1252dd7da2e53b0b6757bf392139cdf.svg",
                        "isPro": false,
                        "fullname": "Lixin Gu",
                        "user": "gulixin0922",
                        "type": "user"
                    },
                    "name": "Lixin Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-26T10:50:33.244Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d60",
                    "user": {
                        "_id": "648a1e44fe11ebd7489c289c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648a1e44fe11ebd7489c289c/WvszTywGnuY8OV4500_qh.jpeg",
                        "isPro": false,
                        "fullname": "Hengjun Pu",
                        "user": "MIASANMIA",
                        "type": "user"
                    },
                    "name": "Hengjun Pu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-26T10:50:39.288Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d61",
                    "name": "Long Cui",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d62",
                    "user": {
                        "_id": "6771045f076645f7feb4798d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/IsYgpeAGvRAcqnYKwnj0T.jpeg",
                        "isPro": false,
                        "fullname": "Xingguang Wei",
                        "user": "WesKwong",
                        "type": "user"
                    },
                    "name": "Xingguang Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:07.170Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d63",
                    "name": "Zhaoyang Liu",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d64",
                    "user": {
                        "_id": "6692b867c7cfcb719e6ea7fb",
                        "avatarUrl": "/avatars/198b9cb13d2a5ef50ceabba1f007dc4d.svg",
                        "isPro": false,
                        "fullname": "Linglin",
                        "user": "jinglinglin",
                        "type": "user"
                    },
                    "name": "Linglin Jing",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-26T10:50:55.076Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d65",
                    "user": {
                        "_id": "64804866c7f87934d082bb25",
                        "avatarUrl": "/avatars/41761226c79ac16e48d4c4cb84362adb.svg",
                        "isPro": false,
                        "fullname": "Yeshenglong",
                        "user": "Yeshenglong",
                        "type": "user"
                    },
                    "name": "Shenglong Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-26T10:51:06.121Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d66",
                    "name": "Jie Shao",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d67",
                    "user": {
                        "_id": "665d4b515fdfe8f923e347a7",
                        "avatarUrl": "/avatars/d114b24c02dadfca0a8aee104755a8ec.svg",
                        "isPro": false,
                        "fullname": "Zhaokai Wang",
                        "user": "wzk1015",
                        "type": "user"
                    },
                    "name": "Zhaokai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-26T10:51:17.286Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d68",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d69",
                    "name": "Hongjie Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d6a",
                    "user": {
                        "_id": "6565d7149afd51867e55520b",
                        "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
                        "isPro": false,
                        "fullname": "Ganlin Yang",
                        "user": "ganlinyang",
                        "type": "user"
                    },
                    "name": "Ganlin Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:02.487Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d6b",
                    "user": {
                        "_id": "64d83ee0763279bb4ddd5ba8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d83ee0763279bb4ddd5ba8/exbvc1hhIXXKPVmJpxRtP.jpeg",
                        "isPro": false,
                        "fullname": "Haomin Wang",
                        "user": "KiyotakaWang",
                        "type": "user"
                    },
                    "name": "Haomin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:12.956Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d6c",
                    "name": "Qi Wei",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d6d",
                    "name": "Jinhui Yin",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d6e",
                    "name": "Wenhao Li",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d6f",
                    "name": "Erfei Cui",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d70",
                    "name": "Guanzhou Chen",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d71",
                    "user": {
                        "_id": "642b9861bb77f8456634b048",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg",
                        "isPro": false,
                        "fullname": "Zichen Ding",
                        "user": "heroding77",
                        "type": "user"
                    },
                    "name": "Zichen Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:09.178Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d72",
                    "name": "Changyao Tian",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d73",
                    "name": "Zhenyu Wu",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d74",
                    "name": "Jingjing Xie",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d75",
                    "name": "Zehao Li",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d76",
                    "name": "Bowen Yang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d77",
                    "name": "Yuchen Duan",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d78",
                    "name": "Xuehui Wang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d79",
                    "user": {
                        "_id": "64acbbd51aee69ece03c6c0c",
                        "avatarUrl": "/avatars/604df1cabc5faeda55022ae4c1997e56.svg",
                        "isPro": false,
                        "fullname": "Songze Li",
                        "user": "SongzeLi",
                        "type": "user"
                    },
                    "name": "Songze Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:04.603Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d7a",
                    "name": "Xiangyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d7b",
                    "name": "Haodong Duan",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d7c",
                    "name": "Nianchen Deng",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d7d",
                    "name": "Bin Fu",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d7e",
                    "name": "Yinan He",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d7f",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d80",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d81",
                    "name": "Botian Shi",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d82",
                    "name": "Junjun He",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d83",
                    "name": "Yingtong Xiong",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d84",
                    "name": "Han Lv",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d85",
                    "name": "Lijun Wu",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d86",
                    "name": "Wenqi Shao",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d87",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d88",
                    "name": "Huipeng Deng",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d89",
                    "name": "Biqing Qi",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d8a",
                    "name": "Jiaye Ge",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d8b",
                    "name": "Qipeng Guo",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d8c",
                    "name": "Wenwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d8d",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d8e",
                    "name": "Limin Wang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d8f",
                    "name": "Min Dou",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d90",
                    "name": "Xizhou Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d91",
                    "name": "Tong Lu",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d92",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d93",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d94",
                    "name": "Bowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d95",
                    "name": "Weijie Su",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d96",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d97",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d98",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d99",
                    "name": "Gen Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T17:58:17.000Z",
            "submittedOnDailyAt": "2025-08-26T01:22:14.339Z",
            "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05times inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.",
            "upvotes": 92,
            "discussionId": "68ad214d86b21a0e2e358d9a",
            "ai_summary": "InternVL 3.5 introduces Cascade RL, ViR, and DvD to enhance reasoning, efficiency, and performance in multimodal models.",
            "ai_keywords": [
                "Cascade RL",
                "offline RL",
                "online RL",
                "Visual Resolution Router",
                "ViR",
                "Decoupled Vision-Language Deployment",
                "DvD",
                "multimodal models",
                "reasoning performance",
                "inference speedup",
                "GUI interaction",
                "embodied agency"
            ]
        },
        "translation_title": "InternVL3.5: 다재다능함과 추론, 효율성을 향상시킨 오픈 소스 멀티모달 모델",
        "purpose": "다양성과 추론 능력, 효율성을 개선하기 위한 새로운 오픈 소스 멀티모달 모델 개발",
        "method": [
            "Cascade Reinforcement Learning 프레임워크를 통해 두 단계의 과정으로 추론 능력을 향상시킴(We introduce a key innovation, the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process.)",
            "Visual Resolution Router를 제안하여 성능 저하 없이 시각적 토큰의 해상도를 동적으로 조정함(To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance.)",
            "Decoupled Vision-Language Deployment 전략으로 시각 인코더와 언어 모델을 서로 다른 GPU에 분리하여 컴퓨팅 부하를 효과적으로 분산함(Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs.)"
        ],
        "conclusion": "InternVL3.5는 전반적인 추론 성능에서 +16.0% 향상과 4.05배의 추론 속도 개선을 달성하였으며, GUI 상호작용 및 구체적 행동과 같은 새로운 기능을 지원함.",
        "keywords": [
            "Multimodal Learning",
            "Reasoning",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2508.18032",
            "authors": [
                {
                    "_id": "68ad236486b21a0e2e358db3",
                    "name": "Yaqi Li",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358db4",
                    "name": "Peng Chen",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358db5",
                    "name": "Mingyang Han",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358db6",
                    "name": "Bu Pi",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358db7",
                    "name": "Haoxiang Shi",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358db8",
                    "name": "Runzhou Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358db9",
                    "name": "Yang Yao",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358dba",
                    "name": "Xuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358dbb",
                    "name": "Jun Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T13:53:02.000Z",
            "submittedOnDailyAt": "2025-08-26T01:31:03.330Z",
            "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance\n  for Text-to-Image Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Despite the promising progress of recent autoregressive models in\ntext-to-image (T2I) generation, their ability to handle multi-attribute and\nambiguous prompts remains limited. To address these limitations, existing works\nhave applied chain-of-thought (CoT) to enable stage-aware visual synthesis and\nemployed reinforcement learning (RL) to improve reasoning capabilities.\nHowever, most models provide reward signals only at the end of the generation\nstage. This monolithic final-only guidance makes it difficult to identify which\nstages contribute positively to the final outcome and may lead to suboptimal\npolicies. To tackle this issue, we propose a Visual-Chain of Guidance\n(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process\nrefining, and outcome evaluation, with stage-aware rewards providing immediate\nguidance throughout the image generation pipeline. We further construct a\nvisual cognition benchmark, VisCog-Bench, which comprises four subtasks to\nevaluate the effectiveness of semantic reasoning. Comprehensive evaluations on\nGenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,\n5%, and 19%, respectively, demonstrating the superior performance of the\nproposed Visual-CoG. We will release all the resources soon.",
            "upvotes": 31,
            "discussionId": "68ad236586b21a0e2e358dbc",
            "ai_summary": "The Visual-Chain of Guidance (Visual-CoG) paradigm enhances text-to-image generation by providing stage-aware rewards, improving performance across multiple benchmarks.",
            "ai_keywords": [
                "autoregressive models",
                "text-to-image (T2I) generation",
                "chain-of-thought (CoT)",
                "reinforcement learning (RL)",
                "stage-aware visual synthesis",
                "semantic reasoning",
                "process refining",
                "outcome evaluation",
                "Visual-Chain of Guidance (Visual-CoG)",
                "VisCog-Bench",
                "GenEval",
                "T2I-CompBench"
            ]
        },
        "translation_title": "Visual-CoG: 단계 인식 강화 학습을 통한 Text-to-Image 생성을 위한 안내 체계",
        "purpose": "다양한 속성과 모호한 프롬프트를 처리하는 Text-to-Image 생성 모델의 한계를 극복하기 위한 새로운 방법 연구",
        "method": [
            "시맨틱 추론, 프로세스 정제, 결과 평가의 세 단계로 구성된 Visual-Chain of Guidance(Visual-CoG) 패러다임을 제안함(To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation.)",
            "단계 인식 보상을 통해 이미지 생성 파이프라인 전반에 걸쳐 즉각적인 안내를 제공함(with stage-aware rewards providing immediate guidance throughout the image generation pipeline.)",
            "시맨틱 추론의 효과성을 평가하기 위해 4개의 하위 작업을 포함하는 시각 인지 기준인 VisCog-Bench를 구축함(We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning.)"
        ],
        "conclusion": "제안한 Visual-CoG 방식은 GenEval, T2I-CompBench 및 VisCog-Bench에서 각각 15%, 5%, 19%의 성능 향상을 보였으며, 우수한 성능을 입증함.",
        "keywords": [
            "Image Generation",
            "Reinforcement Learning",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2508.16577",
            "authors": [
                {
                    "_id": "68ac574486b21a0e2e358c17",
                    "user": {
                        "_id": "680645323889e86c69a3daf6",
                        "avatarUrl": "/avatars/3a361a01ee36965b523d73ff292ce1ef.svg",
                        "isPro": false,
                        "fullname": "Yosef Dayani",
                        "user": "yosepyossi",
                        "type": "user"
                    },
                    "name": "Yosef Dayani",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T12:36:57.099Z",
                    "hidden": false
                },
                {
                    "_id": "68ac574486b21a0e2e358c18",
                    "user": {
                        "_id": "64543a1ccd09ceba0e14ecfd",
                        "avatarUrl": "/avatars/d4f3aca9aa8bb4188f68ffd9e0d1f881.svg",
                        "isPro": false,
                        "fullname": "Omer Benishu",
                        "user": "omerbenishu",
                        "type": "user"
                    },
                    "name": "Omer Benishu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T19:31:52.191Z",
                    "hidden": false
                },
                {
                    "_id": "68ac574486b21a0e2e358c19",
                    "user": {
                        "_id": "6345a9b9a8c2ff9f1377faab",
                        "avatarUrl": "/avatars/a5f2b999ef8b967b2af9f41afcd9d475.svg",
                        "isPro": false,
                        "fullname": "Sagie Benaim",
                        "user": "sagiebenaim",
                        "type": "user"
                    },
                    "name": "Sagie Benaim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T19:31:54.300Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-22T17:59:40.000Z",
            "submittedOnDailyAt": "2025-08-26T06:12:31.342Z",
            "title": "MV-RAG: Retrieval Augmented Multiview Diffusion",
            "submittedOnDailyBy": {
                "_id": "680645323889e86c69a3daf6",
                "avatarUrl": "/avatars/3a361a01ee36965b523d73ff292ce1ef.svg",
                "isPro": false,
                "fullname": "Yosef Dayani",
                "user": "yosepyossi",
                "type": "user"
            },
            "summary": "Text-to-3D generation approaches have advanced significantly by leveraging\npretrained 2D diffusion priors, producing high-quality and 3D-consistent\noutputs. However, they often fail to produce out-of-domain (OOD) or rare\nconcepts, yielding inconsistent or inaccurate results. To this end, we propose\nMV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images\nfrom a large in-the-wild 2D database and then conditions a multiview diffusion\nmodel on these images to synthesize consistent and accurate multiview outputs.\nTraining such a retrieval-conditioned model is achieved via a novel hybrid\nstrategy bridging structured multiview data and diverse 2D image collections.\nThis involves training on multiview data using augmented conditioning views\nthat simulate retrieval variance for view-specific reconstruction, alongside\ntraining on sets of retrieved real-world 2D images using a distinctive held-out\nview prediction objective: the model predicts the held-out view from the other\nviews to infer 3D consistency from 2D data. To facilitate a rigorous OOD\nevaluation, we introduce a new collection of challenging OOD prompts.\nExperiments against state-of-the-art text-to-3D, image-to-3D, and\npersonalization baselines show that our approach significantly improves 3D\nconsistency, photorealism, and text adherence for OOD/rare concepts, while\nmaintaining competitive performance on standard benchmarks.",
            "upvotes": 25,
            "discussionId": "68ac574486b21a0e2e358c1a",
            "projectPage": "https://yosefdayani.github.io/MV-RAG/",
            "githubRepo": "https://github.com/yosefdayani/MV-RAG",
            "ai_summary": "MV-RAG enhances text-to-3D generation by retrieving 2D images and conditioning a multiview diffusion model to improve consistency and accuracy, especially for out-of-domain concepts.",
            "ai_keywords": [
                "pretrained 2D diffusion priors",
                "MV-RAG",
                "multiview diffusion model",
                "retrieval-conditioned model",
                "hybrid strategy",
                "augmented conditioning views",
                "held-out view prediction objective",
                "OOD prompts",
                "text-to-3D",
                "image-to-3D",
                "personalization baselines",
                "3D consistency",
                "photorealism",
                "text adherence"
            ],
            "githubStars": 10
        },
        "translation_title": "MV-RAG: 검색 증강 다중 시점 확산",
        "purpose": "OOD(Out-of-Domain) 및 희귀 개념 생성에서의 일관성과 정확성을 개선하기 위해 새로운 텍스트-3D 파이프라인 제안",
        "method": [
            "대규모 2D 데이터베이스에서 관련 2D 이미지를 검색하는 방법으로 시작함(we propose MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images from a large in-the-wild 2D database)",
            "다중 시점 확산 모델을 이러한 이미지에 조건화해 일관된 다중 시점 출력을 생성함(and then conditions a multiview diffusion model on these images to synthesize consistent and accurate multiview outputs)",
            "하이브리드 전략을 통해 다중 시점 데이터와 다양한 2D 이미지 컬렉션을 연결하여 모델을 훈련함(This involves training on multiview data using augmented conditioning views that simulate retrieval variance for view-specific reconstruction)"
        ],
        "conclusion": "우리의 접근 방식은 OOD/희귀 개념의 3D 일관성, 포토리얼리즘 및 텍스트 준수를 크게 향상시키면서도 표준 벤치마크에서 경쟁력 있는 성능을 유지함.",
        "keywords": [
            "3D Vision",
            "Image Generation",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.17472",
            "authors": [
                {
                    "_id": "68ad304f86b21a0e2e358e22",
                    "user": {
                        "_id": "63640ce5ff4b318d1b7b6f5c",
                        "avatarUrl": "/avatars/9336d1dab1491f3c8e84b1ea287eb891.svg",
                        "isPro": false,
                        "fullname": "Kaiyue Sun",
                        "user": "Kaiyue",
                        "type": "user"
                    },
                    "name": "Kaiyue Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:49:28.939Z",
                    "hidden": false
                },
                {
                    "_id": "68ad304f86b21a0e2e358e23",
                    "name": "Rongyao Fang",
                    "hidden": false
                },
                {
                    "_id": "68ad304f86b21a0e2e358e24",
                    "name": "Chengqi Duan",
                    "hidden": false
                },
                {
                    "_id": "68ad304f86b21a0e2e358e25",
                    "name": "Xian Liu",
                    "hidden": false
                },
                {
                    "_id": "68ad304f86b21a0e2e358e26",
                    "name": "Xihui Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/RwCZGuI1OB7yLiQgBLmd_.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/1HhE9m118CtQHrrvgQmbo.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/KNE5RJ6_y_em3ZRemlUSW.png"
            ],
            "publishedAt": "2025-08-24T17:59:38.000Z",
            "submittedOnDailyAt": "2025-08-26T02:30:30.581Z",
            "title": "T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image\n  Generation",
            "submittedOnDailyBy": {
                "_id": "63640ce5ff4b318d1b7b6f5c",
                "avatarUrl": "/avatars/9336d1dab1491f3c8e84b1ea287eb891.svg",
                "isPro": false,
                "fullname": "Kaiyue Sun",
                "user": "Kaiyue",
                "type": "user"
            },
            "summary": "We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of\ntext-to-image (T2I) models. It consists of four dimensions: Idiom\nInterpretation, Textual Image Design, Entity-Reasoning and\nScientific-Reasoning. We propose a two-stage evaluation protocol to assess the\nreasoning accuracy and image quality. We benchmark various T2I generation\nmodels, and provide comprehensive analysis on their performances.",
            "upvotes": 19,
            "discussionId": "68ad304f86b21a0e2e358e27",
            "githubRepo": "https://github.com/KaiyueSun98/T2I-ReasonBench",
            "ai_summary": "T2I-ReasonBench evaluates the reasoning capabilities of text-to-image models across four dimensions using a two-stage protocol, analyzing their performance comprehensively.",
            "ai_keywords": [
                "text-to-image",
                "T2I",
                "Idiom Interpretation",
                "Textual Image Design",
                "Entity-Reasoning",
                "Scientific-Reasoning",
                "two-stage evaluation protocol"
            ],
            "githubStars": 17
        },
        "translation_title": "T2I-ReasonBench: Reasoning 기반 Text-to-Image 생성 평가",
        "purpose": "Text-to-Image 모델의 추론 능력을 평가하기 위한 기준 마련",
        "method": [
            "추론 정확성과 이미지 품질을 평가하기 위한 두 단계 평가 프로토콜을 제안함 (We propose a two-stage evaluation protocol to assess the reasoning accuracy and image quality.)",
            "T2I 모델의 성능을 평가하기 위해 다양한 텍스트-이미지 생성 모델을 벤치마킹 함 (We benchmark various T2I generation models, and provide comprehensive analysis on their performances.)"
        ],
        "conclusion": "T2I-ReasonBench를 통해 모델의 추론 능력과 이미지 품질에 대한 전반적인 분석을 제공함.",
        "keywords": [
            "Image Generation",
            "Image Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2508.16745",
            "authors": [
                {
                    "_id": "68ad769486b21a0e2e358f20",
                    "user": {
                        "_id": "63c1ac8cc58fcfeac186bda2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c1ac8cc58fcfeac186bda2/MajMhLgsvCBN81tmO9PZc.jpeg",
                        "isPro": false,
                        "fullname": "Ivan Rodkin",
                        "user": "irodkin",
                        "type": "user"
                    },
                    "name": "Ivan Rodkin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:49:10.644Z",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f21",
                    "name": "Daniil Orel",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f22",
                    "name": "Konstantin Smirnov",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f23",
                    "name": "Arman Bolatov",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f24",
                    "user": {
                        "_id": "64219641eaad1bcb28af7d2b",
                        "avatarUrl": "/avatars/8edc9e4f889832f4b25e296b2454ce8c.svg",
                        "isPro": true,
                        "fullname": "BIlal Elbouardi",
                        "user": "b1l4lx1",
                        "type": "user"
                    },
                    "name": "Bilal Elbouardi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T11:37:55.584Z",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f25",
                    "name": "Besher Hassan",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f26",
                    "user": {
                        "_id": "618b9540682ec1c38327e586",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618b9540682ec1c38327e586/v_ZBkfh8O9Zh6C2YQpuBX.jpeg",
                        "isPro": false,
                        "fullname": "Yury Kuratov",
                        "user": "yurakuratov",
                        "type": "user"
                    },
                    "name": "Yuri Kuratov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:49:12.960Z",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f27",
                    "name": "Aydar Bulatov",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f28",
                    "name": "Preslav Nakov",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f29",
                    "name": "Timothy Baldwin",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f2a",
                    "name": "Artem Shelmanov",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f2b",
                    "user": {
                        "_id": "639c6e978a34ed9a404c6a7b",
                        "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
                        "isPro": false,
                        "fullname": "MIKHAIL BURTSEV",
                        "user": "mbur",
                        "type": "user"
                    },
                    "name": "Mikhail Burtsev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:49:16.139Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/639c6e978a34ed9a404c6a7b/-_DXOswK6JJc0jmMdFlbP.png"
            ],
            "publishedAt": "2025-08-22T18:57:08.000Z",
            "submittedOnDailyAt": "2025-08-26T07:29:32.605Z",
            "title": "Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory\n  and Test-Time Compute Scaling",
            "submittedOnDailyBy": {
                "_id": "639c6e978a34ed9a404c6a7b",
                "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
                "isPro": false,
                "fullname": "MIKHAIL BURTSEV",
                "user": "mbur",
                "type": "user"
            },
            "summary": "Reasoning is a core capability of large language models, yet understanding\nhow they learn and perform multi-step reasoning remains an open problem. In\nthis study, we explore how different architectures and training methods affect\nmodel multi-step reasoning capabilities within a cellular automata framework.\nBy training on state sequences generated with random Boolean functions for\nrandom initial conditions to exclude memorization, we demonstrate that most\nneural architectures learn to abstract the underlying rules. While models\nachieve high accuracy in next-state prediction, their performance declines\nsharply if multi-step reasoning is required. We confirm that increasing model\ndepth plays a crucial role for sequential computations. We demonstrate that an\nextension of the effective model depth with recurrence, memory, and test-time\ncompute scaling substantially enhances reasoning capabilities.",
            "upvotes": 15,
            "discussionId": "68ad769486b21a0e2e358f2c",
            "githubRepo": "https://github.com/RodkinIvan/associative-recurrent-memory-transformer/tree/ACT",
            "ai_summary": "Models trained on random Boolean functions in a cellular automata framework show that increasing depth, recurrence, memory, and test-time compute scaling enhances multi-step reasoning capabilities.",
            "ai_keywords": [
                "cellular automata",
                "Boolean functions",
                "multi-step reasoning",
                "model depth",
                "recurrence",
                "memory",
                "test-time compute scaling"
            ],
            "githubStars": 45
        },
        "translation_title": "기억을 넘어: 반복, 메모리 및 테스트 시간 계산 확장을 통한 추론 깊이 증대",
        "purpose": "모델의 다단계 추론 능력을 향상시키기 위한 아키텍처와 훈련 방법의 영향을 탐구",
        "method": [
            "무작위 불리언 함수로 생성된 상태 시퀀스에서 학습하여 기억화의 영향을 배제함(We demonstrate that most neural architectures learn to abstract the underlying rules by training on state sequences generated with random Boolean functions for random initial conditions to exclude memorization.)",
            "모델 깊이를 증가시키는 것이 순차 계산에 중요한 역할을 한다는 것을 확인함(We confirm that increasing model depth plays a crucial role for sequential computations.)",
            "반복, 메모리, 테스트 시간 계산 확장을 통한 모델 깊이의 유효한 연장을 보여주어 추론 능력을 상당히 향상시킴(We demonstrate that an extension of the effective model depth with recurrence, memory, and test-time compute scaling substantially enhances reasoning capabilities.)"
        ],
        "conclusion": "이 연구는 반복과 메모리, 테스트 시간 계산 확장을 통해 모델의 다단계 추론 능력을 크게 향상시킬 수 있음을 보여줌.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    }
]