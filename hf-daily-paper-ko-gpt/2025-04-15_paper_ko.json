[
    {
        "paper": {
            "id": "2504.10479",
            "authors": [
                {
                    "_id": "67fdd08bda7816922cb67e54",
                    "name": "Jinguo Zhu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e55",
                    "user": {
                        "_id": "619507e7b74b6c591f794340",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                        "isPro": false,
                        "fullname": "Weiyun Wang",
                        "user": "Weiyun1025",
                        "type": "user"
                    },
                    "name": "Weiyun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:09:23.250Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e56",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e57",
                    "name": "Zhaoyang Liu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e58",
                    "user": {
                        "_id": "64804866c7f87934d082bb25",
                        "avatarUrl": "/avatars/41761226c79ac16e48d4c4cb84362adb.svg",
                        "isPro": false,
                        "fullname": "Yeshenglong",
                        "user": "Yeshenglong",
                        "type": "user"
                    },
                    "name": "Shenglong Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:09:57.293Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e59",
                    "user": {
                        "_id": "6541efc9109d78427198ea40",
                        "avatarUrl": "/avatars/c1252dd7da2e53b0b6757bf392139cdf.svg",
                        "isPro": false,
                        "fullname": "Lixin Gu",
                        "user": "gulixin0922",
                        "type": "user"
                    },
                    "name": "Lixin Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:10:04.312Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e5a",
                    "user": {
                        "_id": "6495a4d6d1cec0e2c2c96cdd",
                        "avatarUrl": "/avatars/e8b42301514094c8af6fa1e5fcb53119.svg",
                        "isPro": false,
                        "fullname": "Duan Yuchen",
                        "user": "duanyuchen",
                        "type": "user"
                    },
                    "name": "Yuchen Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:10:30.238Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e5b",
                    "name": "Hao Tian",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e5c",
                    "user": {
                        "_id": "63e4562f9db5da2dc1f3b520",
                        "avatarUrl": "/avatars/f4eecf1396b05e1c72436e7026d85cef.svg",
                        "isPro": false,
                        "fullname": "Weijie Su",
                        "user": "jackroos",
                        "type": "user"
                    },
                    "name": "Weijie Su",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:10:39.196Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e5d",
                    "name": "Jie Shao",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e5e",
                    "name": "Zhangwei Gao",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e5f",
                    "user": {
                        "_id": "6579b818563044badca392fc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6579b818563044badca392fc/XTKQ9Lhceibp9dnQADPQF.jpeg",
                        "isPro": false,
                        "fullname": "cuierfei",
                        "user": "cuierfei",
                        "type": "user"
                    },
                    "name": "Erfei Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:10:59.950Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e60",
                    "user": {
                        "_id": "6571382c7644d1128561cebe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bnJ-T3k_w1g1Mr7b7LglK.jpeg",
                        "isPro": false,
                        "fullname": "Cao Yue",
                        "user": "yuecao0119",
                        "type": "user"
                    },
                    "name": "Yue Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:11:07.485Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e61",
                    "name": "Yangzhou Liu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e62",
                    "name": "Weiye Xu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e63",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e64",
                    "name": "Jiahao Wang",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e65",
                    "user": {
                        "_id": "67c6fd7d85d2167189fce0e4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qpvzOJWcUJvD03l5yTBcN.jpeg",
                        "isPro": false,
                        "fullname": "Han Lv",
                        "user": "Hanrandom",
                        "type": "user"
                    },
                    "name": "Han Lv",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:11:58.666Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e66",
                    "name": "Dengnian Chen",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e67",
                    "user": {
                        "_id": "6751a10224431db3bed1f701",
                        "avatarUrl": "/avatars/06dcc5ba6cec8aff217a8bbc7a7d3b73.svg",
                        "isPro": false,
                        "fullname": "Songze Li",
                        "user": "CatCatCat36",
                        "type": "user"
                    },
                    "name": "Songze Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:12:32.493Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e68",
                    "user": {
                        "_id": "65b9d9961fe588f824fde191",
                        "avatarUrl": "/avatars/a9245958cc998a4b4b870bf2490fdaee.svg",
                        "isPro": false,
                        "fullname": "Yinan He",
                        "user": "yinanhe",
                        "type": "user"
                    },
                    "name": "Yinan He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:12:39.591Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e69",
                    "name": "Tan Jiang",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e6a",
                    "user": {
                        "_id": "647ea4aae4d52fe0e021bce4",
                        "avatarUrl": "/avatars/9e68004d04403acff004113c856451a6.svg",
                        "isPro": false,
                        "fullname": "Jiapeng Luo",
                        "user": "woolpeeker",
                        "type": "user"
                    },
                    "name": "Jiapeng Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:12:51.845Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e6b",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e6c",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:13:07.750Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e6d",
                    "name": "Botian Shi",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e6e",
                    "name": "Xingcheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e6f",
                    "user": {
                        "_id": "64b3fd42eec33e27dcc4c941",
                        "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
                        "isPro": false,
                        "fullname": "Wenqi Shao",
                        "user": "wqshao126",
                        "type": "user"
                    },
                    "name": "Wenqi Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:15:27.767Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e70",
                    "user": {
                        "_id": "66b593026cef22e6ba6adb8a",
                        "avatarUrl": "/avatars/8a94d55b85177e84d65dd0bd537e335f.svg",
                        "isPro": false,
                        "fullname": "JunjunHe",
                        "user": "JunjunHe",
                        "type": "user"
                    },
                    "name": "Junjun He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:15:19.980Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e71",
                    "user": {
                        "_id": "654395472cfe8660a3a492bb",
                        "avatarUrl": "/avatars/2010370112200e83ced979d3d4c87735.svg",
                        "isPro": false,
                        "fullname": "xiong",
                        "user": "xiongyingtong",
                        "type": "user"
                    },
                    "name": "Yingtong Xiong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:15:12.302Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e72",
                    "user": {
                        "_id": "6684eb7045d8ceb446ffe9ff",
                        "avatarUrl": "/avatars/30586f289031696253842fcd4a8788b9.svg",
                        "isPro": false,
                        "fullname": "wenwenQu",
                        "user": "wenwenQu",
                        "type": "user"
                    },
                    "name": "Wenwen Qu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:14:47.178Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e73",
                    "name": "Peng Sun",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e74",
                    "name": "Penglong Jiao",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e75",
                    "name": "Lijun Wu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e76",
                    "user": {
                        "_id": "63527f4e7d071f23d085ad45",
                        "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
                        "isPro": false,
                        "fullname": "KAIPENG ZHANG",
                        "user": "kpzhang",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:13:25.647Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e77",
                    "name": "Huipeng Deng",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e78",
                    "name": "Jiaye Ge",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e79",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e7a",
                    "name": "Limin Wang",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e7b",
                    "name": "Min Dou",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e7c",
                    "user": {
                        "_id": "65ead3ea908526a39082e641",
                        "avatarUrl": "/avatars/dcf870695fd56b06ca03d82f831e9019.svg",
                        "isPro": false,
                        "fullname": "Lewei Lu",
                        "user": "luotto",
                        "type": "user"
                    },
                    "name": "Lewei Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:14:05.716Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e7d",
                    "name": "Xizhou Zhu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e7e",
                    "name": "Tong Lu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e7f",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:13:59.084Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e80",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e81",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e82",
                    "user": {
                        "_id": "64d1c560c0c627dfa71bdbe0",
                        "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
                        "isPro": false,
                        "fullname": "wenhai.wang",
                        "user": "wangwhcore",
                        "type": "user"
                    },
                    "name": "Wenhai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:13:51.523Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yA_DVt5PKVaFjiY-E93e5.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/94PuKH6M3nhmkVS8RXtb4.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/qvaW8f5868-P75pyzr6XT.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/4GqLJBNhM2rUqS_4mawkk.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yBSGw85-QcThBlXv2tLj0.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/CHvbRf2VJg6amEGS9OlEE.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VCBvp4nhvzfokH-viUz1g.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/HIC3iirmK6jaOax4GlDYt.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/TLlWnFPBguYJsvuGGqykz.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/xJ-sFLuCkv1bqL_TXwxgH.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/Hsv_hehORgnRmL68xSqmd.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/S1fnELtJWFU_FlEozV0Ik.png"
            ],
            "publishedAt": "2025-04-14T17:59:25.000Z",
            "submittedOnDailyAt": "2025-04-15T01:57:22.403Z",
            "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "619507e7b74b6c591f794340",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                "isPro": false,
                "fullname": "Weiyun Wang",
                "user": "Weiyun1025",
                "type": "user"
            },
            "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.",
            "upvotes": 157,
            "discussionId": "67fdd08cda7816922cb67ec2",
            "projectPage": "https://internvl.github.io/blog/2025-04-11-InternVL-3.0/",
            "githubRepo": "https://github.com/OpenGVLab/InternVL",
            "ai_keywords": [
                "multimodal pre-training paradigm",
                "text-only large language model (LLM)",
                "multimodal large language model (MLLM)",
                "diverse multimodal data",
                "pure-text corpora",
                "unified training paradigm",
                "variable visual position encoding (V2PE)",
                "extended multimodal contexts",
                "supervised fine-tuning (SFT)",
                "mixed preference optimization (MPO)",
                "test-time scaling strategies",
                "optimized training infrastructure",
                "MMMU benchmark",
                "open-source MLLMs",
                "ChatGPT-4o",
                "Claude 3.5 Sonnet",
                "Gemini 2.5 Pro"
            ]
        },
        "translation_title": "InternVL3: 오픈소스 멀티모달 모델을 위한 고급 훈련 및 테스트 시간 레시피 탐색",
        "purpose": "멀티모달 대형 언어 모델의 성능 향상 및 복잡성 문제 해결을 위한 새로운 훈련 접근 방식을 개발",
        "method": [
            "InternVL3는 다양한 멀티모달 데이터와 텍스트 데이터를 함께 사용하여 훈련하는 통합 훈련 패러다임을 적용함(InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage.)",
            "가변 시각 위치 인코딩(V2PE)을 도입하여 확장된 멀티모달 컨텍스트를 지원함(InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts.)",
            "종합적인 성능 향상을 위해 감독 세부 조정(SFT) 및 혼합 선호 최적화(MPO)와 같은 고급 후 훈련 기법을 적용함(employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO))."
        ],
        "conclusion": "InternVL3는 다양한 멀티모달 작업에서 우수한 성능을 발휘하며, 새로운 최첨단 성과를 달성했다.",
        "keywords": [
            "Multimodal Learning",
            "Large Language Models",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2504.08791",
            "authors": [
                {
                    "_id": "67fdbde764a418633ee9fa1b",
                    "user": {
                        "_id": "647466b8b68461d5cf795e3c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
                        "isPro": false,
                        "fullname": "LIKirin",
                        "user": "LIKirin",
                        "type": "user"
                    },
                    "name": "Zonghang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:53.716Z",
                    "hidden": false
                },
                {
                    "_id": "67fdbde764a418633ee9fa1c",
                    "user": {
                        "_id": "669e0c108b279f0a2704a5ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669e0c108b279f0a2704a5ba/uPyioyK0RWSG7CT52xBao.png",
                        "isPro": false,
                        "fullname": "TaoLi",
                        "user": "LiPhilip",
                        "type": "user"
                    },
                    "name": "Tao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:50.518Z",
                    "hidden": false
                },
                {
                    "_id": "67fdbde764a418633ee9fa1d",
                    "user": {
                        "_id": "6513c6c9c1fbded3b1977acc",
                        "avatarUrl": "/avatars/d169b3462f952c5639e1471ed8bf84c9.svg",
                        "isPro": false,
                        "fullname": "Wenjiao Feng",
                        "user": "NeuronNomad",
                        "type": "user"
                    },
                    "name": "Wenjiao Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:15:55.453Z",
                    "hidden": false
                },
                {
                    "_id": "67fdbde764a418633ee9fa1e",
                    "name": "Mohsen Guizani",
                    "hidden": false
                },
                {
                    "_id": "67fdbde764a418633ee9fa1f",
                    "name": "Hongfang Yu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/647466b8b68461d5cf795e3c/Z-XPxpdhg3sBxHCfogGzf.mp4"
            ],
            "publishedAt": "2025-04-07T13:46:21.000Z",
            "submittedOnDailyAt": "2025-04-15T00:38:03.208Z",
            "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
            "submittedOnDailyBy": {
                "_id": "647466b8b68461d5cf795e3c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
                "isPro": false,
                "fullname": "LIKirin",
                "user": "LIKirin",
                "type": "user"
            },
            "summary": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers\nfor running frontier large language models (LLMs) on home devices. While\nconsumer hardware is getting stronger and model quantization is improving,\nexisting end-side solutions still demand GPU clusters, large RAM/VRAM, and high\nbandwidth, far beyond what a common home cluster can handle. This paper\nintroduces prima.cpp, a distributed inference system that runs 70B-scale models\non everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and\ncross-platform support. It uses mmap to manage model weights and introduces\npiped-ring parallelism with prefetching to hide disk loading. By modeling\nheterogeneity in computation, communication, disk, memory (and its management\nbehavior), and OS, it optimally assigns model layers to each device's CPU and\nGPU, further reducing token latency. An elegant algorithm named Halda is\nproposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a\ncommon four-node home cluster. It outperforms llama.cpp, exo, and dllama on\n30B+ models while keeping memory pressure below 6%. This brings frontier\n30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home\nassistants, making advanced AI truly accessible to individuals. The code is\nopen source and available at https://github.com/Lizonghang/prima.cpp.",
            "upvotes": 85,
            "discussionId": "67fdbdeb64a418633ee9fb58",
            "projectPage": "https://github.com/Lizonghang/prima.cpp",
            "githubRepo": "https://github.com/Lizonghang/prima.cpp",
            "ai_keywords": [
                "distributed inference system",
                "mmap",
                "piped-ring parallelism",
                "prefetching",
                "heterogeneity",
                "token latency",
                "Halda algorithm",
                "NP-hard assignment problem",
                "llama.cpp",
                "exo",
                "dllama",
                "Llama 3",
                "DeepSeek R1",
                "Qwen 2.5",
                "QwQ"
            ]
        },
        "translation_title": "PRIMA.CPP: 저자원 일상 홈 클러스터에서 70B 규모 LLM 추론 속도 증가",
        "purpose": "일반 가정용 기기에서 70B 규모 모델을 효율적으로 실행하기 위한 분산 추론 시스템 구현",
        "method": [
            "CPU/GPU, 저메모리 RAM/VRAM, Wi-Fi를 혼합 사용하여 모델을 실행하는 분산 추론 시스템 prima.cpp를 소개함(This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support.)",
            "모델 가중치를 관리하기 위해 mmap을 사용하고, 디스크 로드를 숨기기 위해 piped-ring 병렬 처리를 도입함(It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading.)",
            "장치의 CPU와 GPU에 최적의 모델 계층을 할당하기 위한 할다(Halda)라는 알고리즘을 제안함(An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem.)"
        ],
        "conclusion": "상기 방법을 통해 30B-70B 모델을 일반 가정용 기기에서 실행 가능하게 하여 AI 접근성을 높였으며, 코드도 오픈 소스임.",
        "keywords": [
            "Large Language Models",
            "Robotics",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2504.09925",
            "authors": [
                {
                    "_id": "67fdb5f1913c97aa32f130bd",
                    "user": {
                        "_id": "6625ef13605f46d05c1d0031",
                        "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
                        "isPro": false,
                        "fullname": "Zheng Liu",
                        "user": "starriver030515",
                        "type": "user"
                    },
                    "name": "Zheng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:57.129Z",
                    "hidden": false
                },
                {
                    "_id": "67fdb5f1913c97aa32f130be",
                    "user": {
                        "_id": "672dbf1bec84b9c33412488f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/V6o6K28ydpkaS9XFA4Ac3.png",
                        "isPro": false,
                        "fullname": "Mengjie Liu",
                        "user": "Balalauuoo",
                        "type": "user"
                    },
                    "name": "Mengjie Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:16:15.647Z",
                    "hidden": false
                },
                {
                    "_id": "67fdb5f1913c97aa32f130bf",
                    "name": "Jingzhou Chen",
                    "hidden": false
                },
                {
                    "_id": "67fdb5f1913c97aa32f130c0",
                    "user": {
                        "_id": "672da19bb2f2dc21e176b0de",
                        "avatarUrl": "/avatars/a942ad1c467b865cb9530927fe13f2b7.svg",
                        "isPro": false,
                        "fullname": "Jingwei Xu",
                        "user": "jingwei-xu-00",
                        "type": "user"
                    },
                    "name": "Jingwei Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:16:53.642Z",
                    "hidden": false
                },
                {
                    "_id": "67fdb5f1913c97aa32f130c1",
                    "name": "Bin Cui",
                    "hidden": false
                },
                {
                    "_id": "67fdb5f1913c97aa32f130c2",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:17:01.171Z",
                    "hidden": false
                },
                {
                    "_id": "67fdb5f1913c97aa32f130c3",
                    "name": "Wentao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T06:33:29.000Z",
            "submittedOnDailyAt": "2025-04-15T00:17:19.508Z",
            "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
            "submittedOnDailyBy": {
                "_id": "6625ef13605f46d05c1d0031",
                "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
                "isPro": false,
                "fullname": "Zheng Liu",
                "user": "starriver030515",
                "type": "user"
            },
            "summary": "We introduce FUSION, a family of multimodal large language models (MLLMs)\nwith a fully vision-language alignment and integration paradigm. Unlike\nexisting methods that primarily rely on late-stage modality interaction during\nLLM decoding, our approach achieves deep, dynamic integration throughout the\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\nEncoding, incorporating textual information in vision encoding to achieve\npixel-level integration. We further design Context-Aware Recursive Alignment\nDecoding that recursively aggregates visual features conditioned on textual\ncontext during decoding, enabling fine-grained, question-level semantic\nintegration. To guide feature mapping and mitigate modality discrepancies, we\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\nfeature integration. Building on these foundations, we train FUSION at two\nscales-3B, 8B-and demonstrate that our full-modality integration approach\nsignificantly outperforms existing methods with only 630 vision tokens.\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\nLLaVA-NeXT on over half of the benchmarks under same configuration without\ndynamic resolution, highlighting the effectiveness of our approach. We release\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION",
            "upvotes": 32,
            "discussionId": "67fdb5f3913c97aa32f13141",
            "githubRepo": "https://github.com/starriver030515/FUSION",
            "ai_keywords": [
                "multimodal large language models (MLLMs)",
                "fully vision-language alignment",
                "late-stage modality interaction",
                "LLM decoding",
                "Text-Guided Unified Vision Encoding",
                "pixel-level integration",
                "Context-Aware Recursive Alignment Decoding",
                "fine-grained, question-level semantic integration",
                "Dual-Supervised Semantic Mapping Loss",
                "Synthesized Language-Driven Question-Answer (QA)",
                "data synthesis method",
                "high-quality QA pairs",
                "text-guided feature integration",
                "full-modality integration",
                "vision tokens",
                "Cambrian-1 8B",
                "Florence-VL 8B",
                "ablation studies",
                "LLaVA-NeXT",
                "dynamic resolution"
            ]
        },
        "translation_title": "FUSION: 비전-언어 표현의 완전한 통합을 통한 심층 교차 모달 이해",
        "purpose": "심층적이고 동적인 비전-언어 통합을 통해 성능을 향상시키기 위한 연구",
        "method": [
            "Text-Guided Unified Vision Encoding을 통해 비전 인코딩에 텍스트 정보를 포함시켜 픽셀 수준 통합을 달성함(we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration.)",
            "Context-Aware Recursive Alignment Decoding을 설계하여 디코딩 과정에서 텍스트 맥락에 따라 시각적 특징을 집계함(we further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding.)",
            "Dual-Supervised Semantic Mapping Loss를 개발하여 특징 매핑을 안내하고 모달리티 간 불일치를 완화함(we develop Dual-Supervised Semantic Mapping Loss to guide feature mapping and mitigate modality discrepancies.)"
        ],
        "conclusion": "FUSION은 630개의 비전 토큰으로도 기존 방법보다 훨씬 우수한 성능을 보였으며, 특히 FUSION 3B 모델이 Cambrian-1 8B와 Florence-VL 8B를 능가함.",
        "keywords": [
            "Multimodal Learning",
            "Vision-Language Models",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2504.08837",
            "authors": [
                {
                    "_id": "67fdc483ba0d61664fb0a19d",
                    "user": {
                        "_id": "65bf52f0259bc6caeb74f8bf",
                        "avatarUrl": "/avatars/b38392e954466df784a5760ded5df804.svg",
                        "isPro": false,
                        "fullname": "Haozhe Wang",
                        "user": "JasperHaozhe",
                        "type": "user"
                    },
                    "name": "Haozhe Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:20.455Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc483ba0d61664fb0a19e",
                    "name": "Chao Qu",
                    "hidden": false
                },
                {
                    "_id": "67fdc483ba0d61664fb0a19f",
                    "user": {
                        "_id": "6772524ed6f92f429bd343a3",
                        "avatarUrl": "/avatars/211e0c4641b2d048b0136d7cdeef2483.svg",
                        "isPro": false,
                        "fullname": "Zuming Huang",
                        "user": "zuminghuang",
                        "type": "user"
                    },
                    "name": "Zuming Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:17:55.505Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc483ba0d61664fb0a1a0",
                    "name": "Wei Chu",
                    "hidden": false
                },
                {
                    "_id": "67fdc483ba0d61664fb0a1a1",
                    "name": "Fangzhen Lin",
                    "hidden": false
                },
                {
                    "_id": "67fdc483ba0d61664fb0a1a2",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-15T02:29:24.168Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T17:41:56.000Z",
            "submittedOnDailyAt": "2025-04-15T01:01:12.820Z",
            "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6313a86154e6e5d9f0f94e04",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                "isPro": false,
                "fullname": "Wenhu Chen",
                "user": "wenhu",
                "type": "user"
            },
            "summary": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated\ngreat potential in solving challenging problems through explicit reflection.\nThey significantly outperform the best fast-thinking models, such as GPT-4o, on\nvarious math and science benchmarks. However, their multimodal reasoning\ncapabilities remain on par with fast-thinking models. For instance, GPT-o1's\nperformance on benchmarks like MathVista, MathVerse, and MathVision is similar\nto fast-thinking models. In this paper, we aim to enhance the slow-thinking\ncapabilities of vision-language models using reinforcement learning (without\nrelying on distillation) to advance the state of the art. First, we adapt the\nGRPO algorithm with a novel technique called Selective Sample Replay (SSR) to\naddress the vanishing advantages problem. While this approach yields strong\nperformance, the resulting RL-trained models exhibit limited self-reflection or\nself-verification. To further encourage slow-thinking, we introduce Forced\nRethinking, which appends a textual rethinking trigger to the end of initial\nrollouts in RL training, explicitly enforcing a self-reflection reasoning step.\nBy combining these two techniques, our model, VL-Rethinker, advances\nstate-of-the-art scores on MathVista, MathVerse, and MathVision to achieve\n80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source\nSoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench,\nnarrowing the gap with GPT-o1.",
            "upvotes": 32,
            "discussionId": "67fdc484ba0d61664fb0a1db",
            "projectPage": "https://tiger-ai-lab.github.io/VL-Rethinker/",
            "githubRepo": "https://github.com/TIGER-AI-Lab/VL-Rethinker/",
            "ai_keywords": [
                "GRPO algorithm",
                "Selective Sample Replay (SSR)",
                "reinforcement learning",
                "vanishing advantages problem",
                "self-reflection",
                "self-verification",
                "Forced Rethinking",
                "VL-Rethinker",
                "MathVista",
                "MathVerse",
                "MathVision",
                "MMMU-Pro",
                "EMMA",
                "MEGA-Bench",
                "multi-disciplinary benchmarks",
                "state-of-the-art (SoTA)"
            ]
        },
        "translation_title": "VL-Rethinker: 비전-언어 모델의 자기 반성을 강화하기 위한 강화 학습 접근법",
        "purpose": "비전-언어 모델의 느린 사고 능력을 강화하여 최신 기술을 발전시키고자 함",
        "method": [
            "GRPO 알고리즘을 Selective Sample Replay(SSR)이라는 새로운 기법과 함께 적용하여 사라지는 이점 문제를 해결함(First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem.)",
            "강화 학습 훈련의 초기 롤아웃 끝에 텍스트 재고찰 트리거를 추가하여 자기 반성 추론 단계를 강제로 시행하는 Forced Rethinking 기법을 도입함(To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step.)",
            "이 두 가지 기법을 결합하여 VL-Rethinker 모델을 구성하고 다양한 벤치마크에서 성능을 향상시킴(By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively.)"
        ],
        "conclusion": "VL-Rethinker는 수학 및 다학제 벤치마크에서 최신 성과를 달성하며, GPT-o1과의 격차를 좁힘.",
        "keywords": [
            "Reinforcement Learning",
            "Multimodal Learning",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2504.09643",
            "authors": [
                {
                    "_id": "67fe391f684ac7f04ce21df7",
                    "name": "Nikita Sorokin",
                    "hidden": false
                },
                {
                    "_id": "67fe391f684ac7f04ce21df8",
                    "name": "Ivan Sedykh",
                    "hidden": false
                },
                {
                    "_id": "67fe391f684ac7f04ce21df9",
                    "name": "Valentin Malykh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-13T16:34:17.000Z",
            "submittedOnDailyAt": "2025-04-15T09:27:11.869Z",
            "title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking",
            "submittedOnDailyBy": {
                "_id": "636a9a07e3ad78bc68b1a5a2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668020490988-636a9a07e3ad78bc68b1a5a2.jpeg",
                "isPro": false,
                "fullname": "Dmitry Abulkhanov",
                "user": "mponty",
                "type": "user"
            },
            "summary": "Generating high-quality code that solves complex programming tasks is\nchallenging, especially with current decoder-based models that produce highly\nstochastic outputs. In code generation, even minor errors can easily break the\nentire solution. Leveraging multiple sampled solutions can significantly\nimprove the overall output quality.\n  One effective way to enhance code generation is by pairing a code generation\nmodel with a reranker model, which selects the best solution from the generated\nsamples. We propose a novel iterative self-training approach for self-training\nreranker models using Proximal Policy Optimization (PPO), aimed at improving\nboth reranking accuracy and the overall code generation process. Unlike\ntraditional PPO approaches, where the focus is on optimizing a generative model\nwith a reward model, our approach emphasizes the development of a robust\nreward/reranking model. This model improves the quality of generated code\nthrough reranking and addresses problems and errors that the reward model might\noverlook during PPO alignment with the reranker. Our method iteratively refines\nthe training dataset by re-evaluating outputs, identifying high-scoring\nnegative examples, and incorporating them into the training loop, that boosting\nmodel performance.\n  Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter\nmodel outperforms a 33B model in code generation quality while being three\ntimes faster. Moreover, it achieves performance comparable to GPT-4 and\nsurpasses it in one programming language.",
            "upvotes": 27,
            "discussionId": "67fe394e684ac7f04ce229e8",
            "ai_keywords": [
                "Proximal Policy Optimization (PPO)",
                "reranker model",
                "iterative self-training",
                "reward/reranking model",
                "MultiPL-E dataset",
                "GPT-4"
            ]
        },
        "translation_title": "강화된 재정렬을 통한 코드 생성을 위한 반복 자기 훈련",
        "purpose": "복잡한 프로그래밍 과제 해결을 위한 고품질 코드 생성을 향상시키는 방법 탐색",
        "method": [
            "코드 생성 모델과 재정렬 모델을 결합하여 생성된 샘플 중 최상의 솔루션을 선택함(One effective way to enhance code generation is by pairing a code generation model with a reranker model, which selects the best solution from the generated samples.)",
            "Proximal Policy Optimization (PPO)를 사용해 자기 훈련 재정렬 모델을 위한 새로운 반복 자기 훈련 접근법을 제안함(We propose a novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process.)",
            "훈련 데이터셋을 반복적으로 정제하고 높은 점수를 받은 부정적인 예제를 재훈련 루프에 통합하여 모델 성능을 향상시킴(This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker.)"
        ],
        "conclusion": "우리의 방법을 통해 코드 생성 품질에서 33B 모델을 초월하며, 13.4B 파라미터 모델이 3배 빠른 성능을 보여줌.",
        "keywords": [
            "Code Generation",
            "Reinforcement Learning",
            "Natural Language Processing"
        ]
    }
]