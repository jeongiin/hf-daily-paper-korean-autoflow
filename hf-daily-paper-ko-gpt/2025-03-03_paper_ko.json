[
    {
        "paper": {
            "id": "2502.20730",
            "authors": [
                {
                    "_id": "67c514aba3d873e41624a082",
                    "user": {
                        "_id": "63664c8fa2abcdf2fd6425ed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
                        "isPro": false,
                        "fullname": "Li Zhuoqun",
                        "user": "lzq2021",
                        "type": "user"
                    },
                    "name": "Zhuoqun Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T08:07:26.218Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a083",
                    "user": {
                        "_id": "64a4ceda9a90f701134189b7",
                        "avatarUrl": "/avatars/859a189c5d2ae2fcb9aa2d79104fbfe7.svg",
                        "isPro": false,
                        "fullname": "Haiyang Yu",
                        "user": "yhycai",
                        "type": "user"
                    },
                    "name": "Haiyang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T09:31:12.493Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a084",
                    "user": {
                        "_id": "63ef664304b0e373992a2633",
                        "avatarUrl": "/avatars/cba554ff88bd8b68ae51bea8ee991d13.svg",
                        "isPro": false,
                        "fullname": "Xuanang Chen",
                        "user": "xuanang",
                        "type": "user"
                    },
                    "name": "Xuanang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:29:31.384Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a085",
                    "user": {
                        "_id": "6711c702f858a456b4b9f3a4",
                        "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
                        "isPro": false,
                        "fullname": "Hongyu  Lin",
                        "user": "sanmusunrise",
                        "type": "user"
                    },
                    "name": "Hongyu Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:28:09.791Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a086",
                    "user": {
                        "_id": "6216496a9b34d2fb49144599",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
                        "isPro": false,
                        "fullname": "Yaojie Lu",
                        "user": "luyaojie",
                        "type": "user"
                    },
                    "name": "Yaojie Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:29:38.957Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a087",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a088",
                    "user": {
                        "_id": "65e99a77e71555ed193609cf",
                        "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
                        "isPro": false,
                        "fullname": "Xianpei Han",
                        "user": "xphan",
                        "type": "user"
                    },
                    "name": "Xianpei Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:29:51.007Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a089",
                    "user": {
                        "_id": "66641b2fd8e1e34bc621e688",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
                        "isPro": false,
                        "fullname": "Yongbin Li",
                        "user": "Yongbin-Li",
                        "type": "user"
                    },
                    "name": "Yongbin Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:29:57.561Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a08a",
                    "name": "Le Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-28T05:23:10.000Z",
            "title": "DeepSolution: Boosting Complex Engineering Solution Design via\n  Tree-based Exploration and Bi-point Thinking",
            "summary": "Designing solutions for complex engineering challenges is crucial in human\nproduction activities. However, previous research in the retrieval-augmented\ngeneration (RAG) field has not sufficiently addressed tasks related to the\ndesign of complex engineering solutions. To fill this gap, we introduce a new\nbenchmark, SolutionBench, to evaluate a system's ability to generate complete\nand feasible solutions for engineering problems with multiple complex\nconstraints. To further advance the design of complex engineering solutions, we\npropose a novel system, SolutionRAG, that leverages the tree-based exploration\nand bi-point thinking mechanism to generate reliable solutions. Extensive\nexperimental results demonstrate that SolutionRAG achieves state-of-the-art\n(SOTA) performance on the SolutionBench, highlighting its potential to enhance\nthe automation and reliability of complex engineering solution design in\nreal-world applications.",
            "upvotes": 18,
            "discussionId": "67c514aca3d873e41624a10b",
            "githubRepo": "https://github.com/Li-Z-Q/DeepSolution"
        },
        "translation_title": "DeepSolution: 복잡한 공학 솔루션 설계를 향상시키기 위한 트리 기반 탐색 및 바이 포인트 사고",
        "purpose": "복잡한 공학 문제에 대한 완전하고 실행 가능한 솔루션 생성을 평가하기 위한 새로운 벤치마크 및 시스템 연구",
        "method": [
            "새로운 벤치마크인 SolutionBench를 도입해 복잡한 제한 조건을 가진 공학 문제를 위한 솔루션 능력을 평가함(To fill this gap, we introduce a new benchmark, SolutionBench, to evaluate a system's ability to generate complete and feasible solutions for engineering problems with multiple complex constraints.)",
            "트리 기반 탐색과 바이 포인트 사고 메커니즘을 활용하여 신뢰성 있는 솔루션을 생성하는 시스템 SolutionRAG를 제안함(To further advance the design of complex engineering solutions, we propose a novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions.)",
            "SolutionRAG가 SolutionBench에서 최첨단 성능(SOTA)을 달성했음을 실험적으로 입증함(Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench.)"
        ],
        "conclusion": "SolutionRAG는 복잡한 공학 솔루션 설계의 자동화와 신뢰성을 향상시키는 잠재력을 보여줌.",
        "keywords": [
            "Robotics",
            "Image Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2502.18600",
            "authors": [
                {
                    "_id": "67c0a8058589d8ecb79d472b",
                    "user": {
                        "_id": "6594b1bb57a556fbe162915e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594b1bb57a556fbe162915e/WuYxqbbvaJaT-xsk5KhoT.jpeg",
                        "isPro": false,
                        "fullname": "Silei Xu",
                        "user": "sileixu",
                        "type": "user"
                    },
                    "name": "Silei Xu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-27T18:01:14.543Z",
                    "hidden": false
                },
                {
                    "_id": "67c0a8058589d8ecb79d472c",
                    "name": "Wenhao Xie",
                    "hidden": false
                },
                {
                    "_id": "67c0a8058589d8ecb79d472d",
                    "name": "Lingxiao Zhao",
                    "hidden": false
                },
                {
                    "_id": "67c0a8058589d8ecb79d472e",
                    "user": {
                        "_id": "5efd09cf49ed724c8a135868",
                        "avatarUrl": "/avatars/af12bc94657979677a9f26183f0c9727.svg",
                        "isPro": false,
                        "fullname": "Pengcheng He",
                        "user": "DeBERTa",
                        "type": "user"
                    },
                    "name": "Pengcheng He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:30:43.479Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T19:36:06.000Z",
            "title": "Chain of Draft: Thinking Faster by Writing Less",
            "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nsolving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT)\nprompting, which emphasizes verbose, step-by-step reasoning. However, humans\ntypically employ a more efficient strategy: drafting concise intermediate\nthoughts that capture only essential information. In this work, we propose\nChain of Draft (CoD), a novel paradigm inspired by human cognitive processes,\nwhere LLMs generate minimalistic yet informative intermediate reasoning outputs\nwhile solving tasks. By reducing verbosity and focusing on critical insights,\nCoD matches or surpasses CoT in accuracy while using as little as only 7.6% of\nthe tokens, significantly reducing cost and latency across various reasoning\ntasks.",
            "upvotes": 11,
            "discussionId": "67c0a8078589d8ecb79d47ed"
        },
        "translation_title": "Chain of Draft: 적게 써서 더 빨리 생각하기",
        "purpose": "효율적인 중간 사고 포착을 통한 문제 해결 접근 방식 개선",
        "method": [
            "인간의 인지 과정을 바탕으로 LLM이 최소한의 정보로 중간 추론 출력을 생성하는 Chain of Draft(CoD) 패러다임을 제안함(In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks.)",
            "CoD는 7.6%의 토큰만 사용하면서 verbosity를 줄이고 핵심 통찰에 집중하여 CoT와 유사하거나 더 높은 정확성을 달성함(By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens.)"
        ],
        "conclusion": "CoD는 다양한 추론 작업에서 비용과 대기 시간을 크게 줄이면서도 높은 정확성을 유지함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Reasoning Tasks"
        ]
    },
    {
        "paper": {
            "id": "2502.21318",
            "authors": [
                {
                    "_id": "67c5c13ca10c7059c3d3d4c9",
                    "name": "L. Degeorge",
                    "hidden": false
                },
                {
                    "_id": "67c5c13ca10c7059c3d3d4ca",
                    "user": {
                        "_id": "66f971c83d94062a4aa808ef",
                        "avatarUrl": "/avatars/f1d6c4d85d20fd4a614278ecd784c772.svg",
                        "isPro": false,
                        "fullname": "Arijit Ghosh",
                        "user": "arijitghosh",
                        "type": "user"
                    },
                    "name": "A. Ghosh",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-03T14:48:37.617Z",
                    "hidden": false
                },
                {
                    "_id": "67c5c13ca10c7059c3d3d4cb",
                    "name": "N. Dufour",
                    "hidden": false
                },
                {
                    "_id": "67c5c13ca10c7059c3d3d4cc",
                    "name": "D. Picard",
                    "hidden": false
                },
                {
                    "_id": "67c5c13ca10c7059c3d3d4cd",
                    "name": "V. Kalogeiton",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-28T18:59:42.000Z",
            "title": "How far can we go with ImageNet for Text-to-Image generation?",
            "summary": "Recent text-to-image (T2I) generation models have achieved remarkable results\nby training on billion-scale datasets, following a `bigger is better' paradigm\nthat prioritizes data quantity over quality. We challenge this established\nparadigm by demonstrating that strategic data augmentation of small,\nwell-curated datasets can match or outperform models trained on massive\nweb-scraped collections. Using only ImageNet enhanced with well-designed text\nand image augmentations, we achieve a +2 overall score over SD-XL on GenEval\nand +5 on DPGBench while using just 1/10th the parameters and 1/1000th the\ntraining images. Our results suggest that strategic data augmentation, rather\nthan massive datasets, could offer a more sustainable path forward for T2I\ngeneration.",
            "upvotes": 9,
            "discussionId": "67c5c145a10c7059c3d3d693",
            "projectPage": "https://lucasdegeorge.github.io/projects/t2i_imagenet/",
            "githubRepo": "https://github.com/lucasdegeorge/T2I-ImageNet"
        },
        "translation_title": "ImageNet을 활용한 Text-to-Image 생성의 한계는 어디까지인가?",
        "purpose": "대량 데이터셋이 아닌 소규모, 잘 정리된 데이터셋의 전략적 데이터 증강을 통해 성능을 향상시키기 위함",
        "method": [
            "ImageNet에 잘 설계된 텍스트 및 이미지 증강을 추가하여 데이터셋을 강화함(Using only ImageNet enhanced with well-designed text and image augmentations)",
            "SD-XL에 비해 GenEval에서 +2, DPGBench에서 +5의 점수를 달성함(we achieve a +2 overall score over SD-XL on GenEval and +5 on DPGBench)",
            "모델 파라미터 수를 1/10로, 훈련 이미지 수를 1/1000으로 줄임(while using just 1/10th the parameters and 1/1000th the training images)"
        ],
        "conclusion": "전략적 데이터 증강이 T2I 생성에 더 지속 가능한 경로를 제공할 수 있음을 보여줌.",
        "keywords": [
            "Image Generation",
            "Computer Vision",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2502.18017",
            "authors": [
                {
                    "_id": "67bef5a6070ec160042d99f4",
                    "user": {
                        "_id": "657429d833e5a4bf5b278615",
                        "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
                        "isPro": false,
                        "fullname": "QiuchenWang",
                        "user": "autumncc",
                        "type": "user"
                    },
                    "name": "Qiuchen Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T12:15:57.850Z",
                    "hidden": false
                },
                {
                    "_id": "67bef5a6070ec160042d99f5",
                    "name": "Ruixue Ding",
                    "hidden": false
                },
                {
                    "_id": "67bef5a6070ec160042d99f6",
                    "user": {
                        "_id": "64892d31cbda0d1cdb956897",
                        "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg",
                        "isPro": false,
                        "fullname": "Zehui Chen",
                        "user": "lovesnowbest",
                        "type": "user"
                    },
                    "name": "Zehui Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:32:18.129Z",
                    "hidden": false
                },
                {
                    "_id": "67bef5a6070ec160042d99f7",
                    "user": {
                        "_id": "65351cbe6141b3927afaed17",
                        "avatarUrl": "/avatars/5abf5f2c4ab329e63a7f45c15c9dfb93.svg",
                        "isPro": false,
                        "fullname": "weiqi wu",
                        "user": "vickywu",
                        "type": "user"
                    },
                    "name": "Weiqi Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:32:12.075Z",
                    "hidden": false
                },
                {
                    "_id": "67bef5a6070ec160042d99f8",
                    "user": {
                        "_id": "62e8efb14210d3fe69eacb42",
                        "avatarUrl": "/avatars/2feadd75274bf353b910f4679ef72b39.svg",
                        "isPro": false,
                        "fullname": "Shihang Wang",
                        "user": "shihang",
                        "type": "user"
                    },
                    "name": "Shihang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:32:05.679Z",
                    "hidden": false
                },
                {
                    "_id": "67bef5a6070ec160042d99f9",
                    "user": {
                        "_id": "63a091e42fabbbb89991f5ce",
                        "avatarUrl": "/avatars/d55485b06461764c36c9edf9d6e8892c.svg",
                        "isPro": false,
                        "fullname": "pengjun xie",
                        "user": "xpjandy",
                        "type": "user"
                    },
                    "name": "Pengjun Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:31:59.813Z",
                    "hidden": false
                },
                {
                    "_id": "67bef5a6070ec160042d99fa",
                    "name": "Feng Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T09:26:12.000Z",
            "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic\n  Iterative Reasoning Agents",
            "summary": "Understanding information from visually rich documents remains a significant\nchallenge for traditional Retrieval-Augmented Generation (RAG) methods.\nExisting benchmarks predominantly focus on image-based question answering (QA),\noverlooking the fundamental challenges of efficient retrieval, comprehension,\nand reasoning within dense visual documents. To bridge this gap, we introduce\nViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich\ndocuments requiring complex reasoning. Based on it, we identify key limitations\nin current RAG approaches: (i) purely visual retrieval methods struggle to\neffectively integrate both textual and visual features, and (ii) previous\napproaches often allocate insufficient reasoning tokens, limiting their\neffectiveness. To address these challenges, we propose ViDoRAG, a novel\nmulti-agent RAG framework tailored for complex reasoning across visual\ndocuments. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy\nto effectively handle multi-modal retrieval. To further elicit the model's\nreasoning capabilities, we introduce an iterative agent workflow incorporating\nexploration, summarization, and reflection, providing a framework for\ninvestigating test-time scaling in RAG domains. Extensive experiments on\nViDoSeek validate the effectiveness and generalization of our approach.\nNotably, ViDoRAG outperforms existing methods by over 10% on the competitive\nViDoSeek benchmark.",
            "upvotes": 5,
            "discussionId": "67bef5a7070ec160042d9a65"
        },
        "translation_title": "ViDoRAG: 비주얼 문서 검색 증강 생성을 위한 동적 반복 추론 에이전트",
        "purpose": "비주얼 문서에서 정보를 이해하는 데 있어 기존 Retrieval-Augmented Generation(RAG) 방법의 한계를 극복하기 위한 연구",
        "method": [
            "ViDoSeek라는 새로운 데이터세트를 도입해 시각적으로 풍부한 문서에서 RAG 성능을 평가함(we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning.)",
            "현재 RAG 접근 방식의 주요 한계를 식별하고 비주얼 및 텍스트 기능을 효과적으로 통합하지 못하는 문제를 해결하기 위해 ViDoRAG라는 새로운 다중 에이전트 RAG 프레임워크를 제안함(we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features).",
            "ViDoRAG는 Gaussian Mixture Model(GMM)-기반 혼합 전략을 사용해 다중 모달 검색을 효과적으로 처리하며, 반복 에이전트 워크플로를 통해 모델의 추론 능력을 발휘하도록 함(ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval.)"
        ],
        "conclusion": "광범위한 실험을 통해 ViDoRAG가 기존 방법보다 10% 이상 향상된 성능을 보임을 확인하였으며, 이를 통해 RAG 도메인에서 테스트 시간 확장에 대한 새로운 연구를 제공함.",
        "keywords": [
            "Natural Language Processing",
            "Multimodal Learning",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2502.20545",
            "authors": [
                {
                    "_id": "67c51b459d5807d6674b3d3c",
                    "name": "Kechen Li",
                    "hidden": false
                },
                {
                    "_id": "67c51b459d5807d6674b3d3d",
                    "name": "Wenqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "67c51b459d5807d6674b3d3e",
                    "name": "Coralia Cartis",
                    "hidden": false
                },
                {
                    "_id": "67c51b459d5807d6674b3d3f",
                    "user": {
                        "_id": "64bb61e876a6e2efcc728e22",
                        "avatarUrl": "/avatars/b0ed1c9f13fd1f2c99d202155001e39b.svg",
                        "isPro": false,
                        "fullname": "Tianbo Ji",
                        "user": "jitianbo",
                        "type": "user"
                    },
                    "name": "Tianbo Ji",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:35:49.782Z",
                    "hidden": false
                },
                {
                    "_id": "67c51b459d5807d6674b3d40",
                    "user": {
                        "_id": "65b04d2291e63920a7898c9e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b04d2291e63920a7898c9e/iUHs235G4bqK-KnH_94ti.jpeg",
                        "isPro": false,
                        "fullname": "Liu",
                        "user": "Shiweiliuiiiiiii",
                        "type": "user"
                    },
                    "name": "Shiwei Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T11:14:45.635Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T21:41:43.000Z",
            "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
            "summary": "Large Language Models (LLMs) have achieved human-level proficiency across\ndiverse tasks, but their ability to perform rigorous mathematical problem\nsolving remains an open challenge. In this work, we investigate a fundamental\nyet computationally intractable problem: determining whether a given\nmultivariate polynomial is nonnegative. This problem, closely related to\nHilbert's Seventeenth Problem, plays a crucial role in global polynomial\noptimization and has applications in various fields. First, we introduce\nSoS-1K, a meticulously curated dataset of approximately 1,000 polynomials,\nalong with expert-designed reasoning instructions based on five progressively\nchallenging criteria. Evaluating multiple state-of-the-art LLMs, we find that\nwithout structured guidance, all models perform only slightly above the random\nguess baseline 50%. However, high-quality reasoning instructions significantly\nimprove accuracy, boosting performance up to 81%. Furthermore, our 7B model,\nSoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3\nand GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation\ntime needed for letters, respectively. Our findings highlight the potential of\nLLMs to push the boundaries of mathematical reasoning and tackle NP-hard\nproblems.",
            "upvotes": 5,
            "discussionId": "67c51b469d5807d6674b3d88"
        },
        "translation_title": "SoS1: O1 및 R1-유사한 추론 LLM은 합의 제곱 해결사입니다",
        "purpose": "다양한 수학적 문제 해결에서 LLM의 성능을 향상시키기 위한 방법 연구",
        "method": [
            "SoS-1K라는 약 1,000개의 다항식으로 구성된 데이터셋과 5단계의 난이도에 맞춘 전문가 설계의 추론 지침을 도입함(First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria.)",
            "구조화된 안내 없이 모든 모델이 무작위 추측 기준인 50%의 정확도를 겨우 초과함을 발견함(we find that without structured guidance, all models perform only slightly above the random guess baseline 50%).",
            "고품질 추론 지침이 정확성을 크게 향상시켜 성능이 81%로 증가함(However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%)."
        ],
        "conclusion": "LLM은 수학적 추론과 NP-어려운 문제를 해결하는 데 잠재력을 가지며, 우리의 모델이 기존 모델보다 더 높은 정확도를 달성함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    }
]