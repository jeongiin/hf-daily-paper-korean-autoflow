[
    {
        "paper": {
            "id": "2509.08827",
            "authors": [
                {
                    "_id": "68c228e829b8ec9932cd08ca",
                    "name": "Kaiyan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08cb",
                    "name": "Yuxin Zuo",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08cc",
                    "name": "Bingxiang He",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08cd",
                    "name": "Youbang Sun",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08ce",
                    "name": "Runze Liu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08cf",
                    "name": "Che Jiang",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d0",
                    "name": "Yuchen Fan",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d1",
                    "name": "Kai Tian",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d2",
                    "name": "Guoli Jia",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d3",
                    "name": "Pengfei Li",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d4",
                    "name": "Yu Fu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d5",
                    "name": "Xingtai Lv",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d6",
                    "name": "Yuchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d7",
                    "name": "Sihang Zeng",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d8",
                    "name": "Shang Qu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d9",
                    "name": "Haozhan Li",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08da",
                    "name": "Shijie Wang",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08db",
                    "name": "Yuru Wang",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08dc",
                    "name": "Xinwei Long",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08dd",
                    "name": "Fangfu Liu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08de",
                    "name": "Xiang Xu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08df",
                    "name": "Jiaze Ma",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e0",
                    "name": "Xuekai Zhu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e1",
                    "name": "Ermo Hua",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e2",
                    "name": "Yihao Liu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e3",
                    "name": "Zonglin Li",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e4",
                    "name": "Huayu Chen",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e5",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e6",
                    "name": "Yafu Li",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e7",
                    "name": "Weize Chen",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e8",
                    "name": "Zhenzhao Yuan",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e9",
                    "name": "Junqi Gao",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08ea",
                    "name": "Dong Li",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08eb",
                    "name": "Zhiyuan Ma",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08ec",
                    "name": "Ganqu Cui",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08ed",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08ee",
                    "name": "Biqing Qi",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08ef",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08f0",
                    "name": "Bowen Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-10T17:59:43.000Z",
            "submittedOnDailyAt": "2025-09-11T00:49:36.249Z",
            "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "60bc94cd85a3ab33829b6211",
                "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
                "isPro": false,
                "fullname": "Kaiyan Zhang",
                "user": "iseesaw",
                "type": "user"
            },
            "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
            "upvotes": 81,
            "discussionId": "68c228e929b8ec9932cd08f1",
            "projectPage": "https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
            "githubRepo": "https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
            "ai_summary": "Reinforcement Learning enhances Large Language Models for complex reasoning tasks, facing challenges in scalability and infrastructure as the field advances.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Large Language Models",
                "LRMs",
                "DeepSeek-R1",
                "Artificial SuperIntelligence"
            ],
            "githubStars": 871
        },
        "translation_title": "대규모 추론 모델을 위한 강화 학습에 대한 조사",
        "purpose": "대규모 추론 모델(LLMs)의 능력을 향상시키기 위한 강화 학습(RL)의 최근 발전을 조사하고 그 방향성을 재평가하기 위함",
        "method": [
            "RL 기술이 LLM의 복잡한 논리적 작업을 해결하는 데 성공을 거두었다는 점을 강조함(RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding.)",
            "RL의 발전 과정을 재조명하고 ASI(Artificial SuperIntelligence)를 향한 확장성 향상의 전략을 탐구함(To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence.)",
            "DeepSeek-R1 출시 이후 LLM과 LRM을 위한 RL 적용 연구를 살펴보며 기초 구성 요소와 훈련 자원 등을 분석함(In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications.)"
        ],
        "conclusion": "이 조사는 RL의 확장성을 향상시키고 향후 연구 기회를 모색하는 데 기여할 것으로 기대됨.",
        "keywords": [
            "Large Language Models",
            "Reinforcement Learning",
            "Artificial SuperIntelligence"
        ]
    },
    {
        "paper": {
            "id": "2509.08826",
            "authors": [
                {
                    "_id": "68c2308c29b8ec9932cd08f3",
                    "name": "Jie Wu",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08f4",
                    "name": "Yu Gao",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08f5",
                    "name": "Zilyu Ye",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08f6",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08f7",
                    "name": "Liang Li",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08f8",
                    "name": "Hanzhong Guo",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08f9",
                    "name": "Jie Liu",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08fa",
                    "name": "Zeyue Xue",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08fb",
                    "name": "Xiaoxia Hou",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08fc",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08fd",
                    "name": "Yan Zeng",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08fe",
                    "name": "Weilin Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-10T17:59:31.000Z",
            "submittedOnDailyAt": "2025-09-11T00:44:48.257Z",
            "title": "RewardDance: Reward Scaling in Visual Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Reward Models (RMs) are critical for improving generation models via\nReinforcement Learning (RL), yet the RM scaling paradigm in visual generation\nremains largely unexplored. It primarily due to fundamental limitations in\nexisting approaches: CLIP-based RMs suffer from architectural and input\nmodality constraints, while prevalent Bradley-Terry losses are fundamentally\nmisaligned with the next-token prediction mechanism of Vision-Language Models\n(VLMs), hindering effective scaling. More critically, the RLHF optimization\nprocess is plagued by Reward Hacking issue, where models exploit flaws in the\nreward signal without improving true quality. To address these challenges, we\nintroduce RewardDance, a scalable reward modeling framework that overcomes\nthese barriers through a novel generative reward paradigm. By reformulating the\nreward score as the model's probability of predicting a \"yes\" token, indicating\nthat the generated image outperforms a reference image according to specific\ncriteria, RewardDance intrinsically aligns reward objectives with VLM\narchitectures. This alignment unlocks scaling across two dimensions: (1) Model\nScaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context\nScaling: Integration of task-specific instructions, reference examples, and\nchain-of-thought (CoT) reasoning. Extensive experiments demonstrate that\nRewardDance significantly surpasses state-of-the-art methods in text-to-image,\ntext-to-video, and image-to-video generation. Crucially, we resolve the\npersistent challenge of \"reward hacking\": Our large-scale RMs exhibit and\nmaintain high reward variance during RL fine-tuning, proving their resistance\nto hacking and ability to produce diverse, high-quality outputs. It greatly\nrelieves the mode collapse problem that plagues smaller models.",
            "upvotes": 47,
            "discussionId": "68c2308c29b8ec9932cd08ff",
            "ai_summary": "RewardDance is a scalable reward modeling framework that aligns with VLM architectures, enabling effective scaling of RMs and resolving reward hacking issues in generation models.",
            "ai_keywords": [
                "CLIP-based RMs",
                "Bradley-Terry losses",
                "Vision-Language Models (VLMs)",
                "RLHF optimization",
                "Reward Hacking",
                "RewardDance",
                "generative reward paradigm",
                "model scaling",
                "context scaling",
                "task-specific instructions",
                "reference examples",
                "chain-of-thought (CoT) reasoning",
                "text-to-image",
                "text-to-video",
                "image-to-video generation",
                "mode collapse"
            ]
        },
        "translation_title": "RewardDance: 시각 생성에서 보상 스케일링",
        "purpose": "시각 생성 모델의 성과를 개선하기 위한 새로운 보상 모델링 프레임워크를 개발하여 기존의 한계를 극복하고자 함.",
        "method": [
            "전통적인 Reward Models(RMs)의 한계를 극복하기 위해 새로운 generative reward 패러다임을 도입함(To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm.)",
            "보상 점수를 모델의 'yes' 토큰을 예측하는 확률로 재정의함(By reformulating the reward score as the model's probability of predicting a 'yes' token.)",
            "RMs의 체계적인 스케일링과 작업에 특정한 지침, 참고 예시 및 chain-of-thought(CoT) 추론을 통합함(This alignment unlocks scaling across two dimensions: Model Scaling and Context Scaling.)"
        ],
        "conclusion": "RewardDance는 text-to-image, text-to-video 및 image-to-video 생성에서 최신 기술보다 탁월한 성과를 보였으며, 보상 해킹 문제를 해결하여 높은 품질의 다양한 출력을 생산할 수 있는 능력을 입증함.",
        "keywords": [
            "Image Generation",
            "Video Generation",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2509.07996",
            "authors": [
                {
                    "_id": "68c23b0629b8ec9932cd095b",
                    "name": "Lingdong Kong",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd095c",
                    "name": "Wesley Yang",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd095d",
                    "name": "Jianbiao Mei",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd095e",
                    "name": "Youquan Liu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd095f",
                    "name": "Ao Liang",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0960",
                    "name": "Dekai Zhu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0961",
                    "name": "Dongyue Lu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0962",
                    "name": "Wei Yin",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0963",
                    "name": "Xiaotao Hu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0964",
                    "name": "Mingkai Jia",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0965",
                    "name": "Junyuan Deng",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0966",
                    "name": "Kaiwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0967",
                    "name": "Yang Wu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0968",
                    "name": "Tianyi Yan",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0969",
                    "name": "Shenyuan Gao",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd096a",
                    "name": "Song Wang",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd096b",
                    "name": "Linfeng Li",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd096c",
                    "name": "Liang Pan",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd096d",
                    "name": "Yong Liu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd096e",
                    "name": "Jianke Zhu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd096f",
                    "name": "Wei Tsang Ooi",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0970",
                    "name": "Steven C. H. Hoi",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0971",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T17:59:58.000Z",
            "submittedOnDailyAt": "2025-09-11T01:29:36.490Z",
            "title": "3D and 4D World Modeling: A Survey",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "World modeling has become a cornerstone in AI research, enabling agents to\nunderstand, represent, and predict the dynamic environments they inhabit. While\nprior work largely emphasizes generative methods for 2D image and video data,\nthey overlook the rapidly growing body of work that leverages native 3D and 4D\nrepresentations such as RGB-D imagery, occupancy grids, and LiDAR point clouds\nfor large-scale scene modeling. At the same time, the absence of a standardized\ndefinition and taxonomy for ``world models'' has led to fragmented and\nsometimes inconsistent claims in the literature. This survey addresses these\ngaps by presenting the first comprehensive review explicitly dedicated to 3D\nand 4D world modeling and generation. We establish precise definitions,\nintroduce a structured taxonomy spanning video-based (VideoGen),\noccupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and\nsystematically summarize datasets and evaluation metrics tailored to 3D/4D\nsettings. We further discuss practical applications, identify open challenges,\nand highlight promising research directions, aiming to provide a coherent and\nfoundational reference for advancing the field. A systematic summary of\nexisting literature is available at https://github.com/worldbench/survey",
            "upvotes": 31,
            "discussionId": "68c23b0629b8ec9932cd0972",
            "githubRepo": "https://github.com/worldbench/survey",
            "ai_summary": "This survey provides a comprehensive review of 3D and 4D world modeling and generation, establishing definitions, taxonomy, datasets, and evaluation metrics, and discussing applications and challenges.",
            "ai_keywords": [
                "world models",
                "3D world modeling",
                "4D world modeling",
                "generative methods",
                "RGB-D imagery",
                "occupancy grids",
                "LiDAR point clouds",
                "VideoGen",
                "OccGen",
                "LiDARGen",
                "datasets",
                "evaluation metrics",
                "applications",
                "open challenges",
                "research directions"
            ],
            "githubStars": 166
        },
        "translation_title": "3D 및 4D 세계 모델링: 조사",
        "purpose": "AI 연구에서 동적 환경을 이해하고 예측할 수 있는 세계 모델링의 기초를 다지기 위한 종합적인 검토 제공",
        "method": [
            "2D 이미지 및 비디오 데이터 생성 방법을 강조하던 기존 연구의 한계를 극복하고 RGB-D 이미지, 점유 그리드, LiDAR 포인트 클라우드 등을 활용하여 대규모 장면 모델링 방법을 고찰함.(while prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling.)",
            "세계 모델에 대한 표준화된 정의와 분류체계를 수립하고, 비디오 기반(VideoGen), 점유 기반(OccGen), LiDAR 기반(LiDARGen) 접근법으로 나누어 체계적으로 정리함.(we establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings.)",
            "기존 연구 문헌을 체계적으로 요약하고, 실용적인 응용, 열린 문제 및 유망한 연구 방향에 대해 논의함.(We further discuss practical applications, identify open challenges, and highlight promising research directions.)"
        ],
        "conclusion": "3D 및 4D 세계 모델링의 체계적인 접근을 제시함으로써 해당 분야의 발전을 위한 기초 참고자료를 제공함.",
        "keywords": [
            "3D Vision",
            "Computer Vision",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2509.08755",
            "authors": [
                {
                    "_id": "68c2329529b8ec9932cd090a",
                    "name": "Zhiheng Xi",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd090b",
                    "name": "Jixuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd090c",
                    "name": "Chenyang Liao",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd090d",
                    "name": "Baodai Huang",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd090e",
                    "name": "Honglin Guo",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd090f",
                    "name": "Jiaqi Liu",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0910",
                    "name": "Rui Zheng",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0911",
                    "name": "Junjie Ye",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0912",
                    "name": "Jiazheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0913",
                    "name": "Wenxiang Chen",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0914",
                    "name": "Wei He",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0915",
                    "name": "Yiwen Ding",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0916",
                    "name": "Guanyu Li",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0917",
                    "name": "Zehui Chen",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0918",
                    "name": "Zhengyin Du",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0919",
                    "name": "Xuesong Yao",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd091a",
                    "name": "Yufei Xu",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd091b",
                    "name": "Jiecao Chen",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd091c",
                    "name": "Tao Gui",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd091d",
                    "name": "Zuxuan Wu",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd091e",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd091f",
                    "name": "Xuanjing Huang",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0920",
                    "name": "Yu-Gang Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-10T16:46:11.000Z",
            "submittedOnDailyAt": "2025-09-11T00:53:25.051Z",
            "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents.",
            "upvotes": 14,
            "discussionId": "68c2329529b8ec9932cd0921",
            "projectPage": "https://agentgym-rl.github.io/",
            "githubRepo": "https://github.com/WooooDyy/AgentGym-RL",
            "ai_summary": "AgentGym-RL is a modular RL framework for training LLM agents in diverse environments without supervised fine-tuning, featuring ScalingInter-RL for balanced exploration-exploitation.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "modular architecture",
                "decoupled architecture",
                "exploration-exploitation balance",
                "ScalingInter-RL",
                "LLM agents",
                "multi-turn decision-making",
                "diverse environments",
                "commercial models"
            ],
            "githubStars": 89
        },
        "translation_title": "AgentGym-RL: LLM 에이전트를 위한 다중 턴 강화 학습을 통한 장기 결정 만들기 훈련",
        "purpose": "다양한 환경에서 LLM 에이전트를 효과적으로 훈련시키기 위한 통합된 강화 학습 프레임워크 개발",
        "method": [
            "AgentGym-RL 프레임워크를 도입하여 다중 턴 상호작용 의사결정을 위한 RL 훈련을 수행함(To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL.)",
            "모듈화된 구조를 통해 유연성과 확장성을 보장함(The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility.)",
            "ScalingInter-RL 방식을 통해 탐색-이용 균형을 맞추며 안정적인 RL 최적화를 진행함(Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization.)"
        ],
        "conclusion": "우리의 에이전트는 27개 과제에서 다양한 환경에 걸쳐 상업적 모델을 초과하거나 동등한 성능을 보였으며, AgentGym-RL 프레임워크를 오픈소스로 제공할 예정이다.",
        "keywords": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2509.06784",
            "authors": [
                {
                    "_id": "68bfa444207285de11b07c13",
                    "user": {
                        "_id": "66fe2aadd5ede3018ab704bf",
                        "avatarUrl": "/avatars/46f2ae3717d1dfa5773d419e4dc9a891.svg",
                        "isPro": false,
                        "fullname": "Changhangfeng MA",
                        "user": "murcherful",
                        "type": "user"
                    },
                    "name": "Changfeng Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:28.483Z",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c14",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c15",
                    "name": "Xinhao Yan",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c16",
                    "name": "Jiachen Xu",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c17",
                    "name": "Yunhan Yang",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c18",
                    "name": "Chunshi Wang",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c19",
                    "name": "Zibo Zhao",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c1a",
                    "name": "Yanwen Guo",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c1b",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c1c",
                    "name": "Chunchao Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T15:12:17.000Z",
            "submittedOnDailyAt": "2025-09-11T01:06:10.468Z",
            "title": "P3-SAM: Native 3D Part Segmentation",
            "submittedOnDailyBy": {
                "_id": "66fe2aadd5ede3018ab704bf",
                "avatarUrl": "/avatars/46f2ae3717d1dfa5773d419e4dc9a891.svg",
                "isPro": false,
                "fullname": "Changhangfeng MA",
                "user": "murcherful",
                "type": "user"
            },
            "summary": "Segmenting 3D assets into their constituent parts is crucial for enhancing 3D\nunderstanding, facilitating model reuse, and supporting various applications\nsuch as part generation. However, current methods face limitations such as poor\nrobustness when dealing with complex objects and cannot fully automate the\nprocess. In this paper, we propose a native 3D point-promptable part\nsegmentation model termed P3-SAM, designed to fully automate the segmentation\nof any 3D objects into components. Inspired by SAM, P3-SAM consists of a\nfeature extractor, multiple segmentation heads, and an IoU predictor, enabling\ninteractive segmentation for users. We also propose an algorithm to\nautomatically select and merge masks predicted by our model for part instance\nsegmentation. Our model is trained on a newly built dataset containing nearly\n3.7 million models with reasonable segmentation labels. Comparisons show that\nour method achieves precise segmentation results and strong robustness on any\ncomplex objects, attaining state-of-the-art performance. Our code will be\nreleased soon.",
            "upvotes": 13,
            "discussionId": "68bfa444207285de11b07c1d",
            "ai_summary": "P3-SAM, a native 3D point-promptable part segmentation model, achieves precise and robust segmentation of complex 3D objects using a feature extractor, multiple segmentation heads, and an IoU predictor.",
            "ai_keywords": [
                "3D point-promptable part segmentation",
                "P3-SAM",
                "feature extractor",
                "segmentation heads",
                "IoU predictor",
                "interactive segmentation",
                "part instance segmentation",
                "dataset"
            ]
        },
        "translation_title": "P3-SAM: 네이티브 3D 부분 분할",
        "purpose": "3D 자산을 구성 요소로 자동 분할하여 3D 이해도를 향상시키고 모델 재사용을 촉진하는 방법 연구",
        "method": [
            "P3-SAM이라는 네이티브 3D 점 프롬프트 기반 부분 분할 모델을 제안함(we propose a native 3D point-promptable part segmentation model termed P3-SAM)",
            "특징 추출기, 여러 분할 헤드 및 IoU 예측기로 구성하여 사용자 상호작용 분할을 가능하게 함(P3-SAM consists of a feature extractor, multiple segmentation heads, and an IoU predictor, enabling interactive segmentation for users)",
            "모델에 의해 예측된 마스크를 자동으로 선택하고 병합하는 알고리즘을 제시함(We also propose an algorithm to automatically select and merge masks predicted by our model for part instance segmentation)"
        ],
        "conclusion": "P3-SAM은 복잡한 객체에서도 높은 정확도를 가진 분할 결과를 달성하며, 이전 방법들보다 우수한 성능을 보임.",
        "keywords": [
            "3D Vision",
            "Image Segmentation",
            "Computer Vision"
        ]
    }
]