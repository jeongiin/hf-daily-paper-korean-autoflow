[
    {
        "paper": {
            "id": "2506.15675",
            "authors": [
                {
                    "_id": "6853946599bf39f9665c79e0",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e1",
                    "name": "Chuanhao Li",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e2",
                    "name": "Xiaofeng Mao",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e3",
                    "name": "Shaoheng Lin",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e4",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e5",
                    "name": "Shitian Zhao",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e6",
                    "name": "Zhaopan Xu",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e7",
                    "name": "Xinyue Li",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e8",
                    "name": "Yukang Feng",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e9",
                    "name": "Jianwen Sun",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79ea",
                    "name": "Zizhen Li",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79eb",
                    "name": "Fanrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79ec",
                    "name": "Jiaxin Ai",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79ed",
                    "name": "Zhixiang Wang",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79ee",
                    "name": "Yuwei Wu",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79ef",
                    "name": "Tong He",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79f0",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79f1",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79f2",
                    "name": "Yunde Jia",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79f3",
                    "user": {
                        "_id": "63527f4e7d071f23d085ad45",
                        "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
                        "isPro": false,
                        "fullname": "KAIPENG ZHANG",
                        "user": "kpzhang",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-19T10:09:38.100Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
            ],
            "publishedAt": "2025-06-18T17:57:06.000Z",
            "submittedOnDailyAt": "2025-06-19T03:16:13.113Z",
            "title": "Sekai: A Video Dataset towards World Exploration",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.",
            "upvotes": 30,
            "discussionId": "6853946599bf39f9665c79f4",
            "projectPage": "https://lixsp11.github.io/sekai-project/",
            "githubRepo": "https://github.com/Lixsp11/sekai-codebase",
            "ai_summary": "Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.",
            "ai_keywords": [
                "first-person view",
                "worldwide video dataset",
                "rich annotations",
                "FPV",
                "UVA",
                "video collection",
                "pre-processing",
                "camera trajectories",
                "interactive video world exploration model"
            ]
        },
        "translation_title": "Sekai: 세계 탐험을 위한 비디오 데이터세트",
        "purpose": "세계 탐험을 위한 고품질 비디오 데이터 및 모델 개발",
        "method": [
            "세계 탐험에 적합한 비디오 데이터세트 Sekai를 소개함(we introduce Sekai, a high-quality first-person view worldwide video dataset with rich annotations for world exploration.)",
            "750개 도시에서 5000시간 이상의 비디오를 수집하고 위치, 장면, 날씨, 군중 밀도, 캡션, 카메라 경로 등을 주석 처리하는 도구를 개발함(We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories.)",
            "데이터세트의 품질을 실험적으로 확인하고, 하위 집합을 사용해 YUME라는 상호작용 비디오 모델을 훈련함(Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME.)"
        ],
        "conclusion": "Sekai는 비디오 생성 및 세계 탐험 분야에 기여하며, 유용한 응용 프로그램을 촉진할 것으로 기대됨.",
        "keywords": [
            "Video Generation",
            "Video Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.15681",
            "authors": [
                {
                    "_id": "68536fc899bf39f9665c7961",
                    "user": {
                        "_id": "657152eb12f162153b50ec9d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
                        "isPro": false,
                        "fullname": "Byung-Kwan Lee",
                        "user": "BK-Lee",
                        "type": "user"
                    },
                    "name": "Byung-Kwan Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-19T09:10:25.415Z",
                    "hidden": false
                },
                {
                    "_id": "68536fc899bf39f9665c7962",
                    "user": {
                        "_id": "65b33e5f7cd0069ad648c4e8",
                        "avatarUrl": "/avatars/1a746ea535cffa92ea08006e05ea414a.svg",
                        "isPro": false,
                        "fullname": "Ryo Hachiuma",
                        "user": "rhachiuma",
                        "type": "user"
                    },
                    "name": "Ryo Hachiuma",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-19T02:02:49.546Z",
                    "hidden": false
                },
                {
                    "_id": "68536fc899bf39f9665c7963",
                    "name": "Yong Man Ro",
                    "hidden": false
                },
                {
                    "_id": "68536fc899bf39f9665c7964",
                    "name": "Yu-Chiang Frank Wang",
                    "hidden": false
                },
                {
                    "_id": "68536fc899bf39f9665c7965",
                    "name": "Yueh-Hua Wu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
            ],
            "publishedAt": "2025-06-18T17:59:49.000Z",
            "submittedOnDailyAt": "2025-06-19T00:36:10.331Z",
            "title": "GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "657152eb12f162153b50ec9d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
                "isPro": false,
                "fullname": "Byung-Kwan Lee",
                "user": "BK-Lee",
                "type": "user"
            },
            "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.",
            "upvotes": 15,
            "discussionId": "68536fc899bf39f9665c7966",
            "projectPage": "https://byungkwanlee.github.io/GenRecal-page/",
            "ai_summary": "GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.",
            "ai_keywords": [
                "vision-language models",
                "large language models",
                "distillation framework",
                "GenerRecal",
                "recalibration",
                "feature representations",
                "heterogeneous VLMs"
            ]
        },
        "translation_title": "GenRecal: 대규모에서 소규모로의 보정 후 생성",
        "purpose": "자원 제약이 있는 환경에서도 사용할 수 있는 효율적인 Vision-Language Models(VLMs)의 개발을 목표로 함.",
        "method": [
            "다양한 설계의 VLM 아키텍처 간의 피쳐 표현을 정렬하고 조정하는 Recalibrator를 통해 효과적인 지식 전이를 가능하게 함.(GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs.)",
            "다양한 벤치마크에 대한 실험을 통해 GenRecal의 성능 향상을 입증함.(Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances.)"
        ],
        "conclusion": "GenRecal은 대규모 공개 및 비공개 VLMs의 성능을 뛰어넘는 효율적인 작은 모델 생성에 기여함.",
        "keywords": [
            "Large Language Models",
            "Vision-Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.13414",
            "authors": [
                {
                    "_id": "6853f62841586a7b0193a0ba",
                    "name": "Alexander Polok",
                    "hidden": false
                },
                {
                    "_id": "6853f62841586a7b0193a0bb",
                    "name": "Jiangyu Han",
                    "hidden": false
                },
                {
                    "_id": "6853f62841586a7b0193a0bc",
                    "name": "Dominik Klement",
                    "hidden": false
                },
                {
                    "_id": "6853f62841586a7b0193a0bd",
                    "name": "Samuele Cornell",
                    "hidden": false
                },
                {
                    "_id": "6853f62841586a7b0193a0be",
                    "name": "Jan Černocký",
                    "hidden": false
                },
                {
                    "_id": "6853f62841586a7b0193a0bf",
                    "name": "Lukáš Burget",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T12:28:35.000Z",
            "submittedOnDailyAt": "2025-06-19T10:07:26.619Z",
            "title": "BUT System for the MLC-SLM Challenge",
            "submittedOnDailyBy": {
                "_id": "638d2b765e14c2f38677987b",
                "avatarUrl": "/avatars/622c183e897a99e33717f4a92305fbd3.svg",
                "isPro": false,
                "fullname": "Alexander Polok",
                "user": "Lakoc",
                "type": "user"
            },
            "summary": "We present a two-speaker automatic speech recognition (ASR) system that\ncombines DiCoW -- a diarization-conditioned variant of Whisper -- with\nDiariZen, a diarization pipeline built on top of Pyannote. We first evaluate\nboth systems in out-of-domain (OOD) multilingual scenarios without any\nfine-tuning. In this scenario, DiariZen consistently outperforms the baseline\nPyannote diarization model, demonstrating strong generalization. Despite being\nfine-tuned on English-only data for target-speaker ASR, DiCoW retains solid\nmultilingual performance, indicating that encoder modifications preserve\nWhisper's multilingual capabilities. We then fine-tune both DiCoW and DiariZen\non the MLC-SLM challenge data. The fine-tuned DiariZen continues to outperform\nthe fine-tuned Pyannote baseline, while DiCoW sees further gains from domain\nadaptation. Our final system achieves a micro-average tcpWER/CER of 16.75% and\nranks second in Task 2 of the MLC-SLM challenge. Lastly, we identify several\nlabeling inconsistencies in the training data -- such as missing speech\nsegments and incorrect silence annotations -- which can hinder diarization\nfine-tuning. We propose simple mitigation strategies to address these issues\nand improve system robustness.",
            "upvotes": 11,
            "discussionId": "6853f62841586a7b0193a0c0",
            "ai_summary": "The combined DiCoW and DiariZen ASR system demonstrates strong performance in multilingual scenarios, with DiCoW preserving its multilingual capabilities and DiariZen improving through fine-tuning.",
            "ai_keywords": [
                "DiCoW",
                "DiariZen",
                "Whisper",
                "Pyannote",
                "ASR",
                "out-of-domain",
                "multilingual",
                "fine-tuning",
                "micro-average tcpWER/CER",
                "MLC-SLM challenge",
                "labeling inconsistencies"
            ]
        },
        "translation_title": "MLC-SLM 챌린지를 위한 BUT 시스템",
        "purpose": "다중 언어 환경에서의 자동 음성 인식 성능 향상",
        "method": [
            "DiCoW와 DiariZen을 결합한 두 화자 음성 인식 시스템을 제안함(We present a two-speaker automatic speech recognition (ASR) system that combines DiCoW -- a diarization-conditioned variant of Whisper -- with DiariZen, a diarization pipeline built on top of Pyannote.)",
            "DiariZen을 사용해 다중 언어 시나리오에서 성능을 평가하고 기존 모델보다 뛰어난 일반화 성능을 확인함(DiariZen consistently outperforms the baseline Pyannote diarization model, demonstrating strong generalization.)",
            "MLC-SLM 데이터로 두 시스템을 파인튜닝하여 성능을 개선함(The fine-tuned DiariZen continues to outperform the fine-tuned Pyannote baseline, while DiCoW sees further gains from domain adaptation.)"
        ],
        "conclusion": "최종 시스템은 16.75%의 tcpWER/CER을 달성하며 MLC-SLM 챌린지에서 두 번째로 높은 순위를 기록함.",
        "keywords": [
            "Natural Language Processing",
            "Speech Recognition",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2506.15677",
            "authors": [
                {
                    "_id": "68536b2399bf39f9665c794c",
                    "name": "Yining Hong",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c794d",
                    "name": "Rui Sun",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c794e",
                    "name": "Bingxuan Li",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c794f",
                    "name": "Xingcheng Yao",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c7950",
                    "name": "Maxine Wu",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c7951",
                    "name": "Alexander Chien",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c7952",
                    "name": "Da Yin",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c7953",
                    "name": "Ying Nian Wu",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c7954",
                    "name": "Zhecan James Wang",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c7955",
                    "name": "Kai-Wei Chang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T17:58:17.000Z",
            "submittedOnDailyAt": "2025-06-19T00:14:22.240Z",
            "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
            "submittedOnDailyBy": {
                "_id": "6431b64df76c34519e93d1ba",
                "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
                "isPro": true,
                "fullname": "Yining Hong",
                "user": "evelynhong",
                "type": "user"
            },
            "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.",
            "upvotes": 9,
            "discussionId": "68536b2399bf39f9665c7956",
            "ai_summary": "Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.",
            "ai_keywords": [
                "Embodied Web Agents",
                "task environments",
                "simulation platform",
                "3D indoor and outdoor environments",
                "functional web interfaces",
                "Embodied Web Agents Benchmark",
                "systematic assessment",
                "cross-domain intelligence",
                "embodied cognition"
            ]
        },
        "translation_title": "구체화된 웹 에이전트: 통합된 에이전트 지능을 위한 물리-디지털 영역의 연결",
        "purpose": "물리적 지능과 디지털 지능을 통합하여 복잡한 작업을 수행할 수 있는 AI 에이전트 개발",
        "method": [
            "Embodied Web Agents 작업 환경을 개발하여 실제 3D 실내외 환경과 웹 인터페이스를 통합함(We first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces.)",
            "다양한 작업을 포함한 Embodied Web Agents 벤치마크를 구축하고 공개함(Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation.)",
            "실험을 통해 최신 AI 시스템과 인간 능력 간의 성과 차이를 평가함(Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities.)"
        ],
        "conclusion": "구체화된 인지와 웹 지식 접근의 교차점에서 발생하는 도전과 기회를 밝혔다.",
        "keywords": [
            "Robotics",
            "3D Vision",
            "Natural Language Processing"
        ]
    }
]