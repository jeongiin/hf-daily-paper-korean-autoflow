[
    {
        "paper": {
            "id": "2512.09363",
            "authors": [
                {
                    "_id": "693a379e74fced5bf9c32412",
                    "user": {
                        "_id": "6486ff6561053da6442fef1a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486ff6561053da6442fef1a/72sdWErAwWtWNJIV5VZsy.jpeg",
                        "isPro": false,
                        "fullname": "KeXing",
                        "user": "KXingLab",
                        "type": "user"
                    },
                    "name": "Ke Xing",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:13:26.656Z",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32413",
                    "name": "Longfei Li",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32414",
                    "user": {
                        "_id": "64b7ab4c037d6452a31910eb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7ab4c037d6452a31910eb/0UaBtwyQTysBMndFWZdKu.png",
                        "isPro": false,
                        "fullname": "yuyangyin",
                        "user": "yuyangyin",
                        "type": "user"
                    },
                    "name": "Yuyang Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:43:30.256Z",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32415",
                    "name": "Hanwen Liang",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32416",
                    "name": "Guixun Luo",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32417",
                    "name": "Chen Fang",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32418",
                    "name": "Jue Wang",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32419",
                    "name": "Konstantinos N. Plataniotis",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c3241a",
                    "name": "Xiaojie Jin",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c3241b",
                    "name": "Yao Zhao",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c3241c",
                    "name": "Yunchao Wei",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"
            ],
            "publishedAt": "2025-12-10T06:50:16.000Z",
            "submittedOnDailyAt": "2025-12-11T00:46:52.612Z",
            "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.",
            "upvotes": 41,
            "discussionId": "693a379e74fced5bf9c3241d",
            "projectPage": "https://ke-xing.github.io/StereoWorld/",
            "ai_summary": "StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.",
            "ai_keywords": [
                "stereo video",
                "monocular-to-stereo",
                "pretrained video generator",
                "geometry-aware regularization",
                "spatio-temporal tiling",
                "high-definition stereo video dataset",
                "natural human interpupillary distance (IPD)",
                "visual fidelity",
                "geometric consistency"
            ]
        },
        "translation_title": "StereoWorld: 기하학 인식을 고려한 단안에서 스테레오 비디오 생성",
        "purpose": "고품질 스테레오 비디오 생성의 비용 문제 및 아티팩트 발생을 해결하기 위한 방법 연구",
        "method": [
            "사전 학습된 비디오 생성기를 활용하여 단안 비디오 입력에 기반한 고충실도의 스테레오 비디오 생성을 위한 종단 간 프레임워크 StereoWorld를 제안함(we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation.)",
            "기하학적 인지를 고려한 규제를 통해 3D 구조의 충실성을 보장하며, 모델을 명시적으로 감독함(while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity.)",
            "효율적인 고해상도 합성을 가능하게 하기 위해 시공간 타일링 방식을 통합함(A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis.)"
        ],
        "conclusion": "StereoWorld는 기존 방법보다 시각적 충실도와 기하학적 일관성이 뛰어난 스테레오 비디오를 생성하는 데 성공함.",
        "keywords": [
            "Video Generation",
            "3D Vision",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2512.08560",
            "authors": [
                {
                    "_id": "693a7def74fced5bf9c32537",
                    "user": {
                        "_id": "6492c419702103104f9450c4",
                        "avatarUrl": "/avatars/d6700e4bb1b9f172096ea31ba15a83b2.svg",
                        "isPro": false,
                        "fullname": "navve wasserman",
                        "user": "navvew",
                        "type": "user"
                    },
                    "name": "Navve Wasserman",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:07:40.033Z",
                    "hidden": false
                },
                {
                    "_id": "693a7def74fced5bf9c32538",
                    "user": {
                        "_id": "6735088b22d14a01ae17501f",
                        "avatarUrl": "/avatars/23d2eb2bb833dcf7a05434b499fedd5e.svg",
                        "isPro": false,
                        "fullname": "Matias Cosarinsky",
                        "user": "mcosarinsky",
                        "type": "user"
                    },
                    "name": "Matias Cosarinsky",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:45:36.526Z",
                    "hidden": false
                },
                {
                    "_id": "693a7def74fced5bf9c32539",
                    "user": {
                        "_id": "687f44e793eb81b0684b4eee",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/uDef9M8sQxTweYvwcVs07.png",
                        "isPro": false,
                        "fullname": "Yuval Golbari",
                        "user": "yuvalgolbari",
                        "type": "user"
                    },
                    "name": "Yuval Golbari",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:45:41.956Z",
                    "hidden": false
                },
                {
                    "_id": "693a7def74fced5bf9c3253a",
                    "name": "Aude Oliva",
                    "hidden": false
                },
                {
                    "_id": "693a7def74fced5bf9c3253b",
                    "user": {
                        "_id": "6744c6ec6ec99a37d4ba9235",
                        "avatarUrl": "/avatars/a5384b63bb615192f6fa157c6ea89e92.svg",
                        "isPro": false,
                        "fullname": "Antonio",
                        "user": "Antoniotorralbaborruel",
                        "type": "user"
                    },
                    "name": "Antonio Torralba",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:45:57.490Z",
                    "hidden": false
                },
                {
                    "_id": "693a7def74fced5bf9c3253c",
                    "user": {
                        "_id": "63914afeb6b839bb6143a6be",
                        "avatarUrl": "/avatars/da25b555d105f4755c8187479469ca77.svg",
                        "isPro": false,
                        "fullname": "Tamar Rott Shaham",
                        "user": "tamarott",
                        "type": "user"
                    },
                    "name": "Tamar Rott Shaham",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:46:03.740Z",
                    "hidden": false
                },
                {
                    "_id": "693a7def74fced5bf9c3253d",
                    "name": "Michal Irani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T13:01:17.000Z",
            "submittedOnDailyAt": "2025-12-11T05:54:20.652Z",
            "title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain",
            "submittedOnDailyBy": {
                "_id": "6492c419702103104f9450c4",
                "avatarUrl": "/avatars/d6700e4bb1b9f172096ea31ba15a83b2.svg",
                "isPro": false,
                "fullname": "navve wasserman",
                "user": "navvew",
                "type": "user"
            },
            "summary": "Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.",
            "upvotes": 28,
            "discussionId": "693a7def74fced5bf9c3253e",
            "projectPage": "https://navvewas.github.io/BrainExplore/",
            "ai_summary": "An automated framework uses unsupervised decomposition and natural language descriptions to identify and explain visual representations in human brain fMRI data.",
            "ai_keywords": [
                "fMRI",
                "unsupervised decomposition",
                "natural images",
                "natural-language descriptions",
                "voxel patterns"
            ]
        },
        "translation_title": "BrainExplore: 인간 두뇌에서 해석 가능한 비주얼 표현의 대규모 발견",
        "purpose": "인간 두뇌에서 시각적 개념이 어떻게 표현되고, 어떤 뇌 영역에 해당 표현이 인코딩되는지를 이해하기 위한 대규모 자동화된 프레임워크 개발",
        "method": [
            "비지도 학습과 데이터 기반 분해 기법을 통해 fMRI 활동에서 해석 가능한 패턴 후보를 발견하는 단계(First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods.)",
            "각 패턴을 설명하기 위해 자연 이미지 세트를 식별하고, 이들이 공유하는 시각적 의미에 대한 자연어 설명을 생성하는 단계(Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning.)",
            "여러 후보 설명을 테스트하고 정량적 신뢰성 점수를 부여하는 자동화된 파이프라인 도입(To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern.)"
        ],
        "conclusion": "우리의 프레임워크는 수천 개의 해석 가능한 패턴을 밝혀내어 다양한 시각적 개념을 포함하고, 이전에 보고되지 않은 세부 표현도 발견함.",
        "keywords": [
            "Image Understanding",
            "Natural Language Processing",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2512.09824",
            "authors": [
                {
                    "_id": "693a2f6d74fced5bf9c323c2",
                    "user": {
                        "_id": "6428fd124fe87caede856311",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
                        "isPro": false,
                        "fullname": "Xianghao Kong",
                        "user": "refkxh",
                        "type": "user"
                    },
                    "name": "Xianghao Kong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:13:32.571Z",
                    "hidden": false
                },
                {
                    "_id": "693a2f6d74fced5bf9c323c3",
                    "user": {
                        "_id": "65b00730403a23a2fd765110",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b00730403a23a2fd765110/Uw-obs-VymyU9iMVKLURZ.jpeg",
                        "isPro": false,
                        "fullname": "Zeyu Zhang",
                        "user": "ZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:45:03.580Z",
                    "hidden": false
                },
                {
                    "_id": "693a2f6d74fced5bf9c323c4",
                    "name": "Yuwei Guo",
                    "hidden": false
                },
                {
                    "_id": "693a2f6d74fced5bf9c323c5",
                    "user": {
                        "_id": "67f87bc19d597ac661a75b68",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CZiu75y2QIR53MZkEjgJd.png",
                        "isPro": false,
                        "fullname": "Zhuoran Zhao",
                        "user": "Alicezrzhao",
                        "type": "user"
                    },
                    "name": "Zhuoran Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:13:29.933Z",
                    "hidden": false
                },
                {
                    "_id": "693a2f6d74fced5bf9c323c6",
                    "name": "Songchun Zhang",
                    "hidden": false
                },
                {
                    "_id": "693a2f6d74fced5bf9c323c7",
                    "user": {
                        "_id": "63f8130749569335b679af62",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f8130749569335b679af62/vgTu23-y0UKocwAGqNMwT.jpeg",
                        "isPro": false,
                        "fullname": "Anyi Rao",
                        "user": "anyirao",
                        "type": "user"
                    },
                    "name": "Anyi Rao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:45:24.045Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67f87bc19d597ac661a75b68/zeD4CrSjVY6WpEKMkjlat.gif"
            ],
            "publishedAt": "2025-12-10T16:57:31.000Z",
            "submittedOnDailyAt": "2025-12-11T04:44:05.510Z",
            "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
            "submittedOnDailyBy": {
                "_id": "67f87bc19d597ac661a75b68",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CZiu75y2QIR53MZkEjgJd.png",
                "isPro": false,
                "fullname": "Zhuoran Zhao",
                "user": "Alicezrzhao",
                "type": "user"
            },
            "summary": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",
            "upvotes": 22,
            "discussionId": "693a2f6d74fced5bf9c323c8",
            "projectPage": "https://refkxh.github.io/BiCo_Webpage/",
            "githubRepo": "https://github.com/refkxh/bico",
            "githubRepoAddedBy": "user",
            "ai_summary": "Bind & Compose uses Diffusion Transformers with hierarchical binders and temporal strategies to accurately compose complex visual concepts from images and videos.",
            "ai_keywords": [
                "visual concept composition",
                "prompt tokens",
                "cross-attention conditioning",
                "Diffusion Transformers",
                "hierarchical binder structure",
                "Diversify-and-Absorb Mechanism",
                "absorbent token",
                "Temporal Disentanglement Strategy",
                "dual-branch binder structure"
            ],
            "githubStars": 44,
            "organization": {
                "_id": "693a3d43dd71dc07fc3a7cfe",
                "name": "mmlab-hkust",
                "fullname": "MMLab@HKUST",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6428fd124fe87caede856311/u4kVky_MS1iH4OYTzMF0H.jpeg"
            }
        },
        "translation_title": "이미지와 비디오에서 개념을 결합하여 구성하기",
        "purpose": "복잡한 시각 개념을 정확하게 추출하고 이미지와 비디오에서 개념을 유연하게 결합하기 위한 방법 연구",
        "method": [
            "Bind & Compose라는 방법을 도입하여 시각 개념을 프롬프트 토큰과 결합하고 다양한 출처의 바인드 토큰으로 목표 프롬프트를 구성함(We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources.)",
            "계층적 바인더 구조를 사용하여 Diffusion Transformers의 크로스-어텐션 조건을 설정하고, 복잡한 시각 개념을 정확하게 탈분해하기 위해 시각 개념을 프롬프트 토큰에 인코딩함(It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts.)",
            "다양한 프롬프트로 훈련 시 개념과 관련 없는 세부 요소의 영향을 제거하기 위해 추가적인 흡수 토큰을 설계한 Diversify-and-Absorb Mechanism을 수립함(To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts.)",
            "비디오 개념의 훈련 과정을 두 단계로 나누어 이중 분기 바인더 구조를 통해 시간 모델링하는 Temporal Disentanglement Strategy를 제시함(To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling.)"
        ],
        "conclusion": "이 방법은 기존 접근 방식보다 우수한 개념 일관성, 프롬프트 충실도 및 동작 품질을 달성하여 시각적 창의성의 새로운 가능성을 열어줌.",
        "keywords": [
            "Image Generation",
            "Video Generation",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2512.09247",
            "authors": [
                {
                    "_id": "693a372474fced5bf9c323fd",
                    "name": "Cheng Liu",
                    "hidden": false
                },
                {
                    "_id": "693a372474fced5bf9c323fe",
                    "user": {
                        "_id": "64311a95034ecbefddd141ef",
                        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
                        "isPro": false,
                        "fullname": "Yiren Song",
                        "user": "yiren98",
                        "type": "user"
                    },
                    "name": "Yiren Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:48:03.757Z",
                    "hidden": false
                },
                {
                    "_id": "693a372474fced5bf9c323ff",
                    "user": {
                        "_id": "637745113a63a2983ffbde13",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
                        "isPro": false,
                        "fullname": "Haofan Wang",
                        "user": "wanghaofan",
                        "type": "user"
                    },
                    "name": "Haofan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:47:57.012Z",
                    "hidden": false
                },
                {
                    "_id": "693a372474fced5bf9c32400",
                    "user": {
                        "_id": "661ab3da2b14565c7acccf5c",
                        "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
                        "isPro": false,
                        "fullname": "Mike Zheng Shou",
                        "user": "AnalMom",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:47:38.881Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GAFA0NvcooaUISwA3XiJq.mp4"
            ],
            "publishedAt": "2025-12-10T02:09:59.000Z",
            "submittedOnDailyAt": "2025-12-11T00:45:04.130Z",
            "title": "OmniPSD: Layered PSD Generation with Diffusion Transformer",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.",
            "upvotes": 13,
            "discussionId": "693a372474fced5bf9c32401",
            "ai_summary": "OmniPSD, a diffusion framework within the Flux ecosystem, enables text-to-PSD generation and image-to-PSD decomposition, achieving high-fidelity results with transparency awareness.",
            "ai_keywords": [
                "diffusion models",
                "OmniPSD",
                "Flux ecosystem",
                "in-context learning",
                "spatial attention",
                "iterative in-context editing",
                "RGBA-VAE",
                "RGBA-layered dataset",
                "diffusion transformers"
            ]
        },
        "translation_title": "OmniPSD: Diffusion Transformer를 활용한 층별 PSD 생성",
        "purpose": "투명한 알파 채널을 포함한 층별 PSD 파일을 생성하거나 재구성하는 문제를 해결하기 위한 새로운 프레임워크 개발",
        "method": [
            "OmniPSD는 텍스트에서 PSD로 생성과 이미지에서 PSD로 분해를 가능하게 하는 통합된 diffusion framework임(We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning.)",
            "텍스트에서 PSD로의 생성 시 여러 타겟 레이어를 단일 캔버스에 배치하고 공간 관찰을 통해 구성 관계를 학습함(For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention.)",
            "이미지에서 PSD로의 분해 과정에서 반복적인 편집을 통해 단일 평면 이미지에서 PSD 레이어를 재구성함(For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image.)"
        ],
        "conclusion": "OmniPSD는 높은 충실도의 생성, 구조적 일관성 및 투명성 인식을 달성하여 층별 디자인 생성 및 분해에 기여하는 새로운 패러다임을 제시함.",
        "keywords": [
            "Image Generation",
            "Image Understanding",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2512.08829",
            "authors": [
                {
                    "_id": "693a1f0274fced5bf9c32399",
                    "user": {
                        "_id": "66a105bb456284adf458d656",
                        "avatarUrl": "/avatars/b543a324f7e159d6e84bc68915e93d24.svg",
                        "isPro": false,
                        "fullname": "Tao Hongyuan",
                        "user": "HongyuanTao",
                        "type": "user"
                    },
                    "name": "Hongyuan Tao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:46:24.642Z",
                    "hidden": false
                },
                {
                    "_id": "693a1f0274fced5bf9c3239a",
                    "user": {
                        "_id": "6577073fc2bf55b1f6bafb49",
                        "avatarUrl": "/avatars/58803398b1a918b7570db17893e65122.svg",
                        "isPro": false,
                        "fullname": "Bencheng liao",
                        "user": "LegendBC",
                        "type": "user"
                    },
                    "name": "Bencheng Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:46:30.879Z",
                    "hidden": false
                },
                {
                    "_id": "693a1f0274fced5bf9c3239b",
                    "name": "Shaoyu Chen",
                    "hidden": false
                },
                {
                    "_id": "693a1f0274fced5bf9c3239c",
                    "name": "Haoran Yin",
                    "hidden": false
                },
                {
                    "_id": "693a1f0274fced5bf9c3239d",
                    "name": "Qian Zhang",
                    "hidden": false
                },
                {
                    "_id": "693a1f0274fced5bf9c3239e",
                    "user": {
                        "_id": "66c2e7fc934e2f07753542ac",
                        "avatarUrl": "/avatars/f6fa3f94435cf1c1d06daa6c925d07d0.svg",
                        "isPro": false,
                        "fullname": "LWY",
                        "user": "wenyuliu",
                        "type": "user"
                    },
                    "name": "Wenyu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:46:45.682Z",
                    "hidden": false
                },
                {
                    "_id": "693a1f0274fced5bf9c3239f",
                    "user": {
                        "_id": "62600de6d47e3dbae32ce1ce",
                        "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg",
                        "isPro": false,
                        "fullname": "Xinggang Wang",
                        "user": "xinggangw",
                        "type": "user"
                    },
                    "name": "Xinggang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:46:51.670Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T17:18:32.000Z",
            "submittedOnDailyAt": "2025-12-11T03:25:34.652Z",
            "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "66a105bb456284adf458d656",
                "avatarUrl": "/avatars/b543a324f7e159d6e84bc68915e93d24.svg",
                "isPro": false,
                "fullname": "Tao Hongyuan",
                "user": "HongyuanTao",
                "type": "user"
            },
            "summary": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.",
            "upvotes": 11,
            "discussionId": "693a1f0274fced5bf9c323a0",
            "projectPage": "https://github.com/hustvl/InfiniteVL",
            "githubRepo": "https://github.com/hustvl/InfiniteVL",
            "githubRepoAddedBy": "user",
            "ai_summary": "InfiniteVL, a linear-complexity VLM architecture combining sliding window attention and Gated DeltaNet, achieves competitive performance with less data and faster inference than leading Transformer-based models.",
            "ai_keywords": [
                "window attention",
                "linear attention",
                "Vision-Language Models",
                "VLMs",
                "sequence length",
                "window size",
                "sliding window attention",
                "Gated DeltaNet",
                "distillation pretraining",
                "instruction tuning",
                "long-sequence SFT",
                "multimodal performance",
                "long-term memory retention",
                "FlashAttention-2",
                "inference speedup",
                "real-time prefill speed"
            ],
            "githubStars": 26,
            "organization": {
                "_id": "62600e67ffe8827cb1d6180b",
                "name": "hustvl",
                "fullname": "HUST Vision Lab"
            }
        },
        "translation_title": "InfiniteVL: 고효율 무제한 입력 비전-언어 모델을 위한 선형 및 희소 주의의 시너지",
        "purpose": "비전-언어 모델(VLM)의 성능 저하 문제를 해결하고, 효율적이며 동시에 긴 시퀀스 처리가 가능하도록 하는 것이 목표",
        "method": [
            "슬라이딩 윈도우 주의(SWA)와 Gated DeltaNet을 결합한 선형 복잡성 VLM 아키텍처인 InfiniteVL을 제안함(To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet.)",
            "제한된 자원 하에 경쟁력 있는 멀티모달 성능을 달성하기 위해 세 단계의 훈련 전략을 설계함(For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT.)",
            "최소 2%의 훈련 데이터로도 이전의 선형 복잡성 VLMs를 크게 초월하고 Transformer 기반 VLM과 일치하는 성능을 보여줌(Rememberably, using less than 2% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs.)"
        ],
        "conclusion": "InfiniteVL은 3.6배의 추론 속도 향상과 안정적인 긴 메모리 캐시를 유지하며 비전-언어 모델 성능을 개선했음.",
        "keywords": [
            "Vision-Language Models",
            "Multimodal Learning",
            "Long-Term Memory"
        ]
    }
]