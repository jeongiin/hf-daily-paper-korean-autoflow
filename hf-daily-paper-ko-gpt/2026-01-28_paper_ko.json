[
    {
        "paper": {
            "id": "2601.18491",
            "authors": [
                {
                    "_id": "697831d9026bdf0473116e5c",
                    "name": "Dongrui Liu",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e5d",
                    "user": {
                        "_id": "66e2624a436a1798365e4581",
                        "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg",
                        "isPro": false,
                        "fullname": "Qihan Ren",
                        "user": "jasonrqh",
                        "type": "user"
                    },
                    "name": "Qihan Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-28T11:31:15.765Z",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e5e",
                    "name": "Chen Qian",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e5f",
                    "name": "Shuai Shao",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e60",
                    "name": "Yuejin Xie",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e61",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e62",
                    "name": "Zhonghao Yang",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e63",
                    "name": "Haoyu Luo",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e64",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e65",
                    "name": "Qingyu Liu",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e66",
                    "name": "Binxin Hu",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e67",
                    "name": "Ling Tang",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e68",
                    "name": "Jilin Mei",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e69",
                    "name": "Dadi Guo",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e6a",
                    "name": "Leitao Yuan",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e6b",
                    "name": "Junyao Yang",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e6c",
                    "name": "Guanxu Chen",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e6d",
                    "name": "Qihao Lin",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e6e",
                    "name": "Yi Yu",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e6f",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e70",
                    "name": "Jiaxuan Guo",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e71",
                    "name": "Jie Zhang",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e72",
                    "name": "Wenqi Shao",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e73",
                    "name": "Huiqi Deng",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e74",
                    "name": "Zhiheng Xi",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e75",
                    "name": "Wenjie Wang",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e76",
                    "name": "Wenxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e77",
                    "name": "Wen Shen",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e78",
                    "name": "Zhikai Chen",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e79",
                    "name": "Haoyu Xie",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e7a",
                    "name": "Jialing Tao",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e7b",
                    "name": "Juntao Dai",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e7c",
                    "name": "Jiaming Ji",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e7d",
                    "name": "Zhongjie Ba",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e7e",
                    "name": "Linfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e7f",
                    "name": "Yong Liu",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e80",
                    "name": "Quanshi Zhang",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e81",
                    "name": "Lei Zhu",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e82",
                    "name": "Zhihua Wei",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e83",
                    "name": "Hui Xue",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e84",
                    "name": "Chaochao Lu",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e85",
                    "name": "Jing Shao",
                    "hidden": false
                },
                {
                    "_id": "697831d9026bdf0473116e86",
                    "name": "Xia Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-26T13:45:41.000Z",
            "submittedOnDailyAt": "2026-01-28T01:26:49.833Z",
            "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security",
            "submittedOnDailyBy": {
                "_id": "66e2624a436a1798365e4581",
                "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg",
                "isPro": false,
                "fullname": "Qihan Ren",
                "user": "jasonrqh",
                "type": "user"
            },
            "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.",
            "upvotes": 56,
            "discussionId": "697831d9026bdf0473116e87",
            "githubRepo": "https://github.com/AI45Lab/AgentDoG",
            "githubRepoAddedBy": "user",
            "ai_summary": "AI agents face safety and security challenges from autonomous tool use and environmental interactions, requiring advanced guardrail frameworks for risk diagnosis and transparent monitoring.",
            "ai_keywords": [
                "agentic guardrail",
                "three-dimensional taxonomy",
                "agentic safety benchmark",
                "Diagnostic Guardrail framework",
                "agent safety and security",
                "agent trajectories",
                "root cause diagnosis",
                "fine-grained monitoring",
                "model variants",
                "state-of-the-art performance"
            ],
            "githubStars": 174,
            "organization": {
                "_id": "68f716f832b31e42cbc2be7f",
                "name": "AI45Research",
                "fullname": "AI45Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"
            }
        },
        "translation_title": "AgentDoG: AI 에이전트 안전 및 보안을 위한 진단 가드레일 프레임워크",
        "purpose": "AI 에이전트의 안전과 보안을 위한 위기 인식 및 진단 투명성을 제공하는 진단 가드레일 모델 개발",
        "method": [
            "AI 에이전트의 위험을 출처(어디서), 실패 모드(어떻게), 결과(무엇)에 따라 분류하는 통합된 3차원 분류 체계를 제안함(To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what).)",
            "AgentDoG 프레임워크를 통해 진단 기능을 제공하며, 에이전트의 경로 전반에서 세밀하고 맥락적인 모니터링 수행(AgentDoG provides fine-grained and contextual monitoring across agent trajectories.)",
            "비이진 레이블을 넘어 안전하지 않은 행동의 근본 원인을 진단하여 에이전트 정렬을 위해 필요한 투명성을 제공함(AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment.)"
        ],
        "conclusion": "AgentDoG는 다양한 복잡한 상호작용 시나리오에서 최첨단 성능을 달성하며, 모든 모델과 데이터셋이 공개됨.",
        "keywords": [
            "Robotics",
            "Safety and Security",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2601.18631",
            "authors": [
                {
                    "_id": "6978a169026bdf0473117088",
                    "user": {
                        "_id": "66aca01e33f6b27979856f6f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
                        "isPro": false,
                        "fullname": "Mingyang Song",
                        "user": "hitsmy",
                        "type": "user"
                    },
                    "name": "Mingyang Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-28T11:29:30.581Z",
                    "hidden": false
                },
                {
                    "_id": "6978a169026bdf0473117089",
                    "user": {
                        "_id": "63a2a51ef30c464227924fc6",
                        "avatarUrl": "/avatars/e109e85abd25b97bb29dbbe007119e34.svg",
                        "isPro": false,
                        "fullname": "Haoyu Sun",
                        "user": "Mikivis",
                        "type": "user"
                    },
                    "name": "Haoyu Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-28T11:29:26.154Z",
                    "hidden": false
                },
                {
                    "_id": "6978a169026bdf047311708a",
                    "user": {
                        "_id": "645b4819f9d4ec91fdd54852",
                        "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
                        "isPro": false,
                        "fullname": "Jiawei Gu",
                        "user": "kuvvi",
                        "type": "user"
                    },
                    "name": "Jiawei Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-28T11:29:28.392Z",
                    "hidden": false
                },
                {
                    "_id": "6978a169026bdf047311708b",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "6978a169026bdf047311708c",
                    "name": "Luxin Xu",
                    "hidden": false
                },
                {
                    "_id": "6978a169026bdf047311708d",
                    "name": "Ranjay Krishna",
                    "hidden": false
                },
                {
                    "_id": "6978a169026bdf047311708e",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-26T16:04:43.000Z",
            "submittedOnDailyAt": "2026-01-28T01:52:20.218Z",
            "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
            "submittedOnDailyBy": {
                "_id": "66aca01e33f6b27979856f6f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
                "isPro": false,
                "fullname": "Mingyang Song",
                "user": "hitsmy",
                "type": "user"
            },
            "summary": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce AdaReasoner, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.",
            "upvotes": 37,
            "discussionId": "6978a16a026bdf047311708f",
            "projectPage": "https://adareasoner.github.io/",
            "githubRepo": "https://github.com/ssmisya/AdaReasoner",
            "githubRepoAddedBy": "user",
            "ai_summary": "AdaReasoner enables multimodal models to learn tool usage as a general reasoning skill through scalable data curation, reinforcement learning for tool selection, and adaptive learning mechanisms that improve performance on complex visual reasoning tasks.",
            "ai_keywords": [
                "multimodal large language models",
                "tool use",
                "reinforcement learning",
                "end-task success",
                "adaptive learning mechanism",
                "visual reasoning",
                "multimodal models",
                "tool selection",
                "tool sequencing",
                "long-horizon interactions"
            ],
            "githubStars": 42,
            "organization": {
                "_id": "643cb0625fcffe09fb6ca688",
                "name": "Fudan-University",
                "fullname": "Fudan University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
            }
        },
        "translation_title": "AdaReasoner: 반복적 비주얼 추론을 위한 동적 도구 조율",
        "purpose": "다양한 도구를 활용하여 비주얼 추론 능력을 향상시키기 위한 일반적인 추론 기술로서 도구 사용을 학습하는 모델 개발",
        "method": [
            "확장 가능한 데이터 관리 파이프라인을 통해 모델이 장기적인 다단계 도구 상호작용을 경험하도록 함 (AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions.)",
            "Tool-GRPO라는 강화 학습 알고리즘을 통해 최종 작업 성공에 따라 도구 선택 및 순서를 최적화함 (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success.)",
            "적응형 학습 메커니즘을 통해 도구 사용을 동적으로 조절함 (iii) an adaptive learning mechanism that dynamically regulates tool usage."
        ],
        "conclusion": "AdaReasoner는 도구 적응 및 일반화 능력에서 뛰어난 성능을 보여주며, 다양한 벤치마크에서 최첨단 성과를 달성함.",
        "keywords": [
            "Multimodal Learning",
            "Natural Language Processing",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2601.18692",
            "authors": [
                {
                    "_id": "697989f6df44b75fa47e4803",
                    "user": {
                        "_id": "69773d4ee6878183fb90a8c7",
                        "avatarUrl": "/avatars/3e9e4081e3beaf3f69c380387b8ee4c2.svg",
                        "isPro": false,
                        "fullname": "Wei Wu",
                        "user": "Weiww99",
                        "type": "user"
                    },
                    "name": "Wei Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-28T11:15:53.890Z",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4804",
                    "name": "Fan Lu",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4805",
                    "name": "Yunnan Wang",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4806",
                    "user": {
                        "_id": "64548f6c363bb3aaf9cba136",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64548f6c363bb3aaf9cba136/HqJL9HQ5CWVOJCsHyuMOm.jpeg",
                        "isPro": false,
                        "fullname": "Shuai Yang",
                        "user": "ShuaiYang03",
                        "type": "user"
                    },
                    "name": "Shuai Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-28T11:14:06.626Z",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4807",
                    "name": "Shi Liu",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4808",
                    "name": "Fangjing Wang",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4809",
                    "name": "Qian Zhu",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e480a",
                    "user": {
                        "_id": "68234b1167281062097fc34e",
                        "avatarUrl": "/avatars/20ab2a15f27035c8b23d9a123e5ce22c.svg",
                        "isPro": false,
                        "fullname": "HeSun",
                        "user": "he777771",
                        "type": "user"
                    },
                    "name": "He Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-28T11:14:08.792Z",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e480b",
                    "name": "Yong Wang",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e480c",
                    "name": "Shuailei Ma",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e480d",
                    "name": "Yiyu Ren",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e480e",
                    "name": "Kejia Zhang",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e480f",
                    "name": "Hui Yu",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4810",
                    "name": "Jingmei Zhao",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4811",
                    "name": "Shuai Zhou",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4812",
                    "name": "Zhenqi Qiu",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4813",
                    "name": "Houlong Xiong",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4814",
                    "name": "Ziyu Wang",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4815",
                    "name": "Zechen Wang",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4816",
                    "name": "Ran Cheng",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4817",
                    "name": "Yong-Lu Li",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4818",
                    "name": "Yongtao Huang",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e4819",
                    "name": "Xing Zhu",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e481a",
                    "name": "Yujun Shen",
                    "hidden": false
                },
                {
                    "_id": "697989f6df44b75fa47e481b",
                    "name": "Kecheng Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-26T17:08:04.000Z",
            "submittedOnDailyAt": "2026-01-28T03:13:43.716Z",
            "title": "A Pragmatic VLA Foundation Model",
            "submittedOnDailyBy": {
                "_id": "64252045a4f3051f54dd1d53",
                "avatarUrl": "/avatars/0e423a3291091be3b4736a14da3ce495.svg",
                "isPro": false,
                "fullname": "kecheng zheng",
                "user": "zkcys001",
                "type": "user"
            },
            "summary": "Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8times (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.",
            "upvotes": 23,
            "discussionId": "697989f7df44b75fa47e481c",
            "projectPage": "https://technology.robbyant.com/lingbot-vla",
            "githubRepo": "https://github.com/robbyant/lingbot-vla",
            "githubRepoAddedBy": "auto",
            "ai_summary": "A Vision-Language-Action model trained on extensive real-world robotic data demonstrates superior performance and generalization across multiple platforms while offering enhanced efficiency through optimized training infrastructure.",
            "ai_keywords": [
                "Vision-Language-Action",
                "real-world data",
                "robotic manipulation",
                "generalization",
                "efficient codebase",
                "throughput",
                "VLA-oriented codebases"
            ],
            "githubStars": 218,
            "organization": {
                "_id": "69709f892cd08371c1011a2e",
                "name": "robbyant",
                "fullname": "Robbyant",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"
            }
        },
        "translation_title": "실용적인 VLA 기초 모델",
        "purpose": "로봇 조작에 효과적인 Vision-Language-Action(VLA) 기초 모델 개발 및 비용 효율성 달성",
        "method": [
            "약 20,000시간의 실제 데이터로 LingBot-VLA를 개발함(we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations.)",
            "3개의 로봇 플랫폼에서 체계적인 평가를 수행하여 각 플랫폼이 100개의 작업을 수행하도록 함(Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task.)",
            "효율적인 코드베이스를 구축하여 8-GPU 훈련 설정에서 초당 261개의 샘플 처리 속도를 가능하게 함(we have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup.)"
        ],
        "conclusion": "우리 모델은 강력한 성능과 넓은 일반화 능력을 보여주며, 실제 환경에 잘 적합함. 로봇 학습 분야의 발전을 위해 코드와 기본 모델, 벤치마크 데이터에 대한 공개 접근을 제공함.",
        "keywords": [
            "Robotics",
            "Vision-Language Models",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2601.19834",
            "authors": [
                {
                    "_id": "69797e24df44b75fa47e4761",
                    "user": {
                        "_id": "643b866bff50448bcfc7d1d1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
                        "isPro": false,
                        "fullname": "Jialong Wu",
                        "user": "manchery",
                        "type": "user"
                    },
                    "name": "Jialong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-28T11:16:37.217Z",
                    "hidden": false
                },
                {
                    "_id": "69797e24df44b75fa47e4762",
                    "name": "Xiaoying Zhang",
                    "hidden": false
                },
                {
                    "_id": "69797e24df44b75fa47e4763",
                    "name": "Hongyi Yuan",
                    "hidden": false
                },
                {
                    "_id": "69797e24df44b75fa47e4764",
                    "name": "Xiangcheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "69797e24df44b75fa47e4765",
                    "name": "Tianhao Huang",
                    "hidden": false
                },
                {
                    "_id": "69797e24df44b75fa47e4766",
                    "name": "Changjing He",
                    "hidden": false
                },
                {
                    "_id": "69797e24df44b75fa47e4767",
                    "name": "Chaoyi Deng",
                    "hidden": false
                },
                {
                    "_id": "69797e24df44b75fa47e4768",
                    "name": "Renrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "69797e24df44b75fa47e4769",
                    "name": "Youbin Wu",
                    "hidden": false
                },
                {
                    "_id": "69797e24df44b75fa47e476a",
                    "name": "Mingsheng Long",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-27T17:40:07.000Z",
            "submittedOnDailyAt": "2026-01-28T00:46:44.173Z",
            "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
            "submittedOnDailyBy": {
                "_id": "643b866bff50448bcfc7d1d1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "manchery",
                "type": "user"
            },
            "summary": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.",
            "upvotes": 16,
            "discussionId": "69797e24df44b75fa47e476b",
            "projectPage": "https://thuml.github.io/Reasoning-Visual-World/",
            "githubRepo": "https://github.com/thuml/Reasoning-Visual-World",
            "githubRepoAddedBy": "user",
            "ai_summary": "Visual generation enhances reasoning capabilities in multimodal models by providing more natural world models for physical and spatial tasks, while verbal reasoning remains sufficient for abstract domains.",
            "ai_keywords": [
                "chain-of-thought reasoning",
                "large language models",
                "multimodal models",
                "visual generation",
                "world models",
                "visual superiority hypothesis",
                "internal world modeling",
                "interleaved reasoning",
                "VisWorld-Eval"
            ],
            "githubStars": 31,
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "translation_title": "시각적 생성이 다중 모달 세계 모델을 통한 인간과 같은 추론을 가능하게 함",
        "purpose": "시각적 생성이 어떻게 추론에 이점이 있는지를 탐구하여, 인간과 같은 다중 모달 AI의 가능성을 밝히기 위해 연구함.",
        "method": [
            "내부 세계 모델링을 CoT 추론의 핵심 요소로 형식화하고 다양한 형태의 세계 모델 간의 구별을 분석함(We formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models.)",
            "시각적-언어적 CoT 추론이 필요한 작업을 식별하고 새로운 평가 세트인 VisWorld-Eval을 구성함(Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval.)",
            "최신 UMM에서 통제된 실험을 수행하여 시각적 세계 모델링에 유리한 작업에서 언어적 CoT보다 월등한 성과를 보임(Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling.)"
        ],
        "conclusion": "본 연구는 다중 모달 세계 모델링의 잠재력을 명확히 하여, 보다 강력한 인간과 같은 다중 모달 AI를 가능하게 함.",
        "keywords": [
            "Multimodal Learning",
            "Large Language Models",
            "Vision-Language Models"
        ]
    },
    {
        "paper": {
            "id": "2601.09150",
            "authors": [
                {
                    "_id": "6979a32cdf44b75fa47e4831",
                    "name": "Jianwen Sun",
                    "hidden": false
                },
                {
                    "_id": "6979a32cdf44b75fa47e4832",
                    "name": "Yukang Feng",
                    "hidden": false
                },
                {
                    "_id": "6979a32cdf44b75fa47e4833",
                    "name": "Kaining Ying",
                    "hidden": false
                },
                {
                    "_id": "6979a32cdf44b75fa47e4834",
                    "name": "Chuanhao Li",
                    "hidden": false
                },
                {
                    "_id": "6979a32cdf44b75fa47e4835",
                    "name": "Zizhen Li",
                    "hidden": false
                },
                {
                    "_id": "6979a32cdf44b75fa47e4836",
                    "name": "Fanrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "6979a32cdf44b75fa47e4837",
                    "name": "Jiaxin Ai",
                    "hidden": false
                },
                {
                    "_id": "6979a32cdf44b75fa47e4838",
                    "name": "Yifan Chang",
                    "hidden": false
                },
                {
                    "_id": "6979a32cdf44b75fa47e4839",
                    "name": "Yu Dai",
                    "hidden": false
                },
                {
                    "_id": "6979a32cdf44b75fa47e483a",
                    "name": "Yifei Huang",
                    "hidden": false
                },
                {
                    "_id": "6979a32cdf44b75fa47e483b",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/dWe8NkE2o-SBkMqd897MA.mp4"
            ],
            "publishedAt": "2026-01-14T04:45:05.000Z",
            "submittedOnDailyAt": "2026-01-28T03:30:08.387Z",
            "title": "World Craft: Agentic Framework to Create Visualizable Worlds via Text",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) motivate generative agent simulation (e.g., AI Town) to create a ``dynamic world'', holding immense value across entertainment and research. However, for non-experts, especially those without programming skills, it isn't easy to customize a visualizable environment by themselves. In this paper, we introduce World Craft, an agentic world creation framework to create an executable and visualizable AI Town via user textual descriptions. It consists of two main modules, World Scaffold and World Guild. World Scaffold is a structured and concise standardization to develop interactive game scenes, serving as an efficient scaffolding for LLMs to customize an executable AI Town-like environment. World Guild is a multi-agent framework to progressively analyze users' intents from rough descriptions, and synthesizes required structured contents (\\eg environment layout and assets) for World Scaffold . Moreover, we construct a high-quality error-correction dataset via reverse engineering to enhance spatial knowledge and improve the stability and controllability of layout generation, while reporting multi-dimensional evaluation metrics for further analysis. Extensive experiments demonstrate that our framework significantly outperforms existing commercial code agents (Cursor and Antigravity) and LLMs (Qwen3 and Gemini-3-Pro). in scene construction and narrative intent conveyance, providing a scalable solution for the democratization of environment creation.",
            "upvotes": 15,
            "discussionId": "6979a32cdf44b75fa47e483c",
            "githubRepo": "https://github.com/HerzogFL/World-Craft",
            "githubRepoAddedBy": "user",
            "ai_summary": "World Craft enables non-expert users to create executable and visualizable AI environments through textual descriptions by combining structured scaffolding and multi-agent intent analysis.",
            "ai_keywords": [
                "generative agent simulation",
                "AI Town",
                "large language models",
                "agentic world creation framework",
                "world scaffold",
                "world guild",
                "multi-agent framework",
                "scene construction",
                "narrative intent conveyance"
            ],
            "githubStars": 40,
            "organization": {
                "_id": "689f08c50df4fcf7fddc0b08",
                "name": "ShandaAI",
                "fullname": "Shanda AI Research Tokyo",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6099290247dc3dbf8a976612/OV-XOpG-1Pf-yzhFdkQse.png"
            }
        },
        "translation_title": "월드 크래프트: 텍스트를 통해 시각화 가능한 세계를 창조하기 위한 에이전트 프레임워크",
        "purpose": "비전문가들이 텍스트 설명만으로 시각화 가능한 환경을 쉽게 커스터마이즈할 수 있도록 하기 위한 에이전트 기반 세계 생성 프레임워크 개발",
        "method": [
            "사용자 텍스트 설명을 통해 실행 가능하고 시각화 가능한 AI Town을 생성하는 World Craft를 소개함(In this paper, we introduce World Craft, an agentic world creation framework to create an executable and visualizable AI Town via user textual descriptions.)",
            "적극적으로 사용자 의도를 분석하여 환경 레이아웃과 자산 등의 구조화된 콘텐츠를 생성하는 World Guild와 양방향 게임 장면을 개발하기 위한 표준화된 구조인 World Scaffold를 포함함(World Scaffold is a structured and concise standardization to develop interactive game scenes, serving as an efficient scaffolding for LLMs to customize an executable AI Town-like environment.)",
            "공간 지식 향상 및 레이아웃 생성의 안정성과 제어성을 개선하기 위해 역공학을 활용한 고품질 오류 수정 데이터셋을 구축함(we construct a high-quality error-correction dataset via reverse engineering to enhance spatial knowledge and improve the stability and controllability of layout generation.)"
        ],
        "conclusion": "우리의 프레임워크는 기존 상업용 코드 에이전트 및 LLMs보다 현저하게 뛰어난 성능을 보이며, 환경 생성의 민주화를 위한 확장 가능한 솔루션을 제공함.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Robotics"
        ]
    }
]