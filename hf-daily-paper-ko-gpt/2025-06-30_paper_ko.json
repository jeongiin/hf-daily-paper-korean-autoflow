[
    {
        "paper": {
            "id": "2506.17450",
            "authors": [
                {
                    "_id": "68620adf9e7509383d29ab98",
                    "user": {
                        "_id": "655bca95360e4f90cb61ba83",
                        "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
                        "isPro": true,
                        "fullname": "Jiacheng Chen",
                        "user": "cccjc",
                        "type": "user"
                    },
                    "name": "Jiacheng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-30T06:22:01.226Z",
                    "hidden": false
                },
                {
                    "_id": "68620adf9e7509383d29ab99",
                    "name": "Ramin Mehran",
                    "hidden": false
                },
                {
                    "_id": "68620adf9e7509383d29ab9a",
                    "name": "Xuhui Jia",
                    "hidden": false
                },
                {
                    "_id": "68620adf9e7509383d29ab9b",
                    "name": "Saining Xie",
                    "hidden": false
                },
                {
                    "_id": "68620adf9e7509383d29ab9c",
                    "name": "Sanghyun Woo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-20T19:38:34.000Z",
            "submittedOnDailyAt": "2025-06-30T02:33:26.106Z",
            "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
            "submittedOnDailyBy": {
                "_id": "655bca95360e4f90cb61ba83",
                "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
                "isPro": true,
                "fullname": "Jiacheng Chen",
                "user": "cccjc",
                "type": "user"
            },
            "summary": "We present BlenderFusion, a generative visual compositing framework that\nsynthesizes new scenes by recomposing objects, camera, and background. It\nfollows a layering-editing-compositing pipeline: (i) segmenting and converting\nvisual inputs into editable 3D entities (layering), (ii) editing them in\nBlender with 3D-grounded control (editing), and (iii) fusing them into a\ncoherent scene using a generative compositor (compositing). Our generative\ncompositor extends a pre-trained diffusion model to process both the original\n(source) and edited (target) scenes in parallel. It is fine-tuned on video\nframes with two key training strategies: (i) source masking, enabling flexible\nmodifications like background replacement; (ii) simulated object jittering,\nfacilitating disentangled control over objects and camera. BlenderFusion\nsignificantly outperforms prior methods in complex compositional scene editing\ntasks.",
            "upvotes": 33,
            "discussionId": "68620adf9e7509383d29ab9d",
            "projectPage": "https://blenderfusion.github.io/",
            "ai_summary": "A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.",
            "ai_keywords": [
                "diffusion model",
                "source masking",
                "simulated object jittering"
            ]
        },
        "translation_title": "BlenderFusion: 3D 기반 시각 편집 및 생성 합성",
        "purpose": "복잡한 시각 장면 편집 작업에서 성능을 향상시키기 위한 프레임워크 개발",
        "method": [
            "3D 개체로 변환된 시각 입력을 편집 가능한 형태로 세분화함(segmentation and converting visual inputs into editable 3D entities.)",
            "Blender에서 3D 기반 제어로 이를 편집함(editing in Blender with 3D-grounded control.)",
            "생성 합성기를 사용하여 일관된 장면으로 통합함(fusing them into a coherent scene using a generative compositor.)",
            "원본과 편집된 장면을 병행 처리하는 사전 훈련된 diffusion 모델을 확장함(generative compositor extends a pre-trained diffusion model to process both the original and edited scenes in parallel.)"
        ],
        "conclusion": "BlenderFusion은 복잡한 합성 장면 편집 작업에서 기존 방법보다 월등한 성과를 보여줍니다.",
        "keywords": [
            "Computer Vision",
            "Image Generation",
            "3D Vision"
        ]
    },
    {
        "paper": {
            "id": "2506.21862",
            "authors": [
                {
                    "_id": "6861eea79e7509383d29ab2f",
                    "name": "Boyuan Sun",
                    "hidden": false
                },
                {
                    "_id": "6861eea79e7509383d29ab30",
                    "name": "Jiaxing Zhao",
                    "hidden": false
                },
                {
                    "_id": "6861eea79e7509383d29ab31",
                    "name": "Xihan Wei",
                    "hidden": false
                },
                {
                    "_id": "6861eea79e7509383d29ab32",
                    "name": "Qibin Hou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-27T02:29:58.000Z",
            "submittedOnDailyAt": "2025-06-30T00:31:12.107Z",
            "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
            "submittedOnDailyBy": {
                "_id": "6686044047f2a33570e59e31",
                "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
                "isPro": false,
                "fullname": "Jiaxing Zhao",
                "user": "StarJiaxing",
                "type": "user"
            },
            "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
            "upvotes": 28,
            "discussionId": "6861eea89e7509383d29ab33",
            "ai_summary": "LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.",
            "ai_keywords": [
                "token compression strategy",
                "Semantic Connected Components (SCC)",
                "spatio-temporal token compression strategy",
                "video question answering",
                "long video understanding",
                "comprehensive multi-choice benchmarks"
            ]
        },
        "translation_title": "LLaVA-Scissor: 비디오 LLM을 위한 의미 연결 구성 요소를 활용한 토큰 압축",
        "purpose": "비디오 멀티모달 대형 언어 모델을 위한 효율적인 토큰 압축 전략 개발",
        "method": [
            "기존 방법들은 주로 주의 점수를 기반으로 토큰 압축을 시도했으나, 모든 의미 영역을 효과적으로 파악하지 못해 토큰 중복이 발생함(Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy.)",
            "우리의 방법은 의미 연결 구성 요소(SCC) 접근 방식을 활용하여 토큰을 서로 다른 의미 영역에 할당함(We propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set.)",
            "이 전략은 공간적 및 시간적 도메인 모두에서 SCC를 활용하여 전체 비디오를 겹치지 않는 의미 토큰 집합으로 표현함(The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains.)",
            "다양한 비디오 이해 벤치마크에 대해 LLaVA-Scissor의 토큰 압축 기능을 광범위하게 평가함(We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks.)"
        ],
        "conclusion": "LLaVA-Scissor는 다른 토큰 압축 방법을 능가하며, 특히 낮은 토큰 유지 비율에서 비디오 이해 벤치마크에서 우수한 성능을 달성함.",
        "keywords": [
            "Video Understanding",
            "Multimodal Learning",
            "Token Compression"
        ]
    },
    {
        "paper": {
            "id": "2506.21416",
            "authors": [
                {
                    "_id": "685e084071131fa43be08acc",
                    "user": {
                        "_id": "6361dd166945df7441b893fa",
                        "avatarUrl": "/avatars/b3ae6888a41aab8c2a7ef9f7320565c4.svg",
                        "isPro": false,
                        "fullname": "Bowen Chen ",
                        "user": "chenbowen",
                        "type": "user"
                    },
                    "name": "Bowen Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-30T06:22:45.351Z",
                    "hidden": false
                },
                {
                    "_id": "685e084071131fa43be08acd",
                    "name": "Mengyi Zhao",
                    "hidden": false
                },
                {
                    "_id": "685e084071131fa43be08ace",
                    "name": "Haomiao Sun",
                    "hidden": false
                },
                {
                    "_id": "685e084071131fa43be08acf",
                    "name": "Li Chen",
                    "hidden": false
                },
                {
                    "_id": "685e084071131fa43be08ad0",
                    "name": "Xu Wang",
                    "hidden": false
                },
                {
                    "_id": "685e084071131fa43be08ad1",
                    "name": "Kang Du",
                    "hidden": false
                },
                {
                    "_id": "685e084071131fa43be08ad2",
                    "name": "Xinglong Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T16:04:16.000Z",
            "submittedOnDailyAt": "2025-06-30T04:43:14.606Z",
            "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation",
            "submittedOnDailyBy": {
                "_id": "6498038ece9190ebb8693034",
                "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
                "isPro": false,
                "fullname": "Zhao",
                "user": "Mengyi",
                "type": "user"
            },
            "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.",
            "upvotes": 22,
            "discussionId": "685e084071131fa43be08ad3",
            "projectPage": "https://bytedance.github.io/XVerse/",
            "githubRepo": "https://github.com/bytedance/XVerse",
            "ai_summary": "XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.",
            "ai_keywords": [
                "Diffusion Transformers",
                "DiTs",
                "text-to-image generation",
                "multi-subject controlled generation",
                "reference images",
                "token-specific text-stream modulation",
                "image latents",
                "multi-subject image synthesis",
                "semantic attributes"
            ],
            "githubStars": 94
        },
        "translation_title": "XVerse: DiT 변조를 통한 정체성과 의미적 속성의 일관된 다중 주제 제어",
        "purpose": "Text-to-Image 생성에서 다중 주제의 정체성과 의미적 속성에 대한 정밀한 제어를 달성하기 위함",
        "method": [
            "XVerse라는 새로운 다중 주제 제어 생성 모델을 제안함(we propose a novel multi-subject controlled generation model XVerse).",
            "참조 이미지를 토큰 특정 텍스트 흐름 변조를 위한 오프셋으로 변환하여 특정 주제에 대한 정밀한 제어를 가능하게 함(By transforming reference images into offsets for token-specific text-stream modulation, XVerse allows for precise and independent control for specific subject).",
            "XVerse를 사용해 각 개별 주제의 특성과 의미적 속성에 대한 강력한 제어가 가능한 고충실도의 편집 가능한 다중 주제 이미지 합성을 제공함(Consequently, XVerse offers high-fidelity, editable multi-subject image synthesis with robust control over individual subject characteristics and semantic attributes)."
        ],
        "conclusion": "이러한 발전은 개인화되고 복잡한 장면 생성 능력을 크게 향상시킴.",
        "keywords": [
            "Image Generation",
            "Multimodal Learning",
            "Computer Vision"
        ]
    },
    {
        "paper": {
            "id": "2506.21356",
            "authors": [
                {
                    "_id": "6861fb7a9e7509383d29ab4b",
                    "name": "Hongbo Liu",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab4c",
                    "name": "Jingwen He",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab4d",
                    "name": "Yi Jin",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab4e",
                    "name": "Dian Zheng",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab4f",
                    "name": "Yuhao Dong",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab50",
                    "name": "Fan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab51",
                    "name": "Ziqi Huang",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab52",
                    "name": "Yinan He",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab53",
                    "name": "Yangguang Li",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab54",
                    "name": "Weichao Chen",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab55",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab56",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab57",
                    "name": "Shengjie Zhao",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab58",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T15:09:21.000Z",
            "submittedOnDailyAt": "2025-06-30T04:32:34.261Z",
            "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "652965773a416e1f2173443b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
                "isPro": false,
                "fullname": "Yuhao Dong",
                "user": "THUdyh",
                "type": "user"
            },
            "summary": "Cinematography, the fundamental visual language of film, is essential for\nconveying narrative, emotion, and aesthetic quality. While recent\nVision-Language Models (VLMs) demonstrate strong general visual understanding,\ntheir proficiency in comprehending the nuanced cinematic grammar embedded\nwithin individual shots remains largely unexplored and lacks robust evaluation.\nThis critical gap limits both fine-grained visual comprehension and the\nprecision of AI-assisted video generation. To address this, we introduce\nShotBench, a comprehensive benchmark specifically designed for cinematic\nlanguage understanding. It features over 3.5k expert-annotated QA pairs from\nimages and video clips, meticulously curated from over 200 acclaimed\n(predominantly Oscar-nominated) films and spanning eight key cinematography\ndimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their\nsubstantial limitations: even the top-performing model achieves less than 60%\naverage accuracy, particularly struggling with fine-grained visual cues and\ncomplex spatial reasoning. To catalyze advancement in this domain, we construct\nShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic\nQA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning\nand Group Relative Policy Optimization. ShotVL significantly outperforms all\nexisting open-source and proprietary models on ShotBench, establishing new\nstate-of-the-art performance. We open-source our models, data, and code to\nfoster rapid progress in this crucial area of AI-driven cinematic understanding\nand generation.",
            "upvotes": 18,
            "discussionId": "6861fb7a9e7509383d29ab59",
            "projectPage": "https://vchitect.github.io/ShotBench-project/",
            "githubRepo": "https://github.com/Vchitect/ShotBench/tree/main",
            "ai_summary": "ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.",
            "ai_keywords": [
                "Vision-Language Models",
                "VLMs",
                "ShotBench",
                "QA pairs",
                "cinematic grammar",
                "fine-grained visual comprehension",
                "AI-assisted video generation",
                "ShotQA",
                "multimodal dataset",
                "supervised fine-tuning",
                "Group Relative Policy Optimization",
                "ShotVL",
                "AI-driven cinematic understanding",
                "state-of-the-art performance"
            ],
            "githubStars": 7
        },
        "translation_title": "ShotBench: 비전-언어 모델의 전문가 수준 시네마틱 이해",
        "purpose": "시네마틱 언어 이해를 위한 평가 기준을 마련하고, 기존 모델의 한계를 극복하기 위한 연구",
        "method": [
            "시네마틱 언어 이해를 위해 ShotBench라는 포괄적인 벤치마크를 구축함(we introduce ShotBench, a comprehensive benchmark specifically designed for cinematic language understanding.)",
            "200편 이상의 영화에서 전문가 주석이 달린 3.5k 이상의 QA 쌍을 수집함(It features over 3.5k expert-annotated QA pairs from images and video clips, meticulously curated from over 200 acclaimed films.)",
            "ShotQA라는 대규모 다중 모달 데이터셋을 구성하고 이를 기반으로 ShotVL 모델을 개발함(We construct ShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic QA pairs, and develop ShotVL through supervised fine-tuning.)",
            "ShotVL이 기존의 모든 모델보다 현저히 뛰어난 성능을 보임(ShotVL significantly outperforms all existing open-source and proprietary models on ShotBench.)"
        ],
        "conclusion": "ShotVL은 비전-언어 모델의 시네마틱 이해 분야에서 새로운 최첨단 성능을 달성하였으며, 연구자들이 이 분야에서 빠르게 발전할 수 있도록 모델과 데이터를 오픈소스함.",
        "keywords": [
            "Vision-Language Models",
            "Multimodal Learning",
            "Video Understanding"
        ]
    },
    {
        "paper": {
            "id": "2506.20279",
            "authors": [
                {
                    "_id": "686218679e7509383d29abb3",
                    "name": "Changliang Xia",
                    "hidden": false
                },
                {
                    "_id": "686218679e7509383d29abb4",
                    "name": "Chengyou Jia",
                    "hidden": false
                },
                {
                    "_id": "686218679e7509383d29abb5",
                    "name": "Zhuohang Dang",
                    "hidden": false
                },
                {
                    "_id": "686218679e7509383d29abb6",
                    "name": "Minnan Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-25T09:40:50.000Z",
            "submittedOnDailyAt": "2025-06-30T03:24:47.090Z",
            "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
            "submittedOnDailyBy": {
                "_id": "6602548a68d519ed324b47c5",
                "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
                "isPro": false,
                "fullname": "ChengyouJia",
                "user": "ChengyouJia",
                "type": "user"
            },
            "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj",
            "upvotes": 14,
            "discussionId": "686218689e7509383d29abb7",
            "projectPage": "https://xcltql666.github.io/DenseDiTProj/",
            "githubRepo": "https://github.com/xcltql666/DenseDiT",
            "ai_summary": "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.",
            "ai_keywords": [
                "dense prediction",
                "generative models",
                "visual priors",
                "parameter-reuse mechanism",
                "lightweight branches",
                "multi-scale context",
                "DenseWorld",
                "DenseDiT"
            ],
            "githubStars": 16
        },
        "translation_title": "이상에서 현실로: 실제 세계 시나리오를 위한 통합적이고 데이터 효율적인 밀집 예측",
        "purpose": "밀집 예측 작업의 성능을 개선하고 현실 세계에서의 일반화 능력을 향상시키기 위해 시스템적으로 연구",
        "method": [
            "DenseWorld라는 벤치마크를 도입하여 25개의 밀집 예측 작업을 포함시키고 이 작업들이 실제 세계 응용과 연결되도록 함(we first introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction tasks that correspond to urgent real-world applications)",
            "DenseDiT를 제안하여 생성 모델의 시각적 선험을 최대한 활용해 다양한 밀집 예측 작업을 수행함(we propose DenseDiT, which maximally exploits generative models' visual priors to perform diverse real-world dense prediction tasks)",
            "매개변수 재사용 메커니즘과 다중 스케일 컨텍스트를 통합하는 두 가지 경량 브랜치를 통해 0.1% 이하의 추가 매개변수로 작동하도록 함(DenseDiT combines a parameter-reuse mechanism and two lightweight branches that adaptively integrate multi-scale context)"
        ],
        "conclusion": "DenseDiT는 기존 방법들보다 적은 양의 훈련 데이터로 우수한 성능을 달성하여 실제 세계에서의 활용 가능성을 강조함.",
        "keywords": [
            "Computer Vision",
            "Image Segmentation",
            "Multimodal Learning"
        ]
    }
]