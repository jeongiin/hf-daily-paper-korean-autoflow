[
    {
        "paper": {
            "id": "2511.14993",
            "authors": [
                {
                    "_id": "691e819a3c64d32b036458c0",
                    "name": "Vladimir Arkhipkin",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c1",
                    "name": "Vladimir Korviakov",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c2",
                    "name": "Nikolai Gerasimenko",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c3",
                    "name": "Denis Parkhomenko",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c4",
                    "user": {
                        "_id": "64e4c7764af6c29a0697f57b",
                        "avatarUrl": "/avatars/efc4e9f9b105586fd090b22a1bc7dbb7.svg",
                        "isPro": false,
                        "fullname": "Viacheslav Vasilev",
                        "user": "vvasilev",
                        "type": "user"
                    },
                    "name": "Viacheslav Vasilev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:10.246Z",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c5",
                    "user": {
                        "_id": "68838d809080cc7010edf5e2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68838d809080cc7010edf5e2/xBqg5ggt_PfLkiDLmsZxx.jpeg",
                        "isPro": false,
                        "fullname": "Alexey Letunovskiy",
                        "user": "AlexeyLetunovskiy",
                        "type": "user"
                    },
                    "name": "Alexey Letunovskiy",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:55.594Z",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c6",
                    "name": "Maria Kovaleva",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c7",
                    "name": "Nikolai Vaulin",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c8",
                    "name": "Ivan Kirillov",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458c9",
                    "name": "Lev Novitskiy",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458ca",
                    "name": "Denis Koposov",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458cb",
                    "user": {
                        "_id": "6628b73c35d27082500034f2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628b73c35d27082500034f2/CznOeIbjzJ9DmJaGzlWPD.jpeg",
                        "isPro": false,
                        "fullname": "Nikita Kiselev",
                        "user": "kisnikser",
                        "type": "user"
                    },
                    "name": "Nikita Kiselev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:11.927Z",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458cc",
                    "name": "Alexander Varlamov",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458cd",
                    "name": "Dmitrii Mikhailov",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458ce",
                    "name": "Vladimir Polovnikov",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458cf",
                    "name": "Andrey Shutkin",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d0",
                    "name": "Ilya Vasiliev",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d1",
                    "name": "Julia Agafonova",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d2",
                    "name": "Anastasiia Kargapoltseva",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d3",
                    "name": "Anna Dmitrienko",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d4",
                    "name": "Anastasia Maltseva",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d5",
                    "name": "Anna Averchenkova",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d6",
                    "name": "Olga Kim",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d7",
                    "name": "Tatiana Nikulina",
                    "hidden": false
                },
                {
                    "_id": "691e819a3c64d32b036458d8",
                    "user": {
                        "_id": "6669a678465d1d802181e456",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png",
                        "isPro": false,
                        "fullname": "Denis Dimitrov",
                        "user": "dendimitrov",
                        "type": "user"
                    },
                    "name": "Denis Dimitrov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:08.661Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T00:23:22.000Z",
            "submittedOnDailyAt": "2025-11-20T00:19:10.078Z",
            "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
            "upvotes": 113,
            "discussionId": "691e819b3c64d32b036458d9",
            "projectPage": "https://kandinskylab.ai/",
            "githubRepo": "https://github.com/kandinskylab/kandinsky-5",
            "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.",
            "ai_keywords": [
                "foundation models",
                "high-resolution image synthesis",
                "10-second video synthesis",
                "image generation models",
                "text-to-video models",
                "image-to-video models",
                "multi-stage training pipeline",
                "self-supervised fine-tuning",
                "reinforcement learning",
                "pre-training",
                "quality-enhancement techniques",
                "architectural optimizations",
                "training optimizations",
                "inference optimizations",
                "human evaluation",
                "generative framework",
                "open-source code",
                "training checkpoints"
            ],
            "githubStars": 236
        },
        "translation_title": "Kandinsky 5.0: 이미지 및 비디오 생성을 위한 기초 모델 모음",
        "purpose": "고해상도 이미지와 10초 비디오 합성을 위한 최첨단 기초 모델 개발",
        "method": [
            "Kandinsky 5.0 모델은 6B 매개변수를 가진 이미지 생성 모델, 2B 매개변수를 가진 텍스트-비디오 및 이미지-비디오 모델, 19B 매개변수를 가진 비디오 생성 품질을 향상시킨 모델로 구성됨(The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite, Kandinsky 5.0 Video Lite, and Kandinsky 5.0 Video Pro.)",
            "다단계 학습 파이프라인을 통해 데이터 수집, 처리, 필터링 및 클러스터링을 포함한 데이터 관리 프로세스 검토(We provide a comprehensive review of the data curation lifecycle.)",
            "자기 감독 미세 조정(SFT) 및 강화 학습(RL)을 기반으로 한 포스트 트레이닝 기법을 적용하여 생성 품질을 향상시킴(it incorporates quality-enhancement techniques such as self-supervised fine-tuning and RL-based post-training.)"
        ],
        "conclusion": "Kandinsky 5.0은 다양한 작업에서 높은 생성 속도와 최첨단 성능을 달성하며, 공개 코드와 학습 체크포인트와 함께 연구 커뮤니티의 발전에 기여할 것으로 기대됨.",
        "keywords": [
            "Image Generation",
            "Video Generation",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2511.15065",
            "authors": [
                {
                    "_id": "691e828b3c64d32b036458e4",
                    "user": {
                        "_id": "67c443afb753bd020f9c97d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xbACBNLSopWmN5G1K8h_Y.png",
                        "isPro": false,
                        "fullname": "Cheng",
                        "user": "YangC777",
                        "type": "user"
                    },
                    "name": "Cheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:06.809Z",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458e5",
                    "name": "Haiyuan Wan",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458e6",
                    "name": "Yiran Peng",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458e7",
                    "name": "Xin Cheng",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458e8",
                    "user": {
                        "_id": "640dc84b474aa6f89554d518",
                        "avatarUrl": "/avatars/9fcee1023ed5c6cddb3c342e19f18295.svg",
                        "isPro": false,
                        "fullname": "Zhaoyang Yu",
                        "user": "MoshiQAQ",
                        "type": "user"
                    },
                    "name": "Zhaoyang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:02.234Z",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458e9",
                    "name": "Jiayi Zhang",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458ea",
                    "name": "Junchi Yu",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458eb",
                    "user": {
                        "_id": "67d63e228d5c7a132cbcf39b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ynwA3Sya5irwMRCmSeLiC.png",
                        "isPro": false,
                        "fullname": "neil yu",
                        "user": "yxl66666",
                        "type": "user"
                    },
                    "name": "Xinlei Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:49:04.908Z",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458ec",
                    "name": "Xiawu Zheng",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458ed",
                    "name": "Dongzhan Zhou",
                    "hidden": false
                },
                {
                    "_id": "691e828b3c64d32b036458ee",
                    "name": "Chenglin Wu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/syEvNY_b3iyJgYOQZ5cNg.png"
            ],
            "publishedAt": "2025-11-19T03:18:29.000Z",
            "submittedOnDailyAt": "2025-11-20T00:24:18.639Z",
            "title": "Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.",
            "upvotes": 64,
            "discussionId": "691e828b3c64d32b036458ef",
            "projectPage": "https://imyangc7.github.io/VRBench_Web/",
            "githubRepo": "https://github.com/ImYangC7/VR-Bench",
            "ai_summary": "VR-Bench evaluates video models' spatial reasoning capabilities through maze-solving tasks, demonstrating that these models excel in spatial perception and reasoning, outperforming VLMs and benefiting from diverse sampling during inference.",
            "ai_keywords": [
                "video models",
                "video generation",
                "coherent motion dynamics",
                "spatial reasoning",
                "temporal continuity",
                "VR-Bench",
                "maze-solving tasks",
                "procedural generation",
                "spatial planning",
                "multi-step reasoning",
                "SFT",
                "leading VLMs",
                "test-time scaling effect"
            ],
            "githubStars": 10
        },
        "translation_title": "비디오를 통한 추론: 미로 해결 작업을 통한 비디오 모델의 추론 능력 첫 평가",
        "purpose": "비디오 모델의 추론 능력을 평가하기 위한 벤치마크 VR-Bench 개발",
        "method": [
            "비디오 모델이 비디오 생성을 통해 추론할 수 있는지를 탐구함(Can video models reason via video generation?)",
            "미로 해결 작업을 기반으로 한 VR-Bench라는 포괄적인 벤치마크를 소개함(VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities.)",
            "총 7,920개의 프로시저 생성 비디오를 포함한 다양한 미로 유형과 시각적 스타일을 포함하도록 설계함(VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles.)",
            "SFT를 사용해 비디오 모델의 추론 능력을 효과적으로 끌어내는 것을 보여줌(our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model.)"
        ],
        "conclusion": "비디오 모델은 뛰어난 공간 인식을 보여주며, 다양한 시나리오와 작업에서 잘 일반화되고, 추론 신뢰성을 10-20% 향상시키는 테스트 시점 스케일링 효과를 발견함.",
        "keywords": [
            "Video Generation",
            "Video Understanding",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2511.15661",
            "authors": [
                {
                    "_id": "691e8b133c64d32b03645915",
                    "name": "Yicheng He",
                    "hidden": false
                },
                {
                    "_id": "691e8b133c64d32b03645916",
                    "user": {
                        "_id": "62ea79dd01ed9b0e8f61ccd3",
                        "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
                        "isPro": false,
                        "fullname": "Chengsong Huang",
                        "user": "ChengsongHuang",
                        "type": "user"
                    },
                    "name": "Chengsong Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:48:55.509Z",
                    "hidden": false
                },
                {
                    "_id": "691e8b133c64d32b03645917",
                    "name": "Zongxia Li",
                    "hidden": false
                },
                {
                    "_id": "691e8b133c64d32b03645918",
                    "name": "Jiaxin Huang",
                    "hidden": false
                },
                {
                    "_id": "691e8b133c64d32b03645919",
                    "name": "Yonghui Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T17:55:15.000Z",
            "submittedOnDailyAt": "2025-11-20T00:59:58.366Z",
            "title": "VisPlay: Self-Evolving Vision-Language Models from Images",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/",
            "upvotes": 29,
            "discussionId": "691e8b143c64d32b0364591a",
            "projectPage": "https://bruno686.github.io/VisPlay/",
            "githubRepo": "https://github.com/bruno686/VisPlay",
            "ai_summary": "VisPlay, a self-evolving RL framework, uses unlabeled image data to enhance VLMs' reasoning, generalization, and response quality through two interacting roles and GRPO.",
            "ai_keywords": [
                "Reinforcement learning",
                "Vision-Language Models",
                "self-evolving RL framework",
                "Image-Conditioned Questioner",
                "Multimodal Reasoner",
                "Group Relative Policy Optimization",
                "visual reasoning",
                "compositional generalization",
                "hallucination reduction"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "65448bef5b5d9185ba3202b9",
                "name": "UIUC-CS",
                "fullname": "University of Illinois at Urbana-Champaign",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
            }
        },
        "translation_title": "VisPlay: 이미지로부터 자가 발전하는 비전-언어 모델",
        "purpose": "비전-언어 모델의 추론 능력을 향상시키기 위한 자가 발전 Reinforcement Learning 프레임워크 개발",
        "method": [
            "비전-언어 모델이 레이블이 없는 데이터로 스스로 개선하도록 하는 자가 발전 RL 프레임워크인 VisPlay를 도입함(We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data.)",
            "VisPlay는 이미지 기반 질문을 제기하는 역할과 정답을 생성하는 역할을 하는 두 개의 상호작용하는 역할로 모델을 배정함(Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses.)",
            "이 두 역할은 Group Relative Policy Optimization (GRPO)로 공동 학습되어 질문의 복잡성과 정답의 질을 균형 있게 유지하도록 함(These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers.)"
        ],
        "conclusion": "VisPlay는 여러 모델에서 효율적으로 확장되어 비주얼 추론, 조합 일반화 및 허우적임 감소에서 일관된 개선을 달성하였으며, 자가 발전하는 멀티모달 인공지능을 향한 확장 가능한 경로를 보여줌.",
        "keywords": [
            "Vision-Language Models",
            "Reinforcement Learning",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2511.15593",
            "authors": [
                {
                    "_id": "691efc453c64d32b03645a2d",
                    "name": "Alexis Audran-Reiss",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a2e",
                    "name": "Jordi Armengol Estapé",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a2f",
                    "name": "Karen Hambardzumyan",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a30",
                    "name": "Amar Budhiraja",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a31",
                    "name": "Martin Josifoski",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a32",
                    "name": "Edan Toledo",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a33",
                    "name": "Rishi Hazra",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a34",
                    "name": "Despoina Magka",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a35",
                    "name": "Michael Shvartsman",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a36",
                    "name": "Parth Pathak",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a37",
                    "name": "Justine T Kao",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a38",
                    "name": "Lucia Cipolina-Kun",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a39",
                    "name": "Bhavul Gauri",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a3a",
                    "name": "Jean-Christophe Gagnon-Audet",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a3b",
                    "name": "Emanuel Tewolde",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a3c",
                    "name": "Jenny Zhang",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a3d",
                    "name": "Taco Cohen",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a3e",
                    "name": "Yossi Adi",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a3f",
                    "name": "Tatiana Shavrina",
                    "hidden": false
                },
                {
                    "_id": "691efc453c64d32b03645a40",
                    "name": "Yoram Bachrach",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T16:32:18.000Z",
            "submittedOnDailyAt": "2025-11-20T11:07:52.822Z",
            "title": "What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity",
            "submittedOnDailyBy": {
                "_id": "6687ee79eee600e418404cc9",
                "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg",
                "isPro": false,
                "fullname": "Amar Budhiraja",
                "user": "ambud26",
                "type": "user"
            },
            "summary": "AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.",
            "upvotes": 27,
            "discussionId": "691efc463c64d32b03645a41",
            "ai_summary": "Ideation diversity significantly enhances the performance of AI research agents across various models and scaffolds on the MLE-bench benchmark.",
            "ai_keywords": [
                "ideation diversity",
                "AI research agents",
                "MLE-bench",
                "agent trajectories",
                "agent scaffolds"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "translation_title": "훌륭한 AI 연구 에이전트가 되기 위한 조건은 무엇인가? 아이디어 다양성의 역할 연구",
        "purpose": "AI 연구 에이전트의 성과를 높이기 위해 아이디어 다양성이 미치는 영향을 이해하려는 것",
        "method": [
            "MLE-bench라는 벤치마크에서 다양한 모델과 에이전트 스캐폴드를 분석해 아이디어 다양성이 성과에 미치는 영향을 검토함 (We examine the role that ideation diversity plays in agent performance.)",
            "아이디어 다양성이 높은 에이전트들이 더 나은 성과를 올린다는 점을 드러냄 (Our analysis reveals that higher-performing agents tend to have increased ideation diversity.)",
            "아이디어 다양성의 정도를 조절하여 실험을 진행하고, 아이디어 다양성이 높을수록 성과가 강화됨을 입증함 (We run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance.)"
        ],
        "conclusion": "아이디어 다양성이 AI 연구 에이전트의 성과를 높이는 데 중요한 요소임을 확인함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2511.15186",
            "authors": [
                {
                    "_id": "691e88213c64d32b0364590c",
                    "user": {
                        "_id": "66714579562ee91c16be977c",
                        "avatarUrl": "/avatars/fd1a386c1e06fcbc9c06017d18e4fe5f.svg",
                        "isPro": false,
                        "fullname": "Geon Choi",
                        "user": "checkone",
                        "type": "user"
                    },
                    "name": "Geon Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:48:57.580Z",
                    "hidden": false
                },
                {
                    "_id": "691e88213c64d32b0364590d",
                    "user": {
                        "_id": "62a4d58e81a4b10e93064ad6",
                        "avatarUrl": "/avatars/744d5cbc1745a26b816a458260aba050.svg",
                        "isPro": false,
                        "fullname": "hangyulyoon",
                        "user": "hangyulmd",
                        "type": "user"
                    },
                    "name": "Hangyul Yoon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-11-20T08:48:59.480Z",
                    "hidden": false
                },
                {
                    "_id": "691e88213c64d32b0364590e",
                    "name": "Hyunju Shin",
                    "hidden": false
                },
                {
                    "_id": "691e88213c64d32b0364590f",
                    "name": "Hyunki Park",
                    "hidden": false
                },
                {
                    "_id": "691e88213c64d32b03645910",
                    "name": "Sang Hoon Seo",
                    "hidden": false
                },
                {
                    "_id": "691e88213c64d32b03645911",
                    "name": "Eunho Yang",
                    "hidden": false
                },
                {
                    "_id": "691e88213c64d32b03645912",
                    "name": "Edward Choi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-19T07:17:19.000Z",
            "submittedOnDailyAt": "2025-11-20T00:49:39.884Z",
            "title": "Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset",
            "submittedOnDailyBy": {
                "_id": "62a4d58e81a4b10e93064ad6",
                "avatarUrl": "/avatars/744d5cbc1745a26b816a458260aba050.svg",
                "isPro": false,
                "fullname": "hangyulyoon",
                "user": "hangyulmd",
                "type": "user"
            },
            "summary": "The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.",
            "upvotes": 21,
            "discussionId": "691e88213c64d32b03645913",
            "ai_summary": "A new instruction-guided lesion segmentation paradigm using a large-scale dataset and a vision-language model enables diverse CXR lesion segmentation with simple instructions.",
            "ai_keywords": [
                "instruction-guided lesion segmentation",
                "MIMIC-ILS",
                "multimodal pipeline",
                "vision-language model",
                "pixel-level CXR lesion grounding"
            ],
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "translation_title": "자동 생성된 대규모 데이터셋을 활용한 흉부 X-레이의 지침 기반 병변 분할",
        "purpose": "지침 기반 병변 분할 방법을 통해 흉부 X-레이에서의 병변 분할 효율성을 개선하기 위함",
        "method": [
            "흉부 X-레이 이미지를 기반으로 지침-답변 데이터셋 MIMIC-ILS를 구축함(we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation)",
            "자동화된 멀티모달 파이프라인을 사용하여 1.1M 개의 지침-답변 쌍을 생성함(using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports)",
            "MIMIC-ILS를 바탕으로 fine-tuning한 비전-언어 모델 ROSALIA를 소개함(we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS)"
        ],
        "conclusion": "ROSALIA 모델은 다양한 병변을 정확히 분할하고 사용자 지침에 대한 텍스트 설명을 제공하며, 새로운 작업에서 높은 성능을 보여줌으로써 MIMIC-ILS의 활용 가치를 입증함.",
        "keywords": [
            "Image Segmentation",
            "Computer Vision",
            "Multimodal Learning"
        ]
    }
]