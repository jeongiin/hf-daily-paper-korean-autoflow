[
    {
        "paper": {
            "id": "2504.08685",
            "authors": [
                {
                    "_id": "67fc6ffc59b22e7c34d64c2e",
                    "name": "Team Seawead",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c2f",
                    "name": "Ceyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c30",
                    "user": {
                        "_id": "64415957bd0c9726529802f6",
                        "avatarUrl": "/avatars/1132d1ee68fb58ec635d57c8175caacd.svg",
                        "isPro": false,
                        "fullname": "Zhijie Lin",
                        "user": "Ikuinen",
                        "type": "user"
                    },
                    "name": "Zhijie Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-14T13:17:03.433Z",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c31",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c32",
                    "name": "Shanchuan Lin",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c33",
                    "user": {
                        "_id": "645d5a44680734460f9e7742",
                        "avatarUrl": "/avatars/66687e2c413acce55436d967071f8786.svg",
                        "isPro": false,
                        "fullname": "Zhibei Ma",
                        "user": "Brightmzb",
                        "type": "user"
                    },
                    "name": "Zhibei Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-14T13:19:16.342Z",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c34",
                    "name": "Haoyuan Guo",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c35",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c36",
                    "name": "Lu Qi",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c37",
                    "name": "Sen Wang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c38",
                    "name": "Feng Cheng",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c39",
                    "name": "Feilong Zuo Xuejiao Zeng",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c3a",
                    "user": {
                        "_id": "63c98de85fdc575773c52098",
                        "avatarUrl": "/avatars/b66675e781695daed7c00a7c802c91f5.svg",
                        "isPro": false,
                        "fullname": "Ziyan Yang",
                        "user": "ziyany",
                        "type": "user"
                    },
                    "name": "Ziyan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-14T13:20:01.373Z",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c3b",
                    "name": "Fangyuan Kong",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c3c",
                    "name": "Zhiwu Qing",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c3d",
                    "name": "Fei Xiao",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c3e",
                    "name": "Meng Wei",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c3f",
                    "name": "Tuyen Hoang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c40",
                    "name": "Siyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c41",
                    "name": "Peihao Zhu",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c42",
                    "name": "Qi Zhao",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c43",
                    "name": "Jiangqiao Yan",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c44",
                    "name": "Liangke Gui",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c45",
                    "name": "Sheng Bi",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c46",
                    "name": "Jiashi Li",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c47",
                    "name": "Yuxi Ren",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c48",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c49",
                    "name": "Huixia Li",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c4a",
                    "name": "Xuefeng Xiao",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c4b",
                    "name": "Shu Liu",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c4c",
                    "user": {
                        "_id": "636a4e4fa55bbbdf8c877667",
                        "avatarUrl": "/avatars/efdb68c56a4a44fdac52750c07a6cc35.svg",
                        "isPro": false,
                        "fullname": "Ling Feng",
                        "user": "lingff",
                        "type": "user"
                    },
                    "name": "Feng Ling",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T09:46:31.964Z",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c4d",
                    "name": "Heng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c4e",
                    "name": "Houmin Wei",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c4f",
                    "name": "Huafeng Kuang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c50",
                    "name": "Jerry Duncan",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c51",
                    "name": "Junda Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c52",
                    "name": "Junru Zheng",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c53",
                    "name": "Li Sun",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c54",
                    "name": "Manlin Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c55",
                    "name": "Renfei Sun",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c56",
                    "name": "Xiaobin Zhuang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c57",
                    "name": "Xiaojie Li",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c58",
                    "name": "Xin Xia",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c59",
                    "name": "Xuyan Chi",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c5a",
                    "name": "Yanghua Peng",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c5b",
                    "name": "Yuping Wang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c5c",
                    "name": "Yuxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c5d",
                    "name": "Zhongkai Zhao",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c5e",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c5f",
                    "name": "Zuquan Song",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c60",
                    "user": {
                        "_id": "6421183b69a2c2933882d652",
                        "avatarUrl": "/avatars/66813a8fa22915087cccd4dbfb945ca7.svg",
                        "isPro": false,
                        "fullname": "Zhenheng Yang",
                        "user": "zhenheny",
                        "type": "user"
                    },
                    "name": "Zhenheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T09:46:34.053Z",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c61",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c62",
                    "name": "Jianchao Yang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c63",
                    "name": "Lu Jiang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg"
            ],
            "publishedAt": "2025-04-11T16:46:20.000Z",
            "submittedOnDailyAt": "2025-04-14T00:55:27.428Z",
            "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
            "submittedOnDailyBy": {
                "_id": "64a5cba3bea0116f8f7187a7",
                "avatarUrl": "/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg",
                "isPro": false,
                "fullname": "Lu Jiang",
                "user": "roadjiang",
                "type": "user"
            },
            "summary": "This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/",
            "upvotes": 70,
            "discussionId": "67fc700159b22e7c34d64d78",
            "projectPage": "https://seaweed.video/"
        },
        "translation_title": "Seaweed-7B: 비용 효율적인 비디오 생성 기초 모델 훈련 전략",
        "purpose": "비용 효율적으로 비디오 생성 모델을 훈련시키기 위한 전략 연구",
        "method": [
            "약 70억 개의 파라미터를 가진 중간 크기의 연구 모델 Seaweed-7B를 제안하고, 이를 위해 665,000 H100 GPU 시간을 사용하여 처음부터 훈련함(This technical report presents a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours.)",
            "제한된 자원에서도 뛰어난 성능을 보여주기 위해 핵심 설계 결정을 강조함(This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model.)",
            "경쟁 모델들과 비교하여 Seaweed-7B의 일반화 능력을 입증하고 다양한 응용 분야에 효과적으로 적응할 수 있음을 보여줌(our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training.)"
        ],
        "conclusion": "Seaweed-7B는 더 큰 모델들과 비교해도 유사한 또는 더 뛰어난 성능을 보여주며, 비용 효율적인 비디오 생성 모델로서 가능성을 입증함.",
        "keywords": [
            "Video Generation",
            "Computer Vision",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2504.08736",
            "authors": [
                {
                    "_id": "67fc8e37864dfcbd93d3b802",
                    "user": {
                        "_id": "668125557b50b433cda2a211",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
                        "isPro": false,
                        "fullname": "Tianwei Xiong",
                        "user": "YuuTennYi",
                        "type": "user"
                    },
                    "name": "Tianwei Xiong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T09:46:29.330Z",
                    "hidden": false
                },
                {
                    "_id": "67fc8e37864dfcbd93d3b803",
                    "name": "Jun Hao Liew",
                    "hidden": false
                },
                {
                    "_id": "67fc8e37864dfcbd93d3b804",
                    "name": "Zilong Huang",
                    "hidden": false
                },
                {
                    "_id": "67fc8e37864dfcbd93d3b805",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "67fc8e37864dfcbd93d3b806",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T09:46:26.140Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T17:59:58.000Z",
            "submittedOnDailyAt": "2025-04-14T02:57:04.488Z",
            "title": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\n  Autoregressive Image Generation",
            "submittedOnDailyBy": {
                "_id": "668125557b50b433cda2a211",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
                "isPro": false,
                "fullname": "Tianwei Xiong",
                "user": "YuuTennYi",
                "type": "user"
            },
            "summary": "In autoregressive (AR) image generation, visual tokenizers compress images\ninto compact discrete latent tokens, enabling efficient training of downstream\nautoregressive models for visual generation via next-token prediction. While\nscaling visual tokenizers improves image reconstruction quality, it often\ndegrades downstream generation quality -- a challenge not adequately addressed\nin existing literature. To address this, we introduce GigaTok, the first\napproach to simultaneously improve image reconstruction, generation, and\nrepresentation learning when scaling visual tokenizers. We identify the growing\ncomplexity of latent space as the key factor behind the reconstruction vs.\ngeneration dilemma. To mitigate this, we propose semantic regularization, which\naligns tokenizer features with semantically consistent features from a\npre-trained visual encoder. This constraint prevents excessive latent space\ncomplexity during scaling, yielding consistent improvements in both\nreconstruction and downstream autoregressive generation. Building on semantic\nregularization, we explore three key practices for scaling tokenizers:(1) using\n1D tokenizers for better scalability, (2) prioritizing decoder scaling when\nexpanding both encoder and decoder, and (3) employing entropy loss to stabilize\ntraining for billion-scale tokenizers. By scaling to 3 space billion\nparameters, GigaTok achieves state-of-the-art performance in reconstruction,\ndownstream AR generation, and downstream AR representation quality.",
            "upvotes": 27,
            "discussionId": "67fc8e38864dfcbd93d3b836",
            "projectPage": "https://silentview.github.io/GigaTok/",
            "githubRepo": "https://github.com/SilentView/GigaTok"
        },
        "translation_title": "GigaTok: 30억 매개변수로 비주얼 토크나이저 확장하여 자율 회귀 이미지 생성",
        "purpose": "비주얼 토크나이저 확장 시 이미지 재구성, 생성 및 표현 학습을 동시에 개선하기 위한 연구",
        "method": [
            "이미지를 컴팩트한 이산 잠재 토큰으로 압축하는 비주얼 토크나이저를 도입함(In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens.)",
            "semantic regularization을 통해 토크나이저 기능을 사전 훈련된 비주얼 인코더의 의미적으로 일관된 기능과 정렬시킴(We propose semantic regularization, which aligns tokenizer features with semantically consistent features from a pre-trained visual encoder.)",
            "3개의 핵심 실천 방법을 제시해 30억 매개변수로 토크나이저를 확장함(Building on semantic regularization, we explore three key practices for scaling tokenizers.)"
        ],
        "conclusion": "GigaTok은 30억 매개변수에서 최고의 재구성 및 자율 회귀 생성 성능을 달성함.",
        "keywords": [
            "Image Generation",
            "3D Vision",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2504.08388",
            "authors": [
                {
                    "_id": "67fc7367df5f5d1e87c14c6a",
                    "name": "Junliang Guo",
                    "hidden": false
                },
                {
                    "_id": "67fc7367df5f5d1e87c14c6b",
                    "name": "Yang Ye",
                    "hidden": false
                },
                {
                    "_id": "67fc7367df5f5d1e87c14c6c",
                    "name": "Tianyu He",
                    "hidden": false
                },
                {
                    "_id": "67fc7367df5f5d1e87c14c6d",
                    "name": "Haoyu Wu",
                    "hidden": false
                },
                {
                    "_id": "67fc7367df5f5d1e87c14c6e",
                    "name": "Yushu Jiang",
                    "hidden": false
                },
                {
                    "_id": "67fc7367df5f5d1e87c14c6f",
                    "name": "Tim Pearce",
                    "hidden": false
                },
                {
                    "_id": "67fc7367df5f5d1e87c14c70",
                    "name": "Jiang Bian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T09:41:04.000Z",
            "submittedOnDailyAt": "2025-04-14T02:07:06.500Z",
            "title": "MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate 4 to 7 frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released.",
            "upvotes": 16,
            "discussionId": "67fc7367df5f5d1e87c14ca6",
            "githubRepo": "https://github.com/microsoft/MineWorld"
        },
        "translation_title": "MineWorld: Minecraft에서의 실시간 오픈소스 인터랙티브 월드 모델",
        "purpose": "지능형 에이전트가 인간과 효과적으로 상호작용하고 동적 환경에서 작동할 수 있도록 하는 월드 모델을 개발하는 것",
        "method": [
            "MineWorld라는 실시간 인터랙티브 월드 모델을 제안하고 게임 장면과 이에 따른 행동을 입력으로 하는 시각-행동 오토회귀 Transformer를 활용함.(In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling.)",
            "시각 장면과 행동을 각각 이산 토큰 ID로 변환하여 모델의 입력을 구성하고 다음 토큰 예측으로 훈련함.(Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved.)",
            "새로운 병렬 디코딩 알고리즘을 개발하여 각 프레임에서 공간적으로 잉여 토큰을 동시에 예측함.(In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time,)"
        ],
        "conclusion": "MineWorld는 SoTA 오픈소스 기반의 확산 월드 모델보다 뛰어난 성능을 보이며, 코드와 모델이 공개되었음.",
        "keywords": [
            "Computer Vision",
            "Image Generation",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2504.08600",
            "authors": [
                {
                    "_id": "67fc9e72b2383c63dc413dcb",
                    "name": "Peixian Ma",
                    "hidden": false
                },
                {
                    "_id": "67fc9e72b2383c63dc413dcc",
                    "user": {
                        "_id": "6575a625b951d40e7a4d8685",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
                        "isPro": false,
                        "fullname": "zhuangxialie",
                        "user": "ZhuangXialie",
                        "type": "user"
                    },
                    "name": "Xialie Zhuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T09:46:23.810Z",
                    "hidden": false
                },
                {
                    "_id": "67fc9e72b2383c63dc413dcd",
                    "name": "Chengjin Xu",
                    "hidden": false
                },
                {
                    "_id": "67fc9e72b2383c63dc413dce",
                    "name": "Xuhui Jiang",
                    "hidden": false
                },
                {
                    "_id": "67fc9e72b2383c63dc413dcf",
                    "name": "Ran Chen",
                    "hidden": false
                },
                {
                    "_id": "67fc9e72b2383c63dc413dd0",
                    "name": "Jian Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T15:01:30.000Z",
            "submittedOnDailyAt": "2025-04-14T04:07:17.501Z",
            "title": "SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6575a625b951d40e7a4d8685",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
                "isPro": false,
                "fullname": "zhuangxialie",
                "user": "ZhuangXialie",
                "type": "user"
            },
            "summary": "Natural Language to SQL (NL2SQL) enables intuitive interactions with\ndatabases by transforming natural language queries into structured SQL\nstatements. Despite recent advancements in enhancing human-computer interaction\nwithin database applications, significant challenges persist, particularly\nregarding the inference performance in complex scenarios involving multi-table\njoins and nested queries. Current methodologies primarily utilize supervised\nfine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and\ninterpretability in new environments (e.g., finance and healthcare). In order\nto enhance the reasoning performance of the NL2SQL model in the above complex\nsituations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the\nreinforcement learning (RL) algorithms. We design a specialized RL-based reward\nfunction tailored for NL2SQL tasks and discussed the impact of cold start on\nthe effectiveness of intensive training. In addition, we achieve competitive\naccuracy using only a tiny amount of synthetic NL2SQL data for augmented\ntraining and further explore data engineering for RL. In existing experiments,\nSQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider\nand BIRD, respectively, only using the 7B base model.",
            "upvotes": 9,
            "discussionId": "67fc9e73b2383c63dc413e19"
        },
        "translation_title": "SQL-R1: 강화 학습을 통한 자연어에서 SQL로의 추론 모델 훈련",
        "purpose": "복잡한 환경에서 NL2SQL 모델의 추론 성능을 향상시키기 위한 연구",
        "method": [
            "NL2SQL 모델을 훈련하기 위해 강화 학습(RL) 알고리즘을 사용함(In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms.)",
            "NL2SQL 작업에 적합한 RL 기반 보상 함수를 설계함(We design a specialized RL-based reward function tailored for NL2SQL tasks.)",
            "적은 양의 합성 NL2SQL 데이터를 이용해 경쟁력을 갖춘 정확도를 달성함(In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training.)"
        ],
        "conclusion": "SQL-R1은 복잡한 쿼리에서 88.6%와 66.6%의 실행 정확도를 달성하여 강력한 성능을 입증함.",
        "keywords": [
            "Natural Language Processing",
            "Robotics",
            "Document Parsing"
        ]
    },
    {
        "paper": {
            "id": "2504.07963",
            "authors": [
                {
                    "_id": "67f86da6ac109135e18e150f",
                    "name": "Shoufa Chen",
                    "hidden": false
                },
                {
                    "_id": "67f86da6ac109135e18e1510",
                    "name": "Chongjian Ge",
                    "hidden": false
                },
                {
                    "_id": "67f86da6ac109135e18e1511",
                    "name": "Shilong Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f86da6ac109135e18e1512",
                    "name": "Peize Sun",
                    "hidden": false
                },
                {
                    "_id": "67f86da6ac109135e18e1513",
                    "name": "Ping Luo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg"
            ],
            "publishedAt": "2025-04-10T17:59:56.000Z",
            "submittedOnDailyAt": "2025-04-14T04:05:17.975Z",
            "title": "PixelFlow: Pixel-Space Generative Models with Flow",
            "submittedOnDailyBy": {
                "_id": "6412a33900634c4fe9873652",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
                "isPro": false,
                "fullname": "Shoufa Chen",
                "user": "ShoufaChen",
                "type": "user"
            },
            "summary": "We present PixelFlow, a family of image generation models that operate\ndirectly in the raw pixel space, in contrast to the predominant latent-space\nmodels. This approach simplifies the image generation process by eliminating\nthe need for a pre-trained Variational Autoencoder (VAE) and enabling the whole\nmodel end-to-end trainable. Through efficient cascade flow modeling, PixelFlow\nachieves affordable computation cost in pixel space. It achieves an FID of 1.98\non 256times256 ImageNet class-conditional image generation benchmark. The\nqualitative text-to-image results demonstrate that PixelFlow excels in image\nquality, artistry, and semantic control. We hope this new paradigm will inspire\nand open up new opportunities for next-generation visual generation models.\nCode and models are available at https://github.com/ShoufaChen/PixelFlow.",
            "upvotes": 9,
            "discussionId": "67f86da7ac109135e18e154b",
            "githubRepo": "https://github.com/ShoufaChen/PixelFlow"
        },
        "translation_title": "PixelFlow: 픽셀 공간 생성을 위한 생성 모델",
        "purpose": "픽셀 공간에서 직접 작동하는 이미지 생성 모델을 개발하고 그 가능성을 탐구하기 위해",
        "method": [
            "PixelFlow라는 이미지 생성 모델을 제안하고, 이를 통해 기존의 VAE 없이 전체 모델을 엔드 투 엔드로 훈련 가능하게 함(We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models.)",
            "효율적인 cascade flow 모델링을 통해 픽셀 공간에서 비용 효과적인 계산을 구현함(Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space.)",
            "256x256 ImageNet 클래스 조건의 이미지 생성 벤치마크에서 FID 1.98을 달성함(It achieves an FID of 1.98 on 256times256 ImageNet class-conditional image generation benchmark.)"
        ],
        "conclusion": "PixelFlow는 이미지 품질, 예술성, 의미론적 제어에서 뛰어난 성과를 보여주며, 차세대 시각 생성 모델에 대한 새로운 기회를 열 것으로 기대됨.",
        "keywords": [
            "Image Generation",
            "Computer Vision",
            "Image Classification"
        ]
    }
]