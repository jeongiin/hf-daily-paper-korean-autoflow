[
    {
        "paper": {
            "id": "2507.01853",
            "authors": [
                {
                    "_id": "686b4e69213f123a1f88bd76",
                    "name": "Samridhi Raj Sinha",
                    "hidden": false
                },
                {
                    "_id": "686b4e69213f123a1f88bd77",
                    "user": {
                        "_id": "66e1425c919f283fbd7dfb5e",
                        "avatarUrl": "/avatars/b43ee9b9042486b5faf91472b4c49c5d.svg",
                        "isPro": false,
                        "fullname": "Rajvee Sheth",
                        "user": "RajveeSheth",
                        "type": "user"
                    },
                    "name": "Rajvee Sheth",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-07T11:24:11.116Z",
                    "hidden": false
                },
                {
                    "_id": "686b4e69213f123a1f88bd78",
                    "name": "Abhishek Upperwal",
                    "hidden": false
                },
                {
                    "_id": "686b4e69213f123a1f88bd79",
                    "name": "Mayank Singh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-02T16:07:54.000Z",
            "submittedOnDailyAt": "2025-07-07T03:06:37.666Z",
            "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages",
            "submittedOnDailyBy": {
                "_id": "66e1425c919f283fbd7dfb5e",
                "avatarUrl": "/avatars/b43ee9b9042486b5faf91472b4c49c5d.svg",
                "isPro": false,
                "fullname": "Rajvee Sheth",
                "user": "RajveeSheth",
                "type": "user"
            },
            "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.",
            "upvotes": 2,
            "discussionId": "686b4e69213f123a1f88bd7a",
            "ai_summary": "EKA-EVAL is a comprehensive multilingual evaluation framework for large language models, supporting diverse benchmarks and features for efficient distributed inference and GPU usage.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "unified evaluation framework",
                "production-ready",
                "Indic-specific datasets",
                "reasoning",
                "mathematics",
                "tool use",
                "long-context understanding",
                "reading comprehension",
                "distributed inference",
                "quantization",
                "multi-GPU usage",
                "end-to-end",
                "extensible evaluation suite",
                "multilingual benchmarking",
                "open-source"
            ]
        },
        "translation_title": "EKA-EVAL: 인도 언어에서 대형 언어 모델을 위한 포괄적인 평가 프레임워크",
        "purpose": "인도와 같은 다양한 언어 지역의 요구에 맞춘 대형 언어 모델(Large Language Models) 평가 체계 개발",
        "method": [
            "35개 이상의 벤치마크와 10개의 인도 특화 데이터 세트를 통합한 통합 평가 프레임워크를 제시함(We present EKA-EVAL, a unified and production-ready evaluation framework that integrates over 35 benchmarks, including 10 Indic-specific datasets.)",
            "범주로는 추론, 수학, 도구 사용, 긴 문맥 이해, 독서 이해가 포함됨(spanning categories like reasoning, mathematics, tool use, long-context understanding, and reading comprehension.)",
            "분산 추론, 양자화, 다중 GPU 사용을 지원하여 더 넓은 벤치마크 범위를 제공함(EKA-EVAL offers broader benchmark coverage, with built-in support for distributed inference, quantization, and multi-GPU usage.)"
        ],
        "conclusion": "EKA-EVAL은 글로벌 및 인도 LLM에 맞춤형으로 설계된 첫 번째 종합적이고 확장 가능한 평가 도구로, 다국어 벤치마킹의 장벽을 낮추는 데 기여함.",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2507.02608",
            "authors": [
                {
                    "_id": "686be4bc364e2ad167eb52bf",
                    "name": "François Rozet",
                    "hidden": false
                },
                {
                    "_id": "686be4bc364e2ad167eb52c0",
                    "name": "Ruben Ohana",
                    "hidden": false
                },
                {
                    "_id": "686be4bc364e2ad167eb52c1",
                    "name": "Michael McCabe",
                    "hidden": false
                },
                {
                    "_id": "686be4bc364e2ad167eb52c2",
                    "name": "Gilles Louppe",
                    "hidden": false
                },
                {
                    "_id": "686be4bc364e2ad167eb52c3",
                    "name": "François Lanusse",
                    "hidden": false
                },
                {
                    "_id": "686be4bc364e2ad167eb52c4",
                    "name": "Shirley Ho",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-03T13:32:50.000Z",
            "submittedOnDailyAt": "2025-07-07T13:46:34.305Z",
            "title": "Lost in Latent Space: An Empirical Study of Latent Diffusion Models for\n  Physics Emulation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "The steep computational cost of diffusion models at inference hinders their\nuse as fast physics emulators. In the context of image and video generation,\nthis computational drawback has been addressed by generating in the latent\nspace of an autoencoder instead of the pixel space. In this work, we\ninvestigate whether a similar strategy can be effectively applied to the\nemulation of dynamical systems and at what cost. We find that the accuracy of\nlatent-space emulation is surprisingly robust to a wide range of compression\nrates (up to 1000x). We also show that diffusion-based emulators are\nconsistently more accurate than non-generative counterparts and compensate for\nuncertainty in their predictions with greater diversity. Finally, we cover\npractical design choices, spanning from architectures to optimizers, that we\nfound critical to train latent-space emulators.",
            "upvotes": 1,
            "discussionId": "686be4bd364e2ad167eb52c5",
            "ai_summary": "The use of latent space diffusion models for faster and accurate emulation of dynamical systems is viable, offering robustness to high compression rates and improved prediction diversity compared to non-generative approaches.",
            "ai_keywords": [
                "diffusion models",
                "inference",
                "latent space",
                "autoencoder",
                "pixel space",
                "dynamical systems",
                "accuracy",
                "compression rates",
                "design choices",
                "architectures",
                "optimizers"
            ]
        },
        "translation_title": "잠재 공간에서의 혼란: 물리 에뮬레이션을 위한 잠재 확산 모델의 실증 연구",
        "purpose": "빠른 물리 에뮬레이터로서 확산 모델의 활용 가능성을 연구하고자 함",
        "method": [
            "자동 인코더의 픽셀 공간 대신 잠재 공간에서 이미지를 생성하는 전략을 활용하여 물리 시스템을 에뮬레이션 함(In the context of image and video generation, this computational drawback has been addressed by generating in the latent space of an autoencoder instead of the pixel space.)",
            "1000배 압축 비율에서도 잠재 공간 에뮬레이션의 정확도가 견고하다는 것을 발견함(We find that the accuracy of latent-space emulation is surprisingly robust to a wide range of compression rates (up to 1000x).)",
            "확산 기반 에뮬레이터가 비생성 모델보다 일관되게 더 높은 정확도를 보여주고 불확실성을 더 다양하게 보완함을 입증함(We also show that diffusion-based emulators are consistently more accurate than non-generative counterparts and compensate for uncertainty in their predictions with greater diversity.)"
        ],
        "conclusion": "잠재 공간 에뮬레이터의 설계를 위한 중요한 요소를 규명하고, 확산 기반 접근법의 유용성을 강조함.",
        "keywords": [
            "Image Generation",
            "Video Generation",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2507.01955",
            "authors": [
                {
                    "_id": "686b8347213f123a1f88bdc8",
                    "name": "Rahul Ramachandran",
                    "hidden": false
                },
                {
                    "_id": "686b8347213f123a1f88bdc9",
                    "name": "Ali Garjani",
                    "hidden": false
                },
                {
                    "_id": "686b8347213f123a1f88bdca",
                    "name": "Roman Bachmann",
                    "hidden": false
                },
                {
                    "_id": "686b8347213f123a1f88bdcb",
                    "name": "Andrei Atanov",
                    "hidden": false
                },
                {
                    "_id": "686b8347213f123a1f88bdcc",
                    "name": "Oğuzhan Fatih Kar",
                    "hidden": false
                },
                {
                    "_id": "686b8347213f123a1f88bdcd",
                    "name": "Amir Zamir",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-02T17:59:07.000Z",
            "submittedOnDailyAt": "2025-07-07T06:51:04.452Z",
            "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": true,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.",
            "upvotes": 1,
            "discussionId": "686b8348213f123a1f88bdce",
            "projectPage": "https://fm-vision-evals.epfl.ch/",
            "githubRepo": "https://github.com/EPFL-VILAB/fm-vision-evals",
            "ai_summary": "Multimodal foundation models, despite being primarily trained on image-text tasks, demonstrate respectable performance across various vision tasks when adapted through prompt chaining, though they fall short compared to specialized models.",
            "ai_keywords": [
                "GPT-4o",
                "o4-mini",
                "Gemini 1.5 Pro",
                "Gemini 2.0 Flash",
                "Claude 3.5 Sonnet",
                "Qwen2-VL",
                "Llama 3.2",
                "semantic segmentation",
                "object detection",
                "image classification",
                "depth prediction",
                "surface normal prediction",
                "COCO",
                "ImageNet",
                "prompt chaining",
                "reasoning models",
                "hallucinations",
                "spatial misalignments"
            ],
            "githubStars": 19
        },
        "translation_title": "GPT-4o의 비전 이해력: 표준 컴퓨터 비전 작업에서 멀티모달 기반 모델 평가",
        "purpose": "멀티모달 모델의 비전 이해력을 평가하고, 표준 컴퓨터 비전 작업에 대한 성능을 벤치마킹하기 위함",
        "method": [
            "표준 비전 작업을 텍스트 프롬프트와 API 호환 작업으로 변환하여 정형화된 벤치마킹 프레임워크를 구축함(we address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.)",
            "인기 있는 멀티모달 모델의 성능을 다양한 표준 컴퓨터 비전 작업에서 평가함(we benchmark the performance of popular multimodal foundation models on standard computer vision tasks)."
        ],
        "conclusion": "GPT-4o는 비전 작업에서 다른 비전 전문가 모델에 비해 성능은 미치지 않지만, 일반적인 작업 수행에서 괜찮은 결과를 보이며, 비전 모델에 대한 새로운 평가 기준을 제시함.",
        "keywords": [
            "Computer Vision",
            "Multimodal Learning",
            "Image Classification"
        ]
    }
]