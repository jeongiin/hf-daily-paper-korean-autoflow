[
    {
        "paper": {
            "id": "2601.12993",
            "authors": [
                {
                    "_id": "69705709a8be625b19c2af1f",
                    "user": {
                        "_id": "6708cbdcf8a1d7b26732c038",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CN1WHMPKfjQ8wmfwOe0ni.png",
                        "isPro": false,
                        "fullname": "Hao Luo",
                        "user": "Lightet",
                        "type": "user"
                    },
                    "name": "Hao Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-21T10:15:59.988Z",
                    "hidden": false
                },
                {
                    "_id": "69705709a8be625b19c2af20",
                    "name": "Ye Wang",
                    "hidden": false
                },
                {
                    "_id": "69705709a8be625b19c2af21",
                    "user": {
                        "_id": "640dd700fdeaae139081f598",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
                        "isPro": false,
                        "fullname": "Wanpeng Zhang",
                        "user": "zawnpn",
                        "type": "user"
                    },
                    "name": "Wanpeng Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-21T09:19:56.371Z",
                    "hidden": false
                },
                {
                    "_id": "69705709a8be625b19c2af22",
                    "user": {
                        "_id": "64eac1f496f42afd627d439c",
                        "avatarUrl": "/avatars/aa46265122b8a1170f57475494d7922e.svg",
                        "isPro": false,
                        "fullname": "Sipeng Zheng",
                        "user": "sipeng9527",
                        "type": "user"
                    },
                    "name": "Sipeng Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:49:38.507Z",
                    "hidden": false
                },
                {
                    "_id": "69705709a8be625b19c2af23",
                    "user": {
                        "_id": "66c84a9eab23d3d7dfb2a368",
                        "avatarUrl": "/avatars/b0a50133c6a95ed340dfb462e87820f4.svg",
                        "isPro": false,
                        "fullname": "ziheng xi",
                        "user": "zhenqis123",
                        "type": "user"
                    },
                    "name": "Ziheng Xi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:49:45.981Z",
                    "hidden": false
                },
                {
                    "_id": "69705709a8be625b19c2af24",
                    "user": {
                        "_id": "64bdd5cc76a6e2efccb22100",
                        "avatarUrl": "/avatars/5a0edc24283616dafc76ce5ec97ab5a0.svg",
                        "isPro": false,
                        "fullname": "xuchaoyi",
                        "user": "co1one",
                        "type": "user"
                    },
                    "name": "Chaoyi Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:49:56.014Z",
                    "hidden": false
                },
                {
                    "_id": "69705709a8be625b19c2af25",
                    "user": {
                        "_id": "68872ff6c18b7e1e13115564",
                        "avatarUrl": "/avatars/f908fc3cc89cd81493105359093f299d.svg",
                        "isPro": false,
                        "fullname": "Haiweng Xu",
                        "user": "Seaman05",
                        "type": "user"
                    },
                    "name": "Haiweng Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:50:01.261Z",
                    "hidden": false
                },
                {
                    "_id": "69705709a8be625b19c2af26",
                    "user": {
                        "_id": "644560657a7b94ddc2d445a3",
                        "avatarUrl": "/avatars/09d6447da6ff1bd0b2b00c899c9f1b28.svg",
                        "isPro": false,
                        "fullname": "Haoqi Yuan",
                        "user": "Yaya041",
                        "type": "user"
                    },
                    "name": "Haoqi Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:50:06.048Z",
                    "hidden": false
                },
                {
                    "_id": "69705709a8be625b19c2af27",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "69705709a8be625b19c2af28",
                    "name": "Yiqing Wang",
                    "hidden": false
                },
                {
                    "_id": "69705709a8be625b19c2af29",
                    "name": "Yicheng Feng",
                    "hidden": false
                },
                {
                    "_id": "69705709a8be625b19c2af2a",
                    "user": {
                        "_id": "67d905c0e27ba28109384f5c",
                        "avatarUrl": "/avatars/26712594ac9d43c8d1a3e75e36b5df16.svg",
                        "isPro": false,
                        "fullname": "Zongqing Lu",
                        "user": "chungtsing",
                        "type": "user"
                    },
                    "name": "Zongqing Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:50:24.833Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-19T12:20:38.000Z",
            "submittedOnDailyAt": "2026-01-21T02:12:40.880Z",
            "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization",
            "submittedOnDailyBy": {
                "_id": "640dd700fdeaae139081f598",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
                "isPro": false,
                "fullname": "Wanpeng Zhang",
                "user": "zawnpn",
                "type": "user"
            },
            "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.",
            "upvotes": 53,
            "discussionId": "69705709a8be625b19c2af2b",
            "projectPage": "https://research.beingbeyond.com/being-h05",
            "githubRepo": "https://github.com/BeingBeyond/Being-H",
            "githubRepoAddedBy": "user",
            "ai_summary": "Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.",
            "ai_keywords": [
                "Vision-Language-Action",
                "cross-embodiment generalization",
                "human-centric learning",
                "multimodal data",
                "Unified Action Space",
                "Mixture-of-Transformers",
                "Mixture-of-Flow",
                "manifold-preserving gating",
                "universal async chunking"
            ],
            "githubStars": 262,
            "organization": {
                "_id": "687a8ba5aedd77694bc94386",
                "name": "BeingBeyond",
                "fullname": "BeingBeyond",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"
            }
        },
        "translation_title": "Being-H0.5: 다양한 플랫폼에서의 크로스 임바디먼트 일반화를 위한 인간 중심 로봇 학습 기법",
        "purpose": "다양한 로봇 플랫폼 간의 견고한 크로스 임바디먼트 일반화를 위해 인간 중심의 학습 패러다임 연구",
        "method": [
            "인간 상호작용 흔적을 물리적 상호작용을 위한 보편적인 '모국어'로 간주하는 학습 패러다임 제안(we propose a human-centric learning paradigm that treats human interaction traces as a universal 'mother tongue' for physical interaction.)",
            "UniHand-2.0이라는 대규모 임바디드 사전 훈련 레시피를 제공하며, 30개의 다양한 로봇에 대해 35,000시간 이상의 멀티모달 데이터를 포함함(To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments.)",
            "통합 행동 공간을 도입하여 이질적인 로봇 제어를 의미적으로 정렬된 슬롯으로 매핑함(Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots.)"
        ],
        "conclusion": "Being-H0.5는 시뮬레이션 벤치마크에서 최첨단 성과를 달성하며, 다양한 로봇 플랫폼에서 강력한 크로스 임바디먼트 능력을 보임.",
        "keywords": [
            "Robotics",
            "Vision-Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2601.11655",
            "authors": [
                {
                    "_id": "6970436ba8be625b19c2ae97",
                    "user": {
                        "_id": "64d668bf54bb9eb7040c477e",
                        "avatarUrl": "/avatars/b171b9c1cbb22e2f86e4280099c0bf93.svg",
                        "isPro": false,
                        "fullname": "Caihua Li",
                        "user": "LoisNotLo",
                        "type": "user"
                    },
                    "name": "Caihua Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-21T09:20:05.617Z",
                    "hidden": false
                },
                {
                    "_id": "6970436ba8be625b19c2ae98",
                    "user": {
                        "_id": "64c52b6de356b52a9868bce3",
                        "avatarUrl": "/avatars/43b05cc691f273447e8bc65fe7515176.svg",
                        "isPro": false,
                        "fullname": "Guo",
                        "user": "glh123456",
                        "type": "user"
                    },
                    "name": "Lianghong Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-21T09:20:12.738Z",
                    "hidden": false
                },
                {
                    "_id": "6970436ba8be625b19c2ae99",
                    "user": {
                        "_id": "680ef06cce6b5c5af1f29aec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/aTLjskuvCwTYs5JwjUtEF.png",
                        "isPro": false,
                        "fullname": "DeepSoftwareAnalytics",
                        "user": "Yanlin-Wang",
                        "type": "user"
                    },
                    "name": "Yanlin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-21T09:20:10.460Z",
                    "hidden": false
                },
                {
                    "_id": "6970436ba8be625b19c2ae9a",
                    "user": {
                        "_id": "653df20eaa1f487614da4db1",
                        "avatarUrl": "/avatars/12b27ce2c59f53b7e464039deab36a5d.svg",
                        "isPro": false,
                        "fullname": "Daya Guo",
                        "user": "guoday",
                        "type": "user"
                    },
                    "name": "Daya Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-21T09:20:15.421Z",
                    "hidden": false
                },
                {
                    "_id": "6970436ba8be625b19c2ae9b",
                    "user": {
                        "_id": "6355473d525beaee688b7ba1",
                        "avatarUrl": "/avatars/1fb0d57ed5f1a9b872a1ada8b2973ffb.svg",
                        "isPro": false,
                        "fullname": "Wei Tao",
                        "user": "itaowe",
                        "type": "user"
                    },
                    "name": "Wei Tao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-21T09:20:17.409Z",
                    "hidden": false
                },
                {
                    "_id": "6970436ba8be625b19c2ae9c",
                    "name": "Zhenyu Shan",
                    "hidden": false
                },
                {
                    "_id": "6970436ba8be625b19c2ae9d",
                    "name": "Mingwei Liu",
                    "hidden": false
                },
                {
                    "_id": "6970436ba8be625b19c2ae9e",
                    "name": "Jiachi Chen",
                    "hidden": false
                },
                {
                    "_id": "6970436ba8be625b19c2ae9f",
                    "name": "Haoyu Song",
                    "hidden": false
                },
                {
                    "_id": "6970436ba8be625b19c2aea0",
                    "name": "Duyu Tang",
                    "hidden": false
                },
                {
                    "_id": "6970436ba8be625b19c2aea1",
                    "name": "Hongyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6970436ba8be625b19c2aea2",
                    "name": "Zibin Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-15T18:55:03.000Z",
            "submittedOnDailyAt": "2026-01-21T00:52:01.626Z",
            "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey",
            "submittedOnDailyBy": {
                "_id": "6355473d525beaee688b7ba1",
                "avatarUrl": "/avatars/1fb0d57ed5f1a9b872a1ada8b2973ffb.svg",
                "isPro": false,
                "fullname": "Wei Tao",
                "user": "itaowe",
                "type": "user"
            },
            "summary": "Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.",
            "upvotes": 47,
            "discussionId": "6970436ba8be625b19c2aea3",
            "projectPage": "https://deepsoftwareanalytics.github.io/Awesome-Issue-Resolution/",
            "githubRepo": "https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution",
            "githubRepoAddedBy": "user",
            "ai_summary": "Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.",
            "ai_keywords": [
                "large language models",
                "software engineering",
                "autonomous coding agents",
                "training-free frameworks",
                "supervised fine-tuning",
                "reinforcement learning",
                "data quality",
                "agent behavior"
            ],
            "githubStars": 35,
            "organization": {
                "_id": "680ef1aaccefecd5aee18d1d",
                "name": "Deep-Software-Analytics",
                "fullname": "DeepSoftwareAnalytics"
            }
        },
        "translation_title": "LLM 기반 소프트웨어 공학 문제 해결의 발전과 최전선: 종합 조사",
        "purpose": "소프트웨어 공학에서 문제 해결을 위한 LLM 기반 기술의 최신 동향과 도전 과제를 탐구하고 정리함",
        "method": [
            "데이터 생성 경로를 조사하여 자동 수집 및 합성 방법론을 검토함(To begin by examining data construction pipelines, covering automated collection and synthesis approaches)",
            "훈련 없는 프레임워크 및 supervised fine-tuning과 reinforcement learning을 포함한 훈련 기반 기법을 종합적으로 분석함(We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning)",
            "데이터 품질과 에이전트 행동에 대한 중요 분석을 수행하고, 실용적인 응용 사례를 논의함(Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications)"
        ],
        "conclusion": "이 논문은 LLM 기반 문제 해결의 중요한 도전 과제를 식별하고 미래 연구의 유망한 방향을 제시하며, 관련된 동적 리소스를 제공하는 오픈 소스 저장소를 운영함",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2601.14192",
            "authors": [
                {
                    "_id": "69705a68a8be625b19c2af3a",
                    "user": {
                        "_id": "6745c589d2d740914ec2574f",
                        "avatarUrl": "/avatars/7b2ff6848d42cd140a775df0c2bc9384.svg",
                        "isPro": false,
                        "fullname": "Xiaofang Yang",
                        "user": "fffovo",
                        "type": "user"
                    },
                    "name": "Xiaofang Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:48:39.583Z",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af3b",
                    "name": "Lijun Li",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af3c",
                    "user": {
                        "_id": "660d17d6c9be0dcd31a30b3d",
                        "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg",
                        "isPro": false,
                        "fullname": "Zhou Heng",
                        "user": "henggg",
                        "type": "user"
                    },
                    "name": "Heng Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-21T09:19:49.600Z",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af3d",
                    "name": "Tong Zhu",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af3e",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af3f",
                    "name": "Yuchen Fan",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af40",
                    "user": {
                        "_id": "6952244bfbddb08cb2562f3b",
                        "avatarUrl": "/avatars/70d67319af29604129378fee3f216757.svg",
                        "isPro": false,
                        "fullname": "qianshan wei",
                        "user": "b1intern",
                        "type": "user"
                    },
                    "name": "Qianshan Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:49:04.253Z",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af41",
                    "name": "Rui Ye",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af42",
                    "name": "Li Kang",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af43",
                    "name": "Yiran Qin",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af44",
                    "name": "Zhiqiang Kou",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af45",
                    "name": "Daizong Liu",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af46",
                    "name": "Qi Li",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af47",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af48",
                    "user": {
                        "_id": "65257545b017be1fc1915364",
                        "avatarUrl": "/avatars/9bffd3fb567d2fa1e5c3546d77560b43.svg",
                        "isPro": false,
                        "fullname": "Siheng Chen",
                        "user": "sihengchen",
                        "type": "user"
                    },
                    "name": "Siheng Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:49:26.261Z",
                    "hidden": false
                },
                {
                    "_id": "69705a68a8be625b19c2af49",
                    "name": "Jing Shao",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-20T17:51:56.000Z",
            "submittedOnDailyAt": "2026-01-21T02:28:39.429Z",
            "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
            "submittedOnDailyBy": {
                "_id": "641d3efac3983aa9491677b9",
                "avatarUrl": "/avatars/53565486351c16a1ac8ea863963e2d9b.svg",
                "isPro": false,
                "fullname": "Lijun Li",
                "user": "adwardlee",
                "type": "user"
            },
            "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.",
            "upvotes": 28,
            "discussionId": "69705a69a8be625b19c2af4a",
            "projectPage": "https://efficient-agents.github.io/",
            "githubRepo": "https://github.com/yxf203/Awesome-Efficient-Agents",
            "githubRepoAddedBy": "user",
            "ai_summary": "Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.",
            "ai_keywords": [
                "large language models",
                "agentic systems",
                "memory",
                "tool learning",
                "planning",
                "latency",
                "tokens",
                "steps",
                "reinforcement learning",
                "controlled search mechanisms",
                "Pareto frontier",
                "efficiency metrics",
                "evaluation protocols"
            ],
            "githubStars": 23,
            "organization": {
                "_id": "6747ee5decec679eafb90450",
                "name": "ShanghaiAiLab",
                "fullname": "shanghai ailab "
            }
        },
        "translation_title": "효율적인 에이전트를 향하여: 메모리, 도구 학습 및 계획",
        "purpose": "에이전트 시스템의 효율성을 조사하고 개선하기 위한 연구",
        "method": [
            "메모리, 도구 학습 및 계획이라는 세 가지 핵심 요소에서 에이전트의 효율성을 분석함(Therefore, this paper investigates efficiency from three core components of agents: memory, tool learning, and planning.)",
            "구현 방법은 다르지만 공유하는 높은 수준의 원칙을 강조하는 다양한 최신 접근 방식을 검토함(Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles.)",
            "효율성을 비교하는 두 가지 보완적 방법으로 성과와 비용을 측면으로 분석함(Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness.)"
        ],
        "conclusion": "효율성에 대한 통찰을 제공하고, 에이전트 시스템의 실제 적용에서의 성과와 비용 간의 균형을 이해하는 데 기여함.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Robotics"
        ]
    },
    {
        "paper": {
            "id": "2601.13836",
            "authors": [
                {
                    "_id": "697067f2a8be625b19c2afa5",
                    "name": "Qian Chen",
                    "hidden": false
                },
                {
                    "_id": "697067f2a8be625b19c2afa6",
                    "user": {
                        "_id": "618497ea8aaadc9253c2dfa9",
                        "avatarUrl": "/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg",
                        "isPro": false,
                        "fullname": "Fu Jinlan",
                        "user": "Jinlan",
                        "type": "user"
                    },
                    "name": "Jinlan Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:39:51.571Z",
                    "hidden": false
                },
                {
                    "_id": "697067f2a8be625b19c2afa7",
                    "name": "Changsong Li",
                    "hidden": false
                },
                {
                    "_id": "697067f2a8be625b19c2afa8",
                    "name": "See-Kiong Ng",
                    "hidden": false
                },
                {
                    "_id": "697067f2a8be625b19c2afa9",
                    "user": {
                        "_id": "61457b8deff2c9fdb4de4988",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg",
                        "isPro": false,
                        "fullname": "Xipeng Qiu",
                        "user": "xpqiu",
                        "type": "user"
                    },
                    "name": "Xipeng Qiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:48:17.801Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-20T10:47:20.000Z",
            "submittedOnDailyAt": "2026-01-21T04:50:03.124Z",
            "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs",
            "submittedOnDailyBy": {
                "_id": "618497ea8aaadc9253c2dfa9",
                "avatarUrl": "/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg",
                "isPro": false,
                "fullname": "Fu Jinlan",
                "user": "Jinlan",
                "type": "user"
            },
            "summary": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).",
            "upvotes": 27,
            "discussionId": "697067f2a8be625b19c2afaa",
            "projectPage": "https://openmoss.github.io/FutureOmni",
            "githubRepo": "https://github.com/OpenMOSS/FutureOmni",
            "githubRepoAddedBy": "user",
            "ai_summary": "FutureOmni presents the first benchmark for evaluating multimodal models' ability to forecast future events from audio-visual data, revealing current limitations and proposing an improved training strategy for better performance.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "audio-visual cues",
                "future forecasting",
                "cross-modal causal reasoning",
                "temporal reasoning",
                "internal knowledge",
                "LLM-assisted pipeline",
                "instruction-tuning dataset",
                "Omni-Modal Future Forecasting training strategy"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "613b0dee83ec35d460684607",
                "name": "OpenMOSS-Team",
                "fullname": "OpenMOSS",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
            }
        },
        "translation_title": "FutureOmni: 다중 모달 LLM을 위한 Omni-Modal 미래 예측 평가",
        "purpose": "다중 모달 LLM의 음성-비주얼 신호로부터 미래 사건을 예측하는 능력을 평가하기 위한 기준 마련",
        "method": [
            "FutureOmni라는 첫 번째 벤치마크를 소개하여 음성-비주얼 환경에서의 미래 예측을 평가함(To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments.)",
            "919개의 비디오와 1034개의 다중 선택 QA 쌍을 포함하는 FutureOmni를 LLM 보조, 인간 참여 방식으로 구성함(FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains.)",
            "미래 예측 훈련을 향상시키기 위해 7K 샘플의 instruction-tuning 데이터셋을 큐레이션하고 Omni-Modal Future Forecasting (OFF) 교육 전략을 제안함(To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy.)"
        ],
        "conclusion": "OFF 방법은 미래 예측과 일반화를 향상시키는 것으로 평가됨.",
        "keywords": [
            "Multimodal Learning",
            "Large Language Models",
            "Video Understanding"
        ]
    },
    {
        "paper": {
            "id": "2601.14250",
            "authors": [
                {
                    "_id": "69705b78a8be625b19c2af4c",
                    "user": {
                        "_id": "6697765937d24838267b41e7",
                        "avatarUrl": "/avatars/682bb4cf0a0009812b42748dc26916f9.svg",
                        "isPro": false,
                        "fullname": "PangzeCheung",
                        "user": "PangzeCheung",
                        "type": "user"
                    },
                    "name": "Pengze Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-21T09:19:32.663Z",
                    "hidden": false
                },
                {
                    "_id": "69705b78a8be625b19c2af4d",
                    "name": "Yanze Wu",
                    "hidden": false
                },
                {
                    "_id": "69705b78a8be625b19c2af4e",
                    "user": {
                        "_id": "6805bdfb344d6d8a8fd5b07a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dGLeWvN2CXLmxVP_b9S4R.png",
                        "isPro": false,
                        "fullname": "Mengtian Li",
                        "user": "LemonSky1995",
                        "type": "user"
                    },
                    "name": "Mengtian Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:52:30.670Z",
                    "hidden": false
                },
                {
                    "_id": "69705b78a8be625b19c2af4f",
                    "name": "Xu Bai",
                    "hidden": false
                },
                {
                    "_id": "69705b78a8be625b19c2af50",
                    "name": "Songtao Zhao",
                    "hidden": false
                },
                {
                    "_id": "69705b78a8be625b19c2af51",
                    "user": {
                        "_id": "6339029a76421c0543167075",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png",
                        "isPro": false,
                        "fullname": "fulong ye",
                        "user": "Alon77777",
                        "type": "user"
                    },
                    "name": "Fulong Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T11:46:59.343Z",
                    "hidden": false
                },
                {
                    "_id": "69705b78a8be625b19c2af52",
                    "name": "Chong Mou",
                    "hidden": false
                },
                {
                    "_id": "69705b78a8be625b19c2af53",
                    "user": {
                        "_id": "6752cd83ffaeeb979db974ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
                        "isPro": false,
                        "fullname": "Xinghui Li",
                        "user": "Crayon-Shinchan",
                        "type": "user"
                    },
                    "name": "Xinghui Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:52:58.973Z",
                    "hidden": false
                },
                {
                    "_id": "69705b78a8be625b19c2af54",
                    "user": {
                        "_id": "6304e2dabad6ce7fc0287d57",
                        "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
                        "isPro": false,
                        "fullname": "Zhuowei_Chen",
                        "user": "ZhuoweiChen",
                        "type": "user"
                    },
                    "name": "Zhuowei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:52:20.177Z",
                    "hidden": false
                },
                {
                    "_id": "69705b78a8be625b19c2af55",
                    "name": "Qian He",
                    "hidden": false
                },
                {
                    "_id": "69705b78a8be625b19c2af56",
                    "user": {
                        "_id": "671aa30b496f0bc5ae04da4b",
                        "avatarUrl": "/avatars/902d7f9fd56f84953d67d9229bd9d6b7.svg",
                        "isPro": false,
                        "fullname": "Mingyuan Gao",
                        "user": "GMY1999",
                        "type": "user"
                    },
                    "name": "Mingyuan Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-21T10:52:13.464Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6697765937d24838267b41e7/309vLCCbUoiHGJkN0u3LH.mp4"
            ],
            "publishedAt": "2026-01-20T18:58:11.000Z",
            "submittedOnDailyAt": "2026-01-21T02:29:32.365Z",
            "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
            "submittedOnDailyBy": {
                "_id": "6697765937d24838267b41e7",
                "avatarUrl": "/avatars/682bb4cf0a0009812b42748dc26916f9.svg",
                "isPro": false,
                "fullname": "PangzeCheung",
                "user": "PangzeCheung",
                "type": "user"
            },
            "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.",
            "upvotes": 25,
            "discussionId": "69705b78a8be625b19c2af57",
            "projectPage": "https://pangzecheung.github.io/OmniTransfer/",
            "githubRepo": "https://github.com/PangzeCheung/OmniTransfer",
            "githubRepoAddedBy": "user",
            "ai_summary": "OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.",
            "ai_keywords": [
                "video customization",
                "spatio-temporal video transfer",
                "multi-view information",
                "temporal cues",
                "temporal alignment",
                "appearance consistency",
                "reference-decoupled causal learning",
                "task-adaptive multimodal alignment",
                "pose-guided methods"
            ],
            "githubStars": 36,
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "translation_title": "OmniTransfer: 시공간 비디오 전달을 위한 올인원 프레임워크",
        "purpose": "비디오 생성의 유연성과 일반화를 향상시키기 위한 시공간 비디오 전달의 새로운 프레임워크 개발",
        "method": [
            "다양한 프레임의 다중 관점 정보를 활용하여 모습 일관성을 향상시키고, 시간적 단서를 통해 세밀한 시간 제어를 가능하게 함(To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer.)",
            "Task-aware Positional Bias를 통해 참조 비디오 정보를 적응적으로 활용하여 시간 정렬이나 외형 일관성을 개선함(OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency.)",
            "Reference-decoupled Causal Learning으로 참조와 목표 지점을 분리하여 정확한 참조 전달을 가능하게 하고 효율성을 개선함; Task-adaptive Multimodal Alignment를 통해 다양한 작업을 동적으로 구별하고 해결함(Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks.)"
        ],
        "conclusion": "OmniTransfer는 외형과 시간 전달에서 기존 방법보다 우수한 성능을 보이며, 새로운 비디오 생성 패러다임을 설정함.",
        "keywords": [
            "Video Generation",
            "Multimodal Learning",
            "Image Understanding"
        ]
    }
]