[
    {
        "paper": {
            "id": "2504.20734",
            "authors": [
                {
                    "_id": "6811966ae20ba7d0683b8adc",
                    "user": {
                        "_id": "66d30f5fad293ffc4b7672bc",
                        "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
                        "isPro": false,
                        "fullname": "Woongyeong Yeo",
                        "user": "wgcyeo",
                        "type": "user"
                    },
                    "name": "Woongyeong Yeo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T07:56:03.853Z",
                    "hidden": false
                },
                {
                    "_id": "6811966ae20ba7d0683b8add",
                    "user": {
                        "_id": "66ed7737f2f27a5dfd81ef09",
                        "avatarUrl": "/avatars/f45eea356e92ac7b3db23c2c92dec9fa.svg",
                        "isPro": false,
                        "fullname": "Kangsan Kim",
                        "user": "KangsanKim71",
                        "type": "user"
                    },
                    "name": "Kangsan Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T07:56:00.948Z",
                    "hidden": false
                },
                {
                    "_id": "6811966ae20ba7d0683b8ade",
                    "name": "Soyeong Jeong",
                    "hidden": false
                },
                {
                    "_id": "6811966ae20ba7d0683b8adf",
                    "user": {
                        "_id": "63036b6c5c70c21d0ea79d48",
                        "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
                        "isPro": false,
                        "fullname": "Jinheon Baek",
                        "user": "jinheon",
                        "type": "user"
                    },
                    "name": "Jinheon Baek",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T07:56:07.351Z",
                    "hidden": false
                },
                {
                    "_id": "6811966ae20ba7d0683b8ae0",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T13:18:58.000Z",
            "submittedOnDailyAt": "2025-04-30T01:50:28.624Z",
            "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
            "submittedOnDailyBy": {
                "_id": "66d30f5fad293ffc4b7672bc",
                "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
                "isPro": false,
                "fullname": "Woongyeong Yeo",
                "user": "wgcyeo",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines.",
            "upvotes": 41,
            "discussionId": "6811966ae20ba7d0683b8b0e",
            "projectPage": "https://universalrag.github.io",
            "githubRepo": "https://github.com/wgcyeo/UniversalRAG",
            "ai_keywords": [
                "Retrieval-Augmented Generation (RAG)",
                "factual accuracy",
                "external knowledge",
                "text-only corpus",
                "modality-specific corpus",
                "heterogenous sources",
                "diverse modalities",
                "granularities",
                "modality gap",
                "modality-aware routing mechanism",
                "targeted retrieval",
                "granularity levels",
                "fine-tuned retrieval",
                "multi-modal benchmarks",
                "modality-specific baselines",
                "unified baselines"
            ]
        },
        "translation_title": "UniversalRAG: 다양한 모드와 세부 수준에서 다중 코퍼스에 대한 검색 증강 생성",
        "purpose": "다양한 모드와 세부 수준에서 지식을 검색하고 통합하기 위한 RAG 프레임워크 개발",
        "method": [
            "모든 모드를 단일 결합 코퍼스에서 유도된 통일된 표현 공간에 강제하였을 때 발생하는 모드 간의 갭을 해결하기 위한 모드 인식 라우팅 메커니즘을 제안함(To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities.)",
            "질문의 복잡성과 범위에 맞춘 세밀한 검색을 가능하게 하기 위해 각 모드를 여러 세부 수준으로 조직함(we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query.)",
            "8개의 벤치마크에서 UniversalRAG의 우수성을 검증함(We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.)"
        ],
        "conclusion": "UniversalRAG는 여러 모드의 다양한 지식을 효과적으로 검색하고 통합하여 기존 모드 특정 접근법보다 뛰어난 성능을 보여줌.",
        "keywords": [
            "Natural Language Processing",
            "Multimodal Learning",
            "Image Generation"
        ]
    },
    {
        "paper": {
            "id": "2504.20571",
            "authors": [
                {
                    "_id": "681187ddda5ce4cbd7556714",
                    "user": {
                        "_id": "653586fae778506c5b38a3f1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653586fae778506c5b38a3f1/GL_RShZhAkEZmIinA5_8E.jpeg",
                        "isPro": false,
                        "fullname": "Yiping Wang",
                        "user": "ypwang61",
                        "type": "user"
                    },
                    "name": "Yiping Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T09:58:59.486Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556715",
                    "user": {
                        "_id": "673a83b99e6f1c0d81a771fc",
                        "avatarUrl": "/avatars/f3d8e1bf7d4c36b21adee632ea12ffe0.svg",
                        "isPro": false,
                        "fullname": "Qing Yang",
                        "user": "hushqyang",
                        "type": "user"
                    },
                    "name": "Qing Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T07:56:17.953Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556716",
                    "user": {
                        "_id": "64a85e23b6512b8328f9d9e2",
                        "avatarUrl": "/avatars/4a6b35752d3f76cb03278f52b3b43426.svg",
                        "isPro": false,
                        "fullname": "Zhiyuan Zeng",
                        "user": "ZhiyuanZeng",
                        "type": "user"
                    },
                    "name": "Zhiyuan Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T09:59:12.620Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556717",
                    "user": {
                        "_id": "63815eff4761ddfa00903762",
                        "avatarUrl": "/avatars/3419b239d42e091586f1c51b526d88e5.svg",
                        "isPro": false,
                        "fullname": "Liliang Ren",
                        "user": "renll",
                        "type": "user"
                    },
                    "name": "Liliang Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T09:59:18.310Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556718",
                    "name": "Lucas Liu",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556719",
                    "user": {
                        "_id": "61942296d5c2ba6daa290357",
                        "avatarUrl": "/avatars/594021cc183c4922d48b46f43772a062.svg",
                        "isPro": false,
                        "fullname": "Baolin Peng",
                        "user": "Baolin",
                        "type": "user"
                    },
                    "name": "Baolin Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T09:59:45.735Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd755671a",
                    "name": "Hao Cheng",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd755671b",
                    "user": {
                        "_id": "6310493158d83e8f64dc8c55",
                        "avatarUrl": "/avatars/5f91ac4dfec0d6a5bf7bad6094f0fd0f.svg",
                        "isPro": false,
                        "fullname": "Xuehai He",
                        "user": "Xuehai",
                        "type": "user"
                    },
                    "name": "Xuehai He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T09:59:52.344Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd755671c",
                    "user": {
                        "_id": "633523b131a2be3938ca1016",
                        "avatarUrl": "/avatars/06a18f80927289bb949d9f19ffdc4bda.svg",
                        "isPro": false,
                        "fullname": "Kuan Wang",
                        "user": "Keynes",
                        "type": "user"
                    },
                    "name": "Kuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T09:59:58.392Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd755671d",
                    "user": {
                        "_id": "641904caf9d6f1d772ec7af7",
                        "avatarUrl": "/avatars/4a63eac71eb30f70b1a0e9d4708f26c1.svg",
                        "isPro": false,
                        "fullname": "Jianfeng Gao",
                        "user": "wyngjf",
                        "type": "user"
                    },
                    "name": "Jianfeng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:00:04.685Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd755671e",
                    "user": {
                        "_id": "64da876370446182be5b608d",
                        "avatarUrl": "/avatars/e412fdc71404ecdf638e416846e3ebfb.svg",
                        "isPro": false,
                        "fullname": "Weizhu Chen",
                        "user": "chenweizhu",
                        "type": "user"
                    },
                    "name": "Weizhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:00:10.823Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd755671f",
                    "user": {
                        "_id": "6463b2247572c66a8e625a57",
                        "avatarUrl": "/avatars/7722fb5649d42d966ce1e478946d5f8f.svg",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "Shuohang",
                        "type": "user"
                    },
                    "name": "Shuohang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:00:19.855Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556720",
                    "name": "Simon Shaolei Du",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556721",
                    "user": {
                        "_id": "6454c337a13edf669cd5d8ea",
                        "avatarUrl": "/avatars/a383a0dda7c2ef6a0d6c3c64651f42ff.svg",
                        "isPro": false,
                        "fullname": "Yelong Shen",
                        "user": "uuu6",
                        "type": "user"
                    },
                    "name": "Yelong Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:00:33.179Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T09:24:30.000Z",
            "submittedOnDailyAt": "2025-04-30T00:46:23.617Z",
            "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR",
            "upvotes": 36,
            "discussionId": "681187ddda5ce4cbd7556754",
            "githubRepo": "https://github.com/ypwang61/One-Shot-RLVR",
            "ai_keywords": [
                "reinforcement learning with verifiable reward (RLVR)",
                "1-shot RLVR",
                "large language models (LLMs)",
                "Qwen2.5-Math-1.5B",
                "MATH500",
                "mathematical reasoning benchmarks",
                "Qwen2.5-Math-7B",
                "Llama3.2-3B-Instruct",
                "DeepSeek-R1-Distill-Qwen-1.5B",
                "GRPO",
                "PPO",
                "cross-domain generalization",
                "self-reflection",
                "post-saturation generalization",
                "policy gradient loss",
                "entropic exploration",
                "entropy loss"
            ]
        },
        "translation_title": "하나의 훈련 예제를 이용한 대규모 언어 모델을 위한 강화 학습",
        "purpose": "1-shot RLVR을 사용해 대규모 언어 모델의 수학적 추론 능력을 향상시키기 위한 연구",
        "method": [
            "1-shot RLVR을 적용하여 Qwen2.5-Math-1.5B 모델 성능 개선을 유도함(We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs).)",
            "하나의 예제를 통해 MATH500에서 모델 성능을 36.0%에서 73.6%로, 6개의 일반적인 수학 추론 벤치마크에서 평균 성능을 17.6%에서 35.7%로 향상시킴(Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%).",
            "다양한 모델과 RL 알고리즘에서 비슷한 성능 개선을 관찰함(Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples).",
            "탐색을 촉진하기 위한_entropy loss_ 추가가 1-shot RLVR 훈련에서 중요한 역할을 함(we also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training.)"
        ],
        "conclusion": "1-shot RLVR는 대규모 언어 모델의 수학적 추론 능력을 크게 향상시키며, 탐색을 촉진하는 방법이 성능 향상에 효과적임을 확인함.",
        "keywords": [
            "Natural Language Processing",
            "Reinforcement Learning",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2504.20595",
            "authors": [
                {
                    "_id": "68118a9f4570c2ba44bf4418",
                    "user": {
                        "_id": "6334a0bd31a2be3938c59537",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6334a0bd31a2be3938c59537/kSetFUWAmJbPQ1KSlNKBr.jpeg",
                        "isPro": false,
                        "fullname": "Rulin Shao",
                        "user": "rulins",
                        "type": "user"
                    },
                    "name": "Rulin Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:02:20.688Z",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf4419",
                    "user": {
                        "_id": "64ff618c35ec9717626d1431",
                        "avatarUrl": "/avatars/941befd75925d6b691133f84cce525f9.svg",
                        "isPro": false,
                        "fullname": "Rui Qiao",
                        "user": "volpato30",
                        "type": "user"
                    },
                    "name": "Rui Qiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:02:05.204Z",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf441a",
                    "name": "Varsha Kishore",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf441b",
                    "user": {
                        "_id": "5f1eb362eec0ad2a071ad6e2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
                        "isPro": false,
                        "fullname": "Niklas Muennighoff",
                        "user": "Muennighoff",
                        "type": "user"
                    },
                    "name": "Niklas Muennighoff",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:02:27.811Z",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf441c",
                    "name": "Xi Victoria Lin",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf441d",
                    "name": "Daniela Rus",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf441e",
                    "name": "Bryan Kian Hsiang Low",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf441f",
                    "user": {
                        "_id": "63a76d0de27a6dbd485fe863",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg",
                        "isPro": false,
                        "fullname": "Sewon Min",
                        "user": "sewon",
                        "type": "user"
                    },
                    "name": "Sewon Min",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:01:46.233Z",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf4420",
                    "name": "Wen-tau Yih",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf4421",
                    "user": {
                        "_id": "641b4263abfce26bcf7b27de",
                        "avatarUrl": "/avatars/e91b4205e4f74b0dd8c333c23203a924.svg",
                        "isPro": false,
                        "fullname": "Pang Wei Koh",
                        "user": "pangwei",
                        "type": "user"
                    },
                    "name": "Pang Wei Koh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:01:57.373Z",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf4422",
                    "name": "Luke Zettlemoyer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T09:49:28.000Z",
            "submittedOnDailyAt": "2025-04-30T00:58:16.950Z",
            "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model.",
            "upvotes": 29,
            "discussionId": "68118aa44570c2ba44bf457b",
            "ai_keywords": [
                "retriever",
                "ReasonIR-8B",
                "general reasoning tasks",
                "synthetic data generation pipeline",
                "hard negative",
                "nDCG@10",
                "BRIGHT",
                "information retrieval (IR) benchmark",
                "RAG tasks",
                "MMLU",
                "GPQA",
                "closed-book baseline",
                "LLM reranker",
                "test-time compute",
                "rewritten queries",
                "LLM"
            ]
        },
        "translation_title": "ReasonIR: 추론 작업을 위한 리트리버 훈련",
        "purpose": "일반적인 추론 작업을 잘 수행할 수 있는 리트리버 개발",
        "method": [
            "기존 리트리버는 짧은 사실 쿼리만 다뤄 한계가 있었으며, 이를 해결하기 위해 합성 데이터 생성 파이프라인을 개발함(Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them.)",
            "각 문서에 대해 도전적이고 관련 있는 쿼리 및 유용하지 않은 하드 네거티브를 생성함(our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative.)",
            "합성 데이터와 기존 공개 데이터를 혼합하여 ReasonIR-8B를 훈련함(By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art.)"
        ],
        "conclusion": "ReasonIR-8B는 BRIGHT 벤치마크에서 새로운 최첨단 성능을 달성하였으며, RAG 작업에서도 성능 향상을 보여 다른 리트리버보다 우수함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Information Retrieval"
        ]
    },
    {
        "paper": {
            "id": "2504.20157",
            "authors": [
                {
                    "_id": "68119750ff0764f3840a7f93",
                    "user": {
                        "_id": "61e0c5053a1781f66b4e9aed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642120523097-61e0c5053a1781f66b4e9aed.jpeg",
                        "isPro": false,
                        "fullname": "Zae Myung Kim",
                        "user": "zaemyung",
                        "type": "user"
                    },
                    "name": "Zae Myung Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T07:55:58.262Z",
                    "hidden": false
                },
                {
                    "_id": "68119750ff0764f3840a7f94",
                    "name": "Chanwoo Park",
                    "hidden": false
                },
                {
                    "_id": "68119750ff0764f3840a7f95",
                    "user": {
                        "_id": "60985a0547dc3dbf8a976607",
                        "avatarUrl": "/avatars/3c37bf4b7c9db83a46af7c473ee4eb86.svg",
                        "isPro": false,
                        "fullname": "Vipul Raheja",
                        "user": "machineteacher",
                        "type": "user"
                    },
                    "name": "Vipul Raheja",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:03:14.631Z",
                    "hidden": false
                },
                {
                    "_id": "68119750ff0764f3840a7f96",
                    "user": {
                        "_id": "64356b40a4bd75c62cbc5926",
                        "avatarUrl": "/avatars/5f4c603464e9c8ad613a3a25fa4cacbf.svg",
                        "isPro": false,
                        "fullname": "Dongyeop Kang",
                        "user": "dykang",
                        "type": "user"
                    },
                    "name": "Dongyeop Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:03:26.612Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/tHS8gWUK0ptmNTs6lZck6.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/uMD9av8pogPwYTW-KNFJ2.png"
            ],
            "publishedAt": "2025-04-28T18:02:35.000Z",
            "submittedOnDailyAt": "2025-04-30T02:04:16.540Z",
            "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving\n  Reward Models",
            "submittedOnDailyBy": {
                "_id": "6434b6619bd5a84b5dcfa4de",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
                "isPro": true,
                "fullname": "Young-Jun Lee",
                "user": "passing2961",
                "type": "user"
            },
            "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared.",
            "upvotes": 23,
            "discussionId": "68119751ff0764f3840a7fc5",
            "ai_keywords": [
                "Meta Policy Optimization (MPO)",
                "meta-reward model",
                "reward hacking",
                "prompt engineering",
                "policy optimization",
                "adaptive reward signal",
                "meta-learning approach",
                "prompt design",
                "reward-based RL alignment",
                "question answering",
                "mathematical reasoning"
            ]
        },
        "translation_title": "평가적 사고를 향하여: 발전하는 보상 모델을 통한 메타 정책 최적화",
        "purpose": "Large Language Models(LLMs)의 보상 기반 정렬 방법의 한계를 극복하기 위한 새로운 프레임워크 제안",
        "method": [
            "Meta Policy Optimization(MPO) 프레임워크를 도입하여 보상 모델의 프롬프트를 훈련 중에 동적으로 수정함으로써 보상 신호의 결함 활용을 방지함(We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training.)",
            "메타 보상 모델이 훈련 상황을 모니터링하고 지속적으로 보상 모델의 프롬프트를 조정하여 높은 일치를 유지하도록 함(In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment.)",
            "수작업 보상 프롬프트 설계 필요성을 크게 줄이며 안정적인 정책 최적화를 촉진함(This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design.)"
        ],
        "conclusion": "MPO는 다양한 작업에서 효과를 유지하며, 더 강력하고 적응 가능한 정렬 전략을 위한 길을 열어줌.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2504.20879",
            "authors": [
                {
                    "_id": "6811ae6b7f4f553788e905b8",
                    "user": {
                        "_id": "62c2175d756039dd0dd20509",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677753847837-62c2175d756039dd0dd20509.jpeg",
                        "isPro": false,
                        "fullname": "Shivalika Singh",
                        "user": "shivalikasingh",
                        "type": "user"
                    },
                    "name": "Shivalika Singh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:19:56.166Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905b9",
                    "user": {
                        "_id": "66ad9237091a147c4f154dbb",
                        "avatarUrl": "/avatars/c80258e877bafc08452f241a6ecfec04.svg",
                        "isPro": false,
                        "fullname": "Yiyang Nan",
                        "user": "olivernan",
                        "type": "user"
                    },
                    "name": "Yiyang Nan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:20:27.669Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905ba",
                    "user": {
                        "_id": "62fdf36a594a7b92e671e3b5",
                        "avatarUrl": "/avatars/d3e0b109f4fc7da45873d599659ae5b5.svg",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "AlexWang",
                        "type": "user"
                    },
                    "name": "Alex Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:20:38.051Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905bb",
                    "user": {
                        "_id": "6658011eaba105a066e37e1b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6658011eaba105a066e37e1b/VPwyTv1bnVMQbVMoMQzcf.jpeg",
                        "isPro": false,
                        "fullname": "Daniel D'souza",
                        "user": "dsouzadaniel",
                        "type": "user"
                    },
                    "name": "Daniel D'Souza",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:20:45.388Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905bc",
                    "user": {
                        "_id": "630906fde3246e2be88bfab3",
                        "avatarUrl": "/avatars/bca882c027d17f7feba837baae71ec2d.svg",
                        "isPro": false,
                        "fullname": "Sayash Kapoor",
                        "user": "sayashk",
                        "type": "user"
                    },
                    "name": "Sayash Kapoor",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:20:51.466Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905bd",
                    "user": {
                        "_id": "60d35d7ad7b174177faabd5b",
                        "avatarUrl": "/avatars/4e5403b9d4a845a2d21e8217dc3c16d2.svg",
                        "isPro": false,
                        "fullname": "Ahmet Üstün",
                        "user": "ahmetu",
                        "type": "user"
                    },
                    "name": "Ahmet Üstün",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:20:58.423Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905be",
                    "user": {
                        "_id": "64931e7e2da595588288f161",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64931e7e2da595588288f161/4jOhJOFsU7RVFMgGk5kO7.jpeg",
                        "isPro": false,
                        "fullname": "Sanmi Koyejo",
                        "user": "sanmikoyejo",
                        "type": "user"
                    },
                    "name": "Sanmi Koyejo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:21:04.000Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905bf",
                    "user": {
                        "_id": "63081e15a670ed10f9d44229",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
                        "isPro": true,
                        "fullname": "Yuntian Deng",
                        "user": "yuntian-deng",
                        "type": "user"
                    },
                    "name": "Yuntian Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T09:56:42.033Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905c0",
                    "user": {
                        "_id": "61f4283a81c4d30f58140242",
                        "avatarUrl": "/avatars/a1cf1ef1fd442c36ed65c68e51919fed.svg",
                        "isPro": false,
                        "fullname": "Shayne Longpre",
                        "user": "Shayne",
                        "type": "user"
                    },
                    "name": "Shayne Longpre",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:21:11.011Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905c1",
                    "name": "Noah Smith",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905c2",
                    "user": {
                        "_id": "6634e8c750ea73bfbc59223e",
                        "avatarUrl": "/avatars/9f0a7bcdfd82d5344932437d458fce9f.svg",
                        "isPro": false,
                        "fullname": "Beyza Ermis",
                        "user": "beyzaermis",
                        "type": "user"
                    },
                    "name": "Beyza Ermis",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:21:23.787Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905c3",
                    "user": {
                        "_id": "6441042d5d600fb0951a5f99",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6441042d5d600fb0951a5f99/4CbOaYcEz99BtVAQvnGTn.jpeg",
                        "isPro": false,
                        "fullname": "Marzieh Fadaee",
                        "user": "MarziehFadaee",
                        "type": "user"
                    },
                    "name": "Marzieh Fadaee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:21:31.242Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905c4",
                    "user": {
                        "_id": "63434eb76f59b79da07dbddf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63434eb76f59b79da07dbddf/BEwmVjqPNYlqmutXG0G6e.jpeg",
                        "isPro": false,
                        "fullname": "Sara Hooker",
                        "user": "sarahooker",
                        "type": "user"
                    },
                    "name": "Sara Hooker",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:21:37.932Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T15:48:49.000Z",
            "submittedOnDailyAt": "2025-04-30T03:36:53.331Z",
            "title": "The Leaderboard Illusion",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field",
            "upvotes": 19,
            "discussionId": "6811ae6c7f4f553788e905fc"
        },
        "translation_title": "리더보드의 환상",
        "purpose": "AI 시스템의 성능 평가에 있어 공정한 기준을 마련하고 투명한 벤치마킹을 촉진하기 위한 논의",
        "method": [
            "Chatbot Arena의 기준을 분석하여 문제가 있는 평가 방식을 식별함(we identify systematic issues that have resulted in a distorted playing field.)",
            "비공식적인 테스트 관행이 일부 제공자에게 유리하게 작용하는 방식을 규명함(We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release.)",
            "Chatbot Arena 데이터 접근성이 성능 향상에 미치는 영향을 입증함(We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution.)"
        ],
        "conclusion": "공정하고 투명한 기준 마련을 위한 Chatbot Arena의 평가 방식 개편을 위한 실행 가능한 권장 사항을 제시함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Benchmarks"
        ]
    }
]