[
    {
        "paper": {
            "id": "2505.22617",
            "authors": [
                {
                    "_id": "6837cd8fc537d91527323667",
                    "user": {
                        "_id": "650eba9555dc1e841746f132",
                        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
                        "isPro": false,
                        "fullname": "Ganqu Cui",
                        "user": "ganqu",
                        "type": "user"
                    },
                    "name": "Ganqu Cui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:09.467Z",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323668",
                    "user": {
                        "_id": "66e3f8fb5d97b5bb46923444",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DW806I00-00oQAYvD4ocQ.png",
                        "isPro": false,
                        "fullname": "Yuchen Zhang",
                        "user": "YucZhang2003",
                        "type": "user"
                    },
                    "name": "Yuchen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:14.207Z",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323669",
                    "user": {
                        "_id": "65352acb7139c5dd8d9a8590",
                        "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
                        "isPro": false,
                        "fullname": "JiachengChen",
                        "user": "JC-Chen",
                        "type": "user"
                    },
                    "name": "Jiacheng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T09:40:20.736Z",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d9152732366a",
                    "name": "Lifan Yuan",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d9152732366b",
                    "name": "Zhi Wang",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d9152732366c",
                    "user": {
                        "_id": "622474f38dc6b0b64f5e903d",
                        "avatarUrl": "/avatars/d6b60a014277a8ec7d564163c5f644aa.svg",
                        "isPro": false,
                        "fullname": "Yuxin Zuo",
                        "user": "yuxinzuo",
                        "type": "user"
                    },
                    "name": "Yuxin Zuo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:12.022Z",
                    "hidden": true
                },
                {
                    "_id": "6837cd8fc537d9152732366d",
                    "user": {
                        "_id": "662f638ba9891e43cc4c5125",
                        "avatarUrl": "/avatars/77c22de5511f9b85d98ec75fb0b5e9be.svg",
                        "isPro": false,
                        "fullname": "Li Haozhan",
                        "user": "Haozhan72",
                        "type": "user"
                    },
                    "name": "Haozhan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T09:40:22.720Z",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d9152732366e",
                    "name": "Yuchen Fan",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d9152732366f",
                    "name": "Huayu Chen",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323670",
                    "name": "Weize Chen",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323671",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323672",
                    "name": "Hao Peng",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323673",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323674",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323675",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323676",
                    "name": "Bowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323677",
                    "user": {
                        "_id": "60cf4bcb1ce3775ebb86e5d5",
                        "avatarUrl": "/avatars/12bcd18d215abf91f297f93007733148.svg",
                        "isPro": false,
                        "fullname": "Ning Ding",
                        "user": "stingning",
                        "type": "user"
                    },
                    "name": "Ning Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:16.809Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T17:38:45.000Z",
            "submittedOnDailyAt": "2025-05-29T01:38:57.501Z",
            "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "650eba9555dc1e841746f132",
                "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
                "isPro": false,
                "fullname": "Ganqu Cui",
                "user": "ganqu",
                "type": "user"
            },
            "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.",
            "upvotes": 77,
            "discussionId": "6837cd90c537d9152732369d",
            "githubRepo": "https://github.com/PRIME-RL/Entropy-Mechanism-of-RL",
            "ai_summary": "Entropy dynamics in reinforcement learning with large language models are investigated to prevent policy entropy collapse and improve exploration.",
            "ai_keywords": [
                "policy entropy",
                "reinforcement learning",
                "LLMs",
                "entropy intervention",
                "transformation equation",
                "policy performance",
                "entropy dynamics",
                "covariance",
                "action probability",
                "logits",
                "advantage",
                "Policy Gradient",
                "Clip-Cov",
                "KL-Cov"
            ]
        },
        "translation_title": "추론 언어 모델을 위한 강화 학습의 엔트로피 메커니즘",
        "purpose": "LLM으로 추론을 위한 강화 학습을 확장하기 위한 정책 엔트로피 문제 해결",
        "method": [
            "정책 엔트로피와 최종 성능 간의 변환 방정식 R=-a*e^H+b 설정함(This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion.)",
            "정책 엔트로피의 변화를 동작 확률과 로짓 변화의 공분산에 의해 설명함(Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits.)",
            "고공분산 토큰의 업데이트를 제한하여 엔트로피를 관리하는 두 가지 기법, Clip-Cov와 KL-Cov 제안함(Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively.)"
        ],
        "conclusion": "이 방법들은 탐사를 촉진하고 정책이 엔트로피 붕괴를 피하도록 도와주며, 더 나은 최종 성능을 달성하게 함.",
        "keywords": [
            "Reinforcement Learning",
            "Natural Language Processing",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2505.21600",
            "authors": [
                {
                    "_id": "6837bc9c9937bcb69885799c",
                    "user": {
                        "_id": "6445fd9ba56444c355dcbcba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
                        "isPro": false,
                        "fullname": "Tianyu Fu",
                        "user": "fuvty",
                        "type": "user"
                    },
                    "name": "Tianyu Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:43.123Z",
                    "hidden": false
                },
                {
                    "_id": "6837bc9c9937bcb69885799d",
                    "name": "Yi Ge",
                    "hidden": false
                },
                {
                    "_id": "6837bc9c9937bcb69885799e",
                    "user": {
                        "_id": "66954ebfbcd81f395e9dca37",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66954ebfbcd81f395e9dca37/0C3m5YdxyXuK7dJBu4AdL.png",
                        "isPro": false,
                        "fullname": "Yichen You",
                        "user": "youyc22",
                        "type": "user"
                    },
                    "name": "Yichen You",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:40.790Z",
                    "hidden": false
                },
                {
                    "_id": "6837bc9c9937bcb69885799f",
                    "name": "Enshu Liu",
                    "hidden": false
                },
                {
                    "_id": "6837bc9c9937bcb6988579a0",
                    "name": "Zhihang Yuan",
                    "hidden": false
                },
                {
                    "_id": "6837bc9c9937bcb6988579a1",
                    "name": "Guohao Dai",
                    "hidden": false
                },
                {
                    "_id": "6837bc9c9937bcb6988579a2",
                    "name": "Shengen Yan",
                    "hidden": false
                },
                {
                    "_id": "6837bc9c9937bcb6988579a3",
                    "name": "Huazhong Yang",
                    "hidden": false
                },
                {
                    "_id": "6837bc9c9937bcb6988579a4",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6445fd9ba56444c355dcbcba/edz42EmKaJTdax2mDUmhh.mp4"
            ],
            "publishedAt": "2025-05-27T16:57:20.000Z",
            "submittedOnDailyAt": "2025-05-29T00:18:54.118Z",
            "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing",
            "submittedOnDailyBy": {
                "_id": "6445fd9ba56444c355dcbcba",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
                "isPro": false,
                "fullname": "Tianyu Fu",
                "user": "fuvty",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.",
            "upvotes": 49,
            "discussionId": "6837bc9d9937bcb6988579d1",
            "projectPage": "https://fuvty.github.io/R2R_Project_Page/",
            "githubRepo": "https://github.com/thu-nics/R2R",
            "ai_summary": "Roads to Rome (R2R) selectively utilizes large language models for critical reasoning tasks to enhance efficiency and performance in lightweight models.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Small Language Models (SLMs)",
                "token routing",
                "neural token routing",
                "token divergence",
                "token generation",
                "automatic data generation pipeline",
                "token-level routing labels",
                "R1-1.5B",
                "R1-32B",
                "math benchmarks",
                "coding benchmarks",
                "QA benchmarks",
                "parameter size",
                "activated parameters",
                "test-time scaling efficiency",
                "pareto frontier"
            ]
        },
        "translation_title": "R2R: 작은 모델과 큰 모델의 토큰 라우팅을 통한 효과적인 이탈 추론 경로 탐색",
        "purpose": "효율성을 높이기 위한 비율의 언어 모델이 이탈 추론 경로를 따라갈 수 있도록 개선",
        "method": [
            "LLM이 SLM보다 적은 토큰만 선택적으로 이용하여 이탈 경로가 있는 토큰에 집중하도록 R2R 방법론 제안(We introduce Roads to Rome (R2R), a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens.)",
            "자동 데이터 생성 파이프라인을 개발하여 이탈 토큰을 식별하고, 라우팅 라벨을 생성하여 경량 라우터를 학습시킴(We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router.)",
            "딥시크(DeepSeek) 모델의 R1-1.5B과 R1-32B 모델을 결합하여 다양한 벤치마크에서 평가함(We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks.)"
        ],
        "conclusion": "R2R 방법이 R1-32B보다 2.8배 더 빠른 속도를 내며, 평균 정확도를 1.6배 향상시켜 테스트 시 효율성을 극대화함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2505.22651",
            "authors": [
                {
                    "_id": "6837ffdd1bfb4a669ad6de09",
                    "user": {
                        "_id": "662678dfdd43e904ef1dcd03",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
                        "isPro": false,
                        "fullname": "Yi Ding",
                        "user": "Tuwhy",
                        "type": "user"
                    },
                    "name": "Yi Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:41:19.811Z",
                    "hidden": false
                },
                {
                    "_id": "6837ffdd1bfb4a669ad6de0a",
                    "name": "Ruqi Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T17:58:03.000Z",
            "submittedOnDailyAt": "2025-05-29T06:18:44.515Z",
            "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "662678dfdd43e904ef1dcd03",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
                "isPro": false,
                "fullname": "Yi Ding",
                "user": "Tuwhy",
                "type": "user"
            },
            "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\nbeta for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data.",
            "upvotes": 42,
            "discussionId": "6837ffdf1bfb4a669ad6de71",
            "projectPage": "https://dripnowhy.github.io/Sherlock/",
            "githubRepo": "https://github.com/DripNowhy/Sherlock",
            "ai_summary": "Sherlock, a self-correction and self-improvement framework for reasoning vision-language models, enhances accuracy across benchmarks using limited annotated data.",
            "ai_keywords": [
                "vision-language models",
                "self-correction",
                "trajectory-level self-correction",
                "preference data",
                "visual perturbation",
                "dynamic beta",
                "Llama3.2-Vision-11B",
                "LLaVA-CoT",
                "Mulberry",
                "LlamaV-o1"
            ]
        },
        "translation_title": "Sherlock: Vision-Language 모델에서의 자기 수정 추론",
        "purpose": "Vision-Language 모델의 추론 능력을 향상시키기 위해 자기 수정 전략을 탐색",
        "method": [
            "추론 VLM의 자기 수정 능력에 대한 심층 분석을 수행하고 주요 격차를 파악함(we first conduct an in-depth analysis of reasoning VLMs' self-correction abilities and identify key gaps.)",
            "Sherlock이라는 자기 수정 및 자기 개선 훈련 프레임워크를 도입함(Based on our findings, we introduce Sherlock, a self-correction and self-improvement training framework.)",
            "모델이 20,000개의 무작위 샘플링된 주석 데이터를 이용해 자기 수정 능력을 획득하고 외부 감독 없이 계속해서 자기 개선을 할 수 있도록 함(Once the model acquires self-correction capabilities using only 20k randomly sampled annotated data, it continues to self-improve without external supervision.)"
        ],
        "conclusion": "Sherlock은 8개의 벤치마크에서 뛰어난 성과를 기록하며, 평균 정확도가 64.1에 달하고 자기 수정을 통해 65.4에 도달하였음.",
        "keywords": [
            "Multimodal Learning",
            "Vision-Language Models",
            "Image Understanding"
        ]
    },
    {
        "paper": {
            "id": "2505.22312",
            "authors": [
                {
                    "_id": "6837c342cd1601f5bd670255",
                    "name": "Jujie He",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd670256",
                    "user": {
                        "_id": "65643645b09c0b9ece1b8f0e",
                        "avatarUrl": "/avatars/d5197103b6e92f765bfda7ed2cc8d53e.svg",
                        "isPro": false,
                        "fullname": "Jiacai Liu",
                        "user": "skydownacai",
                        "type": "user"
                    },
                    "name": "Jiacai Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:28.696Z",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd670257",
                    "user": {
                        "_id": "658229ef5f6d83438257fce5",
                        "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
                        "isPro": false,
                        "fullname": "Chris (Yuhao) Liu",
                        "user": "chrisliu298",
                        "type": "user"
                    },
                    "name": "Chris Yuhao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:32.117Z",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd670258",
                    "name": "Rui Yan",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd670259",
                    "name": "Chaojie Wang",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd67025a",
                    "name": "Peng Cheng",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd67025b",
                    "name": "Xiaoyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd67025c",
                    "name": "Fuxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd67025d",
                    "name": "Jiacheng Xu",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd67025e",
                    "name": "Wei Shen",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd67025f",
                    "name": "Siyuan Li",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd670260",
                    "name": "Liang Zeng",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd670261",
                    "name": "Tianwen Wei",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd670262",
                    "name": "Cheng Cheng",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd670263",
                    "name": "Bo An",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd670264",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6837c342cd1601f5bd670265",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T12:56:04.000Z",
            "submittedOnDailyAt": "2025-05-29T00:48:05.741Z",
            "title": "Skywork Open Reasoner 1 Technical Report",
            "submittedOnDailyBy": {
                "_id": "658229ef5f6d83438257fce5",
                "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
                "isPro": false,
                "fullname": "Chris (Yuhao) Liu",
                "user": "chrisliu298",
                "type": "user"
            },
            "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets.",
            "upvotes": 42,
            "discussionId": "6837c344cd1601f5bd6702dd",
            "githubRepo": "https://github.com/SkyworkAI/Skywork-OR1",
            "ai_summary": "Skywork-OR1 is a reinforcement learning approach for long Chain-of-Thought models that improves accuracy over DeepSeek-R1 across various benchmarks by addressing entropy collapse.",
            "ai_keywords": [
                "reinforcement learning",
                "LLMs",
                "Chain-of-Thought",
                "Skywork-OR1",
                "DeepSeek-R1-Distill",
                "AIME24",
                "AIME25",
                "LiveCodeBench",
                "entropy collapse",
                "ablation studies"
            ]
        },
        "translation_title": "Skywork Open Reasoner 1 기술 보고서",
        "purpose": "대형 언어 모델(LLM)의 추론 능력을 향상시키기 위한 효과적이고 확장 가능한 강화 학습(RL) 구현을 제시하기 위해",
        "method": [
            "DeepSeek-R1-Distill 모델 시리즈를 기반으로 한 RL 접근 방식을 통해 AIME24, AIME25, LiveCodeBench에서의 평균 정확도를 향상시킴(Our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench.)",
            "32B 모델에서 57.8%에서 72.8%로 (+15.0%) 정확도를 증가시킴(increasing average accuracy ... from 57.8% to 72.8% (+15.0%) for the 32B model.)",
            "핵심 훈련 파이프라인의 주요 구성 요소에 대한 포괄적인 절단 연구를 수행하여 효과성을 검증함(We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness.)"
        ],
        "conclusion": "Skywork-OR1 모델은 DeepSeek-R1과 Qwen3-32B를 초월하며, 엔트로피 붕괴 문제를 해결하는 것이 테스트 성능 향상에 중요함을 보여줌.",
        "keywords": [
            "Large Language Models",
            "Reinforcement Learning",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2505.20411",
            "authors": [
                {
                    "_id": "683735aff42cc8a1d260e677",
                    "user": {
                        "_id": "654e5e094319c75e3e1b6cbc",
                        "avatarUrl": "/avatars/a8889036fa38f80f2d45aea8d1471395.svg",
                        "isPro": false,
                        "fullname": "Ibragim",
                        "user": "ibragim-bad",
                        "type": "user"
                    },
                    "name": "Ibragim Badertdinov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T09:40:24.705Z",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e678",
                    "user": {
                        "_id": "644e9ffcd6001776ed77d874",
                        "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
                        "isPro": false,
                        "fullname": "Alexander",
                        "user": "djalexj",
                        "type": "user"
                    },
                    "name": "Alexander Golubev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:13:38.030Z",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e679",
                    "name": "Maksim Nekrashevich",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e67a",
                    "name": "Anton Shevtsov",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e67b",
                    "user": {
                        "_id": "65e48cb3a4e46e644ec1277d",
                        "avatarUrl": "/avatars/dd2bf04a6f81bf0a0892080af5d485b2.svg",
                        "isPro": false,
                        "fullname": "Simon Karasik",
                        "user": "sbkarasik",
                        "type": "user"
                    },
                    "name": "Simon Karasik",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T09:40:26.644Z",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e67c",
                    "name": "Andrei Andriushchenko",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e67d",
                    "name": "Maria Trofimova",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e67e",
                    "name": "Daria Litvintseva",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e67f",
                    "name": "Boris Yangel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T18:01:00.000Z",
            "submittedOnDailyAt": "2025-05-29T06:59:04.261Z",
            "title": "SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents",
            "submittedOnDailyBy": {
                "_id": "644e9ffcd6001776ed77d874",
                "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
                "isPro": false,
                "fullname": "Alexander",
                "user": "djalexj",
                "type": "user"
            },
            "summary": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues.",
            "upvotes": 35,
            "discussionId": "683735b0f42cc8a1d260e69f",
            "ai_summary": "A novel pipeline extracts real-world, interactive software engineering tasks from GitHub to create SWE-rebench, improving the evaluation of reinforcement learning models in SWE.",
            "ai_keywords": [
                "LLM-based agents",
                "reinforcement learning",
                "software engineering tasks",
                "GitHub repositories",
                "SWE-rebench",
                "contamination-free benchmark",
                "SWE-bench Verified"
            ]
        },
        "translation_title": "SWE-rebench: 소프트웨어 공학 에이전트의 작업 수집 및 비오염 평가를 위한 자동화 파이프라인",
        "purpose": "소프트웨어 공학(SWE) 에이전트의 훈련과 평가를 위한 고품질 데이터 수집 및 오염 문제 해결",
        "method": [
            "다양한 GitHub 저장소에서 실제 상호작용 SWE 작업을 지속적으로 추출하기 위한 자동화되고 확장 가능한 파이프라인을 도입함(To address these limitations, we introduce a novel, automated, and scalable pipeline to continuously extract real-world interactive SWE tasks from diverse GitHub repositories.)",
            "이 파이프라인을 사용해 21,000개 이상의 인터랙티브 Python 기반 SWE 작업으로 구성된 공개 데이터셋인 SWE-rebench를 생성함(Using this pipeline, we construct SWE-rebench, a public dataset comprising over 21,000 interactive Python-based SWE tasks, suitable for reinforcement learning of SWE agents at scale.)",
            "SWE-rebench 방법론을 활용해 신선한 작업을 지속적으로 공급하여 비오염 벤치마크를 구축함(Additionally, we use continuous supply of fresh tasks collected using SWE-rebench methodology to build a contamination-free benchmark for agentic software engineering.)"
        ],
        "conclusion": "SWE-rebench는 소프트웨어 공학 에이전트 훈련을 위한 대규모 데이터세트를 제공하며, 모델 성능 평가의 오염 문제를 해결하는 데 기여함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Robotics"
        ]
    }
]