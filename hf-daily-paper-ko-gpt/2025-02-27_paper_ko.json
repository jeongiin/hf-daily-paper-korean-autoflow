[
    {
        "paper": {
            "id": "2502.18934",
            "authors": [
                {
                    "_id": "67bfe1bf4426925c82fe5953",
                    "name": "Kanana LLM Team",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5954",
                    "user": {
                        "_id": "64d08bd75de9e1e911b24226",
                        "avatarUrl": "/avatars/e572bb47659393573a0c1fb3d333dd7b.svg",
                        "isPro": false,
                        "fullname": "Yunju Bak",
                        "user": "yunjubak63",
                        "type": "user"
                    },
                    "name": "Yunju Bak",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-27T12:55:35.505Z",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5955",
                    "name": "Hojin Lee",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5956",
                    "user": {
                        "_id": "60436d159e905013ae8715d7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1623809612769-60436d159e905013ae8715d7.jpeg",
                        "isPro": false,
                        "fullname": "Minho Ryu",
                        "user": "bzantium",
                        "type": "user"
                    },
                    "name": "Minho Ryu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:14:17.979Z",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5957",
                    "user": {
                        "_id": "66ebb4fdc5b2c25450fd17de",
                        "avatarUrl": "/avatars/e6b40dcbe2eba838ba21be9221758a3c.svg",
                        "isPro": false,
                        "fullname": "Jiyeon Ham",
                        "user": "jiyeonham",
                        "type": "user"
                    },
                    "name": "Jiyeon Ham",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:14:11.786Z",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5958",
                    "name": "Seungjae Jung",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5959",
                    "user": {
                        "_id": "66c82a50c1b3c03c61aea140",
                        "avatarUrl": "/avatars/3c508f96bdca2f2ce9746d3decd4718e.svg",
                        "isPro": false,
                        "fullname": "daniel nam",
                        "user": "daniel-rl2",
                        "type": "user"
                    },
                    "name": "Daniel Wontae Nam",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:14:09.613Z",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe595a",
                    "name": "Taegyeong Eo",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe595b",
                    "name": "Donghun Lee",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe595c",
                    "user": {
                        "_id": "6142e17fe9e656d4459121e4",
                        "avatarUrl": "/avatars/6baebd4598a845ec7fdb735eb0d53139.svg",
                        "isPro": false,
                        "fullname": "Doohae Jung",
                        "user": "Doohae",
                        "type": "user"
                    },
                    "name": "Doohae Jung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:14:06.858Z",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe595d",
                    "user": {
                        "_id": "60f559be68ee3ef098e407cf",
                        "avatarUrl": "/avatars/e1f00ff1c1c9fa7f591535d39c7d5e44.svg",
                        "isPro": false,
                        "fullname": "Boseop Kim",
                        "user": "seopbo",
                        "type": "user"
                    },
                    "name": "Boseop Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:14:01.989Z",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe595e",
                    "user": {
                        "_id": "6605028007a154c768e1c4c7",
                        "avatarUrl": "/avatars/88678edb83fdb466067e38acd22d07de.svg",
                        "isPro": false,
                        "fullname": "Nayeon Kim",
                        "user": "lana-ny",
                        "type": "user"
                    },
                    "name": "Nayeon Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:14:13.867Z",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe595f",
                    "user": {
                        "_id": "6136f65440e43b8f748a0833",
                        "avatarUrl": "/avatars/f72a5ae3d3e94485de8aed8df94abdad.svg",
                        "isPro": false,
                        "fullname": "Jaesun Park",
                        "user": "jaesun",
                        "type": "user"
                    },
                    "name": "Jaesun Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:14:15.898Z",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5960",
                    "name": "Hyunho Kim",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5961",
                    "user": {
                        "_id": "5fd888cf61e46993190ce543",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634604273263-5fd888cf61e46993190ce543.jpeg",
                        "isPro": false,
                        "fullname": "Hyunwoong Ko",
                        "user": "hyunwoongko",
                        "type": "user"
                    },
                    "name": "Hyunwoong Ko",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-27T12:58:05.546Z",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5962",
                    "user": {
                        "_id": "63d268bb57ab367124ea7b75",
                        "avatarUrl": "/avatars/11312cde1e9f077aa9e5103b48be5de6.svg",
                        "isPro": false,
                        "fullname": "Changmin Lee",
                        "user": "changminlee",
                        "type": "user"
                    },
                    "name": "Changmin Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:14:04.506Z",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5963",
                    "user": {
                        "_id": "62bd31e1d2c8a6542f53fcba",
                        "avatarUrl": "/avatars/4ac18a7bcaf9dd3885b0478dea90818f.svg",
                        "isPro": false,
                        "fullname": "Kyoung-Woon On",
                        "user": "kloud",
                        "type": "user"
                    },
                    "name": "Kyoung-Woon On",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-27T12:58:11.269Z",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5964",
                    "name": "Seulye Baeg",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5965",
                    "name": "Junrae Cho",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5966",
                    "name": "Sunghee Jung",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5967",
                    "name": "Jieun Kang",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5968",
                    "name": "EungGyun Kim",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe5969",
                    "name": "Eunhwa Kim",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe596a",
                    "name": "Byeongil Ko",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe596b",
                    "name": "Daniel Lee",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe596c",
                    "name": "Minchul Lee",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe596d",
                    "name": "Miok Lee",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe596e",
                    "name": "Shinbok Lee",
                    "hidden": false
                },
                {
                    "_id": "67bfe1bf4426925c82fe596f",
                    "user": {
                        "_id": "63148a8f5f47a18962765802",
                        "avatarUrl": "/avatars/bc58a863727794006dddf758efa09411.svg",
                        "isPro": true,
                        "fullname": "gaeunseo",
                        "user": "gaeunseo",
                        "type": "user"
                    },
                    "name": "Gaeun Seo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-27T12:59:39.670Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-26T08:36:20.000Z",
            "title": "Kanana: Compute-efficient Bilingual Language Models",
            "summary": "We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models.",
            "upvotes": 44,
            "discussionId": "67bfe1c04426925c82fe59a1"
        },
        "translation_title": "Kanana: 계산 효율적인 이중 언어 모델",
        "purpose": "Korean과 English에서 성능을 개선하고, 계산 비용을 줄인 이중 언어 모델 개발",
        "method": [
            "고품질 데이터 필터링, 단계적인 사전 학습, 깊이 스케일링, 가지치기 및 증류 등 여러 기술을 사용하여 경쟁력 있는 모델을 사전 학습함(The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation.)",
            "모델의 후처리 과정에서 감독된 미세 조정 및 선호 최적화를 통해 사용자와의 상호작용 능력을 향상시킴(Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users.)"
        ],
        "conclusion": "Kanana 모델 시리즈는 2.1B에서 32.5B 매개변수까지 다양하며, 2.1B 모델(기본, 지침, 임베딩)을 공개하여 한국어 모델 연구를 촉진.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2502.18417",
            "authors": [
                {
                    "_id": "67c02b2eb14cf3cbc800c292",
                    "name": "Alexander Groshev",
                    "hidden": false
                },
                {
                    "_id": "67c02b2eb14cf3cbc800c293",
                    "user": {
                        "_id": "67aafccd7517c92ba71142f2",
                        "avatarUrl": "/avatars/ef4b5c6867250b8b7af2c995dd7ad740.svg",
                        "isPro": false,
                        "fullname": "Anastasiia Iashchenko",
                        "user": "nastasia-y",
                        "type": "user"
                    },
                    "name": "Anastasiia Iashchenko",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:13:49.896Z",
                    "hidden": false
                },
                {
                    "_id": "67c02b2eb14cf3cbc800c294",
                    "name": "Pavel Paramonov",
                    "hidden": false
                },
                {
                    "_id": "67c02b2eb14cf3cbc800c295",
                    "user": {
                        "_id": "6669a678465d1d802181e456",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png",
                        "isPro": false,
                        "fullname": "Denis Dimitrov",
                        "user": "dendimitrov",
                        "type": "user"
                    },
                    "name": "Denis Dimitrov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T12:54:35.272Z",
                    "hidden": false
                },
                {
                    "_id": "67c02b2eb14cf3cbc800c296",
                    "user": {
                        "_id": "643984dceb7c5616ef3f5d54",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
                        "isPro": false,
                        "fullname": "Andrey Kuznetsov",
                        "user": "kuznetsoffandrey",
                        "type": "user"
                    },
                    "name": "Andrey Kuznetsov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T12:54:37.211Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T18:13:55.000Z",
            "title": "GHOST 2.0: generative high-fidelity one shot transfer of heads",
            "summary": "While the task of face swapping has recently gained attention in the research\ncommunity, a related problem of head swapping remains largely unexplored. In\naddition to skin color transfer, head swap poses extra challenges, such as the\nneed to preserve structural information of the whole head during synthesis and\ninpaint gaps between swapped head and background. In this paper, we address\nthese concerns with GHOST 2.0, which consists of two problem-specific modules.\nFirst, we introduce enhanced Aligner model for head reenactment, which\npreserves identity information at multiple scales and is robust to extreme pose\nvariations. Secondly, we use a Blender module that seamlessly integrates the\nreenacted head into the target background by transferring skin color and\ninpainting mismatched regions. Both modules outperform the baselines on the\ncorresponding tasks, allowing to achieve state of the art results in head\nswapping. We also tackle complex cases, such as large difference in hair styles\nof source and target. Code is available at\nhttps://github.com/ai-forever/ghost-2.0",
            "upvotes": 35,
            "discussionId": "67c02b31b14cf3cbc800c34b"
        },
        "translation_title": "GHOST 2.0: 고정밀 얼굴 교환을 위한 생성 모델",
        "purpose": "헤드 스와핑 과정에서 피부 색상 전이와 구조 정보 보존 문제를 해결하기 위해 연구",
        "method": [
            "헤드 리이넥트먼트를 위한 향상된 Aligner 모델을 소개하고(First, we introduce enhanced Aligner model for head reenactment, which preserves identity information at multiple scales and is robust to extreme pose variations.)",
            "Blender 모듈을 사용하여 리이넥트된 헤드를 타겟 배경에 자연스럽게 통합함(Secondly, we use a Blender module that seamlessly integrates the reenacted head into the target background by transferring skin color and inpainting mismatched regions.)",
            "두 모듈 모두 해당 작업에서 기초 모델보다 우수한 성능을 보임(Both modules outperform the baselines on the corresponding tasks, allowing to achieve state of the art results in head swapping.)"
        ],
        "conclusion": "GHOST 2.0는 얼굴 교환에서 최첨단 성과를 달성하며, 복잡한 사례에서도 효과적으로 작동함.",
        "keywords": [
            "Computer Vision",
            "Image Generation",
            "Video Understanding"
        ]
    },
    {
        "paper": {
            "id": "2502.19400",
            "authors": [
                {
                    "_id": "67bfd6f15db054ee3c5a766b",
                    "user": {
                        "_id": "631d760344503b7227837242",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631d760344503b7227837242/3b6JRusFX6GKJpsN9ZdeJ.png",
                        "isPro": false,
                        "fullname": "Max Ku",
                        "user": "vinesmsuic",
                        "type": "user"
                    },
                    "name": "Max Ku",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:14:55.238Z",
                    "hidden": false
                },
                {
                    "_id": "67bfd6f15db054ee3c5a766c",
                    "user": {
                        "_id": "6365d5baa7a1324ccd5ecdb9",
                        "avatarUrl": "/avatars/636d3f410b878e451a878a6cf171dd53.svg",
                        "isPro": false,
                        "fullname": "Thomas Chong",
                        "user": "chongcht",
                        "type": "user"
                    },
                    "name": "Thomas Chong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:14:49.567Z",
                    "hidden": false
                },
                {
                    "_id": "67bfd6f15db054ee3c5a766d",
                    "name": "Jonathan Leung",
                    "hidden": false
                },
                {
                    "_id": "67bfd6f15db054ee3c5a766e",
                    "user": {
                        "_id": "67bfdfdbf856fd8ddbb7e0f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rIR0QnVM3wxMCulG2R9SJ.png",
                        "isPro": false,
                        "fullname": "Krish Shah",
                        "user": "KrishKrosh",
                        "type": "user"
                    },
                    "name": "Krish Shah",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:14:47.269Z",
                    "hidden": false
                },
                {
                    "_id": "67bfd6f15db054ee3c5a766f",
                    "user": {
                        "_id": "6696061aa8dbb9a9997dfff6",
                        "avatarUrl": "/avatars/d8f0bbff362fd630e6e60aab141076d3.svg",
                        "isPro": false,
                        "fullname": "Alvin Yu",
                        "user": "AlvinYuVotee",
                        "type": "user"
                    },
                    "name": "Alvin Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:14:52.146Z",
                    "hidden": false
                },
                {
                    "_id": "67bfd6f15db054ee3c5a7670",
                    "name": "Wenhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-26T18:50:09.000Z",
            "title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem\n  Understanding",
            "summary": "Understanding domain-specific theorems often requires more than just\ntext-based reasoning; effective communication through structured visual\nexplanations is crucial for deeper comprehension. While large language models\n(LLMs) demonstrate strong performance in text-based theorem reasoning, their\nability to generate coherent and pedagogically meaningful visual explanations\nremains an open challenge. In this work, we introduce TheoremExplainAgent, an\nagentic approach for generating long-form theorem explanation videos (over 5\nminutes) using Manim animations. To systematically evaluate multimodal theorem\nexplanations, we propose TheoremExplainBench, a benchmark covering 240 theorems\nacross multiple STEM disciplines, along with 5 automated evaluation metrics.\nOur results reveal that agentic planning is essential for generating detailed\nlong-form videos, and the o3-mini agent achieves a success rate of 93.8% and an\noverall score of 0.77. However, our quantitative and qualitative studies show\nthat most of the videos produced exhibit minor issues with visual element\nlayout. Furthermore, multimodal explanations expose deeper reasoning flaws that\ntext-based explanations fail to reveal, highlighting the importance of\nmultimodal explanations.",
            "upvotes": 26,
            "discussionId": "67bfd6f25db054ee3c5a7699"
        },
        "translation_title": "TheoremExplainAgent: LLM 정리를 위한 다중 모드 설명의 방향",
        "purpose": "정리의 깊은 이해를 위해 시각적 설명을 효과적으로 생성하는 방법 연구",
        "method": [
            "TheoremExplainAgent라는 에이전트 기반 접근 방식을 도입하여 5분 이상의 정리 설명 비디오를 생성함(we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes)).",
            "240개의 정리를 포함한 TheoremExplainBench라는 벤치마크를 제안하여 다중 모드 정리 설명을 체계적으로 평가함(we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines).",
            "o3-mini 에이전트를 통해 93.8%의 성공률과 0.77의 종합 점수를 달성함(the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77)."
        ],
        "conclusion": "다중 모드 설명이 텍스트 기반 설명이 드러내지 못하는 심층적인 추론 결점을 보여주어 다중 모드 설명의 중요성을 강조함.",
        "keywords": [
            "Natural Language Processing",
            "Multimodal Learning",
            "Video Generation"
        ]
    },
    {
        "paper": {
            "id": "2502.19328",
            "authors": [
                {
                    "_id": "67bfcb774d22a9379b29334c",
                    "name": "Hao Peng",
                    "hidden": false
                },
                {
                    "_id": "67bfcb774d22a9379b29334d",
                    "name": "Yunjia Qi",
                    "hidden": false
                },
                {
                    "_id": "67bfcb774d22a9379b29334e",
                    "name": "Xiaozhi Wang",
                    "hidden": false
                },
                {
                    "_id": "67bfcb774d22a9379b29334f",
                    "name": "Zijun Yao",
                    "hidden": false
                },
                {
                    "_id": "67bfcb774d22a9379b293350",
                    "name": "Bin Xu",
                    "hidden": false
                },
                {
                    "_id": "67bfcb774d22a9379b293351",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "67bfcb774d22a9379b293352",
                    "name": "Juanzi Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-26T17:19:12.000Z",
            "title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable\n  Correctness Signals for Reliable Reward Systems",
            "summary": "Reward models (RMs) are crucial for the training and inference-time scaling\nup of large language models (LLMs). However, existing reward models primarily\nfocus on human preferences, neglecting verifiable correctness signals which\nhave shown strong potential in training LLMs. In this paper, we propose agentic\nreward modeling, a reward system that combines reward models with verifiable\ncorrectness signals from different aspects to provide reliable rewards. We\nempirically implement a reward agent, named RewardAgent, that combines human\npreference rewards with two verifiable signals: factuality and instruction\nfollowing, to provide more reliable rewards. We conduct comprehensive\nexperiments on existing reward model benchmarks and inference time best-of-n\nsearches on real-world downstream tasks. RewardAgent significantly outperforms\nvanilla reward models, demonstrating its effectiveness. We further construct\ntraining preference pairs using RewardAgent and train an LLM with the DPO\nobjective, achieving superior performance on various NLP benchmarks compared to\nconventional reward models. Our codes are publicly released to facilitate\nfurther research (https://github.com/THU-KEG/Agentic-Reward-Modeling).",
            "upvotes": 14,
            "discussionId": "67bfcb784d22a9379b29338f"
        },
        "translation_title": "Agentic Reward Modeling: 인간 선호와 검증 가능한 정확성 신호를 통합한 신뢰할 수 있는 보상 시스템",
        "purpose": "신뢰할 수 있는 보상 시스템을 구축하기 위해 인간 선호와 검증 가능한 정확성 신호를 통합하는 방법 연구",
        "method": [
            "Agentic reward modeling을 제안하여 여러 측면에서의 검증 가능한 정확성 신호와 인간 선호 보상을 결합함(we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects)",
            "RewardAgent라는 보상 에이전트를 구현하여 사실성과 지침 준수를 포함한 두 가지 검증 가능한 신호를 결합함(we empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following)",
            "기존 보상 모델 벤치마크에서 포괄적인 실험을 수행하고, 실제 정의된 하위 작업에 대해 인퍼런스 시간의 검색을 진행함(We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks)"
        ],
        "conclusion": "RewardAgent는 기존의 보상 모델보다 우수한 성능을 보여주며, DPO 목표로 LLM을 훈련하여 여러 NLP 벤치마크에서 더 뛰어난 성과를 달성함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Reward Models"
        ]
    },
    {
        "paper": {
            "id": "2502.19361",
            "authors": [
                {
                    "_id": "67bfe435ca6e3c22b6e29442",
                    "name": "Yancheng He",
                    "hidden": false
                },
                {
                    "_id": "67bfe435ca6e3c22b6e29443",
                    "name": "Shilong Li",
                    "hidden": false
                },
                {
                    "_id": "67bfe435ca6e3c22b6e29444",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "67bfe435ca6e3c22b6e29445",
                    "name": "Weixun Wang",
                    "hidden": false
                },
                {
                    "_id": "67bfe435ca6e3c22b6e29446",
                    "name": "Xingyuan Bu",
                    "hidden": false
                },
                {
                    "_id": "67bfe435ca6e3c22b6e29447",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:13:58.959Z",
                    "hidden": false
                },
                {
                    "_id": "67bfe435ca6e3c22b6e29448",
                    "name": "Zhongyuan Peng",
                    "hidden": false
                },
                {
                    "_id": "67bfe435ca6e3c22b6e29449",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67bfe435ca6e3c22b6e2944a",
                    "name": "Wenbo Su",
                    "hidden": false
                },
                {
                    "_id": "67bfe435ca6e3c22b6e2944b",
                    "name": "Bo Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-26T17:59:27.000Z",
            "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought\n  Reasoning?",
            "summary": "Recently, o1-like models have drawn significant attention, where these models\nproduce the long Chain-of-Thought (CoT) reasoning steps to improve the\nreasoning abilities of existing Large Language Models (LLMs). In this paper, to\nunderstand the qualities of these long CoTs and measure the critique abilities\nof existing LLMs on these long CoTs, we introduce the DeltaBench, including the\ngenerated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for\ndifferent reasoning tasks (e.g., Math, Code, General Reasoning), to measure the\nability to detect errors in long CoT reasoning. Based on DeltaBench, we first\nperform fine-grained analysis of the generated long CoTs to discover the\neffectiveness and efficiency of different o1-like models. Then, we conduct\nextensive evaluations of existing process reward models (PRMs) and critic\nmodels to detect the errors of each annotated process, which aims to\ninvestigate the boundaries and limitations of existing PRMs and critic models.\nFinally, we hope that DeltaBench could guide developers to better understand\nthe long CoT reasoning abilities of their models.",
            "upvotes": 12,
            "discussionId": "67bfe438ca6e3c22b6e2948e"
        },
        "translation_title": "대형 언어 모델이 긴 Chain-of-Thought 오류를 감지할 수 있나요?",
        "purpose": "기존 대형 언어 모델의 긴 Chain-of-Thought의 질을 이해하고 오류 감지 능력을 측정하기 위한 연구",
        "method": [
            "DeltaBench라는 새로운 벤치마크를 소개하여 다양한 o1과 유사한 모델에서 생성된 긴 Chain-of-Thought를 포함함(we introduce the DeltaBench, including the generated long CoTs from different o1-like models)",
            "긴 Chain-of-Thought를 분석하여 다양한 o1 같은 모델의 효과성과 효율성을 발견함(Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models.)",
            "기존 프로세스 보상 모델 및 비평 모델을 광범위하게 평가하여 주석이 달린 프로세스의 오류를 감지함(Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process.)"
        ],
        "conclusion": "DeltaBench를 통해 모델의 긴 Chain-of-Thought 추론 능력을 더 잘 이해할 수 있도록 개발자에게 도움을 줄 수 있을 것으로 기대함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    }
]